# 第六篇:生成模型(GAN/Diffusion)

> **目标读者**:掌握CNN和Transformer基础,希望深入理解生成式AI的读者
>
> **学习重点**:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成

---

## 篇章概述

生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。

### 为什么学习生成模型?

- **AIGC时代核心技术**:Midjourney、Stable Diffusion、DALL-E等产品的底层技术
- **多模态理解基础**:理解文生图是学习VLM的前置知识
- **实用价值高**:图像生成、编辑、超分辨率等多种应用
- **技术快速迭代**:从DDPM到FLUX,扩散模型仍在快速发展

### 技术演进时间线

```
2014-2019: GAN时代
├── 2014: GAN提出 (Goodfellow)
├── 2015: DCGAN - 稳定训练的GAN
├── 2018: StyleGAN - 高质量人脸生成
└── 2019: StyleGAN2 - 生成质量巅峰

2020-至今: Diffusion崛起
├── 2020: DDPM提出 (Ho et al.)
├── 2021: DALL-E (OpenAI)
├── 2022: Stable Diffusion开源
├── 2023: ControlNet、SDXL
├── 2024: Stable Diffusion 3、FLUX.1
└── 2025: 扩散模型持续迭代
```

---

## 章节安排

### [第14章:生成对抗网络(GAN)](chapter14/README.md)

**快速回顾,不作为重点**

- 14.1 GAN基础原理
  - 生成器与判别器的对抗训练
  - GAN的损失函数
  - 训练稳定性问题

- 14.2 DCGAN:深度卷积GAN
  - 网络架构设计
  - 训练技巧与稳定性改进

- 14.3 StyleGAN系列(简介)
  - StyleGAN的创新点
  - 风格迁移应用

- 14.4 条件GAN与应用
  - Conditional GAN
  - Pix2Pix、CycleGAN

- **实战**:使用DCGAN生成人脸图像

**核心技能**:
- 理解GAN的训练机制
- 掌握DCGAN的实现
- 了解GAN的局限性(为学习Diffusion做准备)

**学习时间**:1-2天(快速过一遍即可)

---

### [第15章:扩散模型(Diffusion)](chapter15/README.md)

**本篇重点,深入学习**

- 15.1 扩散模型基础
  - 前向扩散过程:逐步加噪
  - 反向去噪过程:学习噪声预测
  - DDPM数学原理(简化版)

- 15.2 Stable Diffusion架构
  - Latent Diffusion:潜空间扩散
  - VAE、U-Net、CLIP Text Encoder
  - 采样器:DDIM、Euler、DPM-Solver++

- 15.3 ControlNet:可控生成
  - 条件控制原理
  - 常用控制类型:Canny、Depth、Pose等
  - 多条件组合

- 15.4 FLUX:2024最新扩散模型
  - Black Forest Labs的新架构
  - FLUX.1-dev vs FLUX.1-schnell
  - 性能对比与应用

- 15.5 提示词工程(Prompt Engineering)
  - 高质量提示词结构
  - 负向提示词技巧
  - 提示词模板库

- **实战项目**:
  - 文生图(Text-to-Image)完整流程
  - 图生图(Image-to-Image)风格迁移
  - ControlNet可控生成
  - 批量生成与自动化

**核心技能**:
- 深入理解扩散过程的数学原理
- 熟练使用Hugging Face diffusers库
- 掌握Stable Diffusion的各种应用
- 学会编写高质量提示词

**学习时间**:4-5天(重点学习)

---

## 技术栈

### 环境要求

```bash
# Python >= 3.10
python --version

# GPU要求(强烈建议)
# - VRAM >= 8GB (Stable Diffusion 1.5)
# - VRAM >= 12GB (Stable Diffusion XL)
# - VRAM >= 16GB (FLUX.1)

# 查看GPU信息
nvidia-smi
```

### 核心依赖

```bash
# GAN相关(第14章)
pip install torch torchvision
pip install matplotlib pillow
pip install tqdm

# Diffusion相关(第15章)
pip install diffusers==0.35.1  # 最新版本(2024-11)
pip install transformers accelerate
pip install safetensors
pip install controlnet-aux  # ControlNet预处理

# 可选:模型下载加速
pip install huggingface-hub
export HF_ENDPOINT=https://hf-mirror.com  # 国内镜像
```

### 验证安装

```python
import torch
import diffusers
from diffusers import DiffusionPipeline

print(f"PyTorch: {torch.__version__}")
print(f"Diffusers: {diffusers.__version__}")
print(f"CUDA可用: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
```

---

## 学习建议

### 1. 重点放在Diffusion

GAN虽然是生成模型的开创性工作,但在2024年已基本被Diffusion取代:

| 维度 | GAN | Diffusion |
|------|-----|-----------|
| **生成质量** | 高但有模式崩溃风险 | 非常高且稳定 |
| **训练稳定性** | 难(生成器与判别器需平衡) | 易(标准回归任务) |
| **多样性** | 容易模式崩溃 | 多样性好 |
| **可控性** | 较难 | 容易(ControlNet等) |
| **主流应用** | 较少 | Midjourney、SD等 |

**建议学习策略**:
- 第14章快速过一遍,理解GAN思想即可(1-2天)
- 第15章深入学习,动手实践各种应用(4-5天)

### 2. 理解扩散过程的数学原理

扩散模型的核心是**去噪扩散概率模型(DDPM)**,其数学原理相对复杂。本笔记采用:

- **简化版数学推导**:只讲核心思想,不展开复杂公式
- **直观可视化**:用代码和图示理解前向/反向过程
- **工程实战为主**:重点放在如何使用diffusers库

即使不完全理解数学,也能掌握实战应用!

### 3. 掌握提示词工程

高质量图像生成的关键在于提示词(Prompt):

```
# 低质量提示词
"a cat"

# 高质量提示词
"a fluffy orange cat sitting on a wooden table,
soft natural lighting, shallow depth of field,
bokeh background, photorealistic, 8k uhd,
professional photography"
```

**学习路径**:
1. 理解提示词的结构(主体、风格、质量词、负向词)
2. 学习常用提示词模板
3. 参考优秀案例(Civitai、PromptHero等)
4. 大量实践,形成自己的提示词库

### 4. 循序渐进的实战

```
第一步:跑通Stable Diffusion基础代码
   ↓
第二步:尝试不同的提示词和参数
   ↓
第三步:学习ControlNet可控生成
   ↓
第四步:探索高级技巧(LoRA、IP-Adapter等)
   ↓
第五步:构建自己的图像生成应用
```

---

## 模型资源

### Hugging Face Hub

本篇实战代码使用的模型:

1. **Stable Diffusion 1.5** (基础模型)
   - `runwayml/stable-diffusion-v1-5`
   - VRAM需求:~5GB
   - 速度:快

2. **Stable Diffusion XL** (高质量)
   - `stabilityai/stable-diffusion-xl-base-1.0`
   - VRAM需求:~10GB
   - 生成质量:显著提升

3. **FLUX.1** (2024最新)
   - `black-forest-labs/FLUX.1-dev`
   - `black-forest-labs/FLUX.1-schnell`
   - VRAM需求:~16GB
   - 生成质量:当前最佳

4. **ControlNet模型**
   - `lllyasviel/control_v11p_sd15_canny`
   - `lllyasviel/control_v11f1p_sd15_depth`
   - 等多种控制类型

### 国内下载加速

```bash
# 使用HuggingFace镜像
export HF_ENDPOINT=https://hf-mirror.com

# 或使用ModelScope
pip install modelscope
# 代码中使用modelscope下载
```

---

## 与前后篇的关系

```
第五篇:图像分割
       ↓
  (判别式模型的最后一篇)
       ↓
第六篇:生成模型 ← 当前篇
       ↓
  (生成式模型:GAN → Diffusion)
       ↓
第七篇:视觉大模型
       ↓
  (多模态模型:CLIP、BLIP、LLaVA等)
```

**知识衔接**:
- Transformer基础(第3篇第6章) → Diffusion中的Attention机制
- CNN架构(第3篇) → Diffusion中的U-Net
- 多模态模型(第7篇) → CLIP在SD中的文本编码器

---

## 代码规范

### 目录结构

```
part6_generation/
├── README.md                    # 本文件
├── chapter14/                   # GAN
│   ├── README.md
│   └── code/
│       └── dcgan_generation.py  # DCGAN完整实现
└── chapter15/                   # Diffusion
    ├── README.md
    └── code/
        ├── stable_diffusion_demo.py    # SD基础
        ├── controlnet_demo.py          # ControlNet
        └── utils/
            ├── prompt_templates.py     # 提示词模板
            └── image_utils.py          # 图像处理工具
```

### 代码风格

```python
"""
模块文档字符串:说明功能
"""
import torch
from diffusers import DiffusionPipeline
from typing import List, Optional

def generate_image(
    prompt: str,
    negative_prompt: Optional[str] = None,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    seed: Optional[int] = None
) -> torch.Tensor:
    """
    生成图像的核心函数

    Args:
        prompt: 正向提示词
        negative_prompt: 负向提示词
        num_inference_steps: 采样步数(越大越慢但质量越好)
        guidance_scale: 引导强度(7.5是常用值)
        seed: 随机种子(可复现结果)

    Returns:
        生成的图像Tensor
    """
    # 实现代码...
    pass
```

---

## 常见问题

### Q1: GAN和Diffusion哪个更重要?

**A**: 2024年以后,Diffusion已成为主流。建议:
- GAN:了解基本原理即可,不必深入
- Diffusion:重点学习,这是当前和未来的方向

### Q2: 没有GPU能学习吗?

**A**:
- GAN部分:可以用CPU训练小模型(MNIST等)
- Diffusion部分:强烈建议GPU,否则生成速度极慢
- 替代方案:使用Google Colab免费GPU或Hugging Face Spaces

### Q3: Stable Diffusion版本如何选择?

**A**:
- **SD 1.5**:轻量级,适合学习和低配GPU
- **SDXL**:质量更高,需要更多VRAM
- **SD 3**:最新版本,但生态还不完善
- **FLUX**:2024最强,但对硬件要求高

建议从SD 1.5开始学习!

### Q4: 如何写出高质量提示词?

**A**: 遵循以下结构:

```
[主体描述] + [细节修饰] + [艺术风格] + [技术参数] + [质量词]

示例:
"a majestic lion standing on a cliff,
golden hour lighting, dramatic clouds,
digital painting, trending on artstation,
highly detailed, 8k uhd, masterpiece"
```

第15章会详细讲解提示词工程!

### Q5: 生成的图像质量不好怎么办?

**A**: 依次检查:
1. **提示词质量**:是否足够详细和具体
2. **负向提示词**:添加"low quality, blurry, distorted"等
3. **采样步数**:提高num_inference_steps(20→50)
4. **引导强度**:调整guidance_scale(5-10之间)
5. **采样器**:尝试不同scheduler(Euler、DPM++等)
6. **模型选择**:换用更高质量的模型

---

## 拓展资源

### 官方文档

- **Hugging Face Diffusers**: https://huggingface.co/docs/diffusers
- **Stable Diffusion WebUI**: https://github.com/AUTOMATIC1111/stable-diffusion-webui
- **ComfyUI**: https://github.com/comfyanonymous/ComfyUI (节点式生成)

### 论文阅读

**必读论文**(按时间顺序):
1. GAN (2014) - Generative Adversarial Networks
2. DCGAN (2015) - Unsupervised Representation Learning with DCGAN
3. DDPM (2020) - Denoising Diffusion Probabilistic Models
4. Latent Diffusion (2022) - High-Resolution Image Synthesis
5. ControlNet (2023) - Adding Conditional Control to Text-to-Image

### 社区资源

- **Civitai**: 模型分享社区
- **PromptHero**: 提示词数据库
- **Hugging Face Spaces**: 在线Demo

### 实战平台

- **Midjourney**: 商业化最成功的AI绘画工具
- **Stable Diffusion WebUI**: 本地部署,完全免费
- **Leonardo.ai**: 在线生成平台

---

## 学习路线图

```
第1-2天: 第14章 GAN快速回顾
    ├── 理解GAN原理
    ├── 跑通DCGAN代码
    └── 了解GAN的局限性

第3-7天: 第15章 Diffusion深入学习
    ├── Day 3: 理解扩散过程数学原理
    ├── Day 4: Stable Diffusion架构与使用
    ├── Day 5: 提示词工程与实战
    ├── Day 6: ControlNet可控生成
    └── Day 7: FLUX等前沿模型探索

第8天: 综合实战项目
    └── 构建自己的图像生成应用
```

---

## 实战项目预告

完成本篇学习后,你将能够:

1. **基础应用**
   - 根据文本提示词生成高质量图像
   - 使用ControlNet进行可控生成
   - 图生图风格迁移

2. **进阶应用**
   - 批量生成与自动化
   - 与其他AI工具结合(如ChatGPT生成提示词)
   - 构建Web应用(FastAPI + Diffusers)

3. **商业应用方向**
   - 电商产品图生成
   - 游戏素材制作
   - 建筑效果图渲染
   - 创意设计辅助

---

**准备好进入生成式AI的奇妙世界了吗?让我们从第14章开始!**



---


# 第14章:生成对抗网络(GAN)

> **学习目标**:理解GAN的基本原理,掌握DCGAN实现,了解GAN的局限性
>
> **建议学习时间**:1-2天(快速回顾,不作为重点)
>
> **前置知识**:CNN基础、PyTorch基本操作

---

## 章节导读

生成对抗网络(GAN)在2014年由Ian Goodfellow提出,曾是生成模型的主流方向。虽然在2024年已基本被Diffusion模型取代,但理解GAN的思想对学习生成模型仍有重要意义。

### 为什么要学习GAN?

1. **历史意义**:开创性的生成模型架构
2. **思想价值**:对抗训练的思想影响深远
3. **知识衔接**:理解GAN有助于理解Diffusion的优势
4. **特定应用**:部分领域(如风格迁移)仍在使用

### 本章学习策略

**快速过一遍,不必深入**:
- 理解GAN的核心思想
- 跑通一个DCGAN示例
- 了解GAN的主要问题
- 为学习Diffusion做准备

---

## 14.1 GAN基础原理

### 核心思想:两人零和博弈

GAN的核心是**生成器(Generator)**和**判别器(Discriminator)**的对抗训练:

```
生成器G: 学习生成逼真的假图像,试图欺骗判别器
判别器D: 学习区分真实图像和生成图像

训练过程:
1. G生成假图像
2. D学习区分真假(真实数据标记为1,假数据标记为0)
3. G根据D的反馈改进,生成更逼真的图像
4. D也在改进,提高鉴别能力
5. 最终达到纳什均衡:G生成的图像足够逼真,D无法区分
```

### 数学形式

GAN的目标函数(简化版):

```
min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))]

其中:
- x: 真实数据
- z: 随机噪声(通常是高斯噪声)
- G(z): 生成器根据噪声z生成的假图像
- D(x): 判别器对真实图像的判断(接近1表示认为是真的)
- D(G(z)): 判别器对假图像的判断(接近0表示认为是假的)
```

**直观理解**:
- 判别器D:最大化V,希望正确区分真假
- 生成器G:最小化V,希望骗过判别器

### 训练流程

```python
for epoch in range(num_epochs):
    for real_images, _ in dataloader:
        # ===== 训练判别器 =====
        # 1. 真实图像送入判别器,标签为1
        real_labels = torch.ones(batch_size, 1)
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)

        # 2. 生成假图像,标签为0
        noise = torch.randn(batch_size, latent_dim)
        fake_images = generator(noise)
        fake_labels = torch.zeros(batch_size, 1)
        fake_outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_outputs, fake_labels)

        # 3. 判别器总损失
        d_loss = d_loss_real + d_loss_fake
        # 反向传播更新判别器
        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # ===== 训练生成器 =====
        # 生成器希望判别器认为假图像是真的(标签为1)
        noise = torch.randn(batch_size, latent_dim)
        fake_images = generator(noise)
        outputs = discriminator(fake_images)
        g_loss = criterion(outputs, torch.ones(batch_size, 1))

        # 反向传播更新生成器
        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()
```

---

## 14.2 DCGAN:深度卷积GAN

### 为什么需要DCGAN?

原始GAN使用全连接层,存在以下问题:
- 训练不稳定,容易模式崩溃
- 难以生成高分辨率图像
- 缺乏空间结构信息

**DCGAN** (Deep Convolutional GAN, 2015)通过引入卷积层解决了这些问题。

### DCGAN架构设计原则

DCGAN提出了一套稳定训练的架构指南:

1. **取消池化层**
   - 生成器:使用转置卷积(transposed convolution)进行上采样
   - 判别器:使用步长卷积(strided convolution)进行下采样

2. **使用BatchNorm**
   - 生成器:所有层都加BatchNorm(输出层除外)
   - 判别器:所有层都加BatchNorm(输入层除外)

3. **激活函数选择**
   - 生成器:隐藏层用ReLU,输出层用Tanh
   - 判别器:所有层用LeakyReLU

4. **去除全连接层**
   - 使用全卷积网络

### 生成器架构

```python
class Generator(nn.Module):
    def __init__(self, latent_dim=100, channels=3):
        super().__init__()
        self.model = nn.Sequential(
            # 输入: (batch, latent_dim, 1, 1)
            # 第一层:转置卷积 100 -> 512x4x4
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            # 512x4x4 -> 256x8x8
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # 256x8x8 -> 128x16x16
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # 128x16x16 -> 64x32x32
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # 64x32x32 -> 3x64x64
            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),
            nn.Tanh()  # 输出范围[-1, 1]
        )

    def forward(self, z):
        # z: (batch, latent_dim, 1, 1)
        return self.model(z)
```

**关键点**:
- 输入是100维噪声向量,reshape为(100, 1, 1)
- 通过4次转置卷积,逐步上采样到64x64图像
- 最后用Tanh激活,输出范围[-1, 1]

### 判别器架构

```python
class Discriminator(nn.Module):
    def __init__(self, channels=3):
        super().__init__()
        self.model = nn.Sequential(
            # 输入: 3x64x64
            # 3x64x64 -> 64x32x32
            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # 64x32x32 -> 128x16x16
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            # 128x16x16 -> 256x8x8
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            # 256x8x8 -> 512x4x4
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            # 512x4x4 -> 1x1x1
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()  # 输出范围[0, 1]
        )

    def forward(self, img):
        return self.model(img).view(-1, 1)
```

**关键点**:
- 输入是64x64的图像
- 通过4次步长卷积,逐步下采样
- 最后输出单个值,表示判断为真的概率

### 权重初始化

DCGAN论文建议使用特定的权重初始化:

```python
def weights_init(m):
    """
    从均值0、标准差0.02的正态分布初始化权重
    """
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# 应用初始化
generator.apply(weights_init)
discriminator.apply(weights_init)
```

---

## 14.3 StyleGAN系列(简介)

### StyleGAN的创新

**StyleGAN** (2018, NVIDIA)代表了GAN生成质量的巅峰:

1. **风格迁移机制**
   - 将随机噪声映射到中间潜空间W
   - 在不同层次注入风格信息
   - 实现粗粒度到细粒度的风格控制

2. **渐进式生长**
   - 从低分辨率逐步训练到高分辨率
   - 训练更稳定

3. **应用**
   - 高质量人脸生成
   - 风格混合(style mixing)
   - 图像编辑

### StyleGAN2和StyleGAN3

- **StyleGAN2** (2019):修复artifact问题,提升质量
- **StyleGAN3** (2021):解决纹理粘连问题

**注意**:StyleGAN系列架构复杂,训练难度大,本章不深入讲解。

---

## 14.4 条件GAN与应用

### Conditional GAN (cGAN)

在GAN中引入条件信息(如类别标签、文本描述):

```python
# 生成器和判别器都接受条件输入
class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)
        # 输入是噪声+条件拼接
        self.model = nn.Sequential(...)

    def forward(self, noise, labels):
        # 将标签嵌入与噪声拼接
        gen_input = torch.cat([noise, self.label_emb(labels)], -1)
        return self.model(gen_input)
```

### 经典应用

1. **Pix2Pix** (2016)
   - 图像到图像的翻译(如素描→照片、白天→黑夜)
   - 使用配对数据训练

2. **CycleGAN** (2017)
   - 无需配对数据的风格迁移
   - 使用循环一致性损失

3. **StarGAN** (2018)
   - 多领域图像翻译
   - 单个模型处理多种转换

**2024年现状**:这些任务现在更多使用Diffusion模型(如ControlNet)。

---

## 14.5 GAN的主要问题

### 1. 训练不稳定

**模式崩溃(Mode Collapse)**:
- 生成器只学会生成少数几种模式
- 缺乏多样性

```
理想情况: 数据分布有10个模式,生成器都能覆盖
模式崩溃: 生成器只生成其中2-3个模式
```

### 2. 梯度消失

当判别器过强时,生成器梯度接近0,无法学习。

### 3. 难以评估

缺乏客观的评估指标,常用指标:
- **IS (Inception Score)**:质量和多样性
- **FID (Fréchet Inception Distance)**:生成分布与真实分布的距离

但这些指标都不完美。

### 4. 超参数敏感

学习率、架构设计等超参数对结果影响巨大。

---

## 14.6 实战:DCGAN生成人脸

### 数据集:CelebA

CelebA包含20万张名人人脸图像,常用于GAN训练。

```python
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.Resize(64),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 归一化到[-1, 1]
])

dataset = datasets.CelebA(
    root='./data',
    split='train',
    transform=transform,
    download=True
)

dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=128,
    shuffle=True,
    num_workers=4
)
```

### 训练循环

完整代码见: [code/chapter14_gan/dcgan_generation.py](../../chapter29/code/chapter14_gan/dcgan_generation.py)

### 训练技巧

1. **标签平滑**
   ```python
   # 不使用硬标签1和0,而是0.9和0.1
   real_labels = torch.ones(batch_size, 1) * 0.9
   fake_labels = torch.zeros(batch_size, 1) + 0.1
   ```

2. **单侧标签平滑**
   - 只对真实标签平滑,假标签保持0

3. **特征匹配**
   - 生成器不仅要骗过判别器,还要匹配真实数据的统计特性

4. **学习率调整**
   - 生成器和判别器使用不同学习率
   - 常用:lr_D = 2e-4, lr_G = 2e-4

---

## 14.7 GAN与Diffusion对比

| 维度 | GAN | Diffusion |
|------|-----|-----------|
| **训练稳定性** | 难,需要平衡G和D | 稳定,标准去噪任务 |
| **模式崩溃** | 容易发生 | 不存在 |
| **生成质量** | 高(StyleGAN) | 更高(Stable Diffusion) |
| **多样性** | 有限 | 非常好 |
| **可控性** | 需要特殊设计(cGAN) | 天然支持(ControlNet) |
| **训练时间** | 相对快 | 慢 |
| **推理时间** | 快(单次前向) | 慢(多步去噪) |
| **2024主流应用** | 较少 | Midjourney、SD等 |

**结论**:Diffusion在大多数方面优于GAN,这也是为什么本篇重点学习Diffusion。

---

## 14.8 总结与展望

### 本章要点回顾

1. **GAN核心思想**:生成器与判别器的对抗训练
2. **DCGAN架构**:使用卷积层的稳定训练方案
3. **GAN的问题**:训练不稳定、模式崩溃等
4. **历史地位**:开创性工作,但已被Diffusion超越

### GAN的现代应用

虽然图像生成领域GAN已不是主流,但在以下领域仍有应用:
- **视频生成**:部分工作仍使用GAN
- **超分辨率**:ESRGAN等
- **特定领域**:医学图像、工业检测等数据受限场景

### 下一章预告

第15章我们将深入学习**扩散模型(Diffusion)**:
- 理解前向扩散和反向去噪过程
- 掌握Stable Diffusion的使用
- 学习ControlNet可控生成
- 探索FLUX等最新模型

扩散模型已成为生成式AI的核心技术,是本篇的重点内容!

---

## 参考资源

### 论文

1. **GAN** (2014)
   - Goodfellow et al., "Generative Adversarial Networks"
   - [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)

2. **DCGAN** (2015)
   - Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
   - [arXiv:1511.06434](https://arxiv.org/abs/1511.06434)

3. **StyleGAN** (2018)
   - Karras et al., "A Style-Based Generator Architecture for Generative Adversarial Networks"
   - [arXiv:1812.04948](https://arxiv.org/abs/1812.04948)

### 代码资源

- **PyTorch官方DCGAN教程**: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
- **StyleGAN3 (NVIDIA)**: https://github.com/NVlabs/stylegan3

### 在线课程

- **Coursera GAN Specialization** (deeplearning.ai)
- **Fast.ai Practical Deep Learning** (Part 2包含GAN)

---

**下一步**:进入[第15章:扩散模型](../chapter15/README.md),学习当前最先进的生成技术!



---


# 第15章:扩散模型(Diffusion Models)

> **学习目标**:深入理解扩散模型原理,熟练使用Stable Diffusion,掌握ControlNet可控生成
>
> **建议学习时间**:4-5天(本篇重点,深入学习)
>
> **前置知识**:CNN基础、Transformer基础(第3篇第6章)

---

## 章节导读

扩散模型是2024年生成式AI的核心技术,Midjourney、Stable Diffusion、DALL-E 3等明星产品都基于扩散模型。本章将深入讲解扩散模型的原理与实战应用。

### 为什么扩散模型如此重要?

1. **生成质量最高**:超越GAN,成为当前最强生成技术
2. **训练稳定**:不存在GAN的模式崩溃问题
3. **可控性强**:通过ControlNet等技术实现精准控制
4. **生态完善**:Hugging Face diffusers库,易于使用

### 本章学习路线

```
15.1 扩散模型基础
  ↓ 理解前向加噪和反向去噪
15.2 Stable Diffusion架构
  ↓ 掌握Latent Diffusion的优势
15.3 ControlNet可控生成
  ↓ 学习条件控制技术
15.4 FLUX最新模型
  ↓ 了解2024前沿进展
15.5 提示词工程
  ↓ 编写高质量提示词
实战项目
  ↓ 文生图、图生图、ControlNet
```

---

## 15.1 扩散模型基础

### 核心思想:逐步加噪再去噪

扩散模型的核心思想非常直观:

**前向过程(Forward Diffusion)**:逐步向图像添加高斯噪声,直到变成纯噪声
**反向过程(Reverse Diffusion)**:学习如何一步步去除噪声,从纯噪声恢复图像

```
原始图像 → +噪声 → +噪声 → ... → 纯噪声  (前向过程,固定的)
          ↓        ↓        ↓        ↓
纯噪声 → -噪声 → -噪声 → ... → 生成图像  (反向过程,需要学习)
```

### 前向扩散过程

给定一张图像 x₀,前向过程逐步添加高斯噪声:

```
x₀ → x₁ → x₂ → ... → x_T

每一步:
x_t = √(1-β_t) · x_{t-1} + √β_t · ε

其中:
- β_t: 第t步的噪声系数(通常从0.0001到0.02)
- ε ~ N(0, I): 标准高斯噪声
- T: 总步数(通常1000步)
```

**关键性质**:可以直接计算任意步的噪声图像,无需逐步计算:

```
x_t = √(ᾱ_t) · x₀ + √(1-ᾱ_t) · ε

其中:
ᾱ_t = ∏(1-β_i)  (累乘)
```

这个性质使得训练时可以随机采样任意时间步,大大加速训练!

### 反向去噪过程

反向过程学习如何从噪声中恢复图像:

```
x_T → x_{T-1} → x_{T-2} → ... → x₀

每一步:
x_{t-1} = (1/√α_t) · (x_t - (β_t/√(1-ᾱ_t)) · ε_θ(x_t, t))
```

**核心**:神经网络 ε_θ(x_t, t) 学习预测每一步的噪声

### DDPM训练目标

**Denoising Diffusion Probabilistic Models** (DDPM, 2020)的训练目标非常简单:

```
损失函数:
L = E[||ε - ε_θ(x_t, t)||²]

其中:
- ε: 真实噪声
- ε_θ(x_t, t): 模型预测的噪声
```

**直观理解**:训练一个神经网络,让它学会预测"我们添加了什么噪声"。

### 训练流程伪代码

```python
for epoch in range(num_epochs):
    for x0 in dataloader:  # 原始图像
        # 1. 随机选择时间步 t ∈ [1, T]
        t = random.randint(1, T)

        # 2. 生成随机噪声
        epsilon = torch.randn_like(x0)

        # 3. 根据公式计算 x_t
        xt = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * epsilon

        # 4. 模型预测噪声
        epsilon_pred = model(xt, t)

        # 5. 计算损失(简单的MSE)
        loss = (epsilon - epsilon_pred).pow(2).mean()

        # 6. 反向传播
        loss.backward()
        optimizer.step()
```

**关键点**:
- 每次训练只采样一个时间步,无需完整的T步
- 损失函数就是简单的L2损失
- 训练稳定,不需要GAN那样的对抗训练

### 采样流程(生成图像)

```python
# 从纯噪声开始
x_T = torch.randn(1, 3, H, W)

# 逐步去噪
for t in reversed(range(1, T+1)):
    # 预测噪声
    epsilon_pred = model(x_t, t)

    # 去噪一步
    x_{t-1} = denoise_step(x_t, epsilon_pred, t)

# 最终得到 x_0,即生成的图像
generated_image = x_0
```

**问题**:需要1000步才能生成一张图像,太慢了!

**解决方案**:DDIM、DPM-Solver等快速采样器,可以用20-50步达到相似质量。

---

## 15.2 Stable Diffusion架构

### 为什么需要Latent Diffusion?

原始DDPM直接在像素空间操作,存在问题:
- **计算量大**:1024x1024图像太大,显存吃不消
- **效率低**:高分辨率下训练和推理都慢

**Latent Diffusion Model** (LDM, 2022)的创新:
在**压缩的潜空间(Latent Space)**进行扩散,而非像素空间!

```
像素空间(512x512x3) → VAE编码 → 潜空间(64x64x4)
                                    ↓
                              在这里进行扩散
                                    ↓
潜空间(64x64x4) → VAE解码 → 像素空间(512x512x3)
```

**优势**:
- 潜空间维度小,计算快8倍以上
- 保留语义信息,生成质量不降低
- 可以在消费级GPU上运行

### Stable Diffusion三大组件

Stable Diffusion由三个核心模块组成:

#### 1. VAE (Variational Autoencoder)

**作用**:图像 ↔ 潜空间的相互转换

```python
# 编码器:图像 → 潜空间
latent = vae.encode(image)  # (3, 512, 512) → (4, 64, 64)

# 解码器:潜空间 → 图像
image = vae.decode(latent)  # (4, 64, 64) → (3, 512, 512)
```

**特点**:
- 压缩比8:1(在空间维度上)
- 4通道潜空间(不是RGB的3通道)
- VAE提前训练好,扩散时冻结

#### 2. U-Net:噪声预测器

**作用**:预测噪声 ε_θ(z_t, t, c)

架构:
```
输入: 噪声潜空间 z_t + 时间步 t + 条件 c(文本嵌入)
        ↓
    Encoder (下采样)
        ↓
    Bottleneck (Attention层处理)
        ↓
    Decoder (上采样,带Skip Connection)
        ↓
输出: 预测的噪声
```

**关键特性**:
- ResNet块 + Attention层
- Cross-Attention融合文本条件
- 时间步嵌入(类似Transformer的位置编码)

#### 3. CLIP Text Encoder

**作用**:将文本提示词转换为向量嵌入

```python
# 文本 → 向量
prompt = "a beautiful sunset over the ocean"
text_embedding = clip_text_encoder(prompt)  # (1, 77, 768)
```

**特点**:
- 使用OpenAI的CLIP模型
- 最大长度77个token
- 768维嵌入向量
- 通过Cross-Attention影响U-Net

### Stable Diffusion生成流程

完整的文生图流程:

```python
# 1. 文本编码
text_embedding = clip_text_encoder(prompt)

# 2. 初始化随机噪声(在潜空间)
latent = torch.randn(1, 4, 64, 64)  # 对应512x512图像

# 3. 迭代去噪
for t in scheduler.timesteps:  # 通常50步
    # 预测噪声
    noise_pred = unet(latent, t, text_embedding)

    # 去噪一步
    latent = scheduler.step(noise_pred, t, latent).prev_sample

# 4. 解码为图像
image = vae.decode(latent)
```

### 采样器(Scheduler)对比

不同采样器的速度和质量权衡:

| 采样器 | 步数 | 速度 | 质量 | 特点 |
|--------|------|------|------|------|
| **DDPM** | 1000 | 极慢 | 好 | 原始方法 |
| **DDIM** | 50 | 快 | 好 | 确定性采样 |
| **Euler** | 30-50 | 快 | 较好 | 简单稳定 |
| **Euler A** | 20-40 | 很快 | 较好 | 祖先采样 |
| **DPM-Solver++** | 20-30 | 很快 | 好 | 数值求解器 |
| **UniPC** | 20-30 | 很快 | 很好 | 2023新方法 |

**推荐**:
- 日常使用:DPM-Solver++ (20-30步)
- 追求质量:DDIM (50步)
- 快速预览:Euler A (20步)

### 引导强度(Guidance Scale)

**Classifier-Free Guidance**:控制生成结果对提示词的依赖程度

```
最终噪声预测 = 无条件预测 + guidance_scale × (有条件预测 - 无条件预测)
```

**参数影响**:
- **低值(1.0-5.0)**:更随机,更有创意,但可能偏离提示词
- **中值(7.0-9.0)**:平衡,推荐默认值
- **高值(10.0-20.0)**:严格遵循提示词,但可能过饱和

```python
# guidance_scale = 1.0:完全忽略提示词
# guidance_scale = 7.5:标准值
# guidance_scale = 15.0:强烈遵循提示词
```

---

## 15.3 ControlNet:可控生成

### 为什么需要ControlNet?

纯文本提示词的局限性:
- 难以精确控制构图
- 难以指定物体位置
- 难以保持一致的姿态

**ControlNet** (2023)通过额外的视觉条件解决这些问题!

### ControlNet原理

核心思想:在U-Net基础上添加一个可训练的副本,接受视觉条件输入

```
原始U-Net(冻结)
    ↓
  复制
    ↓
ControlNet分支(可训练)
    ↑
视觉条件(Canny边缘、深度图等)
```

**训练方式**:
1. 复制U-Net的编码器权重
2. 冻结原始U-Net
3. 只训练ControlNet分支
4. 通过零卷积(Zero Convolution)连接回主网络

**零卷积的妙处**:
- 初始权重全为0,训练开始时ControlNet对结果无影响
- 逐渐学习如何融入条件信息
- 训练稳定,不破坏原模型

### 常用控制类型

ControlNet支持多种视觉条件:

#### 1. Canny Edge (边缘检测)

**用途**:控制物体轮廓和边界

```python
from controlnet_aux import CannyDetector

canny = CannyDetector()
edge_image = canny(input_image)  # 提取边缘
```

**适用场景**:
- 保持物体形状
- 线稿上色
- 建筑设计

#### 2. Depth Map (深度图)

**用途**:控制场景的3D结构

```python
from controlnet_aux import DepthEstimator

depth_estimator = DepthEstimator.from_pretrained("Intel/dpt-hybrid-midas")
depth_map = depth_estimator(input_image)
```

**适用场景**:
- 保持空间关系
- 3D场景转换
- 虚拟场景生成

#### 3. OpenPose (人体姿态)

**用途**:控制人物姿态和动作

```python
from controlnet_aux import OpenposeDetector

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
pose_image = openpose(input_image)
```

**适用场景**:
- 动作指导
- 虚拟试衣
- 动画生成

#### 4. Scribble (手绘草图)

**用途**:从简单涂鸦生成图像

**适用场景**:
- 快速概念设计
- 艺术创作辅助

#### 5. Segmentation (语义分割)

**用途**:精确控制不同区域的内容

**适用场景**:
- 场景编辑
- 区域替换

### 多ControlNet组合

可以同时使用多个ControlNet,实现更精细的控制:

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

# 加载多个ControlNet
controlnet_canny = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_canny")
controlnet_depth = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth")

# 创建pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=[controlnet_canny, controlnet_depth]
)

# 生成(同时使用边缘和深度控制)
image = pipe(
    prompt="...",
    image=[canny_image, depth_image],
    controlnet_conditioning_scale=[0.5, 0.8]  # 分别控制权重
).images[0]
```

### ControlNet权重调节

`controlnet_conditioning_scale`参数控制条件的影响强度:

- **0.0**:完全忽略ControlNet条件
- **0.5-0.8**:中等强度,推荐范围
- **1.0-1.5**:强烈遵循条件,可能损失创造性

---

## 15.4 FLUX:2024最新扩散模型

### FLUX简介

**FLUX** 由Black Forest Labs(Stability AI创始团队成员创立)于2024年发布,代表了扩散模型的最新进展。

**主要版本**:

1. **FLUX.1-pro** (商业API)
   - 最高质量
   - 仅通过API访问

2. **FLUX.1-dev** (开源,非商用)
   - 接近pro的质量
   - 适合研究和开发

3. **FLUX.1-schnell** (开源,Apache 2.0)
   - 优化速度,1-4步生成
   - 可商用

### FLUX的创新点

1. **架构升级**
   - 更大的模型(12B参数)
   - 改进的Attention机制
   - 更好的文本理解

2. **生成质量**
   - 更准确的提示词理解
   - 更细腻的细节
   - 更自然的光影

3. **速度优化**(schnell版本)
   - 少步生成(1-4步)
   - 保持高质量

### FLUX vs Stable Diffusion

| 维度 | SD 1.5 | SDXL | FLUX.1 |
|------|--------|------|--------|
| **参数量** | 0.9B | 2.6B | 12B |
| **生成质量** | 好 | 很好 | 极好 |
| **提示词理解** | 中等 | 好 | 优秀 |
| **VRAM需求** | 4-6GB | 8-12GB | 16-24GB |
| **速度** | 快 | 中等 | 慢(dev)/快(schnell) |
| **开源许可** | CreativeML | CreativeML | Apache 2.0(schnell) |

### 使用FLUX

```python
from diffusers import FluxPipeline

# 加载FLUX.1-schnell(快速版本)
pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-schnell",
    torch_dtype=torch.bfloat16
).to("cuda")

# 生成(只需4步!)
image = pipe(
    prompt="a cat holding a sign that says 'hello world'",
    num_inference_steps=4,
    guidance_scale=0.0  # schnell版本不需要guidance
).images[0]
```

**注意**:FLUX需要较大显存,建议16GB+。

---

## 15.5 提示词工程(Prompt Engineering)

### 提示词的重要性

同样的模型,提示词质量决定生成效果的80%!

**低质量提示词**:
```
"a cat"
```

**高质量提示词**:
```
"a fluffy orange tabby cat with green eyes,
sitting on a vintage wooden table,
warm golden hour lighting,
shallow depth of field,
bokeh background,
professional pet photography,
8k uhd, sharp focus,
highly detailed fur texture"
```

### 提示词结构

标准的高质量提示词结构:

```
[主体描述] + [细节修饰] + [环境/背景] + [光照] +
[艺术风格] + [技术参数] + [质量词]
```

**示例分解**:

1. **主体描述**(必需)
   ```
   a majestic lion
   ```

2. **细节修饰**
   ```
   with a flowing golden mane, piercing amber eyes
   ```

3. **环境/背景**
   ```
   standing on a rocky cliff at sunset
   ```

4. **光照**
   ```
   dramatic golden hour lighting, rim light
   ```

5. **艺术风格**
   ```
   digital painting, cinematic, epic composition
   ```

6. **技术参数**
   ```
   8k uhd, sharp focus, volumetric lighting
   ```

7. **质量词**
   ```
   masterpiece, award winning, trending on artstation
   ```

**完整提示词**:
```
a majestic lion with a flowing golden mane and piercing amber eyes,
standing on a rocky cliff at sunset,
dramatic golden hour lighting with rim light,
digital painting, cinematic composition,
8k uhd, sharp focus, volumetric lighting,
masterpiece, award winning photography
```

### 负向提示词(Negative Prompt)

告诉模型**不要生成什么**:

**常用负向提示词模板**:
```
低质量相关:
lowres, bad anatomy, bad hands, text, error,
missing fingers, extra digit, fewer digits,
cropped, worst quality, low quality,
normal quality, jpeg artifacts, signature,
watermark, username, blurry

不需要的元素:
ugly, duplicate, morbid, mutilated,
extra fingers, mutated hands, poorly drawn hands,
poorly drawn face, mutation, deformed
```

**使用示例**:
```python
image = pipe(
    prompt="a beautiful landscape...",
    negative_prompt="blurry, low quality, distorted, watermark",
    num_inference_steps=50,
    guidance_scale=7.5
).images[0]
```

### 提示词权重

调整不同部分的重要性:

**语法**:
```
(keyword)     # 权重1.1
((keyword))   # 权重1.1 * 1.1 = 1.21
(keyword:1.5) # 权重1.5
[keyword]     # 权重0.9
```

**示例**:
```
"a ((highly detailed)) portrait,
(red hair:1.3),
[background]"

解释:
- highly detailed: 权重1.21(强调)
- red hair: 权重1.3(非常强调红色头发)
- background: 权重0.9(弱化背景)
```

### 风格提示词库

#### 摄影风格
```
portrait photography:
- "professional portrait photography"
- "studio lighting, soft box"
- "85mm lens, f/1.4"
- "bokeh background"

landscape photography:
- "landscape photography"
- "golden hour, dramatic clouds"
- "wide angle lens"
- "high dynamic range"
```

#### 艺术风格
```
digital art:
- "digital painting"
- "concept art"
- "trending on artstation"

anime:
- "anime style"
- "studio ghibli"
- "makoto shinkai"

oil painting:
- "oil painting"
- "impressionism"
- "visible brush strokes"
```

#### 质量提升词
```
通用质量词:
- "masterpiece"
- "best quality"
- "highly detailed"
- "8k uhd"
- "professional"
- "award winning"

清晰度:
- "sharp focus"
- "high resolution"
- "ultra detailed"
```

### 提示词优化技巧

1. **具体 > 抽象**
   - 差:"a nice scene"
   - 好:"a cozy coffee shop with warm lighting"

2. **使用艺术家名字**
   ```
   "in the style of Greg Rutkowski"
   "painted by Claude Monet"
   "photograph by Annie Leibovitz"
   ```

3. **分段描述复杂场景**
   ```
   前景 + 中景 + 背景:
   "in the foreground, a red rose,
   in the middle ground, a wooden table,
   in the background, a blurred window"
   ```

4. **参考优秀案例**
   - Civitai: https://civitai.com/
   - Lexica: https://lexica.art/
   - PromptHero: https://prompthero.com/

### 常见问题与解决

| 问题 | 原因 | 解决方案 |
|------|------|----------|
| **生成模糊** | 步数太少 | 增加num_inference_steps到50+ |
| **不符合提示词** | guidance_scale太低 | 提高到7.5-10 |
| **过度饱和** | guidance_scale太高 | 降低到7.5以下 |
| **出现文字** | 模型倾向 | 负向提示词加"text, words" |
| **手部畸形** | 模型弱点 | 负向提示词加"bad hands, extra fingers" |
| **构图不佳** | 提示词不够具体 | 明确描述构图和视角 |

---

## 15.6 实战项目

### 项目1:文生图(Text-to-Image)

完整代码见: [code/chapter15_diffusion/stable_diffusion_demo.py](../../chapter29/code/chapter15_diffusion/stable_diffusion_demo.py)

**核心代码**:
```python
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

prompt = """
a serene Japanese garden with a wooden bridge over a koi pond,
cherry blossoms falling, soft morning light,
traditional architecture in background,
photorealistic, 8k uhd, highly detailed
"""

negative_prompt = "blurry, low quality, distorted, watermark"

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=50,
    guidance_scale=7.5,
    height=512,
    width=512
).images[0]

image.save("output.png")
```

### 项目2:图生图(Image-to-Image)

基于参考图进行风格转换或修改:

```python
from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image

pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

# 加载参考图
init_image = Image.open("input.jpg").resize((512, 512))

prompt = "turn this photo into an oil painting, impressionism style"

image = pipe(
    prompt=prompt,
    image=init_image,
    strength=0.75,  # 0.0-1.0,越高变化越大
    guidance_scale=7.5
).images[0]
```

**strength参数**:
- 0.0-0.3:细微调整
- 0.4-0.7:风格转换
- 0.8-1.0:大幅改变

### 项目3:ControlNet可控生成

完整代码见: [code/chapter15_diffusion/controlnet_demo.py](../../chapter29/code/chapter15_diffusion/controlnet_demo.py)

**Canny边缘控制**:
```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from controlnet_aux import CannyDetector
from PIL import Image

# 提取边缘
canny = CannyDetector()
input_image = Image.open("input.jpg")
canny_image = canny(input_image)

# 加载ControlNet
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11p_sd15_canny"
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
).to("cuda")

# 生成
image = pipe(
    prompt="a modern architectural rendering, glass and steel",
    image=canny_image,
    controlnet_conditioning_scale=0.8
).images[0]
```

### 项目4:批量生成与自动化

```python
def batch_generate(prompts: list, output_dir: str):
    """批量生成图像"""
    pipe = StableDiffusionPipeline.from_pretrained(...)

    for i, prompt in enumerate(prompts):
        image = pipe(prompt=prompt).images[0]
        image.save(f"{output_dir}/image_{i:03d}.png")
        print(f"生成 {i+1}/{len(prompts)}")

# 使用
prompts = [
    "a cat in a garden",
    "a dog on a beach",
    "a bird in the sky"
]
batch_generate(prompts, "./outputs")
```

---

## 15.7 高级技巧

### 1. LoRA (Low-Rank Adaptation)

快速微调模型,添加特定风格或概念:

```python
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(...)

# 加载LoRA权重
pipe.load_lora_weights("path/to/lora.safetensors")

# 调整LoRA强度
pipe.set_adapters("default", adapter_weights=[0.8])
```

**LoRA资源**:Civitai上有大量社区训练的LoRA

### 2. Textual Inversion

添加新概念到模型词汇表:

```python
pipe.load_textual_inversion("path/to/embedding.pt", token="<my-concept>")

prompt = "a photo of <my-concept> in a forest"
```

### 3. Inpainting (局部重绘)

只修改图像的某个区域:

```python
from diffusers import StableDiffusionInpaintPipeline

pipe = StableDiffusionInpaintPipeline.from_pretrained(...)

image = pipe(
    prompt="a red car",
    image=original_image,
    mask_image=mask,  # 白色区域会被重绘
).images[0]
```

### 4. 超分辨率(Upscaling)

使用扩散模型进行图像放大:

```python
from diffusers import StableDiffusionUpscalePipeline

pipe = StableDiffusionUpscalePipeline.from_pretrained(
    "stabilityai/stable-diffusion-x4-upscaler"
)

upscaled = pipe(
    prompt="high quality, detailed",
    image=low_res_image
).images[0]
```

---

## 15.8 性能优化

### 显存优化

```python
pipe.enable_attention_slicing()  # 减少显存占用
pipe.enable_vae_slicing()  # VAE分块处理
pipe.enable_xformers_memory_efficient_attention()  # 使用xFormers
```

### 速度优化

```python
# 使用torch.compile(PyTorch 2.0+)
pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

# 使用更快的采样器
from diffusers import DPMSolverMultistepScheduler
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
```

### CPU Offloading

在低配GPU上运行:

```python
pipe.enable_sequential_cpu_offload()  # 组件轮流使用GPU
pipe.enable_model_cpu_offload()  # 更智能的offload
```

---

## 15.9 总结

### 本章要点回顾

1. **扩散过程**:前向加噪 + 反向去噪
2. **Latent Diffusion**:在压缩空间进行扩散,高效实用
3. **ControlNet**:通过视觉条件实现精准控制
4. **FLUX**:2024最新模型,质量显著提升
5. **提示词工程**:高质量生成的关键

### 扩散模型 vs GAN

| 维度 | GAN | Diffusion |
|------|-----|-----------|
| **训练稳定性** | 难 | 易 |
| **生成质量** | 好 | 更好 |
| **多样性** | 易模式崩溃 | 优秀 |
| **可控性** | 需特殊设计 | 天然支持 |
| **2024地位** | 边缘化 | 主流 |

### 学习建议

1. **理解原理**:前向/反向过程,DDPM训练目标
2. **熟练使用**:diffusers库,各种pipeline
3. **掌握提示词**:参考优秀案例,建立自己的提示词库
4. **实践项目**:文生图、ControlNet等

### 下一步方向

完成本章后,你可以:
- 进入第7篇学习**视觉大模型**(CLIP、LLaVA等)
- 深入研究**模型微调**(DreamBooth、LoRA训练)
- 探索**视频生成**(Animate Diff、Gen-2等)
- 学习**3D生成**(DreamFusion、Magic3D等)

---

## 参考资源

### 必读论文

1. **DDPM** (2020)
   - Ho et al., "Denoising Diffusion Probabilistic Models"
   - [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)

2. **Latent Diffusion** (2022)
   - Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"
   - [arXiv:2112.10752](https://arxiv.org/abs/2112.10752)

3. **ControlNet** (2023)
   - Zhang et al., "Adding Conditional Control to Text-to-Image Diffusion Models"
   - [arXiv:2302.05543](https://arxiv.org/abs/2302.05543)

### 官方文档

- **Hugging Face Diffusers**: https://huggingface.co/docs/diffusers
- **Stable Diffusion**: https://stability.ai/stable-diffusion
- **FLUX**: https://blackforestlabs.ai/

### 实战资源

- **Civitai**: https://civitai.com/ (模型与LoRA)
- **Lexica**: https://lexica.art/ (提示词库)
- **PromptHero**: https://prompthero.com/ (提示词数据库)

### 工具

- **Stable Diffusion WebUI**: https://github.com/AUTOMATIC1111/stable-diffusion-webui
- **ComfyUI**: https://github.com/comfyanonymous/ComfyUI

---

**恭喜完成第15章!你已经掌握了2024年最重要的生成式AI技术!**



---

