# 第01章：机器学习概览

> **本章核心**：
> 1. 理解频率派与贝叶斯派的本质区别（白板推导的核心世界观）
> 2. 掌握机器学习的三要素：模型、策略、算法
> 3. 透彻理解过拟合与泛化误差分解

## 1. 频率派 vs 贝叶斯派 (The Two Schools)

在白板推导系列中，shuhuai008 老师开篇就强调了统计机器学习的两个流派。理解这个通过，对后续理解 SVM（频率派巅峰）和 概率图模型（贝叶斯派大本营）至关重要。

### 1.1 频率派 (Frequentist)
- **世界观**：参数 $\theta$ 是一个**未知但固定**的常量。
- **目标**：通过数据 $X$ 去估计这个常量 $\theta$。
- **核心方法**：**极大似然估计 (MLE)**。
  $$ \hat{\theta}_{MLE} = \arg\max_{\theta} P(X|\theta) $$
  即：找到一个 $\theta$，使得通过它观察到当前数据 $X$ 的概率最大。
- **代表算法**：线性回归、逻辑回归、SVM、神经网络。
- **本质**：**优化问题 (Optimization)**。
  $$ \min_{\theta} L(\theta) $$

### 1.2 贝叶斯派 (Bayesian)
- **世界观**：参数 $\theta$ 是一个**随机变量**，它服从某个分布。
- **目标**：求出 $\theta$ 的**后验分布** $P(\theta|X)$。
- **核心方法**：**贝叶斯定理**。
  $$ P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)} $$
  - $P(\theta|X)$: 后验 (Posterior) - 看了数据之后我们对 $\theta$ 的认知
  - $P(X|\theta)$: 似然 (Likelihood) - 数据呈现的样子
  - $P(\theta)$: 先验 (Prior) - 看数据之前我们的信念
  - $P(X)$: 证据 (Evidence) - 归一化常数，积分 $\int P(X|\theta)P(\theta) d\theta$
- **预测**：不使用具体的 $\theta$，而是对所有可能的 $\theta$ 进行**积分**。
  $$ P(x_{new}|X) = \int P(x_{new}|\theta) \cdot P(\theta|X) d\theta $$
- **代表算法**：朴素贝叶斯、贝叶斯线性回归、高斯过程、LDA主题模型。
- **本质**：**积分问题 (Integration)**。

> **白板笔记**：
> - 频率派发展到极致变成了 **统计学习** (优化问题)。
> - 贝叶斯派发展到极致变成了 **概率图模型** (求积分/推断问题)。
> - 现代深度学习其实大部分是频率派（SGD优化），但在不确定性估计时会引入贝叶斯（Bayesian NN）。

---

## 2. 机器学习三要素

李航老师在《统计学习方法》中指出：
$$ \text{方法} = \text{模型} + \text{策略} + \text{算法} $$

1.  **模型 (Model)**：
    *   我们要学习的函数集合 $\mathcal{F}$。
    *   例如：$f(x) = w^T x + b$ (线性模型)。

2.  **策略 (Strategy)**：
    *   评价模型好坏的标准，即**损失函数 (Loss Function)**。
    *   **0-1损失**：$L(Y, f(X)) = \mathbb{I}(Y \neq f(X))$ (难优化)
    *   **平方损失**：$L(Y, f(X)) = (Y - f(X))^2$ (线性回归)
    *   **交叉熵损失**：$L(Y, P(Y|X)) = -\log P(Y|X)$ (逻辑回归/分类)
    *   **结构风险最小化**：Loss + **正则化项** (防止过拟合)
        $$ \min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f) $$

3.  **算法 (Algorithm)**：
    *   求解最优模型的具体计算方法。
    *   例如：最小二乘法的解析解、梯度下降法 (GD)、牛顿法、SMO算法。

---

## 3. 过拟合与泛化误差 (Overfitting)

### 3.1 现象直观
- **欠拟合 (Underfitting)**：模型太简单，连训练集都学不好。 (Bias 高)
- **过拟合 (Overfitting)**：模型太复杂，训练集全对，测试集稀烂。 (Variance 高)

### 3.2 偏差-方差分解 (Bias-Variance Decomposition)
这是理解模型性能最硬核的数学工具。假设真实函数为 $y = f(x) + \epsilon$，其中 $\epsilon \sim N(0, \sigma^2)$ 是噪声。

我们训练得到的模型为 $\hat{f}(x)$。则**泛化误差**可以分解为：
$$
\begin{aligned}
E[(y - \hat{f}(x))^2] &= \text{Bias}^2[\hat{f}(x)] + \text{Var}[\hat{f}(x)] + \text{Noise} \\
\text{误差} &= \text{偏差}^2 + \text{方差} + \text{不可避免的噪声}
\end{aligned}
$$

- **Bias (偏差)**：$E[\hat{f}(x)] - f(x)$
    - 衡量模型的**准确性**。
    - 模型越复杂，Bias 越小（越能拟合数据）。
- **Variance (方差)**：$E[(\hat{f}(x) - E[\hat{f}(x)])^2]$
    - 衡量模型的**稳定性**。
    - 模型越复杂，Variance 越大（对数据扰动越敏感）。

> **图解**：随着模型复杂度增加，Bias 下降，Variance 上升。最佳模型是在两者之间找到平衡点，使得总 Error 最小。

---

## 4. 实战：多项式拟合与正则化

我们用一段最经典的代码来演示：用多项式拟合正弦函数，直观看到过拟合现象，并用正则化解决它。

### 4.1 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge

# 1. 生成真实数据 (正弦函数 + 噪声)
def true_fun(X):
    return np.cos(1.5 * np.pi * X)

np.random.seed(0)
n_samples = 30
degrees = [1, 4, 15] # 分别代表：欠拟合、合适、过拟合

X = np.sort(np.random.rand(n_samples))
y = true_fun(X) + np.random.randn(n_samples) * 0.1

# 2. 绘图对比
plt.figure(figsize=(14, 5))
for i in range(len(degrees)):
    ax = plt.subplot(1, 3, i + 1)

    # 构造多项式模型
    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([
        ("polynomial_features", polynomial_features),
        ("linear_regression", linear_regression)
    ])

    # 训练
    pipeline.fit(X[:, np.newaxis], y)

    # 预测与评估
    X_test = np.linspace(0, 1, 100)
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
    plt.plot(X_test, true_fun(X_test), label="True function", linestyle="--")
    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")

    plt.title(f"Degree {degrees[i]}")
    plt.legend(loc="best")

plt.tight_layout()
plt.show()
```

### 4.2 实验结果分析
1.  **Degree=1 (欠拟合)**：直线无法拟合正弦曲线，**高 Bias**。
2.  **Degree=4 (合适)**：完美拟合了曲线趋势，泛化能力强。
3.  **Degree=15 (过拟合)**：虽然穿过了每个训练点（训练误差接近0），但曲线剧烈震荡，无法预测新数据，**高 Variance**。

### 4.3 解决方案：正则化 (Regularization)
如果我们坚持用 Degree=15，怎么办？
**白板推导**告诉我们：加一个惩罚项（正则化）。

$$ L(w) = \sum (y_i - w^T x_i)^2 + \lambda \|w\|^2 $$

这就是 **Ridge Regression (岭回归)**。加上正则化后，即使 Degree=15，曲线也会变得平滑！这就是数学的魔力。

---

## 5. 本章总结

| 概念 | 频率派 (Frequentist) | 贝叶斯派 (Bayesian) |
| :--- | :--- | :--- |
| **参数观** | 参数是固定的常量 | 参数是随机变量 (分布) |
| **核心操作** | 优化 (Optimization) | 积分 (Integration) |
| **典型方法** | MLE, 正则化 | MAP, 贝叶斯推断 |

**下一步学习：**
下一章我们将进入数学基础的深水区——**第02章：线性代数基础**，复习那个让无数人头秃的**矩阵求导**，它是推导所有机器学习公式的"手术刀"。
