# 第10章：逻辑回归与最大熵模型

> "The entropy of the universe tends to a maximum." —— Rudolf Clausius
>
> **重要提示**：本章将揭示一个惊人的数学事实：**逻辑回归 (Logistic Regression)** 只是 **最大熵模型 (Maximum Entropy Model)** 的一个特例。
>
> 当我们承认由于信息不足而必须保留"最大的不确定性"时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。
>
> 我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。

---

## 目录

1. [引言：分类问题的两种视角](#1-引言分类问题的两种视角)
2. [逻辑回归 (Logistic Regression)](#2-逻辑回归-logistic-regression)
   - 2.1 [Sigmoid 函数的由来](#21-sigmoid-函数的由来)
   - 2.2 [极大似然估计 (MLE)](#22-极大似然估计-mle)
   - 2.3 [信息论视角：最小化交叉熵](#23-信息论视角最小化交叉熵)
3. [最大熵模型 (Maximum Entropy Model)](#3-最大熵模型-maximum-entropy-model)
   - 3.1 [最大熵原理：无知是的智慧](#31-最大熵原理无知是的智慧)
   - 3.2 [最大熵模型的定义](#32-最大熵模型的定义)
   - 3.3 [Lagrange 对偶推导](#33-lagrange-对偶推导)
4. [殊途同归：逻辑回归与最大熵的等价性](#4-殊途同归逻辑回归与最大熵的等价性)
   - 4.1 [从最大熵推导逻辑回归](#41-从最大熵推导逻辑回归)
   - 4.2 [多分类推广：Softmax 回归](#42-多分类推广softmax-回归)
5. [模型学习算法](#5-模型学习算法)
   - 5.1 [改进的迭代尺度法 (IIS)](#51-改进的迭代尺度法-iis)
   - 5.2 [拟牛顿法 (BFGS/L-BFGS)](#52-拟牛顿法-bfgsl-bfgs)
6. [总结](#6-总结)
7. [推荐阅读](#7-推荐阅读)

---

## 1. 引言：分类问题的两种视角

在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带"回归"，但它是一个纯粹的**分类模型**。

理解逻辑回归有两种路径：
1. **统计学视角**：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。
2. **信息论视角**：在满足数据约束的前提下，选择熵最大的分布。

本章将证明，这两种视角最终指向了同一个数学形式。

---

## 2. 逻辑回归 (Logistic Regression)

### 2.1 Sigmoid 函数的由来

对于二分类问题 $y \in \{0, 1\}$，我们希望找到一个模型 $P(Y=1|x)$。既然是概率，取值必须在 $[0, 1]$ 之间。

线性模型 $w^T x + b$ 的取值范围是 $(-\infty, +\infty)$。如何将它映射到 $[0, 1]$？

**几率 (Odds)** 是一个很好的中间量：
$$
\text{Odds} = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{p}{1-p} \in [0, +\infty)
$$

**对数几率 (Log-Odds / Logit)**：
$$
\ln \frac{p}{1-p} \in (-\infty, +\infty)
$$

我们假设**对数几率是 $x$ 的线性函数**：
$$
\ln \frac{P(Y=1|x)}{1 - P(Y=1|x)} = w \cdot x + b
$$

解出 $P(Y=1|x)$：
$$
P(Y=1|x) = \frac{e^{w \cdot x + b}}{1 + e^{w \cdot x + b}} = \frac{1}{1 + e^{-(w \cdot x + b)}}
$$

这就是著名的 **Sigmoid 函数** $\sigma(z) = \frac{1}{1+e^{-z}}$。

![Sigmoid Function](assets/sigmoid_function.svg)

> **物理意义**：Sigmoid 函数将任意实数"挤压"到 $(0, 1)$ 区间，且在 $z=0$ 处变化最快（区分度最高），在两端趋于平缓（通过概率表达不确定性）。

### 2.2 极大似然估计 (MLE)

给定训练数据 $D = \{(x_i, y_i)\}_{i=1}^N$，我们要估计参数 $\theta = (w, b)$。

假设样本独立同分布，似然函数为：
$$
L(\theta) = \prod_{i=1}^N P(y_i | x_i; \theta)
$$

其中：
$$
P(y_i | x_i) = \begin{cases} p_i & \text{if } y_i = 1 \\ 1-p_i & \text{if } y_i = 0 \end{cases} \quad \Rightarrow \quad P(y_i | x_i) = p_i^{y_i} (1-p_i)^{1-y_i}
$$

**对数似然函数**：
$$
\begin{aligned}
\ell(\theta) &= \sum_{i=1}^N \left[ y_i \ln p_i + (1-y_i) \ln (1-p_i) \right] \\
&= \sum_{i=1}^N \left[ y_i \ln \frac{p_i}{1-p_i} + \ln (1-p_i) \right]
\end{aligned}
$$

代入 $p_i = \sigma(w \cdot x_i + b)$，注意到 $\ln \frac{p_i}{1-p_i} = w \cdot x_i + b$：
$$
\ell(w, b) = \sum_{i=1}^N \left[ y_i (w \cdot x_i + b) - \ln (1 + e^{w \cdot x_i + b}) \right]
$$

为了求解最优参数，我们通常**最小化负对数似然 (NLL)**：
$$
J(w, b) = -\ell(w, b) = \sum_{i=1}^N \left[ -y_i (w \cdot x_i + b) + \ln (1 + e^{w \cdot x_i + b}) \right]
$$

这是一个**凸函数**（Hessian 矩阵正定），可以通过梯度下降法或牛顿法找到全局最优解。

### 2.3 信息论视角：最小化交叉熵

仔细观察上面的 $J(w, b)$，它正是**交叉熵 (Cross Entropy)**！

$$
H(P_{\text{data}}, P_{\text{model}}) = -\frac{1}{N} \sum_{i=1}^N \sum_{y \in \{0,1\}} P_{\text{data}}(y|x_i) \ln P_{\text{model}}(y|x_i)
$$

由于 $P_{\text{data}}(y|x_i)$ 是经验分布（one-hot：$y_i=1$ 时概率为1，否则为0），这简化为：
$$
-\frac{1}{N} \sum_{i=1}^N \ln P_{\text{model}}(y_i|x_i)
$$

> **结论**：**最大化似然 = 最小化交叉熵**。这不仅适用于逻辑回归，也适用于所有概率分类模型（包括深度神经网络）。

---

## 3. 最大熵模型 (Maximum Entropy Model)

如果说逻辑回归是从"几率是线性的"这个假设出发，那么最大熵模型则是从更底层的哲学原理出发。

### 3.1 最大熵原理：无知是的智慧

**最大熵原理 (Principle of Maximum Entropy)**：
> 在满足所有已知约束条件的情况下，我们应该选择**熵最大**的概率分布。

**为什么？**
- 熵代表不确定性。
- 最大化熵 = **不作任何没有根据的假设**。
- 如果我们只知道骰子的均值是3.5，我们应该假设它是均匀的（1-6概率相等），而不是假设它只出3和4。

### 3.2 最大熵模型的定义

对于分类问题，我们想要得到条件概率分布 $P(Y|X)$。

**已知信息**由训练数据集 $D = \{(x_i, y_i)\}_{i=1}^N$ 提供。我们可以计算特征函数 $f_k(x, y)$ 在经验分布上的期望：

**经验期望**：
$$
\tilde{E}_P(f_k) = \sum_{x, y} \tilde{P}(x, y) f_k(x, y) = \frac{1}{N} \sum_{i=1}^N f_k(x_i, y_i)
$$

我们要求模型预测的期望与经验期望一致：

**模型期望**：
$$
E_P(f_k) = \sum_{x, y} \tilde{P}(x) P(y|x) f_k(x, y) = \frac{1}{N} \sum_{i=1}^N \sum_{y} P(y|x_i) f_k(x_i, y)
$$

**约束条件**：
$$
E_P(f_k) = \tilde{E}_P(f_k), \quad k = 1, \dots, K
$$

以及概率归一化条件：
$$
\sum_y P(y|x) = 1
$$

**目标**：最大化条件熵
$$
H(P) = -\sum_{x, y} \tilde{P}(x) P(y|x) \ln P(y|x)
$$

### 3.3 Lagrange 对偶推导

这是一个典型的约束优化问题。引入 Lagrange 乘子 $w_k$（对应特征约束）和 $\lambda$（对应归一化约束）：

$$
\min_P \max_{w} L(P, w)
$$

Lagrange 函数：
$$
L(P, w) = -H(P) + \sum_{k=1}^K w_k (E_P(f_k) - \tilde{E}_P(f_k)) + \sum_x \lambda(x) (\sum_y P(y|x) - 1)
$$

对 $P(y|x)$ 求偏导并令为0：
$$
\frac{\partial L}{\partial P(y|x)} = \tilde{P}(x) (1 + \ln P(y|x)) - \tilde{P}(x) \sum_k w_k f_k(x, y) - \lambda(x) = 0
$$

解得：
$$
P(y|x) = \exp \left( \sum_{k=1}^K w_k f_k(x, y) + \frac{\lambda(x)}{\tilde{P}(x)} - 1 \right)
$$

由于 $\sum_y P(y|x) = 1$，我们可以消去 $\lambda(x)$，得到最终形式：

$$
\boxed{
P_w(y|x) = \frac{1}{Z_w(x)} \exp \left( \sum_{k=1}^K w_k f_k(x, y) \right)
}
$$

其中 $Z_w(x)$ 是归一化因子（配分函数）：
$$
Z_w(x) = \sum_y \exp \left( \sum_{k=1}^K w_k f_k(x, y) \right)
$$

> **这就是最大熵模型！** 它的形式是指数族分布 (Exponential Family)。

---

## 4. 殊途同归：逻辑回归与最大熵的等价性

### 4.1 从最大熵推导逻辑回归

看看二分类逻辑回归。假设特征函数就是输入特征本身：
$$
f_k(x, y) = \begin{cases} x^{(k)} & \text{if } y = 1 \\ 0 & \text{if } y = 0 \end{cases}
$$

对于 $y=1$：
$$
P(y=1|x) = \frac{\exp(\sum w_k x^{(k)})}{\exp(0) + \exp(\sum w_k x^{(k)})} = \frac{e^{w \cdot x}}{1 + e^{w \cdot x}} = \frac{1}{1 + e^{-w \cdot x}}
$$

**完全一致！**

逻辑回归正是**特征函数为 $x$ 及其分量**时的最大熵模型。

### 4.2 多分类推广：Softmax 回归

如果在最大熵模型中，$y$ 可以取 $K$ 个类别，$f(x, y)$ 是 Indicator function，那么我们就得到了 **Softmax 回归** (Multinomial Logistic Regression)：

$$
P(y=k|x) = \frac{\exp(w_k \cdot x)}{\sum_{j=1}^K \exp(w_j \cdot x)}
$$

> **深刻洞见**：为什么深度学习最后一层常用 Softmax？因为它是在给定前一层输出作为特征的情况下，**熵最大**的分类分布。它是最"诚实"的概率输出。

---

## 5. 模型学习算法

最大熵模型的目标函数（对偶问题）等价于逻辑回归的对数似然函数。求解 $w$ 通常使用数值优化方法。

### 5.1 改进的迭代尺度法 (IIS)

**Improved Iterative Scaling (IIS)** 是一种专门用于最大熵模型的早期算法。
- 思想：寻找一个增量 $\delta$，使得对数似然下界提升。
- 缺点：收敛慢，现在很少使用。

### 5.2 拟牛顿法 (BFGS / L-BFGS)

现代机器学习库（如 sklearn, liblinear）主要使用**拟牛顿法**。

**牛顿法**需要计算 Hessian 矩阵 $H$（二阶导）：
$$
w_{t+1} = w_t - H^{-1} \nabla J(w_t)
$$
但 $H^{-1}$ 计算太贵 ($O(D^3)$)。

**BFGS**通过迭代近似 $H^{-1}$。
**L-BFGS (Limited-memory BFGS)** 进一步优化，只存储最近 $m$ 次的更新，将空间复杂度从 $O(D^2)$ 降到 $O(D)$。这是目前求解大规模逻辑回归/最大熵模型的**标准算法**。

---

## 6. 总结

本章虽然篇幅不长，但理论密度极高。

1. **逻辑回归**：
   - 形式上是 Sigmoid / Softmax
   - 本质上是线性模型的对数几率推广
   - 学习目标是最小化 Cross Entropy

2. **最大熵模型**：
   - 哲学上是"无知即不假设"
   - 数学上是带有特征约束的最大熵分布
   - 形式上导出了指数族分布

3. **大统一**：
   - **逻辑回归 = 最大熵模型**（在特定特征函数下）
   - 它们都是**对数线性模型 (Log-linear Model)**
   - 它们的损失函数都是**凸函数**，有全局最优解

![MaxEnt vs LR](assets/maxent_structure.svg)

> **"Entia non sunt multiplicanda praeter necessitatem." (Entities should not be multiplied beyond necessity.)** —— Occam's Razor
>
> 最大熵原理是奥卡姆剃刀在统计学中的数学表达：除此之外，不再做任何多余的假设。

---

## 7. 推荐阅读

- **《统计学习方法》 - 第6章** (李航)：详细推导了IIS算法
- **Jaynes, E. T. (1957). "Information Theory and Statistical Mechanics"**：最大熵原理的奠基之作
- **Berger et al. (1996). "A Maximum Entropy Approach to Natural Language Processing"**：将最大熵引入NLP的经典论文
