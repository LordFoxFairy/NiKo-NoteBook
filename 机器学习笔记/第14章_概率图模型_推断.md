# 第14章 概率图模型：推断

> "Probabilistic inference is nothing but counting, in appropriate ways." — Judea Pearl

## 引言

在概率图模型中，**推断（Inference）** 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。

本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。

---

## 14.1 推断问题的分类

### 14.1.1 推断任务的类型

设概率图模型定义在变量集合 $\mathcal{V} = \{X_1, X_2, \ldots, X_n\}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：

- **查询变量（Query Variables）**：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。
- **证据变量（Evidence Variables）**：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。
- **隐变量（Hidden Variables）**：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。

常见的推断任务包括：

#### (1) 边缘推断（Marginal Inference）

计算查询变量的**边缘概率分布**：

$$
P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}
$$

**示例**：在医疗诊断中，给定症状（证据），计算患某种疾病的概率分布。

#### (2) 最大后验概率推断（MAP Inference）

寻找使后验概率最大的变量赋值：

$$
\mathbf{q}^* = \arg\max_{\mathcal{Q}} P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \arg\max_{\mathcal{Q}} \sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})
$$

**示例**：在图像分割中，给定图像观测，找到最可能的像素标签配置。

**特例**：当 $\mathcal{Q} = \mathcal{V} \setminus \mathcal{E}$（即对所有未观测变量求MAP）时，称为 **最大概率解释（MPE, Most Probable Explanation）**：

$$
\mathbf{v}^* = \arg\max_{\mathcal{V} \setminus \mathcal{E}} P(\mathcal{V} \setminus \mathcal{E} | \mathcal{E} = \mathbf{e})
$$

### 14.1.2 精确推断 vs 近似推断

- **精确推断（Exact Inference）**：
  - 计算推断问题的精确解。
  - 适用于树结构图或图结构简单（treewidth小）的情况。
  - 代表算法：变量消除、信念传播、Junction Tree。

- **近似推断（Approximate Inference）**：
  - 当精确推断计算复杂度过高时（NP-hard），使用近似方法。
  - 代表方法：
    - **采样方法**：MCMC（马尔可夫链蒙特卡洛）、重要性采样。
    - **变分推断**：将推断问题转化为优化问题。
    - **Loopy Belief Propagation**：在有环图上运行信念传播（不保证收敛到精确解）。

**复杂度说明**：

- 边缘推断和MAP推断在一般图结构上均为 **NP-hard**。
- 精确推断的复杂度依赖于图的 **树宽（Treewidth）**，复杂度为 $O(n \cdot k^{w+1})$，其中 $n$ 为变量数，$k$ 为变量状态数，$w$ 为树宽。

---

## 14.2 变量消除算法（Variable Elimination）

### 14.2.1 核心思想

**变量消除（Variable Elimination, VE）** 是一种利用概率分布的因子分解结构，通过动态规划思想高效计算边缘概率的算法。

其核心思想是：

1. 将联合概率分布表示为若干**因子（Factors）**的乘积。
2. 按一定顺序逐个**消除（边缘化）隐变量**。
3. 利用**分配律**，将求和操作"推入"乘积内部，避免对所有变量的联合状态求和。

### 14.2.2 算法流程

**问题**：计算 $P(X_Q | \mathbf{e})$，其中 $\mathbf{e}$ 为证据。

**步骤**：

1. **因子初始化**：
   - 将联合分布分解为因子乘积：$P(\mathbf{X}) = \prod_{i} \phi_i(\mathbf{D}_i)$，其中 $\mathbf{D}_i$ 为因子 $\phi_i$ 的作用域。
   - 固定证据变量：将证据 $\mathbf{e}$ 代入因子，得到简化因子 $\phi_i'(\mathbf{D}_i \setminus \mathcal{E})$。

2. **选择消除顺序**：
   - 确定隐变量的消除顺序 $\pi = (H_1, H_2, \ldots, H_m)$。

3. **逐个消除变量**：
   - 对每个隐变量 $H_i$：
     - 收集所有包含 $H_i$ 的因子：$\Phi_i = \{\phi_j : H_i \in \mathbf{D}_j\}$。
     - 计算中间因子：$\tau_i = \sum_{H_i} \prod_{\phi_j \in \Phi_i} \phi_j$。
     - 用 $\tau_i$ 替换 $\Phi_i$ 中的因子。

4. **归一化**：
   - 最终得到关于查询变量 $X_Q$ 的非归一化分布 $\tilde{P}(X_Q, \mathbf{e})$。
   - 归一化：$P(X_Q | \mathbf{e}) = \frac{\tilde{P}(X_Q, \mathbf{e})}{\sum_{X_Q} \tilde{P}(X_Q, \mathbf{e})}$。

### 14.2.3 算法示例

考虑链式贝叶斯网络：$X_1 \to X_2 \to X_3 \to X_4$，联合分布为：

$$
P(X_1, X_2, X_3, X_4) = P(X_1) P(X_2|X_1) P(X_3|X_2) P(X_4|X_3)
$$

**任务**：计算 $P(X_4)$。

**朴素方法**（枚举）：

$$
P(X_4) = \sum_{X_1, X_2, X_3} P(X_1) P(X_2|X_1) P(X_3|X_2) P(X_4|X_3)
$$

需要枚举 $k^3$ 种状态组合（$k$ 为每个变量的状态数）。

**变量消除**：

消除顺序为 $X_1, X_2, X_3$：

1. 消除 $X_1$：
   $$
   \tau_1(X_2) = \sum_{X_1} P(X_1) P(X_2|X_1)
   $$

2. 消除 $X_2$：
   $$
   \tau_2(X_3) = \sum_{X_2} \tau_1(X_2) P(X_3|X_2)
   $$

3. 消除 $X_3$：
   $$
   P(X_4) = \sum_{X_3} \tau_2(X_3) P(X_4|X_3)
   $$

**复杂度**：每步仅需枚举 $O(k^2)$ 种状态，总复杂度为 $O(k^2)$，远小于朴素方法的 $O(k^3)$。

### 14.2.4 计算复杂度与树宽

**关键观察**：变量消除的复杂度取决于**中间因子的作用域大小**。

- 每次消除变量 $H_i$ 时，新因子 $\tau_i$ 的作用域为所有包含 $H_i$ 的因子作用域的并集（去掉 $H_i$）。
- 中间因子的最大作用域大小记为 **induced width**，与图的 **树宽（Treewidth）** 密切相关。

**树宽定义**：

图 $G$ 的树宽 $w$ 是最优三角化（triangulation）后，最大团的大小减1。

**复杂度定理**：

变量消除算法的时间复杂度为：

$$
O(n \cdot k^{w+1})
$$

其中 $n$ 为变量数，$k$ 为变量状态数，$w$ 为树宽。

**消除顺序的影响**：

- 不同的消除顺序会产生不同的中间因子，影响计算复杂度。
- 寻找**最优消除顺序**（最小化induced width）是NP-hard问题。
- 实践中使用启发式方法（如最小填充、最小度数）选择消除顺序。

---

## 14.3 信念传播（Belief Propagation）

### 14.3.1 消息传递机制

**信念传播（Belief Propagation, BP）** 是一种基于 **消息传递（Message Passing）** 的推断算法，特别适用于树结构的图模型。

**核心思想**：

- 图中的节点通过传递**消息（Messages）**来交换信息。
- 每个节点根据接收到的消息计算其**信念（Belief）**，即边缘概率分布。

**消息定义**：

节点 $i$ 向邻居节点 $j$ 发送的消息 $m_{i \to j}(X_j)$ 表示：节点 $i$ 及其子树对 $X_j$ 的"看法"。

### 14.3.2 Sum-Product 算法（边缘推断）

**目标**：计算每个变量的边缘概率分布 $P(X_i)$。

**算法描述**（针对树结构的无向图）：

设无向树 $T = (V, E)$，因子分解为：

$$
P(\mathbf{X}) = \frac{1}{Z} \prod_{(i,j) \in E} \psi_{ij}(X_i, X_j) \prod_{i \in V} \phi_i(X_i)
$$

**消息传递规则**：

节点 $i$ 向邻居节点 $j$ 发送消息：

$$
m_{i \to j}(X_j) = \sum_{X_i} \psi_{ij}(X_i, X_j) \phi_i(X_i) \prod_{k \in \mathcal{N}(i) \setminus \{j\}} m_{k \to i}(X_i)
$$

其中 $\mathcal{N}(i)$ 为节点 $i$ 的邻居集合。

**信念计算**：

节点 $i$ 的信念（边缘分布）为：

$$
b_i(X_i) \propto \phi_i(X_i) \prod_{k \in \mathcal{N}(i)} m_{k \to i}(X_i)
$$

归一化后得到 $P(X_i)$。

**执行流程**：

1. **选择根节点**：在树上选择任意节点作为根。
2. **从叶到根传递消息**：按拓扑顺序，从叶节点向根节点传递消息。
3. **从根到叶传递消息**：从根节点向叶节点传递消息。
4. **计算信念**：每个节点根据收到的消息计算边缘分布。

**算法示意图（占位符）**：

```svg
<!-- SVG Placeholder: Sum-Product Algorithm Message Passing on Tree -->
<!-- 绘制树结构，标注消息方向（双向箭头），节点表示变量，边标注消息m_{i->j} -->
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 400 300">
  <text x="200" y="150" text-anchor="middle" font-size="14">Sum-Product Message Passing Diagram</text>
  <text x="200" y="170" text-anchor="middle" font-size="12">(Tree structure with bidirectional messages)</text>
</svg>
```

**定理（Sum-Product 正确性）**：

在**树结构**的图模型上，Sum-Product 算法经过两轮消息传递后，计算得到的信念 $b_i(X_i)$ 等于精确的边缘概率 $P(X_i)$。

### 14.3.3 Max-Product 算法（MAP推断）

**目标**：寻找最大概率配置（MPE）：

$$
\mathbf{x}^* = \arg\max_{\mathbf{X}} P(\mathbf{X})
$$

**算法描述**：

将 Sum-Product 算法中的 **求和（$\sum$）** 替换为 **求最大值（$\max$）**。

**消息传递规则**：

$$
m_{i \to j}(X_j) = \max_{X_i} \left[ \psi_{ij}(X_i, X_j) \phi_i(X_i) \prod_{k \in \mathcal{N}(i) \setminus \{j\}} m_{k \to i}(X_i) \right]
$$

**最优配置计算**：

1. 运行 Max-Product 算法传递消息。
2. 在根节点找到最优取值：
   $$
   x_{\text{root}}^* = \arg\max_{X_{\text{root}}} \left[ \phi_{\text{root}}(X_{\text{root}}) \prod_{k \in \mathcal{N}(\text{root})} m_{k \to \text{root}}(X_{\text{root}}) \right]
   $$
3. 通过回溯（backtracking）得到其他节点的最优取值。

**对数空间优化（Max-Sum）**：

为避免数值下溢，通常在对数空间进行计算，将乘法转化为加法：

$$
\log m_{i \to j}(X_j) = \max_{X_i} \left[ \log \psi_{ij}(X_i, X_j) + \log \phi_i(X_i) + \sum_{k \in \mathcal{N}(i) \setminus \{j\}} \log m_{k \to i}(X_i) \right]
$$

### 14.3.4 树结构 vs 有环图

#### 树结构的优势

- **无环性保证收敛**：消息传递在有限轮次内终止。
- **计算精确**：Sum-Product 和 Max-Product 算法在树上给出精确解。
- **复杂度线性**：时间复杂度为 $O(|E| \cdot k^2)$，其中 $|E|$ 为边数，$k$ 为变量状态数。

#### Loopy Belief Propagation（有环图）

当图中存在环时，直接应用 BP 算法称为 **Loopy BP**：

- **迭代执行**：反复传递消息直至收敛（或达到最大迭代次数）。
- **不保证收敛**：消息可能在环上震荡，不收敛。
- **不保证精确**：即使收敛，结果也可能是近似解。
- **实践效果**：在许多应用中（如LDPC译码、图像去噪），Loopy BP 效果良好。

**Loopy BP 的理论理解**：

- 可以视为对 **Bethe 自由能（Bethe Free Energy）** 的优化。
- 在某些条件下（如单峰分布、弱耦合），Loopy BP 的不动点对应局部最优解。

**算法流程**：

1. 初始化所有消息为均匀分布。
2. 重复以下步骤直至收敛：
   - 对每条边 $(i, j)$，按上述规则更新消息 $m_{i \to j}(X_j)$。
   - 检查消息变化是否小于阈值。
3. 根据最终消息计算信念。

**收敛性判据**：

$$
\max_{i,j} \| m_{i \to j}^{(t+1)} - m_{i \to j}^{(t)} \| < \epsilon
$$

---

## 14.4 Junction Tree 算法（汇合树算法）

### 14.4.1 动机与思想

**问题**：变量消除和信念传播在有环图上效率低或不精确。

**Junction Tree 算法** 将任意图转化为树结构（团树），然后在树上进行精确推断。

**核心思想**：

1. **构造团树（Clique Tree）**：将原图中的变量分组为"团（Cliques）"，团之间形成树结构。
2. **满足 Running Intersection Property（RIP）**：保证团树能够正确表示原图的联合分布。
3. **在团树上运行信念传播**：团之间传递消息，计算边缘概率。

### 14.4.2 算法流程

**步骤**：

1. **道德化（Moralization）**（针对有向图）：
   - 将有向图转化为无向图：连接所有父节点，去掉边的方向。

2. **三角化（Triangulation）**：
   - 在无向图中添加边，使得图中不存在长度大于3的无弦环。
   - 目标：最小化最大团的大小（树宽）。

3. **构造团树**：
   - 识别所有最大团（maximal cliques）。
   - 构造团树，使得满足 **Running Intersection Property**：
     - 对于任意变量 $X$，所有包含 $X$ 的团在团树上形成连通子树。

4. **初始化团势函数（Clique Potentials）**：
   - 将原图的因子分配到包含其作用域的团上。

5. **消息传递**：
   - 在团树上运行 Sum-Product 算法。
   - 团之间通过**分隔符（Separator）**传递消息。

6. **计算边缘概率**：
   - 从团的势函数中提取查询变量的边缘分布。

### 14.4.3 Running Intersection Property (RIP)

**定义**：

团树 $T = (C, E)$ 满足 RIP，当且仅当：

$$
\forall X \in \mathcal{V}, \quad \{C_i : X \in C_i\} \text{ 在 } T \text{ 上形成连通子树}
$$

**意义**：

- RIP 保证团树能够一致地表示原图的联合分布。
- 消息传递时，变量的信息能够正确传播到所有相关团。

### 14.4.4 算法复杂度

**时间复杂度**：

$$
O(|C| \cdot k^{w+1})
$$

其中 $|C|$ 为团的数量，$w$ 为树宽（最大团大小减1），$k$ 为变量状态数。

**空间复杂度**：

$$
O(|C| \cdot k^{w+1})
$$

需要存储每个团的势函数。

**优化**：

- 使用稀疏表示存储势函数。
- 采用启发式三角化算法（如最小填充、最小度数）降低树宽。

**Junction Tree 算法示意图（占位符）**：

```svg
<!-- SVG Placeholder: Junction Tree Construction -->
<!-- 绘制原图 -> 道德化 -> 三角化 -> 团树 的转换过程 -->
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 500 300">
  <text x="250" y="150" text-anchor="middle" font-size="14">Junction Tree Algorithm Pipeline</text>
  <text x="250" y="170" text-anchor="middle" font-size="12">(Original Graph → Moralization → Triangulation → Clique Tree)</text>
</svg>
```

---

## 14.5 推断算法的比较

| 算法 | 适用图结构 | 精确性 | 复杂度 | 优缺点 |
|------|-----------|--------|--------|--------|
| **变量消除** | 任意图 | 精确 | $O(n \cdot k^{w+1})$ | 简单直观；需选择消除顺序；每次查询需重新计算 |
| **Sum-Product BP** | 树结构 | 精确 | $O(\|E\| \cdot k^2)$ | 高效；仅适用于树 |
| **Loopy BP** | 有环图 | 近似 | $O(T \cdot \|E\| \cdot k^2)$ | 易实现；不保证收敛或精确；实践中效果较好 |
| **Junction Tree** | 任意图 | 精确 | $O(\|C\| \cdot k^{w+1})$ | 通用；支持多次查询；构造团树有开销 |

**选择建议**：

- **树结构**：首选信念传播（BP）。
- **小树宽的图**：Junction Tree 或变量消除。
- **大树宽的图**：使用近似推断（Loopy BP、MCMC、变分推断）。
- **单次查询**：变量消除。
- **多次查询**：Junction Tree（预计算团树）。

---

## 14.6 推断问题的计算复杂度

### 14.6.1 理论结果

**定理**：

- **边缘推断（Marginal Inference）**在一般贝叶斯网络上是 **#P-complete**（计数复杂度类）。
- **MAP 推断**在一般贝叶斯网络上是 **NP-hard**。

**含义**：

- 精确推断在最坏情况下不存在多项式时间算法（除非P=NP）。
- 实际应用中需根据图的结构选择合适算法。

### 14.6.2 可处理的特殊情况

以下图结构的推断问题可在多项式时间内求解：

1. **树结构**：$O(nk^2)$。
2. **固定树宽的图**：$O(n \cdot k^{w+1})$，当 $w$ 为常数时为多项式。
3. **链式模型**：$O(nk^2)$（隐马尔可夫模型）。
4. **二分图的完美匹配结构**：某些特殊因子图。

---

## 14.7 案例分析：隐马尔可夫模型的推断

### 14.7.1 问题描述

**隐马尔可夫模型（HMM）**：

- 隐状态序列：$Z_1, Z_2, \ldots, Z_T$（Markov链）。
- 观测序列：$X_1, X_2, \ldots, X_T$（每个 $X_t$ 仅依赖于 $Z_t$）。

联合分布：

$$
P(Z_{1:T}, X_{1:T}) = P(Z_1) \prod_{t=2}^{T} P(Z_t | Z_{t-1}) \prod_{t=1}^{T} P(X_t | Z_t)
$$

**推断任务**：

1. **前向算法（Forward Algorithm）**：计算 $P(Z_t | X_{1:t})$。
2. **后向算法（Backward Algorithm）**：计算 $P(X_{t+1:T} | Z_t)$。
3. **前向-后向算法（Forward-Backward）**：计算 $P(Z_t | X_{1:T})$（平滑）。
4. **Viterbi 算法**：计算最可能的隐状态序列（MAP）。

### 14.7.2 前向算法

**定义前向概率**：

$$
\alpha_t(z_t) = P(Z_t = z_t, X_{1:t})
$$

**递推公式**：

$$
\alpha_t(z_t) = P(X_t | Z_t = z_t) \sum_{z_{t-1}} \alpha_{t-1}(z_{t-1}) P(Z_t = z_t | Z_{t-1} = z_{t-1})
$$

**初始化**：

$$
\alpha_1(z_1) = P(Z_1 = z_1) P(X_1 | Z_1 = z_1)
$$

**算法对应**：前向算法是变量消除算法在HMM上的特例，消除顺序为 $Z_1, Z_2, \ldots$。

### 14.7.3 Viterbi 算法

**定义**：

$$
\delta_t(z_t) = \max_{z_{1:t-1}} P(Z_{1:t-1}, Z_t = z_t, X_{1:t})
$$

**递推公式**：

$$
\delta_t(z_t) = P(X_t | Z_t = z_t) \max_{z_{t-1}} \left[ \delta_{t-1}(z_{t-1}) P(Z_t = z_t | Z_{t-1} = z_{t-1}) \right]
$$

**回溯**：

记录每步的最优前驱状态 $\psi_t(z_t)$，最后从 $z_T^* = \arg\max_{z_T} \delta_T(z_T)$ 回溯得到完整路径。

**算法对应**：Viterbi 算法是 Max-Product 算法在链式结构上的应用。

---

## 14.8 近似推断简介

当精确推断不可行时，需采用近似方法。以下简要介绍几种常用技术。

### 14.8.1 采样方法（Sampling-based Inference）

**基本思想**：通过从分布中抽取样本，估计边缘概率或期望。

**常用方法**：

1. **前向采样（Forward Sampling）**：
   - 按拓扑顺序依次采样变量。
   - 适用于无证据或证据较少的情况。

2. **拒绝采样（Rejection Sampling）**：
   - 采样后丢弃不满足证据的样本。
   - 当证据概率很小时效率低。

3. **重要性采样（Importance Sampling）**：
   - 从提议分布 $Q$ 中采样，用重要性权重修正。
   - 权重：$w(\mathbf{x}) = \frac{P(\mathbf{x})}{Q(\mathbf{x})}$。

4. **马尔可夫链蒙特卡洛（MCMC）**：
   - **Gibbs 采样**：逐个变量条件采样。
   - **Metropolis-Hastings**：接受-拒绝机制。
   - 收敛到平稳分布（目标分布）。

### 14.8.2 变分推断（Variational Inference）

**基本思想**：将推断问题转化为优化问题。

**方法**：

- 用简单分布 $Q(\mathbf{X})$ 近似真实后验 $P(\mathbf{X} | \mathbf{e})$。
- 最小化 KL 散度：
  $$
  Q^* = \arg\min_{Q} \text{KL}(Q \| P) = \arg\min_{Q} \int Q(\mathbf{X}) \log \frac{Q(\mathbf{X})}{P(\mathbf{X} | \mathbf{e})} d\mathbf{X}
  $$

**平均场近似（Mean Field Approximation）**：

假设 $Q$ 完全分解：

$$
Q(\mathbf{X}) = \prod_{i} Q_i(X_i)
$$

**优化**：坐标上升法，逐个更新 $Q_i(X_i)$：

$$
\log Q_i^*(X_i) = \mathbb{E}_{Q_{-i}}[\log P(\mathbf{X}, \mathbf{e})] + \text{const}
$$

**优点**：

- 计算速度快。
- 可扩展到大规模问题。

**缺点**：

- 近似质量依赖于 $Q$ 的选择。
- 可能低估后验方差。

### 14.8.3 Loopy BP 的应用

如前所述，Loopy BP 在有环图上作为近似推断方法广泛应用，特别是在：

- **纠错码译码**（Turbo 码、LDPC 码）。
- **计算机视觉**（立体匹配、图像分割）。
- **社交网络分析**。

---

## 14.9 本章小结

本章系统介绍了概率图模型中的推断问题及其求解算法：

1. **推断任务**：
   - 边缘推断（计算后验分布）。
   - MAP 推断（寻找最可能配置）。

2. **精确推断算法**：
   - **变量消除**：利用因子分解和动态规划，复杂度依赖于消除顺序和树宽。
   - **信念传播**：在树结构上通过消息传递高效计算边缘概率和MAP。
   - **Junction Tree**：将任意图转化为团树，进行精确推断。

3. **近似推断**：
   - Loopy BP：在有环图上运行信念传播。
   - 采样方法：MCMC、重要性采样。
   - 变分推断：将推断转化为优化问题。

4. **计算复杂度**：
   - 一般图上的推断是NP-hard问题。
   - 树结构和小树宽图可高效求解。

**关键要点**：

- 推断算法的选择取决于图的结构、精度要求和计算资源。
- 精确推断在树和小树宽图上可行，一般图需要近似方法。
- 信念传播是理解和实现推断算法的核心框架。

---

## 习题

1. **变量消除**：
   - 对于链式贝叶斯网络 $X_1 \to X_2 \to \cdots \to X_n$，分析不同消除顺序的复杂度差异。

2. **Sum-Product 算法**：
   - 手动执行 Sum-Product 算法，计算如下树状贝叶斯网络的边缘概率 $P(X_3)$：
     $$
     X_1 \to X_2 \to X_3 \leftarrow X_4
     $$

3. **Loopy BP 实验**：
   - 在包含单个环的图上运行 Loopy BP，观察消息传递的收敛性。

4. **Junction Tree 构造**：
   - 对给定的无向图进行三角化，构造 Junction Tree，并验证 Running Intersection Property。

5. **HMM 推断**：
   - 实现 Viterbi 算法，对观测序列进行最优隐状态解码。

6. **变分推断**：
   - 用平均场近似推导高斯混合模型的变分EM算法。

---

## 参考文献

1. **Koller, D., & Friedman, N.** (2009). *Probabilistic Graphical Models: Principles and Techniques*. MIT Press.
   - 权威教材，详细介绍推断算法。

2. **Pearl, J.** (1988). *Probabilistic Reasoning in Intelligent Systems*. Morgan Kaufmann.
   - 信念传播算法的开创性著作。

3. **Wainwright, M. J., & Jordan, M. I.** (2008). *Graphical Models, Exponential Families, and Variational Inference*. Foundations and Trends in Machine Learning.
   - 变分推断的深入分析。

4. **Murphy, K. P.** (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
   - 第20-22章：推断与学习算法。

5. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer.
   - 第8章：图模型与推断。

6. **Yedidia, J. S., Freeman, W. T., & Weiss, Y.** (2003). "Understanding Belief Propagation and Its Generalizations". *Exploring Artificial Intelligence in the New Millennium*.
   - Loopy BP 的理论分析。

---

> "The art of inference is the art of message passing." — Anonymous

**下一章预告**：第15章将介绍概率图模型的**学习（Learning）**，包括参数学习（极大似然估计、EM算法）和结构学习（评分搜索、约束方法）。
