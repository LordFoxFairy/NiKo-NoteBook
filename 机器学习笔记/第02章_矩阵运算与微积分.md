# 第02章：矩阵运算与微积分

> **前言**
>
> 线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于**你真正需要的那部分**：
> - **向量空间**：理解数据的结构和维度
> - **投影**：理解线性回归的几何本质
> - **矩阵微积分**：理解梯度下降和反向传播
>
> 我们的目标是**几何直觉** + **计算技巧**。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。

---

## 目录

- [2.1 基础数据结构](#21-基础数据结构)
  - [标量、向量、矩阵与张量](#标量向量矩阵与张量)
  - [核心运算](#核心运算)
  - [转置、逆矩阵与伪逆](#转置逆矩阵与伪逆)
  - [迹与行列式](#迹与行列式)
- [2.2 向量空间：理解数据的结构](#22-向量空间理解数据的结构)
  - [线性组合与张成空间](#线性组合与张成空间)
  - [线性相关与线性无关](#线性相关与线性无关)
  - [基与维度](#基与维度)
  - [秩：矩阵的本质维度](#秩矩阵的本质维度)
  - [秩-零化度定理](#秩-零化度定理)
  - [四个基本子空间](#四个基本子空间)
- [2.3 度量与正交](#23-度量与正交)
  - [范数：测量大小](#范数测量大小)
  - [内积与角度](#内积与角度)
  - [正交与正交矩阵](#正交与正交矩阵)
- [2.4 投影：线性回归的几何本质](#24-投影线性回归的几何本质)
  - [为什么我们需要投影](#为什么我们需要投影)
  - [从最小二乘推导投影矩阵](#从最小二乘推导投影矩阵)
  - [投影矩阵的性质](#投影矩阵的性质)
  - [几何直觉](#几何直觉)
- [2.5 矩阵微积分：反向传播的数学基础](#25-矩阵微积分反向传播的数学基础)
  - [为什么需要矩阵求导](#为什么需要矩阵求导)
  - [布局约定：分母布局](#布局约定分母布局)
  - [标量对向量求导](#标量对向量求导)
  - [标量对矩阵求导](#标量对矩阵求导)
  - [向量对向量求导：雅可比矩阵](#向量对向量求导雅可比矩阵)
  - [链式法则](#链式法则)
  - [重要公式推导](#重要公式推导)
  - [实战：线性回归的梯度](#实战线性回归的梯度)

---

## 2.1 基础数据结构

### 标量、向量、矩阵与张量

数学对象的定义由其维度决定：

- **标量 (Scalar)**：$x \in \mathbb{R}$。单个数值，如温度、距离。

- **向量 (Vector)**：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。**本书默认向量为列向量**。

  例如，$\mathbf{x} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$ 表示2D平面上的一个点。

- **矩阵 (Matrix)**：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的**线性变换**。

  $\mathbf{A}_{ij}$ 表示矩阵第 $i$ 行第 $j$ 列的元素。

- **张量 (Tensor)**：$\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_k}$。多维数组。

  例如，图像是三阶张量 $\mathbb{R}^{H \times W \times C}$（高度、宽度、通道数）。

**符号约定**：
- 标量：$x, y, \lambda$
- 向量：$\mathbf{x}, \mathbf{y}, \mathbf{v}$
- 矩阵：$\mathbf{A}, \mathbf{X}, \mathbf{W}$
- 张量：$\mathcal{X}, \mathcal{Y}$

---

### 核心运算

#### 1. 矩阵乘法

对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{B} \in \mathbb{R}^{n \times p}$，其乘积 $\mathbf{C} = \mathbf{A}\mathbf{B} \in \mathbb{R}^{m \times p}$：

$$
\mathbf{C}_{ij} = \sum_{k=1}^n \mathbf{A}_{ik} \mathbf{B}_{kj}
$$

**几何意义**：矩阵乘法 = 线性变换的复合。$\mathbf{AB}\mathbf{x}$ 表示先对 $\mathbf{x}$ 应用 $\mathbf{B}$ 变换，再应用 $\mathbf{A}$ 变换。

**性质**：
- 满足结合律：$(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})$
- **不满足交换律**：$\mathbf{AB} \neq \mathbf{BA}$（这很重要！）

#### 2. 内积（点积）

对于向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$：

$$
\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T \mathbf{y} = \sum_{i=1}^n x_i y_i
$$

**几何意义**：衡量两个向量的相关性。如果夹角小，内积大；如果垂直，内积为0。

#### 3. 外积

对于 $\mathbf{x} \in \mathbb{R}^m$ 和 $\mathbf{y} \in \mathbb{R}^n$，外积 $\mathbf{xy}^T \in \mathbb{R}^{m \times n}$：

$$
(\mathbf{xy}^T)_{ij} = x_i y_j
$$

**几何意义**：生成一个秩为1的矩阵。这在SVD和推荐系统中很有用。

#### 4. 逐元素运算（Hadamard积）

对于相同形状的矩阵 $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$：

$$
(\mathbf{A} \odot \mathbf{B})_{ij} = \mathbf{A}_{ij} \mathbf{B}_{ij}
$$

**应用**：神经网络中的激活函数、dropout等都是逐元素操作。

---

### 转置、逆矩阵与伪逆

#### 转置

矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 的转置 $\mathbf{A}^T \in \mathbb{R}^{n \times m}$：

$$
(\mathbf{A}^T)_{ij} = \mathbf{A}_{ji}
$$

**性质**：
- $(\mathbf{A}^T)^T = \mathbf{A}$
- $(\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T$（注意顺序反转）

#### 逆矩阵

对于方阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$，如果存在 $\mathbf{A}^{-1}$ 使得：

$$
\mathbf{A} \mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A} = \mathbf{I}
$$

则称 $\mathbf{A}$ 可逆。

**性质**：
- $(\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$（注意顺序反转）
- $(\mathbf{A}^T)^{-1} = (\mathbf{A}^{-1})^T$

**实践警告**：在数值计算中，永远不要显式计算逆矩阵！使用 `np.linalg.solve(A, b)` 而不是 `np.dot(np.linalg.inv(A), b)`。前者更快、更稳定。

#### 伪逆（Moore-Penrose逆）

对于非方阵或奇异矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$，其伪逆 $\mathbf{A}^+ \in \mathbb{R}^{n \times m}$ 满足：

$$
\mathbf{A}^+ = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \quad \text{(当 } \mathbf{A} \text{ 列满秩)}
$$

**应用**：求解超定或欠定的线性系统，线性回归中处理奇异设计矩阵。

---

### 迹与行列式

#### 迹

矩阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$ 的迹是对角线元素之和：

$$
\text{tr}(\mathbf{A}) = \sum_{i=1}^n \mathbf{A}_{ii}
$$

**性质**（这些在求导时非常有用）：
1. $\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})$
2. $\text{tr}(c\mathbf{A}) = c \cdot \text{tr}(\mathbf{A})$
3. **循环性**：$\text{tr}(\mathbf{ABC}) = \text{tr}(\mathbf{BCA}) = \text{tr}(\mathbf{CAB})$
4. $\text{tr}(\mathbf{A}^T) = \text{tr}(\mathbf{A})$
5. $\text{tr}(\mathbf{AB}) = \text{tr}(\mathbf{BA})$（即使 $\mathbf{AB} \neq \mathbf{BA}$）

**技巧**：利用迹的循环性，$\mathbf{x}^T \mathbf{A} \mathbf{x} = \text{tr}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = \text{tr}(\mathbf{x} \mathbf{x}^T \mathbf{A}) = \text{tr}(\mathbf{A} \mathbf{x} \mathbf{x}^T)$。这在矩阵求导时很有用。

#### 行列式

行列式 $\det(\mathbf{A})$ 或 $|\mathbf{A}|$ 是方阵的一个标量值，几何上代表**体积缩放因子**。

**性质**：
- $\det(\mathbf{AB}) = \det(\mathbf{A}) \det(\mathbf{B})$
- $\det(\mathbf{A}^T) = \det(\mathbf{A})$
- $\det(\mathbf{A}^{-1}) = 1/\det(\mathbf{A})$
- $\mathbf{A}$ 可逆 $\Leftrightarrow$ $\det(\mathbf{A}) \neq 0$

**应用**：判断矩阵是否可逆，计算体积变换。

---

## 2.2 向量空间：理解数据的结构

### 线性组合与张成空间

给定向量 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \in \mathbb{R}^m$，它们的**线性组合**是：

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n, \quad c_i \in \mathbb{R}
$$

这些向量的**张成空间** $\text{span}(\mathbf{v}_1, \ldots, \mathbf{v}_n)$ 是所有可能的线性组合的集合。

**直觉**：
- 一个非零向量 $\mathbf{v}$ 张成一条直线
- 两个不共线的向量张成一个平面
- 三个不共面的向量张成整个3D空间

**机器学习中的例子**：在线性回归中，$\mathbf{X}\mathbf{w}$ 是设计矩阵 $\mathbf{X}$ 的列向量的线性组合，结果必然在 $\mathbf{X}$ 的列空间中。如果真实标签 $\mathbf{y}$ 不在这个空间里，我们只能找到最近的点（投影）。

---

### 线性相关与线性无关

向量组 $\{\mathbf{v}_1, \ldots, \mathbf{v}_n\}$ **线性相关**，如果存在不全为零的系数 $c_1, \ldots, c_n$ 使得：

$$
c_1 \mathbf{v}_1 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
$$

否则称为**线性无关**。

**直觉**：线性相关 = 有冗余信息。线性无关 = 每个向量都提供新的方向。

**判断方法**：
- 将向量排成矩阵 $\mathbf{A} = [\mathbf{v}_1 \; \mathbf{v}_2 \; \cdots \; \mathbf{v}_n]$
- 如果 $\text{rank}(\mathbf{A}) = n$，则线性无关
- 如果 $\text{rank}(\mathbf{A}) < n$，则线性相关

---

### 基与维度

向量空间 $V$ 的一组**基**是满足以下条件的向量集：
1. 它们线性无关
2. 它们张成整个空间 $V$

**维度** $\dim(V)$ = 基中向量的个数。

**标准基**：$\mathbb{R}^n$ 的标准基是 $\{\mathbf{e}_1, \ldots, \mathbf{e}_n\}$，其中 $\mathbf{e}_i$ 是第 $i$ 个分量为1、其余为0的向量。

**例子**：
- $\mathbb{R}^2$ 的标准基：$\left\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\right\}$
- 另一组基：$\left\{\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ -1 \end{bmatrix}\right\}$

**重要性质**：同一空间的所有基都有相同的向量个数（这就是为什么维度是良定义的）。

---

### 秩：矩阵的本质维度

矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 的**秩** $\text{rank}(\mathbf{A})$ 定义为：

$$
\text{rank}(\mathbf{A}) = \dim(\text{列空间}) = \dim(\text{行空间})
$$

**等价定义**：
- 列空间的维度 = 线性无关的列向量的最大个数
- 行空间的维度 = 线性无关的行向量的最大个数
- 最大的非零奇异值的个数（通过SVD）

**性质**：
- $\text{rank}(\mathbf{A}) \leq \min(m, n)$
- $\text{rank}(\mathbf{AB}) \leq \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))$
- $\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A}^T)$
- $\text{rank}(\mathbf{A}^T \mathbf{A}) = \text{rank}(\mathbf{A})$

**直觉**：秩表示矩阵变换后空间的"真实维度"。如果 $\mathbf{A} \in \mathbb{R}^{100 \times 50}$ 的秩只有10，说明虽然有50个特征，但本质上只有10个独立的方向。

**机器学习中的意义**：
- **低秩假设**：许多真实数据矩阵（如用户-物品评分矩阵）的秩远小于其维度，这是矩阵分解方法的基础
- **正则化**：当设计矩阵不满秩时，$\mathbf{X}^T\mathbf{X}$ 不可逆，需要正则化（Ridge回归）

---

### 秩-零化度定理

对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$：

$$
\text{rank}(\mathbf{A}) + \dim(\text{零空间}) = n
$$

其中零空间（核空间）$\text{null}(\mathbf{A}) = \{\mathbf{x} : \mathbf{A}\mathbf{x} = \mathbf{0}\}$。

**直觉**：
- $n$ 个输入维度
- $\text{rank}(\mathbf{A})$ 个维度被保留（输出空间）
- $n - \text{rank}(\mathbf{A})$ 个维度被压缩到零（丢失信息）

**例子**：如果 $\mathbf{A} \in \mathbb{R}^{3 \times 5}$ 的秩为2，则：
- 列空间是 $\mathbb{R}^3$ 中的2D子空间（一个平面）
- 零空间是 $\mathbb{R}^5$ 中的3D子空间
- $2 + 3 = 5$ ✓

---

### 四个基本子空间

对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$，有四个重要的子空间：

| 子空间 | 定义 | 维度 | 所在空间 |
|--------|------|------|----------|
| 列空间 $C(\mathbf{A})$ | $\mathbf{A}$ 的列向量张成的空间 | $r$ | $\mathbb{R}^m$ |
| 零空间 $N(\mathbf{A})$ | $\{\mathbf{x} : \mathbf{A}\mathbf{x} = \mathbf{0}\}$ | $n - r$ | $\mathbb{R}^n$ |
| 行空间 $C(\mathbf{A}^T)$ | $\mathbf{A}^T$ 的列空间 | $r$ | $\mathbb{R}^n$ |
| 左零空间 $N(\mathbf{A}^T)$ | $\{\mathbf{y} : \mathbf{A}^T\mathbf{y} = \mathbf{0}\}$ | $m - r$ | $\mathbb{R}^m$ |

其中 $r = \text{rank}(\mathbf{A})$。

**正交关系**：
- 行空间 $\perp$ 零空间（在 $\mathbb{R}^n$ 中）
- 列空间 $\perp$ 左零空间（在 $\mathbb{R}^m$ 中）

**图示**：

```
输入空间 ℝⁿ              输出空间 ℝᵐ
┌─────────────┐         ┌─────────────┐
│  行空间 (r) │ ──A──> │  列空间 (r) │
│      ⊕      │         │      ⊕      │
│ 零空间(n-r) │ ──0──> │左零空间(m-r)│
└─────────────┘         └─────────────┘
```

**机器学习中的应用**：
- **列空间**：$\mathbf{Ax} = \mathbf{b}$ 有解 $\Leftrightarrow$ $\mathbf{b} \in C(\mathbf{A})$
- **零空间**：神经网络中，如果两个不同的输入 $\mathbf{x}_1, \mathbf{x}_2$ 产生相同的输出，则 $\mathbf{x}_1 - \mathbf{x}_2 \in N(\mathbf{A})$
- **SVD**：将矩阵分解为这四个子空间的正交基的组合（第3章）

---

## 2.3 度量与正交

### 范数：测量大小

**范数**是向量"长度"的推广，满足：
1. 非负性：$\|\mathbf{x}\| \geq 0$，且 $\|\mathbf{x}\| = 0 \Leftrightarrow \mathbf{x} = \mathbf{0}$
2. 齐次性：$\|c\mathbf{x}\| = |c| \|\mathbf{x}\|$
3. 三角不等式：$\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|$

#### $L^p$ 范数

$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}
$$

**常用范数**：

1. **$L^1$ 范数**（曼哈顿距离）：
   $$
   \|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|
   $$
   **应用**：Lasso回归，促进稀疏解（许多系数为0）

2. **$L^2$ 范数**（欧几里得距离）：
   $$
   \|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{\mathbf{x}^T \mathbf{x}}
   $$
   **应用**：最常用，Ridge回归，神经网络权重衰减

3. **$L^\infty$ 范数**（最大范数）：
   $$
   \|\mathbf{x}\|_\infty = \max_i |x_i|
   $$
   **应用**：对抗攻击中的扰动约束

**为什么 $L^1$ 产生稀疏解？**

考虑约束优化 $\min \|\mathbf{x}\|_p$ subject to $\mathbf{a}^T \mathbf{x} = c$。
- $L^1$ 球是菱形，有尖角 → 最优解倾向于在坐标轴上（某些分量为0）
- $L^2$ 球是圆形，光滑 → 最优解一般不在坐标轴上

#### 矩阵范数

**Frobenius范数**（最常用）：
$$
\|\mathbf{A}\|_F = \sqrt{\sum_{i,j} \mathbf{A}_{ij}^2} = \sqrt{\text{tr}(\mathbf{A}^T \mathbf{A})}
$$

**谱范数**（最大奇异值）：
$$
\|\mathbf{A}\|_2 = \sigma_{\max}(\mathbf{A})
$$

---

### 内积与角度

内积 $\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T \mathbf{y}$ 与角度的关系：

$$
\mathbf{x}^T \mathbf{y} = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2 \cos \theta
$$

其中 $\theta$ 是 $\mathbf{x}$ 和 $\mathbf{y}$ 的夹角。

**余弦相似度**：
$$
\cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2}
$$

- $\cos \theta = 1$：同向
- $\cos \theta = 0$：正交
- $\cos \theta = -1$：反向

**应用**：文本相似度（TF-IDF向量）、推荐系统（用户/物品嵌入）。

---

### 正交与正交矩阵

向量 $\mathbf{x}$ 和 $\mathbf{y}$ **正交**，记作 $\mathbf{x} \perp \mathbf{y}$，如果：

$$
\mathbf{x}^T \mathbf{y} = 0
$$

**正交矩阵** $\mathbf{Q} \in \mathbb{R}^{n \times n}$ 满足：

$$
\mathbf{Q}^T \mathbf{Q} = \mathbf{Q} \mathbf{Q}^T = \mathbf{I}
$$

即 $\mathbf{Q}^{-1} = \mathbf{Q}^T$。

**性质**：
- 正交矩阵的列（行）向量构成标准正交基
- 正交变换**保持长度和角度**：$\|\mathbf{Qx}\|_2 = \|\mathbf{x}\|_2$
- $\det(\mathbf{Q}) = \pm 1$

**为什么正交矩阵很好？**
1. 数值稳定：条件数为1，不会放大误差
2. 计算高效：求逆只需转置
3. 几何清晰：旋转和反射，不改变形状

**应用**：PCA、SVD、QR分解、Gram-Schmidt正交化。

---

## 2.4 投影：线性回归的几何本质

### 为什么我们需要投影

**问题**：给定矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和向量 $\mathbf{b} \in \mathbb{R}^m$，求解 $\mathbf{Ax} = \mathbf{b}$。

**现实**：通常 $\mathbf{b}$ 不在 $\mathbf{A}$ 的列空间 $C(\mathbf{A})$ 中，方程无解！

**例子（线性回归）**：
- $\mathbf{A}$：设计矩阵（样本 × 特征）
- $\mathbf{x}$：权重
- $\mathbf{b}$：真实标签
- 数据有噪声 → $\mathbf{b} \notin C(\mathbf{A})$

**解决方案**：找到 $C(\mathbf{A})$ 中最接近 $\mathbf{b}$ 的点 $\mathbf{p}$，即 $\mathbf{b}$ 在 $C(\mathbf{A})$ 上的**正交投影**。

---

### 从最小二乘推导投影矩阵

**目标**：最小化残差的平方和

$$
\min_{\mathbf{x}} \|\mathbf{b} - \mathbf{Ax}\|_2^2
$$

**推导**：

设投影为 $\mathbf{p} = \mathbf{A}\hat{\mathbf{x}}$，残差为 $\mathbf{e} = \mathbf{b} - \mathbf{p} = \mathbf{b} - \mathbf{A}\hat{\mathbf{x}}$。

**关键几何直觉**：最优解时，残差 $\mathbf{e}$ 必须垂直于列空间 $C(\mathbf{A})$，即：

$$
\mathbf{e} \perp C(\mathbf{A}) \quad \Rightarrow \quad \mathbf{A}^T \mathbf{e} = \mathbf{0}
$$

因为 $\mathbf{A}$ 的每一列都在 $C(\mathbf{A})$ 中。

代入 $\mathbf{e} = \mathbf{b} - \mathbf{A}\hat{\mathbf{x}}$：

$$
\mathbf{A}^T (\mathbf{b} - \mathbf{A}\hat{\mathbf{x}}) = \mathbf{0}
$$

展开得到**正规方程**：

$$
\boxed{\mathbf{A}^T \mathbf{A} \hat{\mathbf{x}} = \mathbf{A}^T \mathbf{b}}
$$

假设 $\mathbf{A}$ 列满秩（即 $\mathbf{A}^T \mathbf{A}$ 可逆），则：

$$
\hat{\mathbf{x}} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
$$

投影 $\mathbf{p} = \mathbf{A}\hat{\mathbf{x}}$：

$$
\mathbf{p} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
$$

定义**投影矩阵**：

$$
\boxed{\mathbf{P} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T}
$$

则 $\mathbf{p} = \mathbf{P}\mathbf{b}$。

---

### 投影矩阵的性质

1. **对称性**：$\mathbf{P}^T = \mathbf{P}$

   证明：$\mathbf{P}^T = [\mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T]^T = \mathbf{A}[(\mathbf{A}^T \mathbf{A})^{-1}]^T \mathbf{A}^T = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T = \mathbf{P}$

2. **幂等性**：$\mathbf{P}^2 = \mathbf{P}$

   证明：
   $$
   \begin{align}
   \mathbf{P}^2 &= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \cdot \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \\
   &= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} (\mathbf{A}^T \mathbf{A}) (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \\
   &= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T = \mathbf{P}
   \end{align}
   $$

   **直觉**：投影两次 = 投影一次（已经在子空间里了）

3. **秩**：$\text{rank}(\mathbf{P}) = \text{rank}(\mathbf{A})$

4. **残差投影矩阵**：$\mathbf{I} - \mathbf{P}$ 将向量投影到 $C(\mathbf{A})$ 的正交补空间

   验证：$(\mathbf{I} - \mathbf{P})^2 = \mathbf{I} - 2\mathbf{P} + \mathbf{P}^2 = \mathbf{I} - \mathbf{P}$ ✓

---

### 几何直觉

**2D例子**：投影到直线

设 $\mathbf{A} = \mathbf{a}$ 是单位向量，则：

$$
\mathbf{P} = \mathbf{a}(\mathbf{a}^T \mathbf{a})^{-1} \mathbf{a}^T = \mathbf{a}\mathbf{a}^T
$$

对于任意 $\mathbf{b}$：

$$
\mathbf{p} = (\mathbf{a}\mathbf{a}^T) \mathbf{b} = \mathbf{a}(\mathbf{a}^T \mathbf{b}) = (\mathbf{a}^T \mathbf{b}) \mathbf{a}
$$

这就是向量 $\mathbf{b}$ 在 $\mathbf{a}$ 方向上的分量！

**图示**：

```
      b
      │╲
      │ ╲ e = b - p
      │  ╲
      │   ╲
    p │────┴─→ a
      │
      └────────
```

**线性回归的几何图像**：

- $\mathbf{y} \in \mathbb{R}^m$：真实标签（$m$ 个样本）
- $C(\mathbf{X})$：特征空间张成的子空间（维度 $< m$）
- $\hat{\mathbf{y}} = \mathbf{X}\hat{\mathbf{w}}$：预测值（投影）
- $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$：残差（垂直于 $C(\mathbf{X})$）

最小二乘法 = 找到 $C(\mathbf{X})$ 中最接近 $\mathbf{y}$ 的点。

---

## 2.5 矩阵微积分：反向传播的数学基础

### 为什么需要矩阵求导

**梯度下降**的核心是计算损失函数关于参数的导数：

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_{\mathbf{w}} L(\mathbf{w}_t)
$$

在深度学习中：
- 参数是矩阵或向量（权重、偏置）
- 需要计算 $\frac{\partial L}{\partial \mathbf{W}}$、$\frac{\partial L}{\partial \mathbf{b}}$ 等

**传统微积分不够用**：
- 标量对标量求导：$\frac{dy}{dx}$
- **我们需要**：向量对向量、矩阵对矩阵的求导

---

### 布局约定：分母布局

矩阵求导有两种布局约定，我们采用**分母布局**（Denominator Layout），这是深度学习中的主流。

| 分子维度 | 分母维度 | 结果形状 | 例子 |
|----------|----------|----------|------|
| 标量 | 向量 $\mathbf{x} \in \mathbb{R}^n$ | $n \times 1$ 列向量 | $\nabla_{\mathbf{x}} f$ |
| 标量 | 矩阵 $\mathbf{X} \in \mathbb{R}^{m \times n}$ | $m \times n$ 矩阵 | $\frac{\partial f}{\partial \mathbf{X}}$ |
| 向量 $\mathbf{f} \in \mathbb{R}^m$ | 向量 $\mathbf{x} \in \mathbb{R}^n$ | $n \times m$ 矩阵 | 雅可比矩阵 $\mathbf{J}$ |

**记忆技巧**：结果的形状是"分母在前"。

---

### 标量对向量求导

设 $f: \mathbb{R}^n \to \mathbb{R}$，则梯度：

$$
\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} \in \mathbb{R}^n
$$

**常用公式**：

1. **线性**：$f(\mathbf{x}) = \mathbf{a}^T \mathbf{x}$
   $$
   \nabla_{\mathbf{x}} (\mathbf{a}^T \mathbf{x}) = \mathbf{a}
   $$

2. **二次型**：$f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}$
   $$
   \nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}
   $$

   特别地，若 $\mathbf{A}$ 对称：
   $$
   \nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}
   $$

3. **范数平方**：$f(\mathbf{x}) = \|\mathbf{x}\|_2^2 = \mathbf{x}^T \mathbf{x}$
   $$
   \nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}
   $$

---

### 标量对矩阵求导

设 $f: \mathbb{R}^{m \times n} \to \mathbb{R}$，则：

$$
\frac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \frac{\partial f}{\partial \mathbf{X}_{11}} & \cdots & \frac{\partial f}{\partial \mathbf{X}_{1n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial f}{\partial \mathbf{X}_{m1}} & \cdots & \frac{\partial f}{\partial \mathbf{X}_{mn}} \end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**常用公式**：

1. **线性**：$f(\mathbf{X}) = \text{tr}(\mathbf{A}^T \mathbf{X})$
   $$
   \frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A}^T \mathbf{X}) = \mathbf{A}
   $$

2. **二次型**：$f(\mathbf{X}) = \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$
   $$
   \frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{X}
   $$

3. **Frobenius范数平方**：$f(\mathbf{X}) = \|\mathbf{X}\|_F^2 = \text{tr}(\mathbf{X}^T \mathbf{X})$
   $$
   \frac{\partial}{\partial \mathbf{X}} \|\mathbf{X}\|_F^2 = 2\mathbf{X}
   $$

---

### 向量对向量求导：雅可比矩阵

设 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，雅可比矩阵：

$$
\mathbf{J} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} \in \mathbb{R}^{m \times n}
$$

**注意**：分母布局下，第 $i$ 行是 $f_i$ 对 $\mathbf{x}$ 的梯度的转置。

**例子**：线性变换 $\mathbf{f}(\mathbf{x}) = \mathbf{A}\mathbf{x}$
$$
\frac{\partial (\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}^T
$$

等等，为什么是 $\mathbf{A}^T$ 而不是 $\mathbf{A}$？因为分母布局！后面会详细解释。

---

### 链式法则

**标量链**：设 $y = f(u)$，$u = g(x)$，则：
$$
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
$$

**向量链**：设 $y = f(\mathbf{u})$，$\mathbf{u} = g(\mathbf{x})$，则：
$$
\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial y}{\partial \mathbf{u}} \frac{\partial \mathbf{u}}{\partial \mathbf{x}}
$$

这里 $\frac{\partial y}{\partial \mathbf{u}} \in \mathbb{R}^{1 \times m}$（行向量），$\frac{\partial \mathbf{u}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$（雅可比）。

**反向传播本质**：链式法则的递归应用。

---

### 重要公式推导

#### 推导1：$\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x})$

**方法1：直接展开**

$$
f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x} = \sum_{i=1}^n \sum_{j=1}^n x_i \mathbf{A}_{ij} x_j
$$

对 $x_k$ 求偏导：
$$
\frac{\partial f}{\partial x_k} = \sum_{j=1}^n \mathbf{A}_{kj} x_j + \sum_{i=1}^n x_i \mathbf{A}_{ik}
$$

第一项来自 $i=k$ 的项，第二项来自 $j=k$ 的项。

用矩阵形式：
$$
\nabla_{\mathbf{x}} f = \mathbf{A}\mathbf{x} + \mathbf{A}^T \mathbf{x} = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}
$$

**方法2：微分法**（更优雅）

计算微分 $df$：
$$
\begin{align}
df &= d(\mathbf{x}^T \mathbf{A} \mathbf{x}) \\
&= (d\mathbf{x}^T) \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{A} (d\mathbf{x}) \\
&= (d\mathbf{x})^T \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{A}^T (d\mathbf{x})^T \quad \text{(利用标量转置)} \\
&= (d\mathbf{x})^T (\mathbf{A}\mathbf{x} + \mathbf{A}^T \mathbf{x}) \\
&= (d\mathbf{x})^T (\mathbf{A} + \mathbf{A}^T) \mathbf{x}
\end{align}
$$

由 $df = \nabla_{\mathbf{x}} f \cdot d\mathbf{x} = (\nabla_{\mathbf{x}} f)^T d\mathbf{x}$，得：
$$
\boxed{\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}}
$$

#### 推导2：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})$

**技巧**：利用迹的循环性和线性

$$
f(\mathbf{X}) = \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})
$$

计算微分：
$$
\begin{align}
df &= d[\text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})] \\
&= \text{tr}(\mathbf{A} \, d\mathbf{X} \, \mathbf{B}) \quad \text{(迹的线性性)} \\
&= \text{tr}(\mathbf{B} \mathbf{A} \, d\mathbf{X}) \quad \text{(迹的循环性)}
\end{align}
$$

已知 $\text{tr}(\mathbf{C}^T d\mathbf{X}) = \langle \mathbf{C}, d\mathbf{X} \rangle$，因此：
$$
df = \text{tr}[(\mathbf{B}\mathbf{A})^T d\mathbf{X}] = \text{tr}[(\mathbf{A}^T \mathbf{B}^T) d\mathbf{X}]
$$

所以：
$$
\boxed{\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B}) = \mathbf{A}^T \mathbf{B}^T}
$$

**特殊情况**：
- $\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}) = \mathbf{I}$
- $\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A}\mathbf{X}) = \mathbf{A}^T$
- $\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}\mathbf{A}) = \mathbf{A}^T$

#### 推导3：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$

$$
\begin{align}
df &= d[\text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})] \\
&= \text{tr}(d\mathbf{X}^T \mathbf{A} \mathbf{X}) + \text{tr}(\mathbf{X}^T \mathbf{A} \, d\mathbf{X}) \\
&= \text{tr}(\mathbf{X}^T \mathbf{A}^T d\mathbf{X}) + \text{tr}(\mathbf{X}^T \mathbf{A} \, d\mathbf{X}) \quad \text{(利用 } \text{tr}(\mathbf{A}^T) = \text{tr}(\mathbf{A})\text{)} \\
&= \text{tr}[(\mathbf{A}^T \mathbf{X})^T d\mathbf{X}] + \text{tr}[(\mathbf{A}\mathbf{X})^T d\mathbf{X}] \\
&= \text{tr}[(\mathbf{A}^T \mathbf{X} + \mathbf{A}\mathbf{X})^T d\mathbf{X}]
\end{align}
$$

因此：
$$
\boxed{\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{X}}
$$

---

### 实战：线性回归的梯度

**问题**：最小化平方损失

$$
L(\mathbf{w}) = \frac{1}{2} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2
$$

其中 $\mathbf{X} \in \mathbb{R}^{m \times n}$，$\mathbf{y} \in \mathbb{R}^m$，$\mathbf{w} \in \mathbb{R}^n$。

**目标**：计算 $\nabla_{\mathbf{w}} L$。

**推导**：

$$
\begin{align}
L(\mathbf{w}) &= \frac{1}{2} (\mathbf{y} - \mathbf{X}\mathbf{w})^T (\mathbf{y} - \mathbf{X}\mathbf{w}) \\
&= \frac{1}{2} (\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X}\mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w})
\end{align}
$$

注意 $\mathbf{y}^T \mathbf{X}\mathbf{w}$ 是标量，所以等于其转置：
$$
\mathbf{y}^T \mathbf{X}\mathbf{w} = \mathbf{w}^T \mathbf{X}^T \mathbf{y}
$$

因此：
$$
L(\mathbf{w}) = \frac{1}{2} \mathbf{y}^T \mathbf{y} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \frac{1}{2} \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}
$$

逐项求导：

1. $\nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y}) = \mathbf{0}$（不含 $\mathbf{w}$）

2. $\nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{y}) = \mathbf{X}^T \mathbf{y}$（线性项）

3. $\nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) = 2\mathbf{X}^T \mathbf{X} \mathbf{w}$（二次型，$\mathbf{X}^T \mathbf{X}$ 对称）

综合：
$$
\boxed{\nabla_{\mathbf{w}} L = -\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})}
$$

令梯度为零：
$$
\mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T \mathbf{y}
$$

这正是**正规方程**！解得：
$$
\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

**连接投影**：注意到 $\hat{\mathbf{y}} = \mathbf{X}\hat{\mathbf{w}} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{P}\mathbf{y}$，正是投影矩阵！

---

## 常用矩阵求导公式速查表

| 函数 $f(\mathbf{x})$ | 梯度 $\nabla_{\mathbf{x}} f$ | 备注 |
|---------------------|------------------------------|------|
| $\mathbf{a}^T \mathbf{x}$ | $\mathbf{a}$ | 线性 |
| $\mathbf{x}^T \mathbf{a}$ | $\mathbf{a}$ | 同上 |
| $\mathbf{x}^T \mathbf{x}$ | $2\mathbf{x}$ | 范数平方 |
| $\mathbf{x}^T \mathbf{A} \mathbf{x}$ | $(\mathbf{A} + \mathbf{A}^T)\mathbf{x}$ | 二次型 |
| $\mathbf{x}^T \mathbf{A} \mathbf{x}$ ($\mathbf{A}$ 对称) | $2\mathbf{A}\mathbf{x}$ | 对称二次型 |
| $\|\mathbf{Ax} - \mathbf{b}\|_2^2$ | $2\mathbf{A}^T(\mathbf{Ax} - \mathbf{b})$ | 最小二乘 |
| $\log \det(\mathbf{X})$ | $\mathbf{X}^{-T}$ | 对数行列式 |

| 函数 $f(\mathbf{X})$ | 梯度 $\frac{\partial f}{\partial \mathbf{X}}$ | 备注 |
|---------------------|----------------------------------------------|------|
| $\text{tr}(\mathbf{X})$ | $\mathbf{I}$ | 迹 |
| $\text{tr}(\mathbf{AX})$ | $\mathbf{A}^T$ | 线性 |
| $\text{tr}(\mathbf{AXB})$ | $\mathbf{A}^T \mathbf{B}^T$ | 三矩阵积 |
| $\text{tr}(\mathbf{X}^T \mathbf{A})$ | $\mathbf{A}$ | 转置迹 |
| $\text{tr}(\mathbf{X}^T \mathbf{AX})$ | $(\mathbf{A} + \mathbf{A}^T)\mathbf{X}$ | 二次型 |
| $\|\mathbf{X}\|_F^2$ | $2\mathbf{X}$ | Frobenius范数 |

---

## 小结

本章构建了机器学习所需的线性代数和微积分工具箱：

**2.1 基础数据结构**
- 矩阵、向量是数据和变换的表示
- 迹和行列式的循环性质在求导中很有用

**2.2 向量空间**
- **秩**：矩阵的本质维度，决定了信息保留量
- **秩-零化度定理**：$\text{rank}(\mathbf{A}) + \dim(\text{null}(\mathbf{A})) = n$
- **四个基本子空间**：列空间、零空间、行空间、左零空间

**2.3 度量与正交**
- $L^1$ 范数产生稀疏解，$L^2$ 范数产生光滑解
- 正交矩阵保持长度和角度，数值稳定

**2.4 投影**
- **核心思想**：数据通常不在我们想要的子空间里，投影找到最接近的点
- **投影矩阵**：$\mathbf{P} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T$
- **线性回归 = 几何投影**：$\hat{\mathbf{y}} = \mathbf{P}\mathbf{y}$

**2.5 矩阵微积分**
- **分母布局**：结果形状是"分母在前"
- **核心技巧**：微分法 + 迹的循环性
- **反向传播 = 链式法则**

**下一章预告**：第3章将深入矩阵分解（特征值、SVD、Cholesky），这是PCA、推荐系统、线性判别分析的基础。

---

**练习题**

1. 证明投影矩阵的幂等性：$\mathbf{P}^2 = \mathbf{P}$。

2. 设 $\mathbf{A} \in \mathbb{R}^{3 \times 5}$ 的秩为2，求零空间的维度。

3. 计算 $\nabla_{\mathbf{w}} \frac{1}{2}\mathbf{w}^T \mathbf{w} + \lambda \|\mathbf{w}\|_1$（提示：$L^1$ 项不可导，考虑次梯度）。

4. 为什么在实践中用 `np.linalg.solve(A, b)` 而不是 `np.dot(np.linalg.inv(A), b)`？

5. 推导逻辑回归损失函数 $L(\mathbf{w}) = -\sum_{i=1}^m [y_i \log \sigma(\mathbf{w}^T \mathbf{x}_i) + (1-y_i)\log(1-\sigma(\mathbf{w}^T \mathbf{x}_i))]$ 的梯度，其中 $\sigma(z) = 1/(1+e^{-z})$。
