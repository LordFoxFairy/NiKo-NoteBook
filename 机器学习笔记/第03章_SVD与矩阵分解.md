# 第03章：SVD与矩阵分解

> **核心思想**：任何矩阵都可以看作"旋转-拉伸-旋转"的组合。SVD 是线性代数的终极武器。

---

## 前言

如果说线性代数有皇冠,那么**奇异值分解 (SVD)** 就是皇冠上的明珠。Gilbert Strang 教授称其为"线性代数的顶峰"。

在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的**万能钥匙**。

本章我们将从**几何变换**的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为**旋转、拉伸、再旋转**。

---

## 目录

1. [引言:从圆到椭圆](#1-引言从圆到椭圆)
   - 1.1 [矩阵变换的本质](#11-矩阵变换的本质)
   - 1.2 [特征值分解的局限](#12-特征值分解的局限)

2. [特征分解(EVD):对称矩阵的美学](#2-特征分解evd对称矩阵的美学)
   - 2.1 [谱定理(Spectral Theorem)](#21-谱定理spectral-theorem)
   - 2.2 [几何直觉](#22-几何直觉)
   - 2.3 [正定性:碗的形状](#23-正定性碗的形状)

3. [奇异值分解(SVD):万能钥匙](#3-奇异值分解svd万能钥匙)
   - 3.1 [核心思想:让非方阵也能对角化](#31-核心思想让非方阵也能对角化)
   - 3.2 [推导 SVD](#32-推导-svd)
   - 3.3 [SVD 的几何图景:旋转-拉伸-旋转](#33-svd-的几何图景旋转-拉伸-旋转)
   - 3.4 [薄 SVD(Reduced SVD)](#34-薄-svdreduced-svd)
   - 3.5 [外积形式(Dyadic Expansion)](#35-外积形式dyadic-expansion)

4. [四个基本子空间的 SVD 视角](#4-四个基本子空间的-svd-视角)
   - 4.1 [回顾:四个基本子空间](#41-回顾四个基本子空间)
   - 4.2 [SVD 的完美切分](#42-svd-的完美切分)
   - 4.3 [正交关系图](#43-正交关系图)
   - 4.4 [伪逆的几何意义](#44-伪逆的几何意义)

5. [低秩近似:SVD 的杀手级应用](#5-低秩近似svd-的杀手级应用)
   - 5.1 [问题设定](#51-问题设定)
   - 5.2 [Eckart-Young-Mirsky 定理](#52-eckart-young-mirsky-定理)
   - 5.3 [直觉:丢弃小奇异值 = 去噪](#53-直觉丢弃小奇异值--去噪)
   - 5.4 [应用 1:图像压缩](#54-应用-1图像压缩)
   - 5.5 [应用 2:推荐系统与矩阵补全](#55-应用-2推荐系统与矩阵补全)
   - 5.6 [应用 3:主成分分析(PCA)](#56-应用-3主成分分析pca)

6. [SVD 与 EVD 的联系](#6-svd-与-evd-的联系)
   - 6.1 [核心关系](#61-核心关系)
   - 6.2 [特殊情况:对称矩阵](#62-特殊情况对称矩阵)

7. [计算方法简述](#7-计算方法简述)
   - 7.1 [直接方法(不推荐)](#71-直接方法不推荐)
   - 7.2 [实际算法](#72-实际算法)

8. [总结](#8-总结)
   - 8.1 [SVD 的核心价值](#81-svd-的核心价值)
   - 8.2 [SVD 的应用场景](#82-svd-的应用场景)
   - 8.3 [理解 SVD 的三个层次](#83-理解-svd-的三个层次)
   - 8.4 [最终洞察](#84-最终洞察)

9. [附录:关键公式速查](#附录关键公式速查)

---
## 1. 引言：从圆到椭圆

### 1.1 矩阵变换的本质

想象在二维平面上画一个单位圆：所有满足 $x^2 + y^2 = 1$ 的点。现在对这个圆施加一个矩阵变换 $A$：

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = A \begin{bmatrix} x \\ y \end{bmatrix}
$$

**奇妙的事情发生了**：圆变成了椭圆！

- 椭圆的长轴、短轴方向：矩阵 $A$ 的"主方向"
- 椭圆的长轴、短轴长度：矩阵 $A$ 的"拉伸程度"

**深刻的问题**：能否找到一组特殊的基，使得矩阵 $A$ 的作用变得简单（仅仅是沿着坐标轴拉伸）？

### 1.2 特征值分解的局限

如果 $A$ 是**方阵**，我们有特征值分解：

$$
A v = \lambda v
$$

**物理意义**：特征向量 $v$ 的方向在变换后保持不变，只是长度变为 $\lambda$ 倍。

**但是**：

- 特征值分解只适用于**方阵**
- 即使是方阵，也不一定可以对角化（如果特征向量不够）
- 非对称矩阵的特征向量不正交，失去几何直观性

**我们需要更强大的工具**：适用于任意 $m \times n$ 矩阵，始终存在，且具有优美几何意义的分解。

这就是**奇异值分解（SVD）**。

---

## 2. 特征分解（EVD）：对称矩阵的美学

在讨论 SVD 之前，先理解对称矩阵的特殊性质。

### 2.1 谱定理（Spectral Theorem）

**定理**：设 $A \in \mathbb{R}^{n \times n}$ 是实对称矩阵（$A = A^T$），则：

$$
A = Q \Lambda Q^T
$$

其中：

- $Q$ 是**正交矩阵**（$Q^T Q = I$），列向量是 $A$ 的特征向量
- $\Lambda$ 是**对角矩阵**，对角元素是 $A$ 的特征值
- 特征值都是**实数**
- 特征向量**相互正交**

### 2.2 几何直觉

对称矩阵有什么特殊性？

**关键洞察**：对称矩阵表示的变换**不产生切变**，只有旋转和伸缩。

分解 $A = Q \Lambda Q^T$ 的三步曲：

1. **$Q^T$**：旋转到特征向量构成的坐标系
2. **$\Lambda$**：沿着新坐标轴拉伸（特征值决定拉伸倍数）
3. **$Q$**：旋转回原坐标系

**例子**：协方差矩阵

协方差矩阵 $\Sigma = \mathbb{E}[(X - \mu)(X - \mu)^T]$ 是对称的。特征分解告诉我们：

- 特征向量：数据的主方向（PCA 的基础）
- 特征值：数据在各主方向上的方差

### 2.3 正定性：碗的形状

考虑二次型：

$$
f(x) = x^T A x
$$

如果 $A = Q \Lambda Q^T$，令 $y = Q^T x$，则：

$$
f(x) = y^T \Lambda y = \sum_{i=1}^{n} \lambda_i y_i^2
$$

**几何意义**：

- 所有 $\lambda_i > 0$（正定）：向上开口的碗，有唯一最小值
- 存在 $\lambda_i < 0$（不定）：马鞍面，有鞍点
- 所有 $\lambda_i \geq 0$，某些为 0（半正定）：退化的碗

**这在优化中至关重要**：Hessian 矩阵的特征值决定了临界点的性质。

---

## 3. 奇异值分解（SVD）：万能钥匙

### 3.1 核心思想：让非方阵也能对角化

**问题**：对于一般的 $A \in \mathbb{R}^{m \times n}$（$m \neq n$），如何分解？

**关键洞察**：虽然 $A$ 不是对称矩阵，但 $A^T A$ 和 $A A^T$ 是！

- $A^T A \in \mathbb{R}^{n \times n}$，对称半正定
- $A A^T \in \mathbb{R}^{m \times m}$，对称半正定

### 3.2 推导 SVD

**步骤 1**：对 $A^T A$ 做特征分解

$$
A^T A = V \Lambda V^T
$$

其中 $V \in \mathbb{R}^{n \times n}$ 正交，$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$，$\lambda_i \geq 0$。

**步骤 2**：定义奇异值

令 $\sigma_i = \sqrt{\lambda_i}$，这些 $\sigma_i$ 称为 $A$ 的**奇异值**。按降序排列：

$$
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0 = \sigma_{r+1} = \cdots = \sigma_{\min(m,n)}
$$

其中 $r = \text{rank}(A)$。

**步骤 3**：构造左奇异向量

对于前 $r$ 个特征向量 $v_i$（对应非零奇异值），定义：

$$
u_i = \frac{1}{\sigma_i} A v_i, \quad i = 1, \ldots, r
$$

**验证正交性**：

$$
u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j = \frac{1}{\sigma_i \sigma_j} v_i^T (\lambda_j v_j) = \frac{\lambda_j}{\sigma_i \sigma_j} v_i^T v_j
$$

当 $i = j$ 时，$u_i^T u_i = \frac{\sigma_i^2}{\sigma_i^2} = 1$；当 $i \neq j$ 时，$u_i^T u_j = 0$。

**步骤 4**：扩展为完整的正交基

将 $\{u_1, \ldots, u_r\}$ 扩展为 $\mathbb{R}^m$ 的标准正交基 $\{u_1, \ldots, u_m\}$（后面的向量在 $A$ 的左零空间中）。

**最终形式**：

$$
A = U \Sigma V^T
$$

其中：

- $U \in \mathbb{R}^{m \times m}$：左奇异向量，正交矩阵
- $\Sigma \in \mathbb{R}^{m \times n}$：对角矩阵（广义，矩形），对角线是奇异值
- $V \in \mathbb{R}^{n \times n}$：右奇异向量，正交矩阵

### 3.3 SVD 的几何图景：旋转-拉伸-旋转

**三步曲**（这是理解 SVD 的最直观方式）：

对于任意向量 $x \in \mathbb{R}^n$，计算 $Ax$：

$$
A x = U \Sigma V^T x
$$

1. **第一步：$V^T x$（旋转到行空间基）**
   - $V^T$ 是正交变换，将 $x$ 旋转到由 $V$ 的列向量（$A^T A$ 的特征向量）张成的坐标系
   - 物理意义：选择"最合适"的输入方向

2. **第二步：$\Sigma (V^T x)$（沿主轴拉伸）**
   - 对角矩阵，沿着各坐标轴独立缩放
   - 第 $i$ 个分量乘以 $\sigma_i$
   - 物理意义：信息的放大/缩小

3. **第三步：$U (\Sigma V^T x)$（旋转到列空间）**
   - $U$ 是正交变换，将结果旋转到由 $U$ 的列向量（$AA^T$ 的特征向量）张成的坐标系
   - 物理意义：映射到"最合适"的输出方向

**核心洞察**：任何矩阵变换都可以分解为"选择方向 → 缩放 → 输出方向"。

### 3.4 薄 SVD（Reduced SVD）

当 $m > n$ 时，$\Sigma$ 的后面 $m - n$ 行全是零，对应的 $U$ 的列向量没有贡献。我们可以截断：

$$
A = U_r \Sigma_r V_r^T
$$

其中：

- $U_r \in \mathbb{R}^{m \times r}$：前 $r$ 个左奇异向量
- $\Sigma_r \in \mathbb{R}^{r \times r}$：非零奇异值组成的对角矩阵
- $V_r \in \mathbb{R}^{n \times r}$：前 $r$ 个右奇异向量

这是最常用的形式，避免了冗余。

### 3.5 外积形式（Dyadic Expansion）

SVD 还可以写成外积和的形式：

$$
A = \sum_{i=1}^{r} \sigma_i u_i v_i^T
$$

**物理意义**：

- 每个 $u_i v_i^T$ 是一个秩-1 矩阵
- $A$ 是 $r$ 个秩-1 矩阵的加权和
- $\sigma_i$ 是第 $i$ 个"成分"的重要性

**这为低秩近似奠定了基础**。

---

## 4. 四个基本子空间的 SVD 视角

这是 SVD 最深刻的几何洞察之一。

### 4.1 回顾：四个基本子空间

对于矩阵 $A \in \mathbb{R}^{m \times n}$，有四个基本子空间：

1. **列空间**（Column Space）：$\mathcal{C}(A) \subseteq \mathbb{R}^m$，维度 $r$
2. **零空间**（Null Space）：$\mathcal{N}(A) \subseteq \mathbb{R}^n$，维度 $n - r$
3. **行空间**（Row Space）：$\mathcal{C}(A^T) \subseteq \mathbb{R}^n$，维度 $r$
4. **左零空间**（Left Null Space）：$\mathcal{N}(A^T) \subseteq \mathbb{R}^m$，维度 $m - r$

### 4.2 SVD 的完美切分

SVD 的 $U$ 和 $V$ 恰好给出了这四个子空间的标准正交基：

$$
V = \begin{bmatrix} \underbrace{v_1 \cdots v_r}_{\text{行空间}} & \underbrace{v_{r+1} \cdots v_n}_{\text{零空间}} \end{bmatrix}
$$

$$
U = \begin{bmatrix} \underbrace{u_1 \cdots u_r}_{\text{列空间}} & \underbrace{u_{r+1} \cdots u_m}_{\text{左零空间}} \end{bmatrix}
$$

**验证**：

- **行空间**：$A^T A v_i = \sigma_i^2 v_i$（$i \leq r$），所以 $A^T (A v_i) \neq 0$，即 $A v_i$ 在 $\mathcal{C}(A^T)$ 中
- **零空间**：$A^T A v_i = 0$（$i > r$），所以 $A v_i = 0$，即 $v_i \in \mathcal{N}(A)$
- **列空间**：$u_i = \frac{1}{\sigma_i} A v_i$（$i \leq r$），是 $A$ 的列向量的线性组合
- **左零空间**：$A^T u_i = 0$（$i > r$），由构造保证

### 4.3 正交关系图

用文字描述的几何图景：

```
输入空间 ℝⁿ                    输出空间 ℝᵐ
┌─────────────────┐           ┌─────────────────┐
│                 │           │                 │
│   行空间        │    A      │   列空间        │
│   (v₁...vᵣ)    │ ────────> │   (u₁...uᵣ)    │
│   维度 r        │           │   维度 r        │
│                 │           │                 │
├─────────────────┤           ├─────────────────┤
│                 │           │                 │
│   零空间        │    A      │   左零空间      │
│   (vᵣ₊₁...vₙ)  │ ────────> │   (uᵣ₊₁...uₘ)  │
│   维度 n-r      │   ↓0      │   维度 m-r      │
│                 │           │                 │
└─────────────────┘           └─────────────────┘
      ⊥                              ⊥
```

**关键性质**：

- 行空间 ⊥ 零空间（在 $\mathbb{R}^n$ 中）
- 列空间 ⊥ 左零空间（在 $\mathbb{R}^m$ 中）
- $A$ 将行空间**一一映射**到列空间（可逆）
- $A$ 将零空间**全部映射**到零向量

### 4.4 伪逆的几何意义

基于 SVD，我们可以定义**Moore-Penrose 伪逆**：

$$
A^+ = V \Sigma^+ U^T
$$

其中 $\Sigma^+$ 是将非零奇异值取倒数：

$$
\Sigma^+ = \begin{bmatrix}
1/\sigma_1 & & & \\
& \ddots & & \\
& & 1/\sigma_r & \\
& & & 0_{(n-r) \times (m-r)}
\end{bmatrix}
$$

**几何意义**：

- 在列空间中，$A^+$ 是 $A$ 的逆（$A^+ A = I$ 在行空间上）
- 在左零空间中，$A^+$ 映射到零
- $A^+$ 给出线性方程组 $Ax = b$ 的**最小范数解**

---

## 5. 低秩近似：SVD 的杀手级应用

### 5.1 问题设定

**问题**：给定矩阵 $A \in \mathbb{R}^{m \times n}$，秩为 $r$。如何找到秩为 $k$ 的矩阵 $A_k$（$k < r$），使得：

$$
\min_{\text{rank}(B) = k} \|A - B\|_F
$$

其中 $\|M\|_F = \sqrt{\sum_{i,j} M_{ij}^2}$ 是 Frobenius 范数（所有元素平方和的平方根）。

**直觉**：用更少的信息（低秩）来近似原矩阵。

### 5.2 Eckart-Young-Mirsky 定理

**定理**：设 $A = U \Sigma V^T$ 是 SVD，定义截断 SVD：

$$
A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T = U_k \Sigma_k V_k^T
$$

则 $A_k$ 是所有秩为 $k$ 的矩阵中，Frobenius 范数下距离 $A$ 最近的矩阵：

$$
\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2} = \text{最小可能误差}
$$

**证明思路**（不严格，但有启发性）：

由外积形式：

$$
A - A_k = \sum_{i=k+1}^{r} \sigma_i u_i v_i^T
$$

因为 $u_i$ 和 $v_i$ 都是标准正交的，所以：

$$
\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2 \|u_i v_i^T\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
$$

任何其他秩为 $k$ 的近似都无法做得更好（需要变分法严格证明）。

### 5.3 直觉：丢弃小奇异值 = 去噪

**信号 vs 噪声**：

- 大的奇异值：主要信息、结构化模式
- 小的奇异值：细节、噪声、随机性

截断 SVD 相当于**自动去噪**：只保留最重要的 $k$ 个"模式"。

**能量视角**：

矩阵的"总能量"：

$$
\|A\|_F^2 = \sum_{i=1}^{r} \sigma_i^2
$$

前 $k$ 个奇异值捕获的能量占比：

$$
\frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{i=1}^{r} \sigma_i^2}
$$

如果前几个奇异值远大于后面的（快速衰减），则低秩近似非常有效。

### 5.4 应用 1：图像压缩

**设定**：灰度图像是 $m \times n$ 的矩阵（每个元素是像素值）。

**原始存储**：$mn$ 个数。

**SVD 压缩**：只存储前 $k$ 个奇异值和对应的奇异向量：

- $\sigma_1, \ldots, \sigma_k$：$k$ 个数
- $u_1, \ldots, u_k$：$mk$ 个数
- $v_1, \ldots, v_k$：$nk$ 个数

**总存储量**：$k(m + n + 1)$。

**压缩比**：

$$
\frac{k(m + n + 1)}{mn}
$$

例如，$m = n = 1000$，$k = 50$，压缩比约为 $10\%$。

**效果**：如果图像有结构（自然图像通常如此），前几个奇异值就能捕获主要轮廓，重建质量很好。

### 5.5 应用 2：推荐系统与矩阵补全

**设定**：用户-物品评分矩阵 $R \in \mathbb{R}^{m \times n}$：

- $R_{ij}$：用户 $i$ 对物品 $j$ 的评分
- 问题：大部分元素是缺失的（用户没有评价所有物品）

**低秩假设**：

- 假设用户的偏好由少数几个"隐因子"决定（如电影的类型）
- 因此 $R$ 应该是**低秩**的（或近似低秩）

**策略**：

1. 对已观测的评分，用 SVD（或矩阵分解）找到低秩近似 $R_k$
2. 用 $R_k$ 的对应元素来预测缺失的评分

**Netflix Prize**：这一思想的成功应用。

### 5.6 应用 3：主成分分析（PCA）

PCA 本质上就是对数据的协方差矩阵（或数据矩阵本身）做 SVD。

**设定**：数据矩阵 $X \in \mathbb{R}^{n \times d}$（$n$ 个样本，$d$ 个特征），已中心化（每列均值为 0）。

**目标**：找到 $k$ 个方向，使得数据在这些方向上的投影方差最大。

**方法**：对 $X$ 做 SVD：

$$
X = U \Sigma V^T
$$

- $V$ 的列向量：主成分方向（特征）
- $\Sigma$ 的对角元素：对应方向上的标准差（奇异值）
- $U \Sigma$：降维后的数据（前 $k$ 列）

**降维**：

$$
X_k = U_k \Sigma_k V_k^T
$$

保留最大的 $k$ 个奇异值，重构误差最小。

---

## 6. SVD 与 EVD 的联系

### 6.1 核心关系

**对于任意矩阵 $A$**：

$$
A^T A = (V \Sigma U^T)(U \Sigma V^T) = V \Sigma^2 V^T
$$

$$
A A^T = (U \Sigma V^T)(V \Sigma U^T) = U \Sigma^2 U^T
$$

**结论**：

- $V$ 是 $A^T A$ 的特征向量矩阵
- $U$ 是 $A A^T$ 的特征向量矩阵
- $A$ 的奇异值 $\sigma_i$ 是 $A^T A$（或 $A A^T$）的特征值 $\lambda_i$ 的平方根：

$$
\sigma_i = \sqrt{\lambda_i}
$$

### 6.2 特殊情况：对称矩阵

如果 $A = A^T$（对称矩阵），则：

$$
A^T A = A^2
$$

设 $A = Q \Lambda Q^T$ 是特征分解，则：

$$
A^2 = Q \Lambda^2 Q^T
$$

此时 SVD 退化为：

$$
A = Q |\Lambda| Q^T
$$

其中 $|\Lambda|$ 是特征值的绝对值组成的对角矩阵。

**注意**：对称矩阵的特征值可以是负数，但奇异值始终非负。

**例子**：

$$
A = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
$$

- 特征值：$\lambda_1 = 1, \lambda_2 = -1$
- 奇异值：$\sigma_1 = 1, \sigma_2 = 1$

---

## 7. 计算方法简述

### 7.1 直接方法（不推荐）

理论上可以：

1. 计算 $A^T A$
2. 求 $A^T A$ 的特征值和特征向量
3. 计算奇异值和左奇异向量

**问题**：

- $A^T A$ 的条件数是 $A$ 的平方，数值不稳定
- 计算量大

### 7.2 实际算法

实际使用的是**Golub-Kahan 双对角化**或**分而治之法**：

1. 将 $A$ 约化为双对角矩阵（通过正交变换）
2. 对双对角矩阵求 SVD（高效且数值稳定）

现代库（NumPy、MATLAB、LAPACK）都实现了这些算法，直接调用即可。

**Python 示例**：

```python
import numpy as np

A = np.random.randn(100, 50)
U, s, Vt = np.linalg.svd(A, full_matrices=False)

# 低秩近似
k = 10
A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
error = np.linalg.norm(A - A_k, 'fro')
```

---

## 8. 总结

### 8.1 SVD 的核心价值

1. **通用性**：适用于任何矩阵（方阵、长矩阵、宽矩阵）
2. **存在性**：始终存在，且数值稳定
3. **几何直观性**：旋转-拉伸-旋转，清晰的物理意义
4. **正交性**：$U$ 和 $V$ 都是正交矩阵，保持几何结构

### 8.2 SVD 的应用场景

| 应用           | 核心思想                       |
| -------------- | ------------------------------ |
| **低秩近似**   | 截断小奇异值，去噪/压缩        |
| **PCA**        | 找到方差最大的方向             |
| **推荐系统**   | 矩阵补全，隐因子模型           |
| **图像处理**   | 压缩、去噪、特征提取           |
| **伪逆计算**   | 求解欠定/超定方程组            |
| **矩阵秩估计** | 通过奇异值分布判断数值秩       |
| **最小二乘**   | $\min \|Ax - b\|_2$ 的稳定解法 |

### 8.3 理解 SVD 的三个层次

1. **代数层次**：$A = U \Sigma V^T$，矩阵的分解
2. **几何层次**：任何线性变换 = 旋转 + 拉伸 + 旋转
3. **语义层次**：提取数据的"主要模式"，过滤噪声

### 8.4 最终洞察

**SVD 是线性代数的顶峰**：

- 它统一了特征值分解（对称矩阵的特例）
- 它揭示了矩阵的四个基本子空间的完美结构
- 它是现代数据科学的基石（PCA、推荐系统、自然语言处理中的 LSA 等）

**记住这句话**：

> *"Every matrix is a rotation, followed by a stretch, followed by another rotation."*
> — Gilbert Strang

SVD 将这个直觉变成了严格的数学定理，并赋予了它强大的计算能力。

---

## 附录：关键公式速查

| 概念               | 公式                                                         |
| ------------------ | ------------------------------------------------------------ |
| **SVD 完整形式**   | $A = U \Sigma V^T$，$U^T U = I$，$V^T V = I$                 |
| **薄 SVD**         | $A = U_r \Sigma_r V_r^T$，$r = \text{rank}(A)$               |
| **外积形式**       | $A = \sum_{i=1}^{r} \sigma_i u_i v_i^T$                      |
| **奇异值与特征值** | $\sigma_i = \sqrt{\lambda_i(A^T A)} = \sqrt{\lambda_i(A A^T)}$ |
| **低秩近似**       | $A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$                    |
| **近似误差**       | $\|A - A_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}$         |
| **伪逆**           | $A^+ = V \Sigma^+ U^T$，$\Sigma^+_{ii} = 1/\sigma_i$ （若 $\sigma_i \neq 0$） |
| **四个子空间**     | $\mathcal{C}(A) = \text{span}(u_1, \ldots, u_r)$<br>$\mathcal{N}(A) = \text{span}(v_{r+1}, \ldots, v_n)$<br>$\mathcal{C}(A^T) = \text{span}(v_1, \ldots, v_r)$<br>$\mathcal{N}(A^T) = \text{span}(u_{r+1}, \ldots, u_m)$ |

---

**下一章预告**：我们将把这些线性代数工具应用到概率论中，理解多元高斯分布的几何结构，以及协方差矩阵的特征分解如何揭示数据的内在结构。