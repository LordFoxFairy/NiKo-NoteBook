# ç¬¬ä¸‰ç¯‡ LangGraph æ·±å…¥

> **ç›®æ ‡**ï¼šç†è§£ create_agent èƒŒåçš„æœºåˆ¶ï¼ŒæŒæ¡å®Œå…¨è‡ªå®šä¹‰èƒ½åŠ›

åœ¨ç¬¬äºŒç¯‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `create_agent` å¿«é€Ÿæ„å»ºäº† Agentã€‚ä½†åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶ï¼š
- ğŸ¯ è‡ªå®šä¹‰ Agent çš„æ‰§è¡Œæµç¨‹
- ğŸ”„ å®ç°å¤æ‚çš„å¾ªç¯å’Œåˆ†æ”¯é€»è¾‘
- ğŸ’¾ ä¿å­˜å’Œæ¢å¤ Agent çŠ¶æ€
- ğŸ§  å®ç°é•¿æœŸè®°å¿†ç³»ç»Ÿ

è¿™äº›éœ€æ±‚ï¼Œéƒ½éœ€è¦ç†è§£ LangGraph çš„åº•å±‚æœºåˆ¶ã€‚

---

## ç¬¬1ç« ï¼šLangGraph æ ¸å¿ƒåŸç†

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ LangGraph

#### 1.1.1 create_agent çš„å±€é™æ€§

`create_agent` æä¾›äº†å¿«é€Ÿæ„å»º Agent çš„èƒ½åŠ›ï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹å­˜åœ¨å±€é™ï¼š

**å±€é™1ï¼šå›ºå®šçš„æ‰§è¡Œæµç¨‹**

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœï¼š{query}"

# create_agent çš„æ‰§è¡Œæµç¨‹æ˜¯å›ºå®šçš„
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search]
)

# æ‰§è¡Œæµç¨‹ï¼š
# 1. æ¨¡å‹æ€è€ƒ â†’ 2. è°ƒç”¨å·¥å…· â†’ 3. æ¨¡å‹æ€è€ƒ â†’ 4. è¾“å‡ºç­”æ¡ˆ
# æ— æ³•è‡ªå®šä¹‰ï¼š
# - åœ¨è°ƒç”¨å·¥å…·å‰æ·»åŠ éªŒè¯æ­¥éª¤
# - å®ç°å¹¶è¡Œå·¥å…·è°ƒç”¨
# - æ·»åŠ è‡ªå®šä¹‰çš„æ¡ä»¶åˆ†æ”¯
```

**å±€é™2ï¼šæœ‰é™çš„çŠ¶æ€æ§åˆ¶**

```python
# create_agent è‡ªåŠ¨ç®¡ç†çŠ¶æ€
# æ— æ³•ï¼š
# - è®¿é—®ä¸­é—´çŠ¶æ€
# - è‡ªå®šä¹‰çŠ¶æ€ç»“æ„
# - åœ¨ç‰¹å®šæ­¥éª¤ä¿å­˜å¿«ç…§
# - å®ç°å¤æ‚çš„çŠ¶æ€æ›´æ–°é€»è¾‘

result = agent.invoke({"messages": [("user", "æŸ¥è¯¢ä¿¡æ¯")]})

# åªèƒ½è·å–æœ€ç»ˆç»“æœï¼Œçœ‹ä¸åˆ°ä¸­é—´çŠ¶æ€
print(result["messages"][-1].content)
```

**å±€é™3ï¼šç¼ºä¹å¤æ‚æ§åˆ¶æµ**

```python
# éœ€æ±‚ï¼šå¤šæ­¥éª¤å·¥ä½œæµ
# 1. æœç´¢ä¿¡æ¯
# 2. å¦‚æœç»“æœä¸è¶³ï¼Œç»§ç»­æœç´¢
# 3. åˆ†æç»“æœ
# 4. å¦‚æœéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼Œå›åˆ°æ­¥éª¤1
# 5. ç”ŸæˆæŠ¥å‘Š

# create_agent æ— æ³•ç›´æ¥å®ç°è¿™ç§å¤æ‚çš„å¾ªç¯å’Œæ¡ä»¶é€»è¾‘
```

#### 1.1.2 çŠ¶æ€æœºæ€ç»´ä¸ LangGraph çš„å…³ç³»

**çŠ¶æ€æœº (State Machine) æ¦‚å¿µ**ï¼š

```mermaid
stateDiagram-v2
    [*] --> åˆ†æé—®é¢˜
    åˆ†æé—®é¢˜ --> éœ€è¦å·¥å…·: éœ€è¦å¤–éƒ¨ä¿¡æ¯
    åˆ†æé—®é¢˜ --> ç”Ÿæˆç­”æ¡ˆ: ä¿¡æ¯å……è¶³
    éœ€è¦å·¥å…· --> è°ƒç”¨å·¥å…·
    è°ƒç”¨å·¥å…· --> åˆ†æé—®é¢˜: ç»§ç»­åˆ†æ
    ç”Ÿæˆç­”æ¡ˆ --> [*]

    note right of åˆ†æé—®é¢˜
        çŠ¶æ€ï¼šmessages, iterations
    end note
```

**LangGraph = çŠ¶æ€æœºè¿è¡Œæ—¶**ï¼š

- **çŠ¶æ€ (State)**ï¼šå­˜å‚¨å½“å‰çš„æ•°æ®ï¼ˆå¦‚æ¶ˆæ¯å†å²ã€è¿­ä»£æ¬¡æ•°ï¼‰
- **èŠ‚ç‚¹ (Node)**ï¼šçŠ¶æ€è½¬ç§»çš„å¤„ç†å‡½æ•°
- **è¾¹ (Edge)**ï¼šå®šä¹‰çŠ¶æ€è½¬ç§»è·¯å¾„
- **è¿è¡Œæ—¶**ï¼šæŒ‰ç…§å›¾çš„å®šä¹‰æ‰§è¡ŒçŠ¶æ€è½¬ç§»

```python
# LangGraph è®©ä½ å®Œå…¨æ§åˆ¶çŠ¶æ€æœºçš„æ¯ä¸ªç»†èŠ‚
from langgraph.graph import StateGraph, END

# 1. å®šä¹‰çŠ¶æ€
class GraphState(TypedDict):
    messages: list[BaseMessage]
    iterations: int
    search_results: list[str]

# 2. å®šä¹‰èŠ‚ç‚¹ï¼ˆçŠ¶æ€è½¬ç§»å‡½æ•°ï¼‰
def analyze_node(state):
    # åˆ†æå½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥
    pass

def search_node(state):
    # æ‰§è¡Œæœç´¢ï¼Œæ›´æ–°çŠ¶æ€
    pass

# 3. å®šä¹‰è¾¹ï¼ˆè½¬ç§»æ¡ä»¶ï¼‰
def should_search(state):
    # æ¡ä»¶åˆ¤æ–­
    return "search" if need_more_info else "end"

# 4. æ„å»ºå›¾
workflow = StateGraph(GraphState)
workflow.add_node("analyze", analyze_node)
workflow.add_node("search", search_node)
workflow.add_conditional_edges("analyze", should_search, {
    "search": "search",
    "end": END
})
```

**å¯¹æ¯”ï¼šcreate_agent vs LangGraph**

| ç‰¹æ€§ | create_agent | LangGraph |
|------|--------------|-----------|
| **æ˜“ç”¨æ€§** | âœ… ä¸€è¡Œä»£ç åˆ›å»º | âš ï¸ éœ€è¦å®šä¹‰çŠ¶æ€å’ŒèŠ‚ç‚¹ |
| **çµæ´»æ€§** | âš ï¸ å›ºå®šæµç¨‹ | âœ… å®Œå…¨è‡ªå®šä¹‰ |
| **çŠ¶æ€æ§åˆ¶** | âš ï¸ è‡ªåŠ¨ç®¡ç† | âœ… å®Œå…¨å¯è§å’Œå¯æ§ |
| **æ¡ä»¶é€»è¾‘** | âš ï¸ æœ‰é™ | âœ… ä»»æ„å¤æ‚åº¦ |
| **è°ƒè¯•èƒ½åŠ›** | âš ï¸ é»‘ç›’ | âœ… å¯è¿½è¸ªæ¯ä¸ªçŠ¶æ€ |
| **é€‚ç”¨åœºæ™¯** | 80% çš„å¸¸è§„ä»»åŠ¡ | å¤æ‚ã€éœ€è¦ç²¾ç»†æ§åˆ¶çš„ä»»åŠ¡ |

---

### 1.2 åŸºæœ¬å…ƒç´ 

#### 1.2.1 State - çŠ¶æ€å®šä¹‰

**State æ˜¯ä»€ä¹ˆ**ï¼šå­˜å‚¨åœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„æ•°æ®ç»“æ„ã€‚

```python
from typing import Annotated, TypedDict
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

# æœ€ç®€å•çš„çŠ¶æ€ï¼šåªåŒ…å«æ¶ˆæ¯
class SimpleState(TypedDict):
    messages: list[BaseMessage]

# å¸¦ Reducer çš„çŠ¶æ€ï¼šè‡ªåŠ¨è¿½åŠ æ¶ˆæ¯
class MessageState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

# å¤æ‚çŠ¶æ€ï¼šåŒ…å«å¤šä¸ªå­—æ®µ
class ComplexState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    iterations: int              # è¿­ä»£æ¬¡æ•°
    search_results: list[str]    # æœç´¢ç»“æœ
    current_task: str            # å½“å‰ä»»åŠ¡
    is_complete: bool            # æ˜¯å¦å®Œæˆ
```

**Reducer å‡½æ•°**ï¼š

```python
# ä¸ä½¿ç”¨ Reducerï¼šæ¯æ¬¡è¦†ç›–æ•´ä¸ªåˆ—è¡¨
class State1(TypedDict):
    messages: list[BaseMessage]

# èŠ‚ç‚¹è¿”å›
return {"messages": [new_message]}  # ä¼šè¦†ç›–åŸæœ‰çš„ messages

# ä½¿ç”¨ add_messages Reducerï¼šè‡ªåŠ¨è¿½åŠ 
class State2(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

# èŠ‚ç‚¹è¿”å›
return {"messages": [new_message]}  # ä¼šè¿½åŠ åˆ° messages åˆ—è¡¨æœ«å°¾
```

**add_messages çš„å·¥ä½œåŸç†**ï¼š

```python
# add_messages æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ Reducer

# åˆå§‹çŠ¶æ€
state = {"messages": [HumanMessage(content="Hello")]}

# èŠ‚ç‚¹1è¿”å›
update1 = {"messages": [AIMessage(content="Hi")]}

# add_messages å¤„ç†å
# state["messages"] = [HumanMessage("Hello"), AIMessage("Hi")]

# èŠ‚ç‚¹2è¿”å›
update2 = {"messages": [HumanMessage(content="How are you?")]}

# add_messages å¤„ç†å
# state["messages"] = [
#     HumanMessage("Hello"),
#     AIMessage("Hi"),
#     HumanMessage("How are you?")
# ]
```

#### 1.2.2 Nodes - èŠ‚ç‚¹å‡½æ•°

**èŠ‚ç‚¹æ˜¯ä»€ä¹ˆ**ï¼šæ¥æ”¶çŠ¶æ€ï¼Œæ‰§è¡Œæ“ä½œï¼Œè¿”å›çŠ¶æ€æ›´æ–°çš„å‡½æ•°ã€‚

```python
from langchain_openai import ChatOpenAI

# èŠ‚ç‚¹ç­¾å
def node_function(state: AgentState) -> dict:
    """
    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        dict: çŠ¶æ€æ›´æ–°ï¼ˆä¼šåˆå¹¶åˆ°å½“å‰çŠ¶æ€ï¼‰
    """
    # 1. è¯»å–å½“å‰çŠ¶æ€
    messages = state["messages"]
    iterations = state.get("iterations", 0)

    # 2. æ‰§è¡Œæ“ä½œ
    model = ChatOpenAI(model="gpt-4")
    response = model.invoke(messages)

    # 3. è¿”å›çŠ¶æ€æ›´æ–°
    return {
        "messages": [response],
        "iterations": iterations + 1
    }
```

**å¸¸è§èŠ‚ç‚¹ç±»å‹**ï¼š

```python
# 1. æ¨¡å‹èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM
def call_model(state: AgentState) -> dict:
    model = ChatOpenAI(model="gpt-4")
    response = model.invoke(state["messages"])
    return {"messages": [response]}

# 2. å·¥å…·èŠ‚ç‚¹ï¼šæ‰§è¡Œå·¥å…·
def call_tools(state: AgentState) -> dict:
    last_message = state["messages"][-1]
    tool_calls = last_message.tool_calls

    tool_messages = []
    for tool_call in tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]

        # æ‰§è¡Œå·¥å…·
        result = execute_tool(tool_name, tool_args)

        tool_messages.append(
            ToolMessage(
                content=result,
                tool_call_id=tool_call["id"]
            )
        )

    return {"messages": tool_messages}

# 3. éªŒè¯èŠ‚ç‚¹ï¼šæ£€æŸ¥å’Œè¿‡æ»¤
def validate(state: AgentState) -> dict:
    messages = state["messages"]
    last_message = messages[-1]

    # æ£€æŸ¥æ˜¯å¦è¿åå®‰å…¨è§„åˆ™
    if contains_sensitive_info(last_message.content):
        return {
            "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæ— æ³•å¤„ç†è¯¥è¯·æ±‚")]
        }

    return {}  # ä¸æ›´æ–°çŠ¶æ€

# 4. èšåˆèŠ‚ç‚¹ï¼šæ•´åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯
def aggregate(state: AgentState) -> dict:
    search_results = state.get("search_results", [])

    # æ•´åˆæœç´¢ç»“æœ
    summary = summarize(search_results)

    return {
        "messages": [AIMessage(content=summary)],
        "search_results": []  # æ¸…ç©º
    }
```

#### 1.2.3 Edges - è¾¹è¿æ¥ï¼ˆæ™®é€šè¾¹ã€æ¡ä»¶è¾¹ï¼‰

**æ™®é€šè¾¹ (Edge)**ï¼šæ— æ¡ä»¶è½¬ç§»

```python
from langgraph.graph import StateGraph, END

workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("node_a", node_a)
workflow.add_node("node_b", node_b)

# æ™®é€šè¾¹ï¼šnode_a æ‰§è¡Œå®Œåï¼Œæ— æ¡ä»¶è¿›å…¥ node_b
workflow.add_edge("node_a", "node_b")

# ç»“æŸè¾¹ï¼šnode_b æ‰§è¡Œå®Œåï¼Œç»“æŸ
workflow.add_edge("node_b", END)
```

**æ¡ä»¶è¾¹ (Conditional Edge)**ï¼šæ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥

```python
from typing import Literal

def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """
    æ¡ä»¶å‡½æ•°ï¼šæ ¹æ®çŠ¶æ€è¿”å›ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°

    Returns:
        "tools": éœ€è¦è°ƒç”¨å·¥å…·
        "end": ç»“æŸæ‰§è¡Œ
    """
    last_message = state["messages"][-1]

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    if last_message.tool_calls:
        return "tools"

    return "end"

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "agent",                    # ä»å“ªä¸ªèŠ‚ç‚¹å‡ºå‘
    should_continue,            # æ¡ä»¶å‡½æ•°
    {
        "tools": "call_tools",  # æ˜ å°„ï¼šæ¡ä»¶è¿”å›å€¼ -> ç›®æ ‡èŠ‚ç‚¹
        "end": END
    }
)
```

**æ¡ä»¶è¾¹ç¤ºä¾‹ï¼šå¤æ‚è·¯ç”±**

```python
def route_question(state: AgentState) -> Literal["search", "calculate", "general"]:
    """æ ¹æ®é—®é¢˜ç±»å‹è·¯ç”±"""
    question = state["messages"][-1].content

    if "å¤©æ°”" in question or "æ–°é—»" in question:
        return "search"
    elif any(op in question for op in ["+", "-", "*", "/", "è®¡ç®—"]):
        return "calculate"
    else:
        return "general"

workflow.add_conditional_edges(
    "classify",
    route_question,
    {
        "search": "search_node",
        "calculate": "calc_node",
        "general": "general_node"
    }
)
```

#### 1.2.4 Entry Point ä¸ End

**Entry Point**ï¼šå›¾çš„èµ·å§‹èŠ‚ç‚¹

```python
workflow = StateGraph(AgentState)

workflow.add_node("start", start_node)
workflow.add_node("process", process_node)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("start")

# ç­‰ä»·äº
workflow.add_edge("__start__", "start")
```

**END**ï¼šå›¾çš„ç»ˆæ­¢æ ‡è®°

```python
from langgraph.graph import END

# æ–¹å¼1ï¼šç›´æ¥è¾¹åˆ° END
workflow.add_edge("final_node", END)

# æ–¹å¼2ï¼šæ¡ä»¶è¾¹åˆ° END
workflow.add_conditional_edges(
    "decision_node",
    should_end,
    {
        "continue": "next_node",
        "end": END
    }
)
```

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
from typing import Annotated, TypedDict, Literal
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# 1. å®šä¹‰çŠ¶æ€
class ChatState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

# 2. å®šä¹‰èŠ‚ç‚¹
def call_model(state: ChatState) -> dict:
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.invoke(state["messages"])
    return {"messages": [response]}

def call_tools(state: ChatState) -> dict:
    # ç®€åŒ–ï¼šç›´æ¥è¿”å›æ¨¡æ‹Ÿç»“æœ
    return {"messages": [AIMessage(content="å·¥å…·æ‰§è¡Œç»“æœ")]}

# 3. å®šä¹‰æ¡ä»¶å‡½æ•°
def should_continue(state: ChatState) -> Literal["tools", "end"]:
    last_message = state["messages"][-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"

    return "end"

# 4. æ„å»ºå›¾
workflow = StateGraph(ChatState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)

workflow.add_edge("tools", "agent")

# 5. ç¼–è¯‘
app = workflow.compile()

# 6. æ‰§è¡Œ
result = app.invoke({
    "messages": [HumanMessage(content="Hello")]
})

print(result["messages"][-1].content)
```

---

### 1.3 Graph ç±»å‹ä¸æ‰§è¡Œ

#### 1.3.1 Graph ç±»å‹ä¸ç¼–è¯‘æœºåˆ¶

LangGraph ä½¿ç”¨**"æ„å»ºå™¨-ç¼–è¯‘å™¨-è¿è¡Œæ—¶"**çš„è®¾è®¡æ¨¡å¼,ç†è§£è¿™ä¸ªæ¨¡å¼æ˜¯æŒæ¡ LangGraph çš„å…³é”®ã€‚

**1. StateGraph - é€šç”¨çŠ¶æ€å›¾æ„å»ºå™¨**

`StateGraph` æ˜¯æœ€å¸¸ç”¨çš„å›¾æ„å»ºå™¨,é€‚ç”¨äºä»»ä½•è‡ªå®šä¹‰çŠ¶æ€ç»“æ„:

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

# å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€
class WorkflowState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    step_count: int
    processed_data: list[dict]

# åˆ›å»º StateGraph å®ä¾‹
workflow = StateGraph(WorkflowState)

# æ·»åŠ èŠ‚ç‚¹(å¤„ç†é€»è¾‘)
def process_node(state: WorkflowState) -> dict:
    return {
        "step_count": state["step_count"] + 1,
        "processed_data": state["processed_data"] + [{"step": state["step_count"]}]
    }

workflow.add_node("process", process_node)
workflow.set_entry_point("process")
workflow.add_edge("process", END)

# æ³¨æ„:æ­¤æ—¶ workflow è¿˜ä¸èƒ½æ‰§è¡Œ,éœ€è¦ç¼–è¯‘
```

**å…³é”®ç‰¹ç‚¹:**
- âœ… å®Œå…¨è‡ªå®šä¹‰çŠ¶æ€ç»“æ„
- âœ… é€‚åˆå¤æ‚ä¸šåŠ¡é€»è¾‘
- âœ… ç±»å‹å®‰å…¨(TypedDict æä¾›ç±»å‹æç¤º)

**2. MessagesState - ç®€åŒ–çš„æ¶ˆæ¯å›¾**

å¦‚æœåªéœ€è¦å¤„ç†æ¶ˆæ¯å†å²,å¯ä»¥ç›´æ¥ä½¿ç”¨å†…ç½®çš„ `MessagesState`:

```python
from langgraph.graph import StateGraph, MessagesState, END

# MessagesState ç­‰ä»·äº:
# class MessagesState(TypedDict):
#     messages: Annotated[list[BaseMessage], add_messages]

# ç›´æ¥ä½¿ç”¨ MessagesState,æ— éœ€è‡ªå®šä¹‰çŠ¶æ€
workflow = StateGraph(MessagesState)

def chat_node(state: MessagesState) -> dict:
    # ç›´æ¥å¤„ç† messages
    response = model.invoke(state["messages"])
    return {"messages": [response]}

workflow.add_node("chat", chat_node)
workflow.set_entry_point("chat")
workflow.add_edge("chat", END)
```

**3. ç¼–è¯‘: StateGraph â†’ CompiledGraph**

`StateGraph` æ˜¯æ„å»ºå™¨,ä¸èƒ½ç›´æ¥æ‰§è¡Œã€‚é€šè¿‡ `compile()` å°†å…¶ç¼–è¯‘æˆ `CompiledGraph`:

```python
# ç¼–è¯‘å‰: StateGraph (ä¸å¯æ‰§è¡Œ)
workflow = StateGraph(WorkflowState)
workflow.add_node("node1", func1)
workflow.set_entry_point("node1")
workflow.add_edge("node1", END)

# ç¼–è¯‘å: CompiledGraph (å¯æ‰§è¡Œ)
app = workflow.compile()

# CompiledGraph æä¾›æ‰§è¡Œæ¥å£
result = app.invoke({"messages": [], "step_count": 0, "processed_data": []})
```

**ç¼–è¯‘åšäº†ä»€ä¹ˆ?**
- ğŸ” éªŒè¯å›¾ç»“æ„(æ˜¯å¦æœ‰å…¥å£ç‚¹ã€æ˜¯å¦æœ‰æ— æ³•åˆ°è¾¾çš„èŠ‚ç‚¹)
- ğŸ”— æ„å»ºæ‰§è¡Œå¼•æ“(å†³å®šèŠ‚ç‚¹æ‰§è¡Œé¡ºåºã€çŠ¶æ€ä¼ é€’æœºåˆ¶)
- ğŸš€ ä¼˜åŒ–æ‰§è¡Œè·¯å¾„
- ğŸ’¾ é›†æˆ Checkpointer(å¦‚æœé…ç½®äº†æŒä¹…åŒ–)

**4. æ„å»ºå™¨ vs è¿è¡Œæ—¶å¯¹æ¯”**

| å¯¹æ¯”é¡¹ | StateGraph (æ„å»ºå™¨) | CompiledGraph (è¿è¡Œæ—¶) |
|--------|---------------------|------------------------|
| **ä½œç”¨** | å®šä¹‰å›¾ç»“æ„ | æ‰§è¡Œå›¾é€»è¾‘ |
| **å¯ä¿®æ”¹** | âœ… å¯ä»¥æ·»åŠ èŠ‚ç‚¹/è¾¹ | âŒ ä¸å¯ä¿®æ”¹,åªè¯» |
| **å¯æ‰§è¡Œ** | âŒ ä¸èƒ½æ‰§è¡Œ | âœ… æä¾› invoke/stream ç­‰æ–¹æ³• |
| **ç±»å‹** | `StateGraph` | `CompiledGraph` |
| **ä½•æ—¶ä½¿ç”¨** | æ„å»ºé˜¶æ®µ | è¿è¡Œé˜¶æ®µ |

**5. compile() çš„é«˜çº§å‚æ•°**

```python
from langgraph.checkpoint.sqlite import SqliteSaver

checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

app = workflow.compile(
    checkpointer=checkpointer,      # æŒä¹…åŒ–åç«¯
    interrupt_before=["human_review"],  # åœ¨æŒ‡å®šèŠ‚ç‚¹å‰ä¸­æ–­
    interrupt_after=["critical_step"],  # åœ¨æŒ‡å®šèŠ‚ç‚¹åä¸­æ–­
    debug=True                       # å¼€å¯è°ƒè¯•æ¨¡å¼
)
```

**æœ€ä½³å®è·µ:**
- ğŸ—ï¸ æ„å»ºé˜¶æ®µä½¿ç”¨ `StateGraph`,çµæ´»æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
- ğŸ”’ ç¼–è¯‘åçš„ `CompiledGraph` ä¸å¯ä¿®æ”¹,ç¡®ä¿è¿è¡Œæ—¶çš„ç¨³å®šæ€§
- â™»ï¸ å¦‚éœ€ä¿®æ”¹å›¾ç»“æ„,é‡æ–°æ„å»º `StateGraph` å¹¶ç¼–è¯‘

#### 1.3.2 åŒæ­¥ã€å¼‚æ­¥ã€æµå¼æ‰§è¡Œ

**åŒæ­¥æ‰§è¡Œ (invoke)**

```python
app = workflow.compile()

# åŒæ­¥è°ƒç”¨ï¼šé˜»å¡ç›´åˆ°å®Œæˆ
result = app.invoke({
    "messages": [HumanMessage(content="ä½ å¥½")]
})

print(result["messages"][-1].content)
```

**å¼‚æ­¥æ‰§è¡Œ (ainvoke)**

```python
import asyncio

async def main():
    app = workflow.compile()

    # å¼‚æ­¥è°ƒç”¨
    result = await app.ainvoke({
        "messages": [HumanMessage(content="ä½ å¥½")]
    })

    print(result["messages"][-1].content)

asyncio.run(main())
```

**æµå¼æ‰§è¡Œ (stream)**

```python
# æµå¼è¾“å‡ºï¼šæ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåç«‹å³è¿”å›

app = workflow.compile()

for chunk in app.stream({
    "messages": [HumanMessage(content="è®²ä¸ªç¬‘è¯")]
}):
    # chunk æ ¼å¼ï¼š{"node_name": state_update}
    node_name = list(chunk.keys())[0]
    state_update = chunk[node_name]

    print(f"\nèŠ‚ç‚¹ï¼š{node_name}")
    print(f"çŠ¶æ€æ›´æ–°ï¼š{state_update}")
```

**æµå¼æ‰§è¡Œç¤ºä¾‹**ï¼š

```python
from langgraph.graph import StateGraph, END, MessagesState
from langchain_openai import ChatOpenAI

def call_model(state):
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.invoke(state["messages"])
    return {"messages": [response]}

workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.set_entry_point("agent")
workflow.add_edge("agent", END)

app = workflow.compile()

# æµå¼æ‰§è¡Œ
print("=== æµå¼æ‰§è¡Œ ===")
for chunk in app.stream({
    "messages": [HumanMessage(content="1+1ç­‰äºå‡ ï¼Ÿ")]
}):
    node_name = list(chunk.keys())[0]
    print(f"\n[{node_name}] æ‰§è¡Œå®Œæˆ")

    if "messages" in chunk[node_name]:
        messages = chunk[node_name]["messages"]
        if messages:
            print(f"è¾“å‡ºï¼š{messages[-1].content}")
```

**å¼‚æ­¥æµå¼æ‰§è¡Œ (astream)**

```python
async def stream_example():
    app = workflow.compile()

    async for chunk in app.astream({
        "messages": [HumanMessage(content="ä½ å¥½")]
    }):
        node_name = list(chunk.keys())[0]
        print(f"èŠ‚ç‚¹ï¼š{node_name}")

asyncio.run(stream_example())
```

**æ‰§è¡Œæ¨¡å¼å¯¹æ¯”**

| æ¨¡å¼ | æ–¹æ³• | é˜»å¡ | è¿”å›æ–¹å¼ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|----------|
| åŒæ­¥ | invoke | æ˜¯ | ä¸€æ¬¡æ€§è¿”å› | ç®€å•è„šæœ¬ã€æµ‹è¯• |
| å¼‚æ­¥ | ainvoke | å¦ | ä¸€æ¬¡æ€§è¿”å› | é«˜å¹¶å‘ã€Web æœåŠ¡ |
| æµå¼ | stream | æ˜¯ | é€èŠ‚ç‚¹è¿”å› | è¿›åº¦å±•ç¤ºã€è°ƒè¯• |
| å¼‚æ­¥æµå¼ | astream | å¦ | é€èŠ‚ç‚¹è¿”å› | å®æ—¶ UIã€WebSocket |

---

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº† LangGraph çš„æ ¸å¿ƒæ¦‚å¿µï¼š

#### æ ¸å¿ƒæ¦‚å¿µ

1. **ä¸ºä»€ä¹ˆéœ€è¦ LangGraph**
   - create_agent çš„å±€é™æ€§ï¼šå›ºå®šæµç¨‹ã€æœ‰é™çŠ¶æ€æ§åˆ¶
   - çŠ¶æ€æœºæ€ç»´ï¼šStateã€Nodesã€Edges

2. **åŸºæœ¬å…ƒç´ **
   - **State**ï¼šçŠ¶æ€å®šä¹‰ã€Reducer å‡½æ•°ï¼ˆadd_messagesï¼‰
   - **Nodes**ï¼šèŠ‚ç‚¹å‡½æ•°ï¼ˆæ¨¡å‹ã€å·¥å…·ã€éªŒè¯ã€èšåˆï¼‰
   - **Edges**ï¼šæ™®é€šè¾¹ï¼ˆæ— æ¡ä»¶è½¬ç§»ï¼‰ã€æ¡ä»¶è¾¹ï¼ˆæ ¹æ®çŠ¶æ€è·¯ç”±ï¼‰
   - **Entry Point & END**ï¼šèµ·ç‚¹å’Œç»ˆç‚¹

3. **Graph ç±»å‹**
   - StateGraphï¼šé€šç”¨çŠ¶æ€å›¾
   - MessagesStateï¼šé¢„å®šä¹‰çš„æ¶ˆæ¯çŠ¶æ€
   - CompiledGraphï¼šç¼–è¯‘åçš„å¯æ‰§è¡Œå›¾

4. **æ‰§è¡Œæ¨¡å¼**
   - invokeï¼šåŒæ­¥æ‰§è¡Œ
   - ainvokeï¼šå¼‚æ­¥æ‰§è¡Œ
   - streamï¼šæµå¼æ‰§è¡Œ
   - astreamï¼šå¼‚æ­¥æµå¼æ‰§è¡Œ

#### ä¸‹ä¸€æ­¥

åœ¨ç¬¬8ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹  **State ç®¡ç†ä¸ Memory ç³»ç»Ÿ**ï¼ŒæŒæ¡ï¼š
- State æ›´æ–°æœºåˆ¶
- Checkpointer æŒä¹…åŒ–
- LangMem SDKï¼ˆEpisodicã€Proceduralã€Semantic Memoryï¼‰
- Graph æ„å»ºæœ€ä½³å®è·µ

---

### æ€è€ƒä¸ç»ƒä¹ 

#### æ€è€ƒé¢˜

1. create_agent å’Œ LangGraph çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
2. add_messages Reducer å¦‚ä½•å·¥ä½œï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ
3. æ¡ä»¶è¾¹å’Œæ™®é€šè¾¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿå„è‡ªé€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ
4. æµå¼æ‰§è¡Œå’ŒåŒæ­¥æ‰§è¡Œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

#### ç»ƒä¹ é¢˜

**ç»ƒä¹ 1ï¼šæ„å»ºç®€å•çš„å¯¹è¯ Agent**

è¦æ±‚ï¼š
- ä½¿ç”¨ StateGraph æ„å»º
- åŒ…å« call_model èŠ‚ç‚¹
- ä½¿ç”¨ MessagesState
- æµ‹è¯• invoke å’Œ stream

**ç»ƒä¹ 2ï¼šå®ç°æ¡ä»¶è·¯ç”±**

è¦æ±‚ï¼š
- æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹
- å®ç° route_question æ¡ä»¶å‡½æ•°
- åŒ…å«è‡³å°‘3ä¸ªä¸åŒçš„å¤„ç†èŠ‚ç‚¹

**ç»ƒä¹ 3ï¼šç†è§£ Reducer**

è¦æ±‚ï¼š
- åˆ›å»ºä¸ä½¿ç”¨ add_messages çš„çŠ¶æ€
- åˆ›å»ºä½¿ç”¨ add_messages çš„çŠ¶æ€
- å¯¹æ¯”ä¸¤è€…çš„è¡Œä¸ºå·®å¼‚

---

## ç¬¬2ç« ï¼šState ç®¡ç†ä¸ Memory ç³»ç»Ÿ

### 2.1 State å®šä¹‰ä¸æ›´æ–°

#### 2.1.1 è‡ªå®šä¹‰çŠ¶æ€ (TypedDict)

å½“éœ€è¦å®Œå…¨æ§åˆ¶çŠ¶æ€ç»“æ„æ—¶,å¯ä»¥ä½¿ç”¨ `TypedDict` è‡ªå®šä¹‰çŠ¶æ€ã€‚

**åŸºç¡€ç”¨æ³•**

```python
from typing import TypedDict, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

# æœ€ç®€å•çš„çŠ¶æ€å®šä¹‰
class BasicState(TypedDict):
    """åŸºç¡€çŠ¶æ€ï¼šåªåŒ…å«å¿…éœ€å­—æ®µ"""
    messages: list[BaseMessage]
    counter: int
```

**ä½¿ç”¨ Reducer**

Reducer å®šä¹‰äº†çŠ¶æ€å­—æ®µå¦‚ä½•æ›´æ–°ã€‚æœ€å¸¸ç”¨çš„æ˜¯ `add_messages`,å®ƒä¼šè¿½åŠ æ¶ˆæ¯è€Œä¸æ˜¯æ›¿æ¢:

```python
# å¸¦ Reducer çš„çŠ¶æ€
class StateWithReducer(TypedDict):
    """ä½¿ç”¨ add_messages Reducer è‡ªåŠ¨è¿½åŠ æ¶ˆæ¯"""
    messages: Annotated[list[BaseMessage], add_messages]
    counter: int

# ä½¿ç”¨è¿™ä¸ªçŠ¶æ€æ—¶:
# - messages ä¼šè‡ªåŠ¨è¿½åŠ æ–°æ¶ˆæ¯
# - counter ä¼šè¢«æ›¿æ¢
```

**å¤æ‚çŠ¶æ€ç¤ºä¾‹**

```python
from typing import Optional

class WorkflowState(TypedDict):
    """å¤æ‚ä¸šåŠ¡åœºæ™¯çš„çŠ¶æ€å®šä¹‰"""
    # å¯¹è¯å†å² (ä½¿ç”¨ Reducer è‡ªåŠ¨è¿½åŠ )
    messages: Annotated[list[BaseMessage], add_messages]

    # ä¸šåŠ¡å­—æ®µ
    iterations: int              # è¿­ä»£æ¬¡æ•°
    search_results: list[dict]   # æœç´¢ç»“æœ
    current_plan: str            # å½“å‰è®¡åˆ’
    is_complete: bool            # æ˜¯å¦å®Œæˆ

    # å¯é€‰å­—æ®µ
    metadata: Optional[dict]     # å…ƒæ•°æ®
```

**è‡ªå®šä¹‰çŠ¶æ€æœ€ä½³å®è·µ**

```python
class BestPracticeState(TypedDict):
    """çŠ¶æ€å®šä¹‰æœ€ä½³å®è·µ

    1. æ·»åŠ æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²
    2. ä½¿ç”¨ç±»å‹æ³¨è§£
    3. ä¸ºåˆ—è¡¨/å­—å…¸å­—æ®µä½¿ç”¨ Reducer
    4. ä½¿ç”¨ Optional æ ‡è®°å¯é€‰å­—æ®µ
    5. å­—æ®µå‘½åæ¸…æ™°ã€è¯­ä¹‰æ˜ç¡®
    """

    # å¿…éœ€å­—æ®µï¼šå¯¹è¯å†å²
    messages: Annotated[list[BaseMessage], add_messages]

    # å¿…éœ€å­—æ®µï¼šä¸šåŠ¡æ•°æ®
    user_id: str
    session_id: str

    # å¯é€‰å­—æ®µï¼šä¸­é—´çŠ¶æ€
    current_step: Optional[str]
    temp_data: Optional[dict]
```

#### 2.1.2 é¢„å®šä¹‰çŠ¶æ€ç±»å‹

LangGraph å’Œ LangChain æä¾›äº†ä¸¤ç§å¸¸ç”¨çš„é¢„å®šä¹‰çŠ¶æ€ç±»å‹,é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯ã€‚

**MessagesState - ç®€å•å¯¹è¯çŠ¶æ€**

`MessagesState` æ˜¯æœ€å¸¸ç”¨çš„çŠ¶æ€ç±»å‹,é€‚åˆåªéœ€è¦ç®¡ç†æ¶ˆæ¯å†å²çš„åœºæ™¯:

```python
from langgraph.graph import MessagesState, StateGraph

# MessagesState å®šä¹‰ç­‰ä»·äº:
# class MessagesState(TypedDict):
#     messages: Annotated[list[BaseMessage], add_messages]

# ç›´æ¥ä½¿ç”¨
workflow = StateGraph(MessagesState)

def chat_node(state: MessagesState):
    """å¤„ç†å¯¹è¯çš„èŠ‚ç‚¹"""
    messages = state["messages"]
    # ... å¤„ç†é€»è¾‘
    return {"messages": [response]}

workflow.add_node("chat", chat_node)
```

**æ‰©å±• MessagesState**

å¦‚æœéœ€è¦åœ¨æ¶ˆæ¯åŸºç¡€ä¸Šæ·»åŠ é¢å¤–å­—æ®µ:

```python
class ExtendedMessagesState(MessagesState):
    """æ‰©å±• MessagesState æ·»åŠ ä¸šåŠ¡å­—æ®µ"""
    user_id: str
    session_type: str
    context: dict

workflow = StateGraph(ExtendedMessagesState)
```

**AgentState - å®˜æ–¹ Agent çŠ¶æ€**

`AgentState` æ˜¯ LangChain æä¾›çš„æ ‡å‡† Agent çŠ¶æ€,åŒ…å«æ¶ˆæ¯å†å²å’Œè¿­ä»£æ§åˆ¶:

```python
from langchain.agents import AgentState

# AgentState å®šä¹‰:
# class AgentState(TypedDict):
#     messages: Annotated[list[BaseMessage], add_messages]
#     remaining_steps: int  # é˜²æ­¢æ— é™å¾ªç¯
```

**ä½¿ç”¨åœºæ™¯1: create_agent é»˜è®¤ä½¿ç”¨**

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

# create_agent å†…éƒ¨é»˜è®¤ä½¿ç”¨ AgentState
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search_tool],
    # state_schema é»˜è®¤ä¸º AgentState
)

# AgentState è‡ªåŠ¨ç®¡ç† messages å’Œ remaining_steps
result = agent.invoke({"messages": [("user", "æœç´¢ä¿¡æ¯")]})
```

**ä½¿ç”¨åœºæ™¯2: åœ¨ StateGraph ä¸­ä½¿ç”¨**

```python
from langchain.agents import AgentState
from langgraph.graph import StateGraph

# ç›´æ¥ä½¿ç”¨å®˜æ–¹ AgentState
workflow = StateGraph(AgentState)

def agent_node(state: AgentState):
    """Agent èŠ‚ç‚¹ä¼šè‡ªåŠ¨è·å¾— remaining_steps"""
    steps_left = state.get("remaining_steps", 10)
    if steps_left <= 0:
        return {"messages": [AIMessage("è¾¾åˆ°æœ€å¤§æ­¥æ•°")]}

    # æ‰§è¡Œé€»è¾‘...
    return {
        "messages": [response],
        "remaining_steps": steps_left - 1
    }

workflow.add_node("agent", agent_node)
```

**ä½¿ç”¨åœºæ™¯3: æ‰©å±• AgentState**

```python
from langchain.agents import AgentState

class CustomAgentState(AgentState):
    """æ‰©å±• AgentState æ·»åŠ è‡ªå®šä¹‰å­—æ®µ"""
    user_context: dict
    tool_history: list[str]
    # messages å’Œ remaining_steps è‡ªåŠ¨ç»§æ‰¿

workflow = StateGraph(CustomAgentState)
```

**çŠ¶æ€ç±»å‹é€‰æ‹©æŒ‡å—**

| çŠ¶æ€ç±»å‹ | é€‚ç”¨åœºæ™¯ | åŒ…å«å­—æ®µ | ä½•æ—¶ä½¿ç”¨ |
|---------|---------|---------|---------|
| `MessagesState` | ç®€å•å¯¹è¯æµç¨‹ | `messages` | åªéœ€ç®¡ç†æ¶ˆæ¯å†å² |
| `AgentState` | Agent åº”ç”¨ | `messages`, `remaining_steps` | ä½¿ç”¨ `create_agent` æˆ–éœ€è¦æ­¥æ•°é™åˆ¶ |
| è‡ªå®šä¹‰ `TypedDict` | å¤æ‚ä¸šåŠ¡é€»è¾‘ | å®Œå…¨è‡ªå®šä¹‰ | éœ€è¦é¢å¤–ä¸šåŠ¡å­—æ®µ |
| æ‰©å±•é¢„å®šä¹‰ç±»å‹ | æ ‡å‡†åœºæ™¯+æ‰©å±• | ç»§æ‰¿ + è‡ªå®šä¹‰ | åœ¨æ ‡å‡†åŸºç¡€ä¸Šæ·»åŠ å­—æ®µ |

**é€‰æ‹©å»ºè®®:**
- ğŸ¯ **ä¼˜å…ˆä½¿ç”¨é¢„å®šä¹‰ç±»å‹**: `MessagesState` æˆ– `AgentState` èƒ½æ»¡è¶³80%çš„éœ€æ±‚
- ğŸ”§ **æ‰©å±•è€Œéé‡å†™**: éœ€è¦é¢å¤–å­—æ®µæ—¶,æ‰©å±•é¢„å®šä¹‰ç±»å‹è€Œéä»é›¶å¼€å§‹
- ğŸ“¦ **è‡ªå®šä¹‰ä»…åœ¨å¿…è¦æ—¶**: åªæœ‰é¢„å®šä¹‰ç±»å‹å®Œå…¨ä¸é€‚ç”¨æ—¶æ‰è‡ªå®šä¹‰

#### 2.1.3 State æ›´æ–°æœºåˆ¶

ç†è§£çŠ¶æ€å¦‚ä½•æ›´æ–°æ˜¯ä½¿ç”¨ LangGraph çš„å…³é”®ã€‚æœ‰ä¸‰ç§æ›´æ–°æœºåˆ¶ã€‚

**æœºåˆ¶1ï¼šå®Œå…¨æ›¿æ¢**

```python
class State(TypedDict):
    counter: int
    data: dict

def node(state: State) -> dict:
    # è¿”å›çš„å­—æ®µä¼šå®Œå…¨æ›¿æ¢çŠ¶æ€ä¸­çš„å¯¹åº”å­—æ®µ
    return {
        "counter": 10,  # æ›¿æ¢ state["counter"]
        "data": {"new": "value"}  # æ›¿æ¢ state["data"]
    }

# åˆå§‹çŠ¶æ€
initial = {"counter": 0, "data": {"old": "value"}}

# èŠ‚ç‚¹æ‰§è¡Œå
# final = {"counter": 10, "data": {"new": "value"}}
# âš ï¸ {"old": "value"} è¢«å®Œå…¨æ›¿æ¢
```

**æœºåˆ¶2ï¼šå¢é‡æ›´æ–°ï¼ˆä½¿ç”¨ Reducerï¼‰**

```python
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

def node(state: State) -> dict:
    # add_messages Reducer ä¼šè¿½åŠ ï¼Œè€Œä¸æ˜¯æ›¿æ¢
    return {
        "messages": [AIMessage(content="æ–°æ¶ˆæ¯")]
    }

# åˆå§‹çŠ¶æ€
initial = {"messages": [HumanMessage(content="Hello")]}

# èŠ‚ç‚¹æ‰§è¡Œå
# final = {
#     "messages": [
#         HumanMessage(content="Hello"),
#         AIMessage(content="æ–°æ¶ˆæ¯")  # è¿½åŠ 
#     ]
# }
```

**æœºåˆ¶3ï¼šè‡ªå®šä¹‰ Reducer**

```python
from typing import Annotated

def merge_dicts(existing: dict, update: dict) -> dict:
    """è‡ªå®šä¹‰ Reducerï¼šåˆå¹¶å­—å…¸"""
    result = existing.copy()
    result.update(update)
    return result

def append_unique(existing: list, update: list) -> list:
    """è‡ªå®šä¹‰ Reducerï¼šè¿½åŠ ä¸é‡å¤å…ƒç´ """
    result = existing.copy()
    for item in update:
        if item not in result:
            result.append(item)
    return result

class State(TypedDict):
    # ä½¿ç”¨è‡ªå®šä¹‰ Reducer
    metadata: Annotated[dict, merge_dicts]
    tags: Annotated[list[str], append_unique]

# åˆå§‹çŠ¶æ€
initial = {
    "metadata": {"user": "Alice", "session": "123"},
    "tags": ["urgent"]
}

# èŠ‚ç‚¹è¿”å›
update = {
    "metadata": {"priority": "high"},
    "tags": ["urgent", "important"]
}

# åˆå¹¶å
# final = {
#     "metadata": {"user": "Alice", "session": "123", "priority": "high"},
#     "tags": ["urgent", "important"]  # "urgent" ä¸é‡å¤
# }
```

**Reducer æ‰§è¡Œé¡ºåº**

```python
# å¤šä¸ªèŠ‚ç‚¹ä¾æ¬¡æ›´æ–°çŠ¶æ€

# åˆå§‹çŠ¶æ€
state = {"messages": []}

# èŠ‚ç‚¹1
state = apply_reducer(state, {"messages": [msg1]})
# state = {"messages": [msg1]}

# èŠ‚ç‚¹2
state = apply_reducer(state, {"messages": [msg2]})
# state = {"messages": [msg1, msg2]}

# èŠ‚ç‚¹3
state = apply_reducer(state, {"messages": [msg3]})
# state = {"messages": [msg1, msg2, msg3]}
```

---

### 2.2 Checkpointer æŒä¹…åŒ–

Checkpointer å…è®¸ä¿å­˜å’Œæ¢å¤ Agent çš„çŠ¶æ€ï¼Œæ”¯æŒï¼š
- ğŸ’¾ ä¼šè¯æŒä¹…åŒ–ï¼šå…³é—­ç¨‹åºåæ¢å¤å¯¹è¯
- ğŸ”„ ä¸­æ–­æ¢å¤ï¼šåœ¨ä»»æ„èŠ‚ç‚¹æš‚åœå’Œæ¢å¤
- ğŸ•°ï¸ æ—¶é—´æ—…è¡Œï¼šå›åˆ°å†å²çŠ¶æ€

#### 2.2.1 InMemorySaver - å†…å­˜å­˜å‚¨

**åŸºç¡€ä½¿ç”¨**

```python
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, MessagesState, END
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

# å®šä¹‰èŠ‚ç‚¹
def call_model(state):
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.invoke(state["messages"])
    return {"messages": [response]}

# æ„å»ºå›¾
workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.set_entry_point("agent")
workflow.add_edge("agent", END)

# ä½¿ç”¨ InMemorySaver
memory = InMemorySaver()
app = workflow.compile(checkpointer=memory)

# ä¼šè¯1
config1 = {"configurable": {"thread_id": "conversation-1"}}
result1 = app.invoke(
    {"messages": [HumanMessage(content="æˆ‘å«Alice")]},
    config=config1
)

# ä¼šè¯2ï¼ˆç›¸åŒ thread_idï¼Œä¼šç»§ç»­ä¹‹å‰çš„å¯¹è¯ï¼‰
result2 = app.invoke(
    {"messages": [HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]},
    config=config1
)

print(result2["messages"][-1].content)
# è¾“å‡ºï¼š"æ‚¨å« Alice"ï¼ˆè®°ä½äº†ä¹‹å‰çš„å¯¹è¯ï¼‰

# ä¼šè¯3ï¼ˆä¸åŒ thread_idï¼Œæ˜¯å…¨æ–°çš„å¯¹è¯ï¼‰
config2 = {"configurable": {"thread_id": "conversation-2"}}
result3 = app.invoke(
    {"messages": [HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]},
    config=config2
)

print(result3["messages"][-1].content)
# è¾“å‡ºï¼š"æŠ±æ­‰ï¼Œæˆ‘ä¸çŸ¥é“æ‚¨çš„åå­—"
```

**InMemorySaver ç‰¹ç‚¹**

- âœ… ç®€å•ï¼šæ— éœ€é…ç½®ï¼Œå¼€ç®±å³ç”¨
- âœ… å¿«é€Ÿï¼šå†…å­˜æ“ä½œï¼Œæ€§èƒ½é«˜
- âŒ æ˜“å¤±ï¼šç¨‹åºé‡å¯åæ•°æ®ä¸¢å¤±
- âŒ ä¸å…±äº«ï¼šä¸åŒè¿›ç¨‹é—´æ— æ³•å…±äº«

**é€‚ç”¨åœºæ™¯**ï¼š
- å¼€å‘æµ‹è¯•
- å•ä¼šè¯çŸ­æœŸå¯¹è¯
- ä¸éœ€è¦æŒä¹…åŒ–çš„åœºæ™¯

#### 2.2.2 SqliteSaver - æœ¬åœ°æŒä¹…åŒ–

**åŸºç¡€ä½¿ç”¨**

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# è¿æ¥åˆ° SQLite æ•°æ®åº“
with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    # æ‰§è¡Œå¯¹è¯
    config = {"configurable": {"thread_id": "user-123"}}

    result1 = app.invoke(
        {"messages": [HumanMessage(content="è®°ä½ï¼šæˆ‘å–œæ¬¢çŒ«")]},
        config=config
    )

    result2 = app.invoke(
        {"messages": [HumanMessage(content="æˆ‘å–œæ¬¢ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ")]},
        config=config
    )

    print(result2["messages"][-1].content)
    # è¾“å‡ºï¼š"æ‚¨å–œæ¬¢çŒ«"

# ç¨‹åºé‡å¯å
with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "user-123"}}

    # ç»§ç»­ä¹‹å‰çš„å¯¹è¯
    result3 = app.invoke(
        {"messages": [HumanMessage(content="æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ")]},
        config=config
    )

    print(result3["messages"][-1].content)
    # è¾“å‡ºï¼š"æ‚¨å–œæ¬¢çŒ«"ï¼ˆæ¢å¤äº†ä¹‹å‰çš„çŠ¶æ€ï¼‰
```

**é«˜çº§ç”¨æ³•ï¼šæŸ¥çœ‹å†å²çŠ¶æ€**

```python
from langgraph.checkpoint.sqlite import SqliteSaver

with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "user-123"}}

    # è·å–çŠ¶æ€å†å²
    history = app.get_state_history(config)

    print("=== çŠ¶æ€å†å² ===")
    for i, state in enumerate(history):
        print(f"\nçŠ¶æ€ {i}:")
        print(f"  Checkpoint ID: {state.config['configurable'].get('checkpoint_id')}")
        print(f"  Messages: {len(state.values.get('messages', []))}")
```

**SqliteSaver ç‰¹ç‚¹**

- âœ… æŒä¹…åŒ–ï¼šç¨‹åºé‡å¯åæ•°æ®ä¿ç•™
- âœ… è½»é‡ï¼šæ— éœ€é¢å¤–æœåŠ¡ï¼Œå•æ–‡ä»¶æ•°æ®åº“
- âš ï¸ å•æœºï¼šä»…æ”¯æŒå•æœºéƒ¨ç½²
- âš ï¸ å¹¶å‘ï¼šå¹¶å‘å†™å…¥æ€§èƒ½æœ‰é™

**é€‚ç”¨åœºæ™¯**ï¼š
- æœ¬åœ°åº”ç”¨
- å•æœºéƒ¨ç½²
- ä¸­å°è§„æ¨¡æ•°æ®

#### 2.2.3 PostgresSaver - ç”Ÿäº§çº§æŒä¹…åŒ–

**åŸºç¡€ä½¿ç”¨**

```python
from langgraph.checkpoint.postgres import PostgresSaver

# è¿æ¥åˆ° PostgreSQL
DB_URI = "postgresql://user:password@localhost:5432/langchain"

with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "user-123"}}

    result = app.invoke(
        {"messages": [HumanMessage(content="Hello")]},
        config=config
    )
```

**ç”Ÿäº§ç¯å¢ƒé…ç½®**

```python
import os
from langgraph.checkpoint.postgres import PostgresSaver
from psycopg_pool import ConnectionPool

# è¿æ¥æ± 
pool = ConnectionPool(
    conninfo=os.environ["DATABASE_URL"],
    min_size=1,
    max_size=10,
    timeout=30
)

# ä½¿ç”¨è¿æ¥æ± 
checkpointer = PostgresSaver(pool)

app = workflow.compile(checkpointer=checkpointer)
```

**PostgresSaver ç‰¹ç‚¹**

- âœ… ç”Ÿäº§çº§ï¼šé«˜æ€§èƒ½ã€é«˜å¯é 
- âœ… åˆ†å¸ƒå¼ï¼šæ”¯æŒå¤šå®ä¾‹éƒ¨ç½²
- âœ… å¹¶å‘ï¼šæ”¯æŒé«˜å¹¶å‘è¯»å†™
- âœ… ACIDï¼šäº‹åŠ¡ä¿è¯
- âš ï¸ å¤æ‚ï¼šéœ€è¦è¿ç»´ PostgreSQL

**é€‚ç”¨åœºæ™¯**ï¼š
- ç”Ÿäº§ç¯å¢ƒ
- å¤šå®ä¾‹éƒ¨ç½²
- é«˜å¹¶å‘åœºæ™¯
- éœ€è¦ ACID ä¿è¯

#### 2.2.4 è‡ªå®šä¹‰ Checkpointer

**Checkpointer æ¥å£**

```python
from langgraph.checkpoint.base import (
    BaseCheckpointSaver,
    Checkpoint,
    CheckpointMetadata,
    CheckpointTuple,
    ChannelVersions
)
from langchain_core.runnables.config import RunnableConfig
from typing import Optional, Iterator, Dict, Any, Sequence, Tuple

class CustomCheckpointer(BaseCheckpointSaver):
    """è‡ªå®šä¹‰ Checkpointer - ç¬¦åˆæœ€æ–°ç‰ˆæœ¬æ¥å£"""

    def __init__(self):
        super().__init__()
        # è¿™é‡Œå¯ä»¥åˆå§‹åŒ–å­˜å‚¨åç«¯ï¼Œå¦‚ Redisã€MongoDB ç­‰
        self.storage = {}  # ç®€å•ç¤ºä¾‹ç”¨å­—å…¸å­˜å‚¨

    def put(
        self,
        config: RunnableConfig,
        checkpoint: Checkpoint,
        metadata: CheckpointMetadata,
        new_versions: ChannelVersions
    ) -> RunnableConfig:
        """ä¿å­˜ checkpoint

        Args:
            config: è¿è¡Œé…ç½®
            checkpoint: è¦ä¿å­˜çš„æ£€æŸ¥ç‚¹
            metadata: æ£€æŸ¥ç‚¹å…ƒæ•°æ®
            new_versions: é€šé“ç‰ˆæœ¬ä¿¡æ¯

        Returns:
            æ›´æ–°åçš„é…ç½®
        """
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = checkpoint["id"]

        # æ„é€ å­˜å‚¨é”®
        key = f"{thread_id}:{checkpoint_ns}:{checkpoint_id}"

        # ä¿å­˜åˆ°å­˜å‚¨ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜åˆ° Redisã€MongoDB ç­‰ï¼‰
        self.storage[key] = {
            "checkpoint": checkpoint,
            "metadata": metadata,
            "config": config,
            "new_versions": new_versions
        }

        return config

    def get(self, config: RunnableConfig) -> Optional[Checkpoint]:
        """è·å–æœ€æ–°çš„ checkpoint"""
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = config["configurable"].get("checkpoint_id")

        if checkpoint_id:
            # è·å–ç‰¹å®šçš„ checkpoint
            key = f"{thread_id}:{checkpoint_ns}:{checkpoint_id}"
            if key in self.storage:
                return self.storage[key]["checkpoint"]
        else:
            # è·å–æœ€æ–°çš„ checkpoint
            prefix = f"{thread_id}:{checkpoint_ns}:"
            matching_keys = [k for k in self.storage.keys() if k.startswith(prefix)]

            if matching_keys:
                latest_key = max(matching_keys,
                               key=lambda k: self.storage[k]["checkpoint"]["ts"])
                return self.storage[latest_key]["checkpoint"]

        return None

    def list(
        self,
        config: Optional[RunnableConfig],
        *,
        filter: Optional[Dict[str, Any]] = None,
        before: Optional[RunnableConfig] = None,
        limit: Optional[int] = None
    ) -> Iterator[CheckpointTuple]:
        """åˆ—å‡ºæ‰€æœ‰ checkpoint"""
        if config is None:
            items = list(self.storage.items())
        else:
            thread_id = config["configurable"]["thread_id"]
            checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
            prefix = f"{thread_id}:{checkpoint_ns}:"

            items = [(k, v) for k, v in self.storage.items()
                    if k.startswith(prefix)]

        # æŒ‰æ—¶é—´æˆ³å€’åºæ’åº
        items.sort(key=lambda x: x[1]["checkpoint"]["ts"], reverse=True)

        # åº”ç”¨ limit
        if limit:
            items = items[:limit]

        # ç”Ÿæˆ CheckpointTuple
        for key, value in items:
            yield CheckpointTuple(
                config=value["config"],
                checkpoint=value["checkpoint"],
                metadata=value["metadata"],
                parent_config=None
            )

    def put_writes(
        self,
        config: RunnableConfig,
        writes: Sequence[Tuple[str, Any]],
        task_id: str,
        task_path: str = ""
    ) -> None:
        """ä¿å­˜å¾…å†™å…¥çš„æ•°æ®"""
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = config["configurable"].get("checkpoint_id")

        key = f"{thread_id}:{checkpoint_ns}:{checkpoint_id}:writes:{task_id}"

        self.storage[key] = {
            "writes": writes,
            "task_id": task_id,
            "task_path": task_path
        }
```

**ç¤ºä¾‹ï¼šRedis Checkpointer**

```python
import json
import redis
from langgraph.checkpoint.base import (
    BaseCheckpointSaver,
    Checkpoint,
    CheckpointMetadata,
    CheckpointTuple,
    ChannelVersions
)
from langchain_core.runnables.config import RunnableConfig

class RedisCheckpointer(BaseCheckpointSaver):
    """åŸºäº Redis çš„ Checkpointer"""

    def __init__(self, redis_client: redis.Redis):
        super().__init__()
        self.redis = redis_client

    def put(
        self,
        config: RunnableConfig,
        checkpoint: Checkpoint,
        metadata: CheckpointMetadata,
        new_versions: ChannelVersions
    ) -> RunnableConfig:
        """ä¿å­˜ checkpoint åˆ° Redis"""
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = checkpoint["id"]

        # æ„é€  Redis key
        key = f"checkpoint:{thread_id}:{checkpoint_ns}:{checkpoint_id}"

        # åºåˆ—åŒ–æ•°æ®
        data = {
            "checkpoint": checkpoint,
            "metadata": metadata,
            "config": config,
            "new_versions": new_versions,
            "ts": checkpoint["ts"]  # ç”¨äºæ’åº
        }

        # ä¿å­˜åˆ° Redis
        self.redis.set(key, json.dumps(data, default=str))

        # æ·»åŠ åˆ°æœ‰åºé›†åˆä»¥ä¾¿æŒ‰æ—¶é—´æ’åº
        score_key = f"checkpoints:{thread_id}:{checkpoint_ns}"
        self.redis.zadd(score_key, {checkpoint_id: checkpoint["ts"]})

        return config

    def get(self, config: RunnableConfig) -> Optional[Checkpoint]:
        """ä» Redis è·å– checkpoint"""
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = config["configurable"].get("checkpoint_id")

        if checkpoint_id:
            # è·å–ç‰¹å®šçš„ checkpoint
            key = f"checkpoint:{thread_id}:{checkpoint_ns}:{checkpoint_id}"
            data = self.redis.get(key)
            if data:
                return json.loads(data)["checkpoint"]
        else:
            # è·å–æœ€æ–°çš„ checkpoint
            score_key = f"checkpoints:{thread_id}:{checkpoint_ns}"
            latest_ids = self.redis.zrevrange(score_key, 0, 0)

            if latest_ids:
                latest_id = latest_ids[0].decode() if isinstance(latest_ids[0], bytes) else latest_ids[0]
                key = f"checkpoint:{thread_id}:{checkpoint_ns}:{latest_id}"
                data = self.redis.get(key)
                if data:
                    return json.loads(data)["checkpoint"]

        return None

    def list(
        self,
        config: Optional[RunnableConfig],
        *,
        filter: Optional[dict] = None,
        before: Optional[RunnableConfig] = None,
        limit: Optional[int] = None
    ) -> Iterator[CheckpointTuple]:
        """ä» Redis åˆ—å‡º checkpoints"""
        if config:
            thread_id = config["configurable"]["thread_id"]
            checkpoint_ns = config["configurable"].get("checkpoint_ns", "")

            score_key = f"checkpoints:{thread_id}:{checkpoint_ns}"

            # è·å–æ‰€æœ‰ checkpoint IDsï¼ŒæŒ‰æ—¶é—´å€’åº
            checkpoint_ids = self.redis.zrevrange(
                score_key,
                0,
                limit - 1 if limit else -1
            )

            for checkpoint_id in checkpoint_ids:
                cid = checkpoint_id.decode() if isinstance(checkpoint_id, bytes) else checkpoint_id
                key = f"checkpoint:{thread_id}:{checkpoint_ns}:{cid}"
                data = self.redis.get(key)

                if data:
                    parsed = json.loads(data)
                    yield CheckpointTuple(
                        config=parsed["config"],
                        checkpoint=parsed["checkpoint"],
                        metadata=parsed["metadata"],
                        parent_config=None
                    )

    def put_writes(
        self,
        config: RunnableConfig,
        writes: Sequence[Tuple[str, Any]],
        task_id: str,
        task_path: str = ""
    ) -> None:
        """ä¿å­˜å†™å…¥æ•°æ®åˆ° Redis"""
        thread_id = config["configurable"]["thread_id"]
        checkpoint_ns = config["configurable"].get("checkpoint_ns", "")
        checkpoint_id = config["configurable"].get("checkpoint_id")

        key = f"writes:{thread_id}:{checkpoint_ns}:{checkpoint_id}:{task_id}"

        data = {
            "writes": list(writes),
            "task_id": task_id,
            "task_path": task_path
        }

        self.redis.set(key, json.dumps(data, default=str))

# ä½¿ç”¨
redis_client = redis.Redis(host="localhost", port=6379)
checkpointer = RedisCheckpointer(redis_client)

app = workflow.compile(checkpointer=checkpointer)
```

---

### 2.3 Memory ç³»ç»Ÿ

#### 2.3.1 Short-term Memoryï¼ˆä¼šè¯å†…è®°å¿†ï¼‰

**Short-term Memory = Checkpointer + Thread ID**

```python
from langgraph.checkpoint.memory import InMemorySaver

memory = InMemorySaver()
app = workflow.compile(checkpointer=memory)

# ä¼šè¯å†…è®°å¿†ï¼šä½¿ç”¨ç›¸åŒçš„ thread_id
config = {"configurable": {"thread_id": "session-123"}}

# ç¬¬1è½®
app.invoke({"messages": [HumanMessage("æˆ‘å« Alice")]}, config=config)

# ç¬¬2è½®ï¼ˆè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼‰
app.invoke({"messages": [HumanMessage("æˆ‘å«ä»€ä¹ˆï¼Ÿ")]}, config=config)

# ç¬¬3è½®ï¼ˆç»§ç»­è®°ä½ï¼‰
app.invoke({"messages": [HumanMessage("å†è¯´ä¸€é")]}, config=config)
```

**æ§åˆ¶è®°å¿†çª—å£**

```python
# é—®é¢˜ï¼šå¯¹è¯å¤ªé•¿ï¼Œè¶…è¿‡æ¨¡å‹ token é™åˆ¶

# è§£å†³æ–¹æ¡ˆï¼šé™åˆ¶æ¶ˆæ¯æ•°é‡
def trim_messages(state):
    """ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯"""
    messages = state["messages"]
    max_messages = 10

    if len(messages) > max_messages:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘çš„æ¶ˆæ¯
        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
        recent_msgs = messages[-max_messages:]

        return {"messages": system_msgs + recent_msgs}

    return {}

# æ·»åŠ åˆ° workflow
workflow.add_node("trim", trim_messages)
workflow.add_edge("trim", "agent")
```

#### 2.3.2 Long-term Memoryï¼ˆè·¨ä¼šè¯è®°å¿†ï¼‰

**è·¨ä¼šè¯è®°å¿†éœ€æ±‚**

```python
# åœºæ™¯ï¼šç”¨æˆ·åœ¨ä¸åŒä¼šè¯ä¸­éƒ½èƒ½è¢«è®°ä½

# ä¼šè¯1ï¼ˆ2024-01-01ï¼‰
config1 = {"configurable": {"thread_id": "session-001", "user_id": "alice"}}
app.invoke({"messages": [HumanMessage("æˆ‘å« Aliceï¼Œå–œæ¬¢çŒ«")]}, config=config1)

# ä¼šè¯2ï¼ˆ2024-01-02ï¼Œæ–°çš„ thread_idï¼‰
config2 = {"configurable": {"thread_id": "session-002", "user_id": "alice"}}
app.invoke({"messages": [HumanMessage("æˆ‘å–œæ¬¢ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ")]}, config=config2)

# æœŸæœ›ï¼šAgent èƒ½è®°ä½ Alice å–œæ¬¢çŒ«ï¼ˆè·¨ä¼šè¯è®°å¿†ï¼‰
```

**å®ç°æ–¹å¼ï¼šå¤–éƒ¨ Memory Store**

```python
# ç®€åŒ–çš„é•¿æœŸè®°å¿†å®ç°

class LongTermMemory:
    """é•¿æœŸè®°å¿†å­˜å‚¨"""

    def __init__(self):
        self.memories = {}

    def save(self, user_id: str, key: str, value: str):
        """ä¿å­˜è®°å¿†"""
        if user_id not in self.memories:
            self.memories[user_id] = {}
        self.memories[user_id][key] = value

    def recall(self, user_id: str, key: str) -> str:
        """å›å¿†"""
        return self.memories.get(user_id, {}).get(key, "")

    def search(self, user_id: str, query: str) -> list[str]:
        """æœç´¢ç›¸å…³è®°å¿†"""
        user_memories = self.memories.get(user_id, {})
        # ç®€åŒ–ï¼šè¿”å›æ‰€æœ‰è®°å¿†ï¼ˆå®é™…åº”è¯¥ç”¨å‘é‡æœç´¢ï¼‰
        return list(user_memories.values())

# å…¨å±€è®°å¿†å­˜å‚¨
long_term_memory = LongTermMemory()

# åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨
def agent_with_memory(state):
    user_id = state.get("user_id")

    # å›å¿†ç›¸å…³è®°å¿†
    memories = long_term_memory.search(user_id, state["messages"][-1].content)

    # æ„å»ºåŒ…å«è®°å¿†çš„æç¤º
    system_prompt = f"ç”¨æˆ·è®°å¿†ï¼š{', '.join(memories)}"

    model = ChatOpenAI(model="gpt-4")
    response = model.invoke([
        SystemMessage(content=system_prompt),
        *state["messages"]
    ])

    # æå–å¹¶ä¿å­˜æ–°çš„è®°å¿†
    if "æˆ‘å«" in state["messages"][-1].content:
        name = extract_name(state["messages"][-1].content)
        long_term_memory.save(user_id, "name", name)

    return {"messages": [response]}
```

#### 2.3.3 Store API - å®˜æ–¹é•¿æœŸè®°å¿†æ–¹æ¡ˆ

**Store API ç®€ä»‹**

LangGraph 1.0 å¼•å…¥çš„ `Store` API æ˜¯å®˜æ–¹æ¨èçš„é•¿æœŸè®°å¿†å­˜å‚¨æ–¹æ¡ˆï¼Œæ”¯æŒè·¨ä¼šè¯çš„æŒä¹…åŒ–è®°å¿†ã€è¯­ä¹‰æœç´¢å’Œå‘½åç©ºé—´éš”ç¦»ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š

1. **åˆ†å±‚å‘½åç©ºé—´**ï¼šæ”¯æŒå¤šå±‚çº§çš„è®°å¿†éš”ç¦»ï¼ˆå¦‚ `("user-123", "preferences")`ï¼‰
2. **è¯­ä¹‰æœç´¢**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢ç›¸å…³è®°å¿†
3. **çµæ´»å­˜å‚¨**ï¼šæ”¯æŒ InMemoryStoreã€PostgresStore ç­‰å¤šç§åç«¯
4. **è‡ªåŠ¨è¿‡æœŸ**ï¼šå¯è®¾ç½®è®°å¿†çš„ç”Ÿå‘½å‘¨æœŸ

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langgraph.store.memory import InMemoryStore

# åˆ›å»º Store
store = InMemoryStore()

# åˆ†å±‚å‘½åç©ºé—´: (user_id, category)
namespace = ("user-123", "preferences")

# å†™å…¥è®°å¿†ï¼ˆputï¼‰
store.put(namespace, "favorite_color", {"color": "blue", "timestamp": "2025-11-17"})
store.put(namespace, "interests", {"items": ["AI", "Python", "LangChain"]})

# è¯»å–è®°å¿†ï¼ˆgetï¼‰
color = store.get(namespace, "favorite_color")
print(color)  # {"color": "blue", "timestamp": "2025-11-17"}

# åˆ—å‡ºå‘½åç©ºé—´ä¸‹çš„æ‰€æœ‰è®°å¿†ï¼ˆlistï¼‰
all_prefs = store.list(namespace)
for item in all_prefs:
    print(f"{item.key}: {item.value}")

# åˆ é™¤è®°å¿†ï¼ˆdeleteï¼‰
store.delete(namespace, "favorite_color")
```

**è¯­ä¹‰æœç´¢ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰**ï¼š

```python
from langgraph.store.postgres import PostgresStore
from langchain_openai import OpenAIEmbeddings

# ä½¿ç”¨ PostgreSQL åç«¯ï¼ˆæ”¯æŒå‘é‡æœç´¢ï¼‰
store = PostgresStore(
    conn_string="postgresql://user:pass@localhost/db",
    embeddings=OpenAIEmbeddings()
)

namespace = ("user-123", "conversations")

# å­˜å‚¨å¯¹è¯è®°å¿†
store.put(namespace, "mem1", {"content": "ç”¨æˆ·å–œæ¬¢çŒ«"})
store.put(namespace, "mem2", {"content": "ç”¨æˆ·ä½åœ¨åŒ—äº¬"})
store.put(namespace, "mem3", {"content": "ç”¨æˆ·æ˜¯Pythonå¼€å‘è€…"})

# è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
results = store.search(
    namespace,
    query="ç”¨æˆ·çš„å® ç‰©åå¥½",
    limit=2,
    filter={"type": "preference"}  # å¯é€‰è¿‡æ»¤æ¡ä»¶
)

for result in results:
    print(f"ç›¸å…³åº¦: {result.score}, å†…å®¹: {result.value['content']}")
# è¾“å‡º: "ç”¨æˆ·å–œæ¬¢çŒ«" (é«˜ç›¸å…³åº¦)
```

**åœ¨ LangGraph ä¸­ä½¿ç”¨ Store**ï¼š

```python
from langgraph.graph import StateGraph, MessagesState
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import InMemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage

class UserState(MessagesState):
    user_id: str

def create_agent_with_store():
    """åˆ›å»ºå¸¦é•¿æœŸè®°å¿†çš„ Agent"""
    # åˆå§‹åŒ– Store
    store = InMemoryStore()
    checkpointer = InMemorySaver()
    model = ChatOpenAI(model="gpt-4")

    def agent_node(state: UserState, *, store):
        """Agent èŠ‚ç‚¹ï¼ˆè‡ªåŠ¨æ¥æ”¶ store å‚æ•°ï¼‰"""
        user_id = state["user_id"]
        namespace = (user_id, "profile")

        # è¯»å–ç”¨æˆ·è®°å¿†
        name_mem = store.get(namespace, "name")
        interests_mem = store.get(namespace, "interests")

        # æ„å»ºç³»ç»Ÿæç¤º
        memory_context = ""
        if name_mem:
            memory_context += f"\nç”¨æˆ·å§“å: {name_mem['value']}"
        if interests_mem:
            memory_context += f"\nå…´è¶£çˆ±å¥½: {', '.join(interests_mem['items'])}"

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke([
            SystemMessage(content=f"ä½ æ˜¯åŠ©æ‰‹ã€‚ç”¨æˆ·ä¿¡æ¯:{memory_context}"),
            *state["messages"]
        ])

        # æå–å¹¶ä¿å­˜æ–°è®°å¿†ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
        user_msg = state["messages"][-1].content
        if "æˆ‘å«" in user_msg:
            # æå–å§“åå¹¶ä¿å­˜
            name = user_msg.split("æˆ‘å«")[1].strip().split()[0]
            store.put(namespace, "name", {"value": name})

        return {"messages": [response]}

    # æ„å»ºå›¾
    workflow = StateGraph(UserState)
    workflow.add_node("agent", agent_node)
    workflow.set_entry_point("agent")
    workflow.set_finish_point("agent")

    # ç¼–è¯‘æ—¶ä¼ å…¥ store
    return workflow.compile(
        checkpointer=checkpointer,
        store=store  # âœ… å…³é”®ï¼šä¼ å…¥ store
    )

# ä½¿ç”¨
app = create_agent_with_store()

# ç¬¬ä¸€æ¬¡å¯¹è¯ï¼šç”¨æˆ·è‡ªæˆ‘ä»‹ç»
config = {"configurable": {"thread_id": "thread-1"}}
result = app.invoke({
    "messages": [("user", "ä½ å¥½ï¼Œæˆ‘å«Alice")],
    "user_id": "user-123"
}, config)

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼šAgent èƒ½è®°ä½ç”¨æˆ·å§“åï¼ˆè·¨ä¼šè¯ï¼‰
result = app.invoke({
    "messages": [("user", "ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ")],
    "user_id": "user-123"
}, config)
# Agent: "å½“ç„¶è®°å¾—ï¼ŒAliceï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
```

**Store æœ€ä½³å®è·µ**ï¼š

1. **å‘½åç©ºé—´è®¾è®¡**ï¼š
   ```python
   # ç”¨æˆ·çº§åˆ«
   ("user-123", "profile")        # ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
   ("user-123", "preferences")    # ç”¨æˆ·åå¥½
   ("user-123", "conversations")  # å¯¹è¯å†å²
   
   # ç»„ç»‡çº§åˆ«
   ("org-456", "team-settings")
   ("org-456", "shared-knowledge")
   ```

2. **è®°å¿†æ›´æ–°ç­–ç•¥**ï¼š
   ```python
   # å¢é‡æ›´æ–°
   existing = store.get(namespace, "interests") or {"items": []}
   existing["items"].append("æ–°å…´è¶£")
   store.put(namespace, "interests", existing)
   
   # å¸¦æ—¶é—´æˆ³
   store.put(namespace, "last_login", {
       "time": datetime.now().isoformat(),
       "ip": "192.168.1.1"
   })
   ```

3. **å†…å­˜ç®¡ç†**ï¼š
   ```python
   # å®šæœŸæ¸…ç†æ—§è®°å¿†
   all_items = store.list(namespace)
   for item in all_items:
       if is_expired(item.value.get("timestamp")):
           store.delete(namespace, item.key)
   ```

**Store vs Checkpointer**ï¼š

| ç‰¹æ€§ | Store (é•¿æœŸè®°å¿†) | Checkpointer (çŸ­æœŸè®°å¿†) |
|------|-----------------|----------------------|
| **ç”Ÿå‘½å‘¨æœŸ** | è·¨ä¼šè¯æŒä¹…åŒ– | ä¼šè¯å†…ä¸´æ—¶ |
| **æ•°æ®èŒƒå›´** | ç”¨æˆ·/ç»„ç»‡çº§åˆ« | çº¿ç¨‹çº§åˆ« |
| **æœç´¢èƒ½åŠ›** | æ”¯æŒè¯­ä¹‰æœç´¢ | æ— æœç´¢åŠŸèƒ½ |
| **ä½¿ç”¨åœºæ™¯** | ç”¨æˆ·ç”»åƒã€çŸ¥è¯†åº“ | å¯¹è¯å†å²ã€çŠ¶æ€æ¢å¤ |
| **å­˜å‚¨å†…å®¹** | ç»“æ„åŒ–æ•°æ® | State å¿«ç…§ |

---

#### 2.3.4 LangMem SDK

**LangMem ç®€ä»‹**

LangMem SDK æ˜¯ LangChain æä¾›çš„é•¿æœŸè®°å¿†ç®¡ç†å·¥å…·ï¼Œç”¨äºåœ¨ LangGraph ä¸­å®ç°æŒä¹…åŒ–è®°å¿†ç³»ç»Ÿã€‚å®ƒæä¾›äº†è®°å¿†çš„æå–ã€ç®¡ç†å’Œæœç´¢åŠŸèƒ½ã€‚

**å®‰è£…**

```bash
pip install langmem
```

**æ ¸å¿ƒæ¦‚å¿µ**

LangMem æä¾›äº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
1. **Memory Manager**: ä»å¯¹è¯ä¸­æå–å’Œç®¡ç†è®°å¿†
2. **Memory Searcher**: æœç´¢ç›¸å…³è®°å¿†

**ä½¿ç”¨ Memory Manager**

```python
from langmem import create_memory_manager, create_memory_searcher
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState
from langchain_core.messages import HumanMessage, AIMessage

# åˆ›å»ºè®°å¿†ç®¡ç†å™¨èŠ‚ç‚¹
memory_manager = create_memory_manager(
    model=ChatOpenAI(model="gpt-4"),
    # å¯é€‰ï¼šè‡ªå®šä¹‰è®°å¿†æå–æŒ‡ä»¤
    instructions="æå–ç”¨æˆ·åå¥½ã€é‡è¦äº‹å®å’Œäº¤äº’æ¨¡å¼"
)

# åˆ›å»ºè®°å¿†æœç´¢å™¨èŠ‚ç‚¹
memory_searcher = create_memory_searcher(
    model=ChatOpenAI(model="gpt-4"),
    prompt="æœç´¢ä¸å½“å‰å¯¹è¯ç›¸å…³çš„è®°å¿†"
)

# åœ¨ LangGraph ä¸­é›†æˆè®°å¿†ç³»ç»Ÿ
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
from operator import add

class MemoryState(TypedDict):
    messages: Annotated[list, add]
    memories: list
    user_id: str

# æ„å»ºå¸¦è®°å¿†çš„å·¥ä½œæµ
def build_memory_graph():
    workflow = StateGraph(MemoryState)

    # æ·»åŠ è®°å¿†æœç´¢èŠ‚ç‚¹
    async def search_memories(state: MemoryState):
        """æœç´¢ç›¸å…³è®°å¿†"""
        # memory_searcher ä¼šè‡ªåŠ¨å¤„ç†æ¶ˆæ¯å¹¶è¿”å›ç›¸å…³è®°å¿†
        memories = await memory_searcher.ainvoke(state)
        return {"memories": memories}

    # æ·»åŠ  Agent èŠ‚ç‚¹
    def agent(state: MemoryState):
        """ä½¿ç”¨è®°å¿†çš„ Agent"""
        model = ChatOpenAI(model="gpt-4")

        # æ„å»ºå¸¦è®°å¿†çš„ç³»ç»Ÿæç¤º
        memory_context = ""
        if state.get("memories"):
            memory_context = "\nç›¸å…³è®°å¿†ï¼š\n" + "\n".join([
                f"- {mem['content']}" for mem in state["memories"]
            ])

        response = model.invoke([
            SystemMessage(content=f"ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†çš„åŠ©æ‰‹ã€‚{memory_context}"),
            *state["messages"]
        ])

        return {"messages": [response]}

    # æ·»åŠ è®°å¿†ç®¡ç†èŠ‚ç‚¹
    async def manage_memories(state: MemoryState):
        """æå–å¹¶ä¿å­˜æ–°è®°å¿†"""
        # memory_manager ä¼šè‡ªåŠ¨ä»å¯¹è¯ä¸­æå–è®°å¿†
        extracted_memories = await memory_manager.ainvoke(state)
        # è®°å¿†ä¼šè‡ªåŠ¨ä¿å­˜åˆ°å­˜å‚¨ä¸­
        return {}

    # æ„å»ºå·¥ä½œæµ
    workflow.add_node("search_memories", search_memories)
    workflow.add_node("agent", agent)
    workflow.add_node("manage_memories", manage_memories)

    # å®šä¹‰è¾¹
    workflow.add_edge("search_memories", "agent")
    workflow.add_edge("agent", "manage_memories")
    workflow.add_edge("manage_memories", END)

    # è®¾ç½®å…¥å£
    workflow.set_entry_point("search_memories")

    return workflow.compile()

# ä½¿ç”¨ç¤ºä¾‹
app = build_memory_graph()

# è¿è¡Œå¯¹è¯
result = await app.ainvoke({
    "messages": [HumanMessage("æˆ‘å« Aliceï¼Œæˆ‘å–œæ¬¢ç§‘å¹»ç”µå½±")],
    "user_id": "alice",
    "memories": []
})

# åç»­å¯¹è¯ä¼šè®°ä½ä¹‹å‰çš„ä¿¡æ¯
result = await app.ainvoke({
    "messages": [HumanMessage("æ¨èä¸€éƒ¨ç”µå½±ç»™æˆ‘")],
    "user_id": "alice",
    "memories": []
})
```

**è‡ªå®šä¹‰è®°å¿†å­˜å‚¨**

```python
from langgraph.store import BaseStore
import json

class CustomMemoryStore(BaseStore):
    """è‡ªå®šä¹‰è®°å¿†å­˜å‚¨å®ç°"""

    def __init__(self):
        self.store = {}

    async def aget(self, namespace: tuple, key: str):
        """è·å–è®°å¿†"""
        ns_key = "/".join(namespace)
        return self.store.get(ns_key, {}).get(key)

    async def aput(self, namespace: tuple, key: str, value: dict):
        """ä¿å­˜è®°å¿†"""
        ns_key = "/".join(namespace)
        if ns_key not in self.store:
            self.store[ns_key] = {}
        self.store[ns_key][key] = value

    async def asearch(self, namespace: tuple, query: str, limit: int = 10):
        """æœç´¢è®°å¿†"""
        ns_key = "/".join(namespace)
        memories = self.store.get(ns_key, {})
        # ç®€å•å®ç°ï¼šè¿”å›æ‰€æœ‰è®°å¿†
        # å®é™…åº”è¯¥ä½¿ç”¨å‘é‡æœç´¢
        return list(memories.values())[:limit]

# ä½¿ç”¨è‡ªå®šä¹‰å­˜å‚¨
store = CustomMemoryStore()

# åˆ›å»ºå¸¦è‡ªå®šä¹‰å­˜å‚¨çš„è®°å¿†ç®¡ç†å™¨
memory_manager = create_memory_manager(
    model=ChatOpenAI(model="gpt-4"),
    # store å‚æ•°éœ€è¦é€šè¿‡ LangGraph é…ç½®ä¼ é€’
)
```

**å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼šæ„å»ºè®°å¿†å¢å¼ºçš„å®¢æœåŠ©æ‰‹**

```python
from langmem import create_memory_manager, create_memory_searcher
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, END
from langchain_core.messages import SystemMessage
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore

class CustomerServiceState(MessagesState):
    """å®¢æœçŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å’Œè®°å¿†"""
    memories: list = []
    user_id: str = ""

def build_customer_service_agent():
    """æ„å»ºå¸¦è®°å¿†çš„å®¢æœåŠ©æ‰‹"""

    # åˆå§‹åŒ–ç»„ä»¶
    model = ChatOpenAI(model="gpt-4")
    store = InMemoryStore()

    # åˆ›å»ºè®°å¿†ç®¡ç†ç»„ä»¶
    memory_manager = create_memory_manager(
        model=model,
        instructions="""
        æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        1. ç”¨æˆ·åå¥½å’Œéœ€æ±‚
        2. ç”¨æˆ·é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
        3. ç”¨æˆ·æ»¡æ„åº¦å’Œåé¦ˆ
        4. é‡è¦çš„ä¸ªäººä¿¡æ¯ï¼ˆå¦‚ä¼šå‘˜ç­‰çº§ã€è´­ä¹°å†å²ï¼‰
        """
    )

    memory_searcher = create_memory_searcher(
        model=model,
        prompt="æœç´¢ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å†å²è®°å½•å’Œè§£å†³æ–¹æ¡ˆ"
    )

    # åˆ›å»ºå·¥ä½œæµ
    workflow = StateGraph(CustomerServiceState)

    # æœç´¢è®°å¿†èŠ‚ç‚¹
    async def search_memories(state: CustomerServiceState):
        """æœç´¢ç”¨æˆ·å†å²è®°å¿†"""
        if not state["messages"]:
            return {"memories": []}

        # ä½¿ç”¨ memory_searcher æœç´¢ç›¸å…³è®°å¿†
        memories = await memory_searcher.ainvoke({
            "messages": state["messages"],
            "user_id": state["user_id"]
        })

        return {"memories": memories}

    # å®¢æœå“åº”èŠ‚ç‚¹
    def customer_service(state: CustomerServiceState):
        """ç”Ÿæˆå®¢æœå“åº”"""
        # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = ""
        if state["memories"]:
            memory_context = "\nç”¨æˆ·å†å²ä¿¡æ¯ï¼š\n"
            for mem in state["memories"]:
                memory_context += f"- {mem.get('content', '')}\n"

        # ç”Ÿæˆå“åº”
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚
        {memory_context}

        æ ¹æ®ç”¨æˆ·çš„å†å²ä¿¡æ¯å’Œå½“å‰é—®é¢˜ï¼Œæä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚
        """

        response = model.invoke([
            SystemMessage(content=system_prompt),
            *state["messages"]
        ])

        return {"messages": [response]}

    # ä¿å­˜è®°å¿†èŠ‚ç‚¹
    async def save_memories(state: CustomerServiceState):
        """ä»å¯¹è¯ä¸­æå–å¹¶ä¿å­˜æ–°è®°å¿†"""
        if len(state["messages"]) < 2:
            return {}

        # ä½¿ç”¨ memory_manager æå–è®°å¿†
        await memory_manager.ainvoke({
            "messages": state["messages"][-2:],  # æœ€åä¸€è½®å¯¹è¯
            "user_id": state["user_id"]
        })

        return {}

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("search_memories", search_memories)
    workflow.add_node("customer_service", customer_service)
    workflow.add_node("save_memories", save_memories)

    # å®šä¹‰æµç¨‹
    workflow.set_entry_point("search_memories")
    workflow.add_edge("search_memories", "customer_service")
    workflow.add_edge("customer_service", "save_memories")
    workflow.add_edge("save_memories", END)

    # ç¼–è¯‘åº”ç”¨
    checkpointer = InMemorySaver()
    app = workflow.compile(
        checkpointer=checkpointer,
        store=store  # ä¼ é€’å­˜å‚¨
    )

    return app

# ä½¿ç”¨å®¢æœåŠ©æ‰‹
app = build_customer_service_agent()

# ç¬¬ä¸€æ¬¡å¯¹è¯
config = {"configurable": {"thread_id": "user-123"}}
result = await app.ainvoke({
    "messages": [HumanMessage("æˆ‘æ˜¯ VIP ä¼šå‘˜ï¼Œä¸Šä¸ªæœˆä¹°çš„å•†å“æœ‰è´¨é‡é—®é¢˜")],
    "user_id": "user-123"
}, config)

# åç»­å¯¹è¯ï¼ˆä¼šè®°ä½ç”¨æˆ·æ˜¯ VIP ä¼šå‘˜ï¼‰
result = await app.ainvoke({
    "messages": [HumanMessage("èƒ½å¦åŠ æ€¥å¤„ç†æˆ‘çš„é€€æ¬¾ï¼Ÿ")],
    "user_id": "user-123"
}, config)
```

**è®°å¿†ç³»ç»Ÿæœ€ä½³å®è·µ**

1. **è®°å¿†åˆ†ç±»**ï¼š
   - çŸ­æœŸè®°å¿†ï¼šä½¿ç”¨ Checkpointerï¼ˆä¼šè¯å†…ï¼‰
   - é•¿æœŸè®°å¿†ï¼šä½¿ç”¨ LangMemï¼ˆè·¨ä¼šè¯ï¼‰
   - å·¥ä½œè®°å¿†ï¼šä½¿ç”¨ Stateï¼ˆå½“å‰ä»»åŠ¡ï¼‰

2. **è®°å¿†ç®¡ç†ç­–ç•¥**ï¼š
   - å®šæœŸæ¸…ç†è¿‡æœŸè®°å¿†
   - åˆå¹¶ç›¸ä¼¼è®°å¿†é¿å…å†—ä½™
   - è®¾ç½®è®°å¿†ä¼˜å…ˆçº§å’Œé‡è¦æ€§

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨å‘é‡æ•°æ®åº“åŠ é€Ÿæœç´¢
   - å®ç°è®°å¿†ç¼“å­˜æœºåˆ¶
   - å¼‚æ­¥å¤„ç†è®°å¿†æ“ä½œ

4. **éšç§å’Œå®‰å…¨**ï¼š
   - åŠ å¯†æ•æ„Ÿè®°å¿†
   - å®ç°ç”¨æˆ·æ•°æ®åˆ é™¤åŠŸèƒ½
   - è®°å¿†è®¿é—®æƒé™æ§åˆ¶

#### 2.3.5 Memory æœ€ä½³å®è·µ

**å®è·µ1ï¼šåˆ†å±‚è®°å¿†æ¶æ„**

```python
class LayeredMemory:
    """åˆ†å±‚è®°å¿†æ¶æ„

    L1: Short-term (Checkpointer) - å½“å‰ä¼šè¯
    L2: Episodic (LangMem) - æœ€è¿‘äº¤äº’
    L3: Semantic (LangMem) - é•¿æœŸçŸ¥è¯†
    """

    def __init__(self):
        self.episodic = EpisodicMemory(...)
        self.semantic = SemanticMemory(...)

    def recall(self, user_id: str, query: str, state: dict):
        """åˆ†å±‚å›å¿†"""

        # L1: å½“å‰ä¼šè¯ï¼ˆå·²åœ¨ state["messages"] ä¸­ï¼‰
        current_session = state["messages"]

        # L2: æœ€è¿‘äº¤äº’ï¼ˆè¿‡å»7å¤©ï¼‰
        recent_interactions = self.episodic.search(
            user_id,
            query,
            limit=3,
            filters={"days_ago": 7}
        )

        # L3: é•¿æœŸçŸ¥è¯†ï¼ˆæ‰€æœ‰æ—¶é—´ï¼‰
        long_term_facts = self.semantic.search(
            user_id,
            query,
            limit=5
        )

        return {
            "current": current_session,
            "recent": recent_interactions,
            "knowledge": long_term_facts
        }
```

**å®è·µ2ï¼šæ™ºèƒ½è®°å¿†æå–**

```python
def extract_memories_from_conversation(messages: list[BaseMessage]):
    """ä»å¯¹è¯ä¸­æå–å€¼å¾—è®°ä½çš„ä¿¡æ¯"""

    model = ChatOpenAI(model="gpt-4")

    extraction_prompt = """
    ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å€¼å¾—é•¿æœŸè®°ä½çš„ä¿¡æ¯ï¼š
    1. ç”¨æˆ·åå¥½
    2. é‡è¦äº‹å®
    3. åé¦ˆå’Œè¯„ä»·

    å¯¹è¯ï¼š
    {conversation}

    è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
    {
        "preferences": ["åå¥½1", "åå¥½2"],
        "facts": ["äº‹å®1", "äº‹å®2"],
        "feedback": ["åé¦ˆ1"]
    }
    """

    conversation = "\n".join([
        f"{'User' if isinstance(m, HumanMessage) else 'AI'}: {m.content}"
        for m in messages
    ])

    response = model.invoke([
        HumanMessage(content=extraction_prompt.format(conversation=conversation))
    ])

    import json
    return json.loads(response.content)
```

**å®è·µ3ï¼šè®°å¿†é‡è¦æ€§è¯„åˆ†**

```python
def score_memory_importance(memory: dict) -> float:
    """è¯„ä¼°è®°å¿†çš„é‡è¦æ€§ï¼ˆ0-1ï¼‰"""

    score = 0.0

    # å› ç´ 1ï¼šæ—¶é—´è¡°å‡
    days_ago = (datetime.now() - memory["timestamp"]).days
    time_factor = 1.0 / (1 + days_ago / 30)  # 30å¤©è¡°å‡ä¸€åŠ

    # å› ç´ 2ï¼šç”¨æˆ·åé¦ˆ
    if memory.get("feedback") == "positive":
        feedback_factor = 1.0
    elif memory.get("feedback") == "negative":
        feedback_factor = 0.3
    else:
        feedback_factor = 0.5

    # å› ç´ 3ï¼šè®¿é—®é¢‘ç‡
    access_count = memory.get("access_count", 0)
    access_factor = min(access_count / 10, 1.0)

    # ç»¼åˆè¯„åˆ†
    score = (time_factor * 0.4 + feedback_factor * 0.3 + access_factor * 0.3)

    return score

# æ¸…ç†ä½é‡è¦æ€§è®°å¿†
def cleanup_memories(user_id: str, threshold: float = 0.2):
    """æ¸…ç†ä½é‡è¦æ€§çš„è®°å¿†"""
    all_memories = memory_store.list(user_id)

    for memory in all_memories:
        if score_memory_importance(memory) < threshold:
            memory_store.delete(memory["id"])
```

---

### 2.4 Graph æ„å»º

#### 2.4.1 èŠ‚ç‚¹è®¾è®¡ä¸èŒè´£åˆ’åˆ†

**å•ä¸€èŒè´£åŸåˆ™**

```python
# âŒ ä¸å¥½ï¼šä¸€ä¸ªèŠ‚ç‚¹åšå¤ªå¤šäº‹
def god_node(state):
    # éªŒè¯è¾“å…¥
    # è°ƒç”¨æ¨¡å‹
    # æ‰§è¡Œå·¥å…·
    # æ ¼å¼åŒ–è¾“å‡º
    # ...
    pass

# âœ… å¥½ï¼šæ¯ä¸ªèŠ‚ç‚¹èŒè´£å•ä¸€
def validate_input(state):
    """åªè´Ÿè´£éªŒè¯"""
    pass

def call_model(state):
    """åªè´Ÿè´£è°ƒç”¨æ¨¡å‹"""
    pass

def execute_tools(state):
    """åªè´Ÿè´£æ‰§è¡Œå·¥å…·"""
    pass

def format_output(state):
    """åªè´Ÿè´£æ ¼å¼åŒ–"""
    pass
```

**èŠ‚ç‚¹ç±»å‹åˆ†ç±»**

```python
# 1. æ•°æ®å¤„ç†èŠ‚ç‚¹
def preprocess(state):
    """é¢„å¤„ç†è¾“å…¥"""
    messages = state["messages"]
    # æ¸…ç†ã€è§„èŒƒåŒ–
    return {"messages": clean(messages)}

def postprocess(state):
    """åå¤„ç†è¾“å‡º"""
    output = state["messages"][-1].content
    # æ ¼å¼åŒ–ã€éªŒè¯
    return {"messages": [AIMessage(content=format(output))]}

# 2. ä¸šåŠ¡é€»è¾‘èŠ‚ç‚¹
def analyze(state):
    """åˆ†æå’Œå†³ç­–"""
    data = state["data"]
    result = perform_analysis(data)
    return {"analysis_result": result}

# 3. å¤–éƒ¨é›†æˆèŠ‚ç‚¹
def call_api(state):
    """è°ƒç”¨å¤–éƒ¨ API"""
    query = state["query"]
    response = requests.get(f"https://api.example.com?q={query}")
    return {"api_result": response.json()}

# 4. æ§åˆ¶æµèŠ‚ç‚¹
def route(state):
    """è·¯ç”±å†³ç­–ï¼ˆåªè¿”å›å…ƒæ•°æ®ï¼Œä¸ä¿®æ”¹çŠ¶æ€ï¼‰"""
    # è¿™ä¸ªèŠ‚ç‚¹é€šå¸¸ä¸è¿”å›çŠ¶æ€æ›´æ–°
    # è€Œæ˜¯åœ¨æ¡ä»¶è¾¹ä¸­ä½¿ç”¨
    return {}
```

#### 2.4.2 æ¡ä»¶è·¯ç”±å®ç°

**åŸºäºå†…å®¹çš„è·¯ç”±**

```python
def classify_question(state) -> Literal["search", "calc", "chat"]:
    """æ ¹æ®é—®é¢˜ç±»å‹è·¯ç”±"""
    question = state["messages"][-1].content

    if any(keyword in question for keyword in ["æœç´¢", "æŸ¥æ‰¾", "æ–°é—»"]):
        return "search"
    elif any(op in question for op in ["+", "-", "*", "/", "è®¡ç®—"]):
        return "calc"
    else:
        return "chat"

workflow.add_conditional_edges(
    "entry",
    classify_question,
    {
        "search": "search_node",
        "calc": "calculator_node",
        "chat": "chat_node"
    }
)
```

**åŸºäºçŠ¶æ€çš„è·¯ç”±**

```python
def check_completeness(state) -> Literal["continue", "end"]:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ"""

    # æ£€æŸ¥1ï¼šæ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    if state.get("iterations", 0) >= 10:
        return "end"

    # æ£€æŸ¥2ï¼šæ˜¯å¦æ‰€æœ‰å­ä»»åŠ¡éƒ½å®Œæˆ
    todos = state.get("todos", [])
    if all(todo["status"] == "completed" for todo in todos):
        return "end"

    # æ£€æŸ¥3ï¼šæ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
    last_message = state["messages"][-1]
    if "æœ€ç»ˆç­”æ¡ˆ" in last_message.content:
        return "end"

    return "continue"

workflow.add_conditional_edges(
    "process",
    check_completeness,
    {
        "continue": "process",
        "end": END
    }
)
```

**ä½¿ç”¨ LLM çš„è·¯ç”±**

```python
from langchain_openai import ChatOpenAI
from pydantic import BaseModel

class RoutingDecision(BaseModel):
    """è·¯ç”±å†³ç­–"""
    route: Literal["technical", "sales", "support"]
    confidence: float
    reason: str

def llm_router(state) -> str:
    """ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½è·¯ç”±"""
    question = state["messages"][-1].content

    model = ChatOpenAI(model="gpt-4").with_structured_output(RoutingDecision)

    decision = model.invoke([
        SystemMessage(content="""
        æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œå†³å®šè·¯ç”±åˆ°å“ªä¸ªéƒ¨é—¨ï¼š
        - technical: æŠ€æœ¯é—®é¢˜
        - sales: é”€å”®å’¨è¯¢
        - support: å”®åæ”¯æŒ
        """),
        HumanMessage(content=question)
    ])

    return decision.route

workflow.add_conditional_edges(
    "classify",
    llm_router,
    {
        "technical": "tech_agent",
        "sales": "sales_agent",
        "support": "support_agent"
    }
)
```

#### 2.4.3 å¾ªç¯æ§åˆ¶ä¸é˜²æ­¢æ­»å¾ªç¯

**è¿­ä»£è®¡æ•°å™¨**

```python
class LoopControlState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    iterations: int  # è¿­ä»£è®¡æ•°å™¨

def agent_node(state):
    """å¢åŠ è¿­ä»£è®¡æ•°"""
    iterations = state.get("iterations", 0)

    # ... æ‰§è¡Œé€»è¾‘

    return {
        "messages": [response],
        "iterations": iterations + 1
    }

def should_continue(state) -> Literal["continue", "end"]:
    """æ£€æŸ¥è¿­ä»£æ¬¡æ•°"""
    if state.get("iterations", 0) >= 10:
        return "end"

    # å…¶ä»–ç»ˆæ­¢æ¡ä»¶...

    return "continue"
```

**recursion_limit é…ç½®**

```python
# åœ¨ invoke æ—¶è®¾ç½® recursion_limit
app = workflow.compile()

result = app.invoke(
    {"messages": [HumanMessage("Hello")]},
    config={"recursion_limit": 50}  # æœ€å¤šæ‰§è¡Œ50ä¸ªèŠ‚ç‚¹
)

# å…¬å¼ï¼šrecursion_limit = 2 Ã— max_iterations + 1
# ä¾‹å¦‚ï¼šæœ€å¤š5æ¬¡è¿­ä»£ â†’ recursion_limit = 11
```

**å¾ªç¯æ£€æµ‹**

```python
class StateWithHistory(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    visited_nodes: list[str]  # è®¿é—®è¿‡çš„èŠ‚ç‚¹

def detect_loop(state) -> bool:
    """æ£€æµ‹æ˜¯å¦é™·å…¥å¾ªç¯"""
    visited = state.get("visited_nodes", [])

    # æ£€æµ‹ï¼šè¿ç»­3æ¬¡è®¿é—®ç›¸åŒèŠ‚ç‚¹
    if len(visited) >= 3:
        if visited[-1] == visited[-2] == visited[-3]:
            return True

    return False

def safe_node(state, node_name: str):
    """å¸¦å¾ªç¯æ£€æµ‹çš„èŠ‚ç‚¹åŒ…è£…å™¨"""
    visited = state.get("visited_nodes", [])
    visited.append(node_name)

    if detect_loop({"visited_nodes": visited}):
        return {
            "messages": [AIMessage(content="æ£€æµ‹åˆ°å¾ªç¯ï¼Œåœæ­¢æ‰§è¡Œ")],
            "visited_nodes": visited
        }

    # æ­£å¸¸æ‰§è¡Œ
    result = original_node(state)
    result["visited_nodes"] = visited
    return result
```

---

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº† State ç®¡ç†ä¸ Memory ç³»ç»Ÿï¼š

#### æ ¸å¿ƒæ¦‚å¿µ

1. **State å®šä¹‰ä¸æ›´æ–°**
   - TypedDict å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€ç»“æ„
   - MessagesState é¢„å®šä¹‰ç±»å‹ï¼ˆä»…åŒ…å« messagesï¼‰
   - AgentState å®˜æ–¹ Agent çŠ¶æ€ï¼ˆåŒ…å« messages å’Œ remaining_stepsï¼‰
   - ä¸‰ç§æ›´æ–°æœºåˆ¶ï¼šæ›¿æ¢ã€å¢é‡ï¼ˆReducerï¼‰ã€è‡ªå®šä¹‰ Reducer

2. **Checkpointer æŒä¹…åŒ–**
   - InMemorySaverï¼šå†…å­˜å­˜å‚¨ï¼Œå¼€å‘æµ‹è¯•
   - SqliteSaverï¼šæœ¬åœ°æŒä¹…åŒ–ï¼Œå•æœºéƒ¨ç½²
   - PostgresSaverï¼šç”Ÿäº§çº§ï¼Œåˆ†å¸ƒå¼éƒ¨ç½²
   - è‡ªå®šä¹‰ Checkpointer

3. **Memory ç³»ç»Ÿ**
   - Short-term Memoryï¼šä¼šè¯å†…è®°å¿†ï¼ˆCheckpointer + Thread IDï¼‰
   - Long-term Memoryï¼šè·¨ä¼šè¯è®°å¿†
   - LangMem SDKï¼š
     - Episodic Memoryï¼ˆæƒ…èŠ‚è®°å¿†ï¼‰ï¼šè¿‡å»äº¤äº’
     - Procedural Memoryï¼ˆè¿‡ç¨‹è®°å¿†ï¼‰ï¼šæ‰§è¡Œæ–¹æ³•
     - Semantic Memoryï¼ˆè¯­ä¹‰è®°å¿†ï¼‰ï¼šäº‹å®çŸ¥è¯†

4. **Graph æ„å»º**
   - èŠ‚ç‚¹è®¾è®¡ï¼šå•ä¸€èŒè´£ã€ç±»å‹åˆ†ç±»
   - æ¡ä»¶è·¯ç”±ï¼šåŸºäºå†…å®¹ã€çŠ¶æ€ã€LLM å†³ç­–
   - å¾ªç¯æ§åˆ¶ï¼šè¿­ä»£è®¡æ•°ã€recursion_limitã€å¾ªç¯æ£€æµ‹

#### ä¸‹ä¸€æ­¥

åœ¨ç¬¬9ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **æŒä¹…åŒ–ä¸ Human-in-the-Loop**ï¼ŒæŒæ¡ï¼š
- Thread ç®¡ç†ä¸æ—¶é—´æ—…è¡Œ
- ä¸­æ–­ä¸å®¡æ‰¹æµç¨‹
- è‡ªå®šä¹‰ Agent å®Œæ•´å®æˆ˜

---

### æ€è€ƒä¸ç»ƒä¹ 

#### æ€è€ƒé¢˜

1. Reducer å‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿadd_messages å¦‚ä½•å·¥ä½œï¼Ÿ
2. InMemorySaverã€SqliteSaverã€PostgresSaver çš„é€‚ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ
3. Episodicã€Proceduralã€Semantic Memory çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
4. å¦‚ä½•é˜²æ­¢ Agent é™·å…¥æ­»å¾ªç¯ï¼Ÿ

#### ç»ƒä¹ é¢˜

**ç»ƒä¹ 1ï¼šå®ç°å¸¦ Checkpointer çš„ Agent**

è¦æ±‚ï¼š
- ä½¿ç”¨ SqliteSaver
- æ”¯æŒå¤šä¸ª thread_id
- æµ‹è¯•è·¨ä¼šè¯è®°å¿†

**ç»ƒä¹ 2ï¼šé›†æˆ LangMem SDK**

è¦æ±‚ï¼š
- å®ç° Episodic Memory
- å®ç° Semantic Memory
- åœ¨ Agent ä¸­ç»¼åˆä½¿ç”¨

**ç»ƒä¹ 3ï¼šå®ç°æ™ºèƒ½è·¯ç”±**

è¦æ±‚ï¼š
- ä½¿ç”¨ LLM è¿›è¡Œè·¯ç”±å†³ç­–
- æ”¯æŒè‡³å°‘3ä¸ªè·¯ç”±ç›®æ ‡
- è®°å½•è·¯ç”±å†³ç­–è¿‡ç¨‹

**ç»ƒä¹ 4ï¼šå¾ªç¯æ§åˆ¶**

è¦æ±‚ï¼š
- å®ç°è¿­ä»£è®¡æ•°å™¨
- å®ç°å¾ªç¯æ£€æµ‹
- æµ‹è¯•é˜²æ­¢æ­»å¾ªç¯

---

## ç¬¬3ç« ï¼šæŒä¹…åŒ–ä¸ Human-in-the-Loop

### 3.1 Thread ç®¡ç†

#### 3.1.1 Thread ID ä¸å¤šä¼šè¯éš”ç¦»

**Thread ID æ¦‚å¿µ**

Thread ID æ˜¯ç”¨äºéš”ç¦»ä¸åŒä¼šè¯çš„æ ‡è¯†ç¬¦ï¼Œæ¯ä¸ª Thread ID å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„çŠ¶æ€å†å²ã€‚

```python
from langgraph.checkpoint.sqlite import SqliteSaver

with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    # ç”¨æˆ·Açš„ä¼šè¯
    config_a = {"configurable": {"thread_id": "user-a-session-1"}}
    app.invoke({"messages": [HumanMessage("æˆ‘å« Alice")]}, config=config_a)

    # ç”¨æˆ·Bçš„ä¼šè¯ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
    config_b = {"configurable": {"thread_id": "user-b-session-1"}}
    app.invoke({"messages": [HumanMessage("æˆ‘å« Bob")]}, config=config_b)

    # ç”¨æˆ·Açš„ç¬¬äºŒè½®ï¼ˆè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼‰
    result_a = app.invoke(
        {"messages": [HumanMessage("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]},
        config=config_a
    )
    print(result_a["messages"][-1].content)  # "æ‚¨å« Alice"

    # ç”¨æˆ·Bçš„ç¬¬äºŒè½®ï¼ˆä¹Ÿè®°ä½è‡ªå·±çš„å¯¹è¯ï¼‰
    result_b = app.invoke(
        {"messages": [HumanMessage("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]},
        config=config_b
    )
    print(result_b["messages"][-1].content)  # "æ‚¨å« Bob"
```

**Thread ID æœ€ä½³å®è·µ**

```python
# æ–¹æ¡ˆ1ï¼šç”¨æˆ·ID + ä¼šè¯ID
thread_id = f"user-{user_id}-session-{session_id}"

# æ–¹æ¡ˆ2ï¼šç”¨æˆ·ID + æ—¥æœŸ
from datetime import datetime
thread_id = f"user-{user_id}-{datetime.now().strftime('%Y%m%d')}"

# æ–¹æ¡ˆ3ï¼šä¸šåŠ¡åœºæ™¯ + UUID
import uuid
thread_id = f"support-ticket-{uuid.uuid4()}"

# ä½¿ç”¨
config = {"configurable": {"thread_id": thread_id}}
result = app.invoke(input, config=config)
```

**åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯**

```python
def list_user_threads(user_id: str, checkpointer) -> list[str]:
    """åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯"""

    # æ³¨æ„ï¼šè¿™éœ€è¦è‡ªå®šä¹‰ Checkpointer æˆ–æŸ¥è¯¢æ•°æ®åº“
    # ä»¥ä¸‹æ˜¯æ¦‚å¿µç¤ºä¾‹

    query = f"SELECT DISTINCT thread_id FROM checkpoints WHERE thread_id LIKE 'user-{user_id}-%'"
    threads = execute_query(query)

    return threads
```

#### 3.1.2 æ—¶é—´æ—…è¡Œè°ƒè¯•ï¼ˆçŠ¶æ€å¿«ç…§ã€å›æº¯ï¼‰

**è·å–çŠ¶æ€å†å²**

```python
from langgraph.checkpoint.sqlite import SqliteSaver

with SqliteSaver.from_conn_string("checkpoints.db") as checkpointer:
    app = workflow.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "debug-session"}}

    # æ‰§è¡Œå¤šè½®å¯¹è¯
    app.invoke({"messages": [HumanMessage("ç¬¬1è½®")]}, config=config)
    app.invoke({"messages": [HumanMessage("ç¬¬2è½®")]}, config=config)
    app.invoke({"messages": [HumanMessage("ç¬¬3è½®")]}, config=config)

    # è·å–çŠ¶æ€å†å²
    history = app.get_state_history(config)

    print("=== çŠ¶æ€å†å² ===")
    for i, state_snapshot in enumerate(history):
        checkpoint_id = state_snapshot.config["configurable"].get("checkpoint_id")
        messages_count = len(state_snapshot.values.get("messages", []))

        print(f"\nçŠ¶æ€ {i}:")
        print(f"  Checkpoint ID: {checkpoint_id}")
        print(f"  æ¶ˆæ¯æ•°é‡: {messages_count}")
        print(f"  æ—¶é—´: {state_snapshot.metadata.get('timestamp')}")
```

**å›åˆ°å†å²çŠ¶æ€**

```python
# è·å–å†å²çŠ¶æ€
history = list(app.get_state_history(config))

# é€‰æ‹©ç¬¬2ä¸ªçŠ¶æ€ï¼ˆå€’æ•°ç¬¬äºŒä¸ª checkpointï¼‰
target_state = history[1]

# ä»è¯¥çŠ¶æ€ç»§ç»­æ‰§è¡Œ
result = app.invoke(
    {"messages": [HumanMessage("ä»å†å²çŠ¶æ€ç»§ç»­")]},
    config=target_state.config  # ä½¿ç”¨å†å²çŠ¶æ€çš„ config
)
```

**è°ƒè¯•å·¥ä½œæµ**

```python
def debug_workflow(app, input_data, thread_id: str):
    """è°ƒè¯•å·¥ä½œæµï¼šæ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€"""

    config = {"configurable": {"thread_id": thread_id}}

    print("=== å¼€å§‹æ‰§è¡Œ ===")
    print(f"è¾“å…¥: {input_data}\n")

    # æµå¼æ‰§è¡Œï¼Œè§‚å¯Ÿæ¯ä¸ªèŠ‚ç‚¹
    for chunk in app.stream(input_data, config=config):
        node_name = list(chunk.keys())[0]
        state_update = chunk[node_name]

        print(f"[èŠ‚ç‚¹: {node_name}]")
        print(f"  çŠ¶æ€æ›´æ–°: {state_update}")
        print()

    # è·å–æœ€ç»ˆçŠ¶æ€
    final_state = app.get_state(config)
    print("=== æœ€ç»ˆçŠ¶æ€ ===")
    print(f"æ¶ˆæ¯æ•°é‡: {len(final_state.values.get('messages', []))}")
    print(f"æœ€åä¸€æ¡æ¶ˆæ¯: {final_state.values['messages'][-1].content}")

    # æŸ¥çœ‹çŠ¶æ€å†å²
    print("\n=== çŠ¶æ€å†å² ===")
    for i, state in enumerate(app.get_state_history(config)):
        print(f"çŠ¶æ€ {i}: {len(state.values.get('messages', []))} æ¡æ¶ˆæ¯")

# ä½¿ç”¨
debug_workflow(
    app,
    {"messages": [HumanMessage("æµ‹è¯•è¾“å…¥")]},
    thread_id="debug-001"
)
```

---

### 3.2 ä¸­æ–­ä¸å®¡æ‰¹

#### 3.2.1 interrupt() å‡½æ•°ä¸ä¸­æ–­ç‚¹

**åŸºç¡€ä¸­æ–­**

```python
from langgraph.graph import StateGraph, END, MessagesState
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import interrupt

def sensitive_action(state):
    """éœ€è¦å®¡æ‰¹çš„æ•æ„Ÿæ“ä½œ"""
    action = state["messages"][-1].content

    # ä½¿ç”¨ interrupt() å‡½æ•°æš‚åœæ‰§è¡Œ,ç­‰å¾…äººå·¥å®¡æ‰¹
    # interrupt() ä¼šæš‚åœæ‰§è¡Œå¹¶è¿”å›æä¾›çš„å€¼ç»™è°ƒç”¨è€…
    approved = interrupt(f"è¯·å®¡æ‰¹æ“ä½œï¼š{action}")

    # å½“æ¢å¤æ‰§è¡Œæ—¶,approved ä¼šæ¥æ”¶æ¢å¤æ—¶ä¼ å…¥çš„å€¼
    if approved:
        return {"messages": [AIMessage("æ“ä½œå·²æ‰¹å‡†å¹¶æ‰§è¡Œ")]}
    else:
        return {"messages": [AIMessage("æ“ä½œè¢«æ‹’ç»")]}

# æ„å»ºå›¾
workflow = StateGraph(MessagesState)
workflow.add_node("check", sensitive_action)
workflow.set_entry_point("check")
workflow.add_edge("check", END)

app = workflow.compile(checkpointer=InMemorySaver())

# ç¬¬ä¸€æ¬¡æ‰§è¡Œ - ä¼šåœ¨ interrupt() å¤„æš‚åœ
config = {"configurable": {"thread_id": "approval-001"}}
result = app.invoke(
    {"messages": [HumanMessage("åˆ é™¤æ•°æ®åº“")]},
    config=config
)

# æŸ¥çœ‹ä¸­æ–­ä¿¡æ¯
print(f"â¸ï¸  ä¸­æ–­ä¿¡æ¯: {result}")

# äººå·¥å®¡æ‰¹å,ä½¿ç”¨ Command æ¢å¤æ‰§è¡Œ
from langgraph.types import Command

# æ‰¹å‡†æ“ä½œ
app.invoke(Command(resume=True), config=config)

# æˆ–æ‹’ç»æ“ä½œ
# app.invoke(Command(resume=False), config=config)
```

**å¸¦æ¡ä»¶çš„å®¡æ‰¹æµç¨‹**

```python
from typing import Literal
from langgraph.types import interrupt

class ApprovalState(MessagesState):
    """å¸¦å®¡æ‰¹çš„çŠ¶æ€"""
    action_description: str

def request_approval(state) -> dict:
    """è¯·æ±‚å®¡æ‰¹èŠ‚ç‚¹"""
    action = state["messages"][-1].content

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•æ„Ÿæ“ä½œ
    if "åˆ é™¤" in action or "ä¿®æ”¹" in action:
        # ä½¿ç”¨ interrupt() æš‚åœå¹¶ç­‰å¾…å®¡æ‰¹
        approval = interrupt({
            "type": "approval_request",
            "action": action,
            "question": f"æ˜¯å¦æ‰¹å‡†æ“ä½œ: {action}?"
        })

        # æ¢å¤æ—¶,approval ä¼šæ˜¯ True æˆ– False
        if approval:
            return {
                "messages": [AIMessage(f"æ“ä½œå·²æ‰¹å‡†: {action}")],
                "action_description": action
            }
        else:
            return {
                "messages": [AIMessage(f"æ“ä½œè¢«æ‹’ç»: {action}")]
            }

    # éæ•æ„Ÿæ“ä½œç›´æ¥é€šè¿‡
    return {"messages": [AIMessage(f"è‡ªåŠ¨æ‰§è¡Œ: {action}")]}

# æ„å»ºå›¾
workflow = StateGraph(ApprovalState)
workflow.add_node("request", request_approval)
workflow.set_entry_point("request")
workflow.add_edge("request", END)

app = workflow.compile(checkpointer=InMemorySaver())

# ä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "approval-002"}}

# ç¬¬ä¸€æ¬¡æ‰§è¡Œ - ä¼šåœ¨ interrupt() å¤„æš‚åœ
result = app.invoke(
    {"messages": [HumanMessage("åˆ é™¤ç”¨æˆ·æ•°æ®")]},
    config=config
)

# äººå·¥å®¡æ‰¹
from langgraph.types import Command
# æ‰¹å‡†
app.invoke(Command(resume=True), config=config)
# æˆ–æ‹’ç»
# app.invoke(Command(resume=False), config=config)
```

#### 3.2.2 å®¡æ‰¹æµç¨‹è®¾è®¡ï¼ˆç­‰å¾…ã€æ‰¹å‡†ã€ä¿®æ”¹ã€æ¢å¤ï¼‰

**å®Œæ•´çš„å®¡æ‰¹ç³»ç»Ÿ**

```python
from enum import Enum
from langgraph.types import interrupt, Command

class ApprovalAction(str, Enum):
    APPROVE = "approve"
    REJECT = "reject"
    MODIFY = "modify"

class ApprovalWorkflowState(MessagesState):
    original_request: str
    modified_request: str

def agent_node(state):
    """Agent èŠ‚ç‚¹ - ç”Ÿæˆéœ€è¦å®¡æ‰¹çš„æ“ä½œ"""
    model = ChatOpenAI(model="gpt-4")
    response = model.invoke(state["messages"])

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å®¡æ‰¹
    if needs_approval(response.content):
        # ä½¿ç”¨ interrupt() è¯·æ±‚å®¡æ‰¹
        approval_result = interrupt({
            "type": "approval_required",
            "request": response.content,
            "options": ["approve", "reject", "modify"]
        })

        # approval_result æ ¼å¼: {"action": "approve"} æˆ– {"action": "modify", "new_request": "..."}
        if approval_result["action"] == "approve":
            return {
                "messages": [response, AIMessage("âœ… è¯·æ±‚å·²æ‰¹å‡†,æ­£åœ¨æ‰§è¡Œ...")],
                "original_request": response.content
            }
        elif approval_result["action"] == "modify":
            # ä½¿ç”¨ä¿®æ”¹åçš„è¯·æ±‚
            new_request = approval_result.get("new_request", response.content)
            return {
                "messages": [AIMessage(f"ğŸ“ è¯·æ±‚å·²ä¿®æ”¹: {new_request}")],
                "original_request": response.content,
                "modified_request": new_request
            }
        else:  # reject
            return {
                "messages": [AIMessage("âŒ è¯·æ±‚è¢«æ‹’ç»")]
            }

    # ä¸éœ€è¦å®¡æ‰¹çš„æ“ä½œç›´æ¥æ‰§è¡Œ
    return {"messages": [response]}

def execute_node(state):
    """æ‰§è¡ŒèŠ‚ç‚¹"""
    request = state.get("modified_request") or state.get("original_request")
    # æ‰§è¡Œå®é™…æ“ä½œ
    result = f"å·²æ‰§è¡Œæ“ä½œ: {request}"
    return {"messages": [AIMessage(result)]}

# æ„å»ºå›¾
workflow = StateGraph(ApprovalWorkflowState)
workflow.add_node("agent", agent_node)
workflow.add_node("execute", execute_node)

workflow.set_entry_point("agent")
workflow.add_edge("agent", "execute")
workflow.add_edge("execute", END)

app = workflow.compile(checkpointer=InMemorySaver())

# ä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "approval-003"}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨ - åœ¨ interrupt() å¤„æš‚åœ
result = app.invoke(
    {"messages": [HumanMessage("åˆ é™¤ç”Ÿäº§æ•°æ®åº“")]},
    config=config
)

# é€‰é¡¹1: æ‰¹å‡†
app.invoke(Command(resume={"action": "approve"}), config=config)

# é€‰é¡¹2: æ‹’ç»
# app.invoke(Command(resume={"action": "reject"}), config=config)

# é€‰é¡¹3: ä¿®æ”¹åæ‰¹å‡†
# app.invoke(Command(resume={
#     "action": "modify",
#     "new_request": "å¤‡ä»½ååˆ é™¤æµ‹è¯•æ•°æ®åº“"
# }), config=config)
```

**å®¡æ‰¹ UI ç¤ºä¾‹**

```python
def approval_ui(app, checkpointer):
    """ç®€å•çš„å®¡æ‰¹ UI"""

    # 1. åˆ—å‡ºæ‰€æœ‰å¾…å®¡æ‰¹çš„è¯·æ±‚
    pending_threads = get_pending_approvals(checkpointer)

    for thread_id in pending_threads:
        config = {"configurable": {"thread_id": thread_id}}
        state = app.get_state(config)

        print(f"\n{'='*60}")
        print(f"è¯·æ±‚ ID: {thread_id}")
        print(f"è¯·æ±‚å†…å®¹: {state.values['original_request']}")
        print(f"çŠ¶æ€: {state.values['approval_status']}")

        # 2. ç®¡ç†å‘˜å†³ç­–
        action = input("\næ“ä½œ [a]æ‰¹å‡† [r]æ‹’ç» [m]ä¿®æ”¹ [s]è·³è¿‡: ")

        if action == "a":
            app.update_state(config, {"approval_status": ApprovalStatus.APPROVED})
            result = app.invoke(None, config=config)
            print(f"âœ… {result['messages'][-1].content}")

        elif action == "r":
            app.update_state(config, {"approval_status": ApprovalStatus.REJECTED})
            result = app.invoke(None, config=config)
            print(f"âŒ {result['messages'][-1].content}")

        elif action == "m":
            modified = input("ä¿®æ”¹åçš„è¯·æ±‚: ")
            app.update_state(config, {
                "approval_status": ApprovalStatus.MODIFIED,
                "modified_request": modified
            })
            result = app.invoke(None, config=config)
            print(f"âœ… {result['messages'][-1].content}")

def get_pending_approvals(checkpointer):
    """ä»æ•°æ®åº“è·å–å¾…å®¡æ‰¹çš„è¯·æ±‚"""
    # å®ç°ï¼šæŸ¥è¯¢æ‰€æœ‰ approval_status == PENDING çš„ thread_id
    pass
```

---

### 3.3 å®æˆ˜ï¼šè‡ªå®šä¹‰ Agent

#### 3.3.1 è‡ªå®šä¹‰ RAG Agent å®Œæ•´ç¤ºä¾‹

```python
from typing import Annotated, TypedDict, Literal
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver

# === 1. å®šä¹‰çŠ¶æ€ ===

class RAGState(TypedDict):
    """RAG Agent çŠ¶æ€"""
    messages: Annotated[list[BaseMessage], add_messages]
    retrieved_docs: list[str]
    iterations: int

# === 2. å®šä¹‰å·¥å…· ===

# åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆç¤ºä¾‹ï¼‰
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(
    [
        "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶",
        "LangGraph æ˜¯ LangChain çš„çŠ¶æ€æœºè¿è¡Œæ—¶",
        "LangSmith æ˜¯ LangChain çš„ç›‘æ§å¹³å°"
    ],
    embeddings
)

@tool
def retrieve(query: str) -> str:
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    docs = vectorstore.similarity_search(query, k=2)
    return "\n".join([doc.page_content for doc in docs])

# === 3. å®šä¹‰èŠ‚ç‚¹ ===

def agent_node(state: RAGState) -> dict:
    """Agent èŠ‚ç‚¹ï¼šå†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢"""
    model = ChatOpenAI(model="gpt-4").bind_tools([retrieve])
    response = model.invoke(state["messages"])

    return {
        "messages": [response],
        "iterations": state.get("iterations", 0) + 1
    }

def retrieve_node(state: RAGState) -> dict:
    """æ£€ç´¢èŠ‚ç‚¹"""
    last_message = state["messages"][-1]
    tool_calls = last_message.tool_calls

    retrieved_docs = []
    tool_messages = []

    for tool_call in tool_calls:
        result = retrieve.invoke({"query": tool_call["args"]["query"]})
        retrieved_docs.append(result)

        tool_messages.append(
            ToolMessage(
                content=result,
                tool_call_id=tool_call["id"]
            )
        )

    return {
        "messages": tool_messages,
        "retrieved_docs": retrieved_docs
    }

def generate_node(state: RAGState) -> dict:
    """ç”ŸæˆèŠ‚ç‚¹ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ"""
    docs = state.get("retrieved_docs", [])

    context = "\n\n".join(docs)
    messages = state["messages"]

    # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤º
    prompt = [
        SystemMessage(content=f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context}"),
        *messages
    ]

    model = ChatOpenAI(model="gpt-4")
    response = model.invoke(prompt)

    return {"messages": [response]}

# === 4. å®šä¹‰æ¡ä»¶å‡½æ•° ===

def should_retrieve(state: RAGState) -> Literal["retrieve", "generate", "end"]:
    """å†³ç­–ï¼šæ˜¯å¦éœ€è¦æ£€ç´¢"""
    last_message = state["messages"][-1]

    # æ£€æŸ¥è¿­ä»£æ¬¡æ•°
    if state.get("iterations", 0) >= 5:
        return "end"

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "retrieve"

    # å·²æœ‰æ£€ç´¢ç»“æœï¼Œç”Ÿæˆç­”æ¡ˆ
    if state.get("retrieved_docs"):
        return "generate"

    return "end"

# === 5. æ„å»ºå›¾ ===

workflow = StateGraph(RAGState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("agent", agent_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)

# è®¾ç½®å…¥å£
workflow.set_entry_point("agent")

# æ·»åŠ è¾¹
workflow.add_conditional_edges(
    "agent",
    should_retrieve,
    {
        "retrieve": "retrieve",
        "generate": "generate",
        "end": END
    }
)

workflow.add_edge("retrieve", "agent")  # æ£€ç´¢åç»§ç»­æ€è€ƒ
workflow.add_edge("generate", END)

# === 6. ç¼–è¯‘ ===

app = workflow.compile(
    checkpointer=SqliteSaver.from_conn_string("rag_agent.db")
)

# === 7. ä½¿ç”¨ ===

from langchain_core.messages import ToolMessage

config = {"configurable": {"thread_id": "rag-session-1"}}

# æµå¼æ‰§è¡Œï¼Œè§‚å¯Ÿè¿‡ç¨‹
print("=== RAG Agent æ‰§è¡Œ ===\n")

for chunk in app.stream(
    {"messages": [HumanMessage("ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ")]},
    config=config
):
    node_name = list(chunk.keys())[0]
    print(f"\n[{node_name}]")

    if "messages" in chunk[node_name]:
        for msg in chunk[node_name]["messages"]:
            print(f"  {type(msg).__name__}: {msg.content[:100]}...")

# è·å–æœ€ç»ˆç»“æœ
final_state = app.get_state(config)
print("\n=== æœ€ç»ˆç­”æ¡ˆ ===")
print(final_state.values["messages"][-1].content)
```

#### 3.3.2 Graph è°ƒè¯•æŠ€å·§ï¼ˆå¯è§†åŒ–ã€æ—¥å¿—ã€æµ‹è¯•ï¼‰

**æŠ€å·§1ï¼šå¯è§†åŒ– Graph**

```python
# ç”Ÿæˆ Mermaid å›¾
from langchain_core.runnables.graph import CurveStyle, NodeStyles

mermaid_code = app.get_graph().draw_mermaid(
    curve_style=CurveStyle.LINEAR,
    node_colors=NodeStyles(
        first="fill:#e1f5ff",
        last="fill:#e8f5e9"
    )
)

print(mermaid_code)

# ä¿å­˜ä¸º PNGï¼ˆéœ€è¦ pygraphvizï¼‰
try:
    app.get_graph().draw_png(
        output_file_path="rag_agent_graph.png",
        fontname="Arial"
    )
    print("âœ… å›¾è¡¨å·²ä¿å­˜")
except Exception as e:
    print(f"âš ï¸  {e}")
```

**æŠ€å·§2ï¼šè¯¦ç»†æ—¥å¿—**

```python
from langchain_core.globals import set_verbose, set_debug

# å¯ç”¨è¯¦ç»†æ—¥å¿—
set_verbose(True)
set_debug(True)

result = app.invoke(input_data, config=config)

# ä¼šè¾“å‡ºï¼š
# - æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥/è¾“å‡º
# - æ¯æ¬¡ LLM è°ƒç”¨çš„è¯¦æƒ…
# - çŠ¶æ€æ›´æ–°è¿‡ç¨‹
```

**æŠ€å·§3ï¼šå•å…ƒæµ‹è¯•èŠ‚ç‚¹**

```python
import unittest

class TestRAGAgent(unittest.TestCase):
    """æµ‹è¯• RAG Agent çš„å„ä¸ªèŠ‚ç‚¹"""

    def test_agent_node(self):
        """æµ‹è¯• Agent èŠ‚ç‚¹"""
        state = {
            "messages": [HumanMessage("ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ")],
            "iterations": 0
        }

        result = agent_node(state)

        # æ–­è¨€
        self.assertIn("messages", result)
        self.assertEqual(result["iterations"], 1)

    def test_retrieve_node(self):
        """æµ‹è¯•æ£€ç´¢èŠ‚ç‚¹"""
        state = {
            "messages": [
                HumanMessage("ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"),
                AIMessage(
                    content="",
                    tool_calls=[{
                        "id": "call_123",
                        "name": "retrieve",
                        "args": {"query": "LangChain"}
                    }]
                )
            ]
        }

        result = retrieve_node(state)

        # æ–­è¨€
        self.assertIn("messages", result)
        self.assertIn("retrieved_docs", result)
        self.assertGreater(len(result["retrieved_docs"]), 0)

    def test_should_retrieve(self):
        """æµ‹è¯•æ¡ä»¶å‡½æ•°"""
        # åœºæ™¯1ï¼šéœ€è¦æ£€ç´¢
        state1 = {
            "messages": [AIMessage(content="", tool_calls=[{"name": "retrieve"}])],
            "iterations": 1
        }
        self.assertEqual(should_retrieve(state1), "retrieve")

        # åœºæ™¯2ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£
        state2 = {
            "messages": [AIMessage(content="ç­”æ¡ˆ")],
            "iterations": 5
        }
        self.assertEqual(should_retrieve(state2), "end")

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    unittest.main()
```

**æŠ€å·§4ï¼šé›†æˆæµ‹è¯•**

```python
def test_full_workflow():
    """å®Œæ•´å·¥ä½œæµæµ‹è¯•"""

    test_cases = [
        {
            "input": "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ",
            "expected_keywords": ["LangChain", "æ¡†æ¶", "LLM"]
        },
        {
            "input": "LangGraph çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_keywords": ["LangGraph", "çŠ¶æ€æœº"]
        }
    ]

    for i, test_case in enumerate(test_cases):
        config = {"configurable": {"thread_id": f"test-{i}"}}

        result = app.invoke(
            {"messages": [HumanMessage(test_case["input"])]},
            config=config
        )

        answer = result["messages"][-1].content

        # éªŒè¯åŒ…å«å…³é”®è¯
        for keyword in test_case["expected_keywords"]:
            assert keyword in answer, f"ç­”æ¡ˆä¸­ç¼ºå°‘å…³é”®è¯ï¼š{keyword}"

        print(f"âœ… æµ‹è¯• {i+1} é€šè¿‡")

test_full_workflow()
```

**æŠ€å·§5ï¼šæ€§èƒ½åˆ†æ**

```python
import time

def profile_workflow(app, input_data, config):
    """æ€§èƒ½åˆ†æ"""

    node_times = {}
    start_time = time.time()

    for chunk in app.stream(input_data, config=config):
        node_name = list(chunk.keys())[0]
        node_start = time.time()

        # è®°å½•èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´
        if node_name not in node_times:
            node_times[node_name] = []

        node_times[node_name].append(time.time() - node_start)

    total_time = time.time() - start_time

    # è¾“å‡ºæŠ¥å‘Š
    print("\n=== æ€§èƒ½åˆ†æ ===")
    print(f"æ€»è€—æ—¶: {total_time:.2f}s\n")

    for node_name, times in node_times.items():
        avg_time = sum(times) / len(times)
        print(f"{node_name}:")
        print(f"  è°ƒç”¨æ¬¡æ•°: {len(times)}")
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.2f}s")
        print(f"  æ€»è€—æ—¶: {sum(times):.2f}s")

# ä½¿ç”¨
profile_workflow(
    app,
    {"messages": [HumanMessage("æµ‹è¯•é—®é¢˜")]},
    {"configurable": {"thread_id": "profile-001"}}
)
```

---

### æœ¬ç« å°ç»“

æœ¬ç« å­¦ä¹ äº†æŒä¹…åŒ–ä¸ Human-in-the-Loopï¼š

#### æ ¸å¿ƒæ¦‚å¿µ

1. **Thread ç®¡ç†**
   - Thread IDï¼šä¼šè¯éš”ç¦»
   - æœ€ä½³å®è·µï¼šå‘½åè§„èŒƒ
   - æ—¶é—´æ—…è¡Œï¼šçŠ¶æ€å¿«ç…§ã€å›æº¯

2. **ä¸­æ–­ä¸å®¡æ‰¹**
   - interrupt() å‡½æ•°ï¼šæš‚åœæ‰§è¡Œç­‰å¾…è¾“å…¥
   - Command(resume=value)ï¼šæ¢å¤æ‰§è¡Œå¹¶ä¼ å€¼
   - å®¡æ‰¹æµç¨‹ï¼šæ‰¹å‡†ã€æ‹’ç»ã€ä¿®æ”¹è¯·æ±‚

3. **å®æˆ˜ï¼šè‡ªå®šä¹‰ Agent**
   - RAG Agent å®Œæ•´å®ç°
   - è°ƒè¯•æŠ€å·§ï¼šå¯è§†åŒ–ã€æ—¥å¿—ã€æµ‹è¯•ã€æ€§èƒ½åˆ†æ

#### ä¸‹ä¸€æ­¥

åœ¨ç¬¬å››ç¯‡ä¸­ï¼Œå­¦ä¹  **Middleware å·¥ç¨‹åŒ–**ï¼ŒæŒæ¡ï¼š
- Middleware Hook ä½“ç³»
- å†…ç½® Middlewareï¼ˆPIIã€Summarizationã€Human-in-the-Loopï¼‰
- è‡ªå®šä¹‰ Middleware å¼€å‘

---

### æ€è€ƒä¸ç»ƒä¹ 

#### æ€è€ƒé¢˜

1. Thread ID çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è®¾è®¡ Thread ID å‘½åè§„èŒƒï¼Ÿ
2. interrupt() å‡½æ•°å¦‚ä½•å·¥ä½œï¼Ÿå¦‚ä½•å®ç°å®¡æ‰¹æµç¨‹ï¼Ÿ
3. æ—¶é—´æ—…è¡Œè°ƒè¯•æœ‰ä»€ä¹ˆç”¨ï¼Ÿå¦‚ä½•å›åˆ°å†å²çŠ¶æ€ï¼Ÿ
4. å¦‚ä½•æµ‹è¯• LangGraph åº”ç”¨ï¼Ÿ

#### ç»ƒä¹ é¢˜

**ç»ƒä¹ 1ï¼šå®ç°å¤šç”¨æˆ·ä¼šè¯ç®¡ç†**

è¦æ±‚ï¼š
- ä½¿ç”¨ Thread ID éš”ç¦»ä¼šè¯
- æ”¯æŒåˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯
- å®ç°ä¼šè¯åˆ‡æ¢

**ç»ƒä¹ 2ï¼šå®ç°å®¡æ‰¹å·¥ä½œæµ**

è¦æ±‚ï¼š
- æ•æ„Ÿæ“ä½œéœ€è¦å®¡æ‰¹
- æ”¯æŒæ‰¹å‡†ã€æ‹’ç»ã€ä¿®æ”¹
- å®ç°ç®€å•çš„å®¡æ‰¹ UI

**ç»ƒä¹ 3ï¼šè‡ªå®šä¹‰ SQL Agent**

è¦æ±‚ï¼š
- åŒ…å« SQL æŸ¥è¯¢å·¥å…·
- SQL æ‰§è¡Œå‰éœ€è¦å®¡æ‰¹
- ä½¿ç”¨ Checkpointer æŒä¹…åŒ–

**ç»ƒä¹ 4ï¼šè°ƒè¯•å’Œä¼˜åŒ–**

è¦æ±‚ï¼š
- å¯è§†åŒ– Graph ç»“æ„
- è®°å½•æ€§èƒ½æ•°æ®
- ç¼–å†™å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

---
## ç¬¬4ç« ï¼šTime Travel - çŠ¶æ€å›æº¯ä¸è°ƒè¯•

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡LangGraphçš„æ€æ‰‹çº§è°ƒè¯•ç‰¹æ€§ï¼Œé‡ç°å’Œä¿®å¤ç”Ÿäº§é—®é¢˜

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒAgentçš„æ‰§è¡Œè¿‡ç¨‹å……æ»¡äº†ä¸ç¡®å®šæ€§ï¼š
- ğŸ› ç”¨æˆ·æŠ¥å‘Šï¼š"ä½ çš„Agentåœ¨ç¬¬5æ­¥ç»™äº†é”™è¯¯ç­”æ¡ˆ"
- ğŸ”„ éœ€æ±‚å˜æ›´ï¼š"èƒ½ä¸èƒ½æµ‹è¯•ä¸€ä¸‹å¦‚æœç¬¬3æ­¥é€‰æ‹©äº†ä¸åŒçš„å·¥å…·ä¼šæ€æ ·ï¼Ÿ"
- ğŸ§ª A/Bæµ‹è¯•ï¼š"å¯¹æ¯”ä¸¤ç§å†³ç­–ç­–ç•¥ï¼Œå“ªä¸ªæ›´å¥½ï¼Ÿ"

**ä¼ ç»Ÿè°ƒè¯•æ–¹å¼çš„å›°å¢ƒ**ï¼š
```python
# é—®é¢˜ï¼šç”¨æˆ·è¯´Agentç¬¬5æ­¥å‡ºé”™äº†
# ä¼ ç»Ÿåšæ³•ï¼š
# 1. é‡æ–°è¿è¡Œæ•´ä¸ªæµç¨‹ï¼ˆè´¹æ—¶è´¹é’±ï¼‰
# 2. å¯èƒ½æ— æ³•å¤ç°ï¼ˆLLMä¸ç¡®å®šæ€§ï¼‰
# 3. æ— æ³•ç²¾ç¡®å®šä½é—®é¢˜ç‚¹
# 4. æ— æ³•æµ‹è¯•ä¿®å¤æ–¹æ¡ˆ
```

**Time Travel çš„ä»·å€¼**ï¼š
```python
# Time Travel åšæ³•ï¼š
# 1. å›æº¯åˆ°ç¬¬4æ­¥çš„çŠ¶æ€
# 2. æŸ¥çœ‹å½“æ—¶çš„ä¸Šä¸‹æ–‡
# 3. ä¿®æ”¹çŠ¶æ€/Prompt
# 4. ä»ç¬¬5æ­¥é‡æ–°æ‰§è¡Œ
# 5. å¯¹æ¯”ä¿®å¤å‰åçš„å·®å¼‚
```

è¿™å°±æ˜¯ LangGraph **Time Travel** çš„æ ¸å¿ƒä»·å€¼ï¼š**è®©Agentçš„æ‰§è¡Œè¿‡ç¨‹å¯é‡ç°ã€å¯å›æº¯ã€å¯ä¿®æ”¹**ã€‚

---

### 4.1 Time Travel æ ¸å¿ƒæ¦‚å¿µ

#### 4.1.1 ä»€ä¹ˆæ˜¯ Time Travel

**å®šä¹‰**ï¼šTime Travel æ˜¯ LangGraph æä¾›çš„çŠ¶æ€å›æº¯æœºåˆ¶ï¼Œå…è®¸ä½ ï¼š
- æŸ¥çœ‹ Agent å†å²æ‰§è¡Œçš„æ¯ä¸€æ­¥çŠ¶æ€
- å›æº¯åˆ°ä»»æ„å†å²èŠ‚ç‚¹
- ä¿®æ”¹çŠ¶æ€åé‡æ–°æ‰§è¡Œ
- å¯¹æ¯”ä¸åŒå†³ç­–è·¯å¾„çš„ç»“æœ

**å‰ç½®æ¡ä»¶**ï¼š
```python
# Time Travel ä¾èµ– Checkpointerï¼ˆæŒä¹…åŒ–ï¼‰
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

graph = StateGraph(...).compile(
    checkpointer=checkpointer  # âœ… å¿…éœ€
)
```

#### 4.1.2 æ ¸å¿ƒ API

**1. get_state_history() - è·å–å†å²çŠ¶æ€**

```python
from langchain.graph import StateGraph, MessagesState

# æ‰§è¡Œ Agent
config = {"configurable": {"thread_id": "user-123"}}
result = graph.invoke({"messages": [("user", "å¸®æˆ‘æŸ¥è¯¢å¤©æ°”")]}, config)

# è·å–å†å²çŠ¶æ€
history = list(graph.get_state_history(config))

print(f"æ€»å…± {len(history)} ä¸ªçŠ¶æ€èŠ‚ç‚¹")

# éå†å†å²
for i, state in enumerate(history):
    print(f"\n=== çŠ¶æ€ {i} ===")
    print(f"èŠ‚ç‚¹: {state.metadata.get('langgraph_node', 'START')}")
    print(f"æ­¥æ•°: {state.metadata.get('step', 0)}")
    print(f"Checkpoint ID: {state.config['configurable']['checkpoint_id']}")
    print(f"æ¶ˆæ¯æ•°: {len(state.values.get('messages', []))}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ€»å…± 7 ä¸ªçŠ¶æ€èŠ‚ç‚¹

=== çŠ¶æ€ 0 ===
èŠ‚ç‚¹: agent
æ­¥æ•°: 3
Checkpoint ID: 1e4f8b3c-...
æ¶ˆæ¯æ•°: 6
(æœ€æ–°çŠ¶æ€)

=== çŠ¶æ€ 1 ===
èŠ‚ç‚¹: tools
æ­¥æ•°: 2
Checkpoint ID: 2a9d7c1e-...
æ¶ˆæ¯æ•°: 5
(å·¥å…·æ‰§è¡Œå)

=== çŠ¶æ€ 2 ===
èŠ‚ç‚¹: agent
æ­¥æ•°: 1
Checkpoint ID: 3f5e2a8d-...
æ¶ˆæ¯æ•°: 3
(é¦–æ¬¡æ¨¡å‹è°ƒç”¨å)

...
```

**2. update_state() - çŠ¶æ€å›æº¯ä¸ä¿®æ”¹**

```python
# å›æº¯åˆ°ç‰¹å®šçŠ¶æ€å¹¶ä¿®æ”¹
checkpoint_id = history[3].config['configurable']['checkpoint_id']

# ä¿®æ”¹çŠ¶æ€ï¼ˆæ³¨å…¥ä¸åŒçš„å€¼ï¼‰
graph.update_state(
    config={"configurable": {"thread_id": "user-123", "checkpoint_id": checkpoint_id}},
    values={
        "messages": [
            ("system", "ä½ ç°åœ¨æ›´åŠ è°¨æ…ï¼Œä¼šä»”ç»†éªŒè¯å·¥å…·ç»“æœ")
        ]
    }
)

# ä»è¯¥çŠ¶æ€é‡æ–°æ‰§è¡Œ
new_result = graph.invoke(None, config={"configurable": {"thread_id": "user-123"}})
```

---

### 4.2 Time Travel å®æˆ˜åœºæ™¯

#### 4.2.1 åœºæ™¯1ï¼šè°ƒè¯•ç”Ÿäº§é—®é¢˜

**é—®é¢˜æè¿°**ï¼šç”¨æˆ·åé¦ˆ Agent åœ¨æŸæ¬¡å¯¹è¯çš„ç¬¬3æ­¥ç»™å‡ºäº†é”™è¯¯ä¿¡æ¯ã€‚

**ä¼ ç»Ÿè°ƒè¯•å›°å¢ƒ**ï¼š
- æ— æ³•é‡ç°ï¼ˆLLM ä¸ç¡®å®šæ€§ï¼‰
- ç”¨æˆ·åªè®°å¾—"ç¬¬3æ­¥å‡ºé”™"ï¼Œä¸è®°å¾—å…·ä½“è¾“å…¥
- é‡æ–°è¿è¡Œè¦èŠ±é’±ï¼ˆAPIè°ƒç”¨ï¼‰

**Time Travel è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.tools import tool

# å·¥å…·å®šä¹‰
@tool
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    # æ¨¡æ‹ŸAPIé”™è¯¯
    if city == "åŒ—äº¬":
        return "é”™è¯¯ï¼šAPIè¶…æ—¶"  # è¿™æ˜¯é—®é¢˜æ‰€åœ¨
    return f"{city}ä»Šå¤©æ™´æœ—ï¼Œ25Â°C"

# åˆ›å»º Agent Graph
checkpointer = MemorySaver()

graph = create_agent(
    ChatOpenAI(model="gpt-4o"),
    tools=[get_weather],
    system_prompt="ä½ æ˜¯å¤©æ°”åŠ©æ‰‹"
).compile(checkpointer=checkpointer)

# æ­¥éª¤1ï¼šé‡ç°ç”¨æˆ·é—®é¢˜ï¼ˆä½¿ç”¨ç›¸åŒçš„ thread_idï¼‰
config = {"configurable": {"thread_id": "reported-issue-123"}}

# å‡è®¾è¿™æ˜¯ç”¨æˆ·å½“æ—¶çš„æ‰§è¡Œï¼ˆå·²ç»å®Œæˆï¼Œä¿å­˜åœ¨Checkpointerä¸­ï¼‰
# result = graph.invoke({"messages": [("user", "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ")]}, config)

# æ­¥éª¤2ï¼šTime Travel è°ƒè¯•
print("=== è·å–å†å²çŠ¶æ€ ===\n")
history = list(graph.get_state_history(config))

for i, state in enumerate(reversed(history)):  # ä»æœ€æ—©åˆ°æœ€æ–°
    step = state.metadata.get('step', 0)
    node = state.metadata.get('langgraph_node', 'START')
    messages = state.values.get('messages', [])

    print(f"æ­¥éª¤ {step} [{node}]:")
    if messages:
        last_msg = messages[-1]
        content = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)
        print(f"  æœ€åæ¶ˆæ¯: {content[:100]}...")

    # å‘ç°é—®é¢˜
    if "é”™è¯¯ï¼šAPIè¶…æ—¶" in str(messages):
        print(f"  âŒ å‘ç°é—®é¢˜ï¼šå·¥å…·è¿”å›é”™è¯¯")
        problem_checkpoint = state.config['configurable']['checkpoint_id']

print(f"\né—®é¢˜å‡ºç°åœ¨ checkpoint: {problem_checkpoint}")

# æ­¥éª¤3ï¼šä¿®æ”¹çŠ¶æ€é‡æ–°æ‰§è¡Œ
print("\n=== ä¿®æ”¹å·¥å…·ç»“æœé‡æ–°æ‰§è¡Œ ===\n")

# è·å–é—®é¢˜èŠ‚ç‚¹çš„çŠ¶æ€
problem_state = next(s for s in history if s.config['configurable']['checkpoint_id'] == problem_checkpoint)

# ä¿®æ”¹æ¶ˆæ¯ï¼ˆæ›¿æ¢é”™è¯¯çš„å·¥å…·ç»“æœï¼‰
modified_messages = problem_state.values['messages'][:-1]  # ç§»é™¤é”™è¯¯æ¶ˆæ¯
modified_messages.append({
    "role": "tool",
    "content": "åŒ—äº¬ä»Šå¤©æ™´æœ—ï¼Œ25Â°C",  # ä¿®æ­£åçš„ç»“æœ
    "name": "get_weather"
})

# æ›´æ–°çŠ¶æ€
graph.update_state(
    config={"configurable": {"thread_id": "reported-issue-123", "checkpoint_id": problem_checkpoint}},
    values={"messages": modified_messages}
)

# ä»è¯¥ç‚¹é‡æ–°æ‰§è¡Œ
fixed_result = graph.invoke(None, config=config)

print("ä¿®å¤åçš„å›ç­”:")
print(fixed_result["messages"][-1].content)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
=== è·å–å†å²çŠ¶æ€ ===

æ­¥éª¤ 1 [agent]:
  æœ€åæ¶ˆæ¯: æˆ‘æ¥å¸®ä½ æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”ã€‚

æ­¥éª¤ 2 [tools]:
  æœ€åæ¶ˆæ¯: é”™è¯¯ï¼šAPIè¶…æ—¶
  âŒ å‘ç°é—®é¢˜ï¼šå·¥å…·è¿”å›é”™è¯¯

æ­¥éª¤ 3 [agent]:
  æœ€åæ¶ˆæ¯: æŠ±æ­‰ï¼Œæ— æ³•è·å–å¤©æ°”ä¿¡æ¯...

é—®é¢˜å‡ºç°åœ¨ checkpoint: 2a9d7c1e-...

=== ä¿®æ”¹å·¥å…·ç»“æœé‡æ–°æ‰§è¡Œ ===

ä¿®å¤åçš„å›ç­”:
åŒ—äº¬ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦25Â°Cï¼Œé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚
```

---

#### 4.2.2 åœºæ™¯2ï¼šA/B æµ‹è¯•ä¸åŒå†³ç­–ç­–ç•¥

**éœ€æ±‚**ï¼šå¯¹æ¯”ä¸¤ç§ Prompt ç­–ç•¥ï¼Œå“ªä¸ªæ•ˆæœæ›´å¥½ï¼Ÿ

```python
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage

# å®šä¹‰çŠ¶æ€
class AgentState(MessagesState):
    iterations: int

# åˆ›å»ºGraph
def agent_node(state: AgentState):
    model = ChatOpenAI(model="gpt-4o")
    response = model.invoke(state["messages"])
    return {
        "messages": state["messages"] + [response],
        "iterations": state.get("iterations", 0) + 1
    }

def should_continue(state: AgentState):
    if state["iterations"] >= 3:
        return "end"
    return "continue"

workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {"continue": "agent", "end": END}
)

checkpointer = MemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

# æ­¥éª¤1ï¼šæ‰§è¡Œç­–ç•¥Aï¼ˆä¿å®ˆç­–ç•¥ï¼‰
config_a = {"configurable": {"thread_id": "test-a"}}

result_a = graph.invoke({
    "messages": [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¿å®ˆçš„åˆ†æå¸ˆï¼Œæ€»æ˜¯å»ºè®®è°¨æ…è¡ŒåŠ¨ã€‚"),
        ("user", "æˆ‘åº”è¯¥æŠ•èµ„è‚¡ç¥¨å—ï¼Ÿ")
    ],
    "iterations": 0
}, config_a)

print("=== ç­–ç•¥Aï¼ˆä¿å®ˆï¼‰ç»“æœ ===")
print(result_a["messages"][-1].content)

# æ­¥éª¤2ï¼šTime Travel - å›åˆ°åˆå§‹çŠ¶æ€ï¼Œæµ‹è¯•ç­–ç•¥B
history_a = list(graph.get_state_history(config_a))
initial_checkpoint = history_a[-1].config['configurable']['checkpoint_id']  # æœ€æ—©çš„çŠ¶æ€

# åˆ›å»ºæ–°çº¿ç¨‹ï¼Œä»ç›¸åŒåˆå§‹çŠ¶æ€å¼€å§‹ï¼Œä½¿ç”¨ç­–ç•¥B
config_b = {"configurable": {"thread_id": "test-b"}}

# æ³¨å…¥ç­–ç•¥Bçš„ç³»ç»Ÿæç¤º
graph.update_state(
    config=config_b,
    values={
        "messages": [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ¿€è¿›çš„åˆ†æå¸ˆï¼Œæ€»æ˜¯å»ºè®®ç§¯æè¡ŒåŠ¨ã€‚"),
            ("user", "æˆ‘åº”è¯¥æŠ•èµ„è‚¡ç¥¨å—ï¼Ÿ")
        ],
        "iterations": 0
    },
    as_node="agent"  # ä»agentèŠ‚ç‚¹å¼€å§‹
)

result_b = graph.invoke(None, config=config_b)

print("\n=== ç­–ç•¥Bï¼ˆæ¿€è¿›ï¼‰ç»“æœ ===")
print(result_b["messages"][-1].content)

# æ­¥éª¤3ï¼šå¯¹æ¯”åˆ†æ
print("\n=== å¯¹æ¯”åˆ†æ ===")
print(f"ç­–ç•¥Aè¿­ä»£æ¬¡æ•°: {result_a['iterations']}")
print(f"ç­–ç•¥Bè¿­ä»£æ¬¡æ•°: {result_b['iterations']}")

# å¯ä»¥è¿›ä¸€æ­¥ç”¨ LLM è¯„ä¼°ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡
```

---

#### 4.2.3 åœºæ™¯3ï¼šå›æº¯ä¿®å¤é”™è¯¯å†³ç­–

**åœºæ™¯**ï¼šAgentåœ¨ä¸­é—´æ­¥éª¤é€‰æ‹©äº†é”™è¯¯çš„å·¥å…·ï¼Œå¯¼è‡´åç»­åç¦»æ–¹å‘ã€‚

```python
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# å·¥å…·å®šä¹‰
@tool
def search_web(query: str) -> str:
    """æœç´¢ç½‘é¡µ"""
    return f"ç½‘é¡µæœç´¢ç»“æœï¼š{query}"

@tool
def search_database(query: str) -> str:
    """æœç´¢æ•°æ®åº“"""
    return f"æ•°æ®åº“æœç´¢ç»“æœï¼š{query}"

# åˆ›å»ºAgent
from langchain.agents import create_agent

checkpointer = MemorySaver()

graph = create_agent(
    ChatOpenAI(model="gpt-4o"),
    tools=[search_web, search_database],
    system_prompt="ä½ æ˜¯æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“æœç´¢å†…éƒ¨æ•°æ®ã€‚"
).compile(checkpointer=checkpointer)

# æ‰§è¡Œ
config = {"configurable": {"thread_id": "debug-session"}}
result = graph.invoke({
    "messages": [("user", "æŸ¥è¯¢æˆ‘ä»¬å…¬å¸å»å¹´çš„é”€å”®é¢")]
}, config)

print("=== åˆæ¬¡æ‰§è¡Œç»“æœ ===")
print(result["messages"][-1].content)

# å‡è®¾Agenté”™è¯¯åœ°ä½¿ç”¨äº†search_webè€Œä¸æ˜¯search_database

# Time Travel è°ƒè¯•
print("\n=== Time Travel æŸ¥æ‰¾é—®é¢˜ ===")
history = list(graph.get_state_history(config))

for state in reversed(history):
    messages = state.values.get('messages', [])
    for msg in messages:
        if hasattr(msg, 'additional_kwargs'):
            tool_calls = msg.additional_kwargs.get('tool_calls', [])
            for tc in tool_calls:
                func_name = tc['function']['name']
                if func_name == 'search_web':
                    print(f"âŒ å‘ç°é”™è¯¯ï¼šä½¿ç”¨äº† {func_name}ï¼Œåº”è¯¥ç”¨ search_database")
                    wrong_checkpoint = state.config['configurable']['checkpoint_id']

# å›æº¯å¹¶ä¿®æ”¹
print("\n=== ä¿®æ”¹å†³ç­–é‡æ–°æ‰§è¡Œ ===")

# è·å–é”™è¯¯å†³ç­–å‰çš„çŠ¶æ€
wrong_state = next(s for s in history if s.config['configurable']['checkpoint_id'] == wrong_checkpoint)

# æ‰‹åŠ¨æ³¨å…¥æ­£ç¡®çš„å·¥å…·è°ƒç”¨ç»“æœ
corrected_messages = wrong_state.values['messages'][:-1]  # ç§»é™¤é”™è¯¯çš„å·¥å…·è°ƒç”¨
corrected_messages.append({
    "role": "tool",
    "content": "æ•°æ®åº“æœç´¢ç»“æœï¼šå…¬å¸å»å¹´é”€å”®é¢ä¸º5000ä¸‡å…ƒ",
    "name": "search_database"
})

graph.update_state(
    config={"configurable": {"thread_id": "debug-session", "checkpoint_id": wrong_checkpoint}},
    values={"messages": corrected_messages}
)

# é‡æ–°æ‰§è¡Œ
fixed_result = graph.invoke(None, config=config)

print("ä¿®å¤åçš„ç»“æœ:")
print(fixed_result["messages"][-1].content)
```

---

### 4.3 Time Travel é«˜çº§æŠ€å·§

#### 4.3.1 å¹¶è¡Œæµ‹è¯•å¤šä¸ªåˆ†æ”¯

```python
from concurrent.futures import ThreadPoolExecutor

def test_branch(branch_name: str, modified_values: dict):
    """æµ‹è¯•ä¸€ä¸ªåˆ†æ”¯"""
    config = {"configurable": {"thread_id": f"branch-{branch_name}"}}

    # æ³¨å…¥ä¿®æ”¹åçš„å€¼
    graph.update_state(
        config=config,
        values=modified_values,
        as_node="agent"
    )

    # æ‰§è¡Œ
    result = graph.invoke(None, config=config)

    return branch_name, result

# å¹¶è¡Œæµ‹è¯•3ä¸ªä¸åŒçš„ç­–ç•¥
branches = {
    "conservative": {
        "messages": [SystemMessage(content="ä¿å®ˆç­–ç•¥ï¼š...")]
    },
    "moderate": {
        "messages": [SystemMessage(content="æ¸©å’Œç­–ç•¥ï¼š...")]
    },
    "aggressive": {
        "messages": [SystemMessage(content="æ¿€è¿›ç­–ç•¥ï¼š...")]
    }
}

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(test_branch, name, values) for name, values in branches.items()]

    results = {}
    for future in futures:
        branch_name, result = future.result()
        results[branch_name] = result

# å¯¹æ¯”ç»“æœ
for name, result in results.items():
    print(f"\n=== {name} ===")
    print(result["messages"][-1].content[:100])
```

#### 4.3.2 çŠ¶æ€å¿«ç…§ä¸æ¢å¤

```python
# ä¿å­˜å…³é”®çŠ¶æ€å¿«ç…§
def save_snapshot(graph, config, snapshot_name: str):
    """ä¿å­˜çŠ¶æ€å¿«ç…§"""
    current_state = graph.get_state(config)

    # ä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶
    snapshot_data = {
        "checkpoint_id": current_state.config['configurable']['checkpoint_id'],
        "values": current_state.values,
        "metadata": current_state.metadata
    }

    # ç¤ºä¾‹ï¼šä¿å­˜åˆ°JSON
    import json
    with open(f"snapshots/{snapshot_name}.json", "w") as f:
        json.dump(snapshot_data, f)

    return snapshot_data

# æ¢å¤å¿«ç…§
def restore_snapshot(graph, snapshot_name: str):
    """æ¢å¤å¿«ç…§"""
    import json
    with open(f"snapshots/{snapshot_name}.json", "r") as f:
        snapshot_data = json.load(f)

    config = {"configurable": {
        "thread_id": "restored",
        "checkpoint_id": snapshot_data['checkpoint_id']
    }}

    graph.update_state(
        config=config,
        values=snapshot_data['values']
    )

    return config

# ä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "main-session"}}

# æ‰§è¡Œåˆ°æŸä¸ªå…³é”®ç‚¹
result = graph.invoke({"messages": [("user", "å¤æ‚ä»»åŠ¡")]}, config)

# ä¿å­˜å¿«ç…§
save_snapshot(graph, config, "before_critical_decision")

# ç»§ç»­æ‰§è¡Œ...
# å¦‚æœå‡ºé”™ï¼Œå¯ä»¥éšæ—¶æ¢å¤
restored_config = restore_snapshot(graph, "before_critical_decision")
```

---

### 4.4 Time Travel æœ€ä½³å®è·µ

#### 4.4.1 ç”Ÿäº§ç¯å¢ƒå»ºè®®

**1. é€‰æ‹©åˆé€‚çš„ Checkpointer**

```python
# å¼€å‘ç¯å¢ƒï¼šMemorySaverï¼ˆå¿«é€Ÿï¼Œä½†é‡å¯ä¸¢å¤±ï¼‰
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()

# ç”Ÿäº§ç¯å¢ƒï¼šPostgresSaverï¼ˆæŒä¹…åŒ–ï¼‰
from langgraph.checkpoint.postgres import PostgresSaver
checkpointer = PostgresSaver.from_conn_string("postgresql://...")

# é«˜æ€§èƒ½åœºæ™¯ï¼šRedisSaver
from langgraph.checkpoint.redis import RedisSaver
checkpointer = RedisSaver.from_conn_string("redis://...")
```

**2. Checkpoint è¿‡æœŸç­–ç•¥**

```python
# é…ç½®TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰
from datetime import timedelta

checkpointer = PostgresSaver.from_conn_string(
    "postgresql://...",
    checkpoint_ttl=timedelta(days=7)  # 7å¤©åè‡ªåŠ¨åˆ é™¤
)
```

**3. ç›‘æ§ Checkpoint æ•°é‡**

```python
# å®šæœŸæ¸…ç†æ—§çš„ checkpoints
def cleanup_old_checkpoints(thread_id: str, keep_last_n: int = 10):
    """åªä¿ç•™æœ€è¿‘Nä¸ªcheckpoint"""
    history = list(graph.get_state_history({"configurable": {"thread_id": thread_id}}))

    if len(history) > keep_last_n:
        for old_state in history[keep_last_n:]:
            # åˆ é™¤æ—§checkpoint
            checkpointer.delete(old_state.config['configurable']['checkpoint_id'])
```

#### 4.4.2 è°ƒè¯•å·¥ä½œæµ

**æ ‡å‡†è°ƒè¯•æµç¨‹**ï¼š

```markdown
1. é‡ç°é—®é¢˜
   - ä½¿ç”¨ç›¸åŒçš„ thread_id
   - è·å–å†å²çŠ¶æ€

2. å®šä½é—®é¢˜èŠ‚ç‚¹
   - éå†å†å²çŠ¶æ€
   - æ‰¾åˆ°å¼‚å¸¸çš„è¾“å‡º

3. å›æº¯ä¿®æ”¹
   - å›åˆ°é—®é¢˜èŠ‚ç‚¹å‰
   - æ³¨å…¥ä¿®æ­£åçš„å€¼

4. éªŒè¯ä¿®å¤
   - é‡æ–°æ‰§è¡Œ
   - å¯¹æ¯”ç»“æœ

5. æŒä¹…åŒ–ä¿®å¤
   - å°†ä¿®å¤åº”ç”¨åˆ°ä»£ç 
   - ç¼–å†™å›å½’æµ‹è¯•
```

---

### 4.5 Time Travel ä¸ LangSmith é›†æˆ

**ç»“åˆ LangSmith å¯è§†åŒ–è°ƒè¯•**ï¼š

```python
import os
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "lsv2_..."

# Time Travel + LangSmith
config = {"configurable": {"thread_id": "debug-with-langsmith"}}

# æ‰§è¡Œä¼šè‡ªåŠ¨è¿½è¸ªåˆ° LangSmith
result = graph.invoke({"messages": [("user", "å¤æ‚æŸ¥è¯¢")]}, config)

# åœ¨ LangSmith UI ä¸­:
# 1. æŸ¥çœ‹å®Œæ•´æ‰§è¡Œæ ‘
# 2. ç‚¹å‡»ä»»æ„èŠ‚ç‚¹æŸ¥çœ‹çŠ¶æ€
# 3. å¤åˆ¶ checkpoint_id

# åœ¨ä»£ç ä¸­å›æº¯ï¼ˆä½¿ç”¨ä»UIå¤åˆ¶çš„checkpoint_idï¼‰
graph.update_state(
    config={"configurable": {"thread_id": "debug-with-langsmith", "checkpoint_id": "ä»UIå¤åˆ¶çš„ID"}},
    values={...}
)
```

---

### 4.6 å°ç»“

**Time Travel çš„æ ¸å¿ƒä»·å€¼**ï¼š

| åœºæ™¯ | ä»·å€¼ | ä¼ ç»Ÿæ–¹å¼ | Time Travel æ–¹å¼ |
|------|------|---------|-----------------|
| **ç”Ÿäº§é—®é¢˜è°ƒè¯•** | â­â­â­â­â­ | é‡ç°å›°éš¾ï¼Œæˆæœ¬é«˜ | ç²¾ç¡®å›æº¯ï¼Œå¿«é€Ÿå®šä½ |
| **A/B æµ‹è¯•** | â­â­â­â­â­ | éœ€è¦å¤šæ¬¡å®Œæ•´æ‰§è¡Œ | ä»åŒä¸€èµ·ç‚¹åˆ†å‰æµ‹è¯• |
| **å†³ç­–ä¼˜åŒ–** | â­â­â­â­ | æ— æ³•å¯¹æ¯”å†å²å†³ç­– | å›æº¯å¹¶æµ‹è¯•ä¸åŒè·¯å¾„ |
| **é”™è¯¯æ¢å¤** | â­â­â­â­ | ä»å¤´å¼€å§‹ | ä»é”™è¯¯ç‚¹ä¿®å¤ |
| **å­¦ä¹ åˆ†æ** | â­â­â­ | åªèƒ½çœ‹æ—¥å¿— | å®Œæ•´çŠ¶æ€å›æ”¾ |

**å…³é”®è¦ç‚¹**ï¼š
1. âœ… Time Travel ä¾èµ– Checkpointerï¼Œç”Ÿäº§ç¯å¢ƒç”¨ PostgresSaver/RedisSaver
2. âœ… `get_state_history()` è·å–å†å²ï¼Œ`update_state()` å›æº¯ä¿®æ”¹
3. âœ… é€‚åˆè°ƒè¯•ã€A/Bæµ‹è¯•ã€å†³ç­–ä¼˜åŒ–
4. âœ… ç»“åˆ LangSmith å¯è§†åŒ–æ•ˆæœæ›´å¥½
5. âš ï¸ æ³¨æ„ Checkpoint æ•°é‡ï¼Œå®šæœŸæ¸…ç†é¿å…å­˜å‚¨è†¨èƒ€

**ä¸‹ä¸€æ­¥**ï¼š
- ç¬¬5ç« ï¼šLangGraph å·¥ç¨‹åŒ–å®è·µï¼ˆlanggraph.jsonã€langgraph devã€æµ‹è¯•ï¼‰
- ç¬¬6ç« ï¼šFunctional APIï¼ˆå‡½æ•°å¼ç¼–ç¨‹é£æ ¼ï¼‰

---

**ç»ƒä¹ **ï¼š

1. **åŸºç¡€ç»ƒä¹ **ï¼š
   - åˆ›å»ºä¸€ä¸ªç®€å•çš„ Agent
   - æ‰§è¡Œå¹¶ä¿å­˜å¤šä¸ª checkpoint
   - ä½¿ç”¨ `get_state_history()` æŸ¥çœ‹å†å²
   - å›æº¯åˆ°ç¬¬2æ­¥é‡æ–°æ‰§è¡Œ

2. **è¿›é˜¶ç»ƒä¹ **ï¼š
   - å®ç°ä¸€ä¸ª "åæ‚”è¯" åŠŸèƒ½ï¼šç”¨æˆ·å¯ä»¥è¯´ "è¿”å›ä¸Šä¸€æ­¥"
   - ä¿å­˜å…³é”®å†³ç­–ç‚¹çš„å¿«ç…§
   - å¹¶è¡Œæµ‹è¯•3ç§ä¸åŒçš„ç­–ç•¥

3. **ç”Ÿäº§åœºæ™¯**ï¼š
   - æ¨¡æ‹Ÿç”Ÿäº§é—®é¢˜ï¼šAgent è°ƒç”¨äº†é”™è¯¯çš„å·¥å…·
   - ä½¿ç”¨ Time Travel å®šä½é—®é¢˜
   - ä¿®æ”¹å·¥å…·ç»“æœé‡æ–°æ‰§è¡Œ
   - å°†ä¿®å¤æŒä¹…åŒ–åˆ°ä»£ç 

---
