# ç¬¬å››ç¯‡ï¼šRAGåŸºç¡€ç¯‡ (LlamaIndex)

## ğŸ“‹ å‰ç½®å‡†å¤‡

### ç¯å¢ƒé…ç½®

```bash
# æ ¸å¿ƒä¾èµ–
pip install llama-index>=0.14.8
pip install llama-index-core>=0.14.8
pip install llama-index-llms-openai>=0.2.0
pip install llama-index-embeddings-openai>=0.2.0

# å‘é‡æ•°æ®åº“é›†æˆ
pip install llama-index-vector-stores-chroma
pip install chromadb>=0.5.0

# å¯é€‰ä¾èµ–
pip install pypdf  # PDFæ”¯æŒ
pip install python-dotenv  # ç¯å¢ƒå˜é‡ç®¡ç†
```

### ç¯å¢ƒå˜é‡

```python
# .env
OPENAI_API_KEY=sk-your-api-key
```

---

# ç¬¬ 1 ç« ï¼šLlamaIndexæ ¸å¿ƒæ¦‚å¿µ

## 1.1 ä¸ºä»€ä¹ˆé€‰æ‹©LlamaIndex

### 1.1.1 LlamaIndexçš„è®¾è®¡å“²å­¦

LlamaIndexæ˜¯ä¸“é—¨ä¸ºRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨è€Œè®¾è®¡çš„æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç†å¿µæ˜¯ï¼š

**æ ¸å¿ƒç†å¿µ**ï¼š

1. **æ•°æ®ä¼˜å…ˆ**ï¼ˆData-Firstï¼‰
   - ä¸€åˆ‡ä»æ•°æ®å¼€å§‹
   - å†…ç½®ä¸°å¯Œçš„æ•°æ®è¿æ¥å™¨
   - æ”¯æŒç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®

2. **ç´¢å¼•å³æŸ¥è¯¢**ï¼ˆIndex as Interfaceï¼‰
   - å¤šç§ç´¢å¼•ç±»å‹é€‚åº”ä¸åŒåœºæ™¯
   - ç´¢å¼•è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥
   - æŸ¥è¯¢å¼•æ“å¼€ç®±å³ç”¨

3. **æ¨¡å—åŒ–è®¾è®¡**ï¼ˆModular Architectureï¼‰
   ```
   Reader â†’ Parser â†’ Index â†’ Retriever â†’ Query Engine
   ```

4. **LLMæ— å…³**ï¼ˆLLM-Agnosticï¼‰
   - æ”¯æŒOpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹
   - ç»Ÿä¸€çš„æ¥å£åˆ‡æ¢æ¨¡å‹

### 1.1.2 LlamaIndexçš„æ ¸å¿ƒä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **å¼€ç®±å³ç”¨** | 5è¡Œä»£ç å®ç°å®Œæ•´RAG | SimpleDirectoryReader + VectorStoreIndex |
| **ä¸°å¯Œçš„åŠ è½½å™¨** | æ”¯æŒ100+ç§æ•°æ®æº | PDFã€Webã€æ•°æ®åº“ã€APIç­‰ |
| **å¤šç§ç´¢å¼•ç±»å‹** | é€‚åº”ä¸åŒåœºæ™¯ | Vectorã€Summaryã€Treeã€Keywordç­‰ |
| **æ™ºèƒ½æŸ¥è¯¢å¼•æ“** | è‡ªåŠ¨ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ | è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ£€ç´¢æ–¹å¼ |
| **æ¨¡å—åŒ–æ¶æ„** | çµæ´»ç»„åˆ | å¯æ›¿æ¢ä»»ä½•ç»„ä»¶ |

---

## 1.2 å¿«é€Ÿå¼€å§‹ï¼š5è¡Œä»£ç å®ç°RAG

### 1.2.1 æœ€ç®€å•çš„RAGåº”ç”¨

```python
"""
5è¡Œä»£ç å®ç°å®Œæ•´RAG - LlamaIndexçš„å¼ºå¤§ä¹‹å¤„
"""
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import os

# è®¾ç½®API Key
os.environ["OPENAI_API_KEY"] = "sk-your-key"

# 1. åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# 2. åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine()

# 4. æŸ¥è¯¢
response = query_engine.query("æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")

# 5. è¾“å‡º
print(response)
```

**å°±è¿™ä¹ˆç®€å•ï¼** LlamaIndexå·²ç»è‡ªåŠ¨å®Œæˆäº†ï¼š
- âœ… æ–‡æ¡£åˆ†å—
- âœ… å‘é‡åŒ–ï¼ˆEmbeddingï¼‰
- âœ… å‘é‡å­˜å‚¨
- âœ… æ£€ç´¢
- âœ… LLMç”Ÿæˆç­”æ¡ˆ

### 1.2.2 æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

print(f"ğŸ“„ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
for i, doc in enumerate(documents[:2], 1):
    print(f"\næ–‡æ¡£ {i}:")
    print(f"  å†…å®¹: {doc.text[:200]}...")
    print(f"  å…ƒæ•°æ®: {doc.metadata}")

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æŸ¥è¯¢ï¼ˆå¸¦æ¥æºï¼‰
query_engine = index.as_query_engine(
    similarity_top_k=3,  # è¿”å›Top-3æœ€ç›¸å…³æ–‡æ¡£
    response_mode="compact"
)

response = query_engine.query("ä»€ä¹ˆæ˜¯LlamaIndexï¼Ÿ")

print(f"\nğŸ’¡ å›ç­”:\n{response}\n")
print("ğŸ“š æ¥æº:")
for i, node in enumerate(response.source_nodes, 1):
    print(f"{i}. {node.text[:100]}... (å¾—åˆ†: {node.score:.4f})")
```

---

## 1.3 æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1.3.1 æ–‡æ¡£ï¼ˆDocumentï¼‰

Documentæ˜¯LlamaIndexçš„åŸºæœ¬æ•°æ®å•å…ƒï¼š

```python
from llama_index.core import Document

# æ‰‹åŠ¨åˆ›å»ºæ–‡æ¡£
doc1 = Document(
    text="è¿™æ˜¯æ–‡æ¡£å†…å®¹",
    metadata={
        "source": "manual",
        "author": "å¼ ä¸‰",
        "date": "2025-11-23"
    }
)

# æŸ¥çœ‹æ–‡æ¡£å±æ€§
print(f"æ–‡æ¡£ID: {doc1.doc_id}")
print(f"å†…å®¹: {doc1.text}")
print(f"å…ƒæ•°æ®: {doc1.metadata}")

# æ‰¹é‡åˆ›å»º
documents = [
    Document(text="æ–‡æ¡£1å†…å®¹", metadata={"id": 1}),
    Document(text="æ–‡æ¡£2å†…å®¹", metadata={"id": 2}),
    Document(text="æ–‡æ¡£3å†…å®¹", metadata={"id": 3})
]
```

### 1.3.2 èŠ‚ç‚¹ï¼ˆNodeï¼‰

Nodeæ˜¯æ–‡æ¡£åˆ†å—åçš„å•å…ƒï¼š

```python
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Document

# åˆ›å»ºæ–‡æ¡£
doc = Document(text="å¾ˆé•¿çš„æ–‡æœ¬å†…å®¹..." * 100)

# åˆ›å»ºåˆ†å—å™¨
parser = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=50
)

# åˆ†å—
nodes = parser.get_nodes_from_documents([doc])

print(f"âœ‚ï¸  åˆ†å‰²æˆ {len(nodes)} ä¸ªèŠ‚ç‚¹")
for i, node in enumerate(nodes[:3], 1):
    print(f"\nèŠ‚ç‚¹ {i}:")
    print(f"  å†…å®¹: {node.text[:100]}...")
    print(f"  é•¿åº¦: {len(node.text)}")
```

### 1.3.3 ç´¢å¼•ï¼ˆIndexï¼‰

ç´¢å¼•æ˜¯LlamaIndexçš„æ ¸å¿ƒï¼š

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºå‘é‡ç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æŒä¹…åŒ–ç´¢å¼•
index.storage_context.persist(persist_dir="./storage")

# ä»ç£ç›˜åŠ è½½ç´¢å¼•
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
loaded_index = load_index_from_storage(storage_context)
```

---

# ç¬¬ 2 ç« ï¼šæ–‡æ¡£åŠ è½½ä¸å¤„ç†

## 2.1 æ–‡æ¡£åŠ è½½å™¨

### 2.1.1 SimpleDirectoryReaderï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
from llama_index.core import SimpleDirectoryReader

# åŸºç¡€ç”¨æ³•ï¼šåŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
documents = SimpleDirectoryReader("./data").load_data()

# æŒ‡å®šæ–‡ä»¶ç±»å‹
documents = SimpleDirectoryReader(
    "./data",
    required_exts=[".pdf", ".txt", ".md"]
).load_data()

# é€’å½’åŠ è½½å­ç›®å½•
documents = SimpleDirectoryReader(
    "./data",
    recursive=True
).load_data()

# æ’é™¤æŸäº›æ–‡ä»¶
documents = SimpleDirectoryReader(
    "./data",
    exclude=["temp.txt", "*.log"]
).load_data()

# è‡ªå®šä¹‰å…ƒæ•°æ®
documents = SimpleDirectoryReader(
    "./data",
    file_metadata=lambda filename: {
        "source": filename,
        "category": "docs"
    }
).load_data()
```

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
- ğŸ“„ æ–‡æœ¬ï¼š`.txt`, `.md`, `.csv`
- ğŸ“˜ æ–‡æ¡£ï¼š`.pdf`, `.docx`, `.pptx`
- ğŸ’» ä»£ç ï¼š`.py`, `.js`, `.java`, `.cpp`
- ğŸŒ ç½‘é¡µï¼š`.html`, `.htm`
- ğŸ“Š æ•°æ®ï¼š`.json`, `.xml`

### 2.1.2 ä¸“ç”¨åŠ è½½å™¨

```python
# PDFåŠ è½½å™¨
from llama_index.readers.file import PDFReader

reader = PDFReader()
documents = reader.load_data(file="paper.pdf")

# ç½‘é¡µåŠ è½½å™¨
from llama_index.readers.web import SimpleWebPageReader

reader = SimpleWebPageReader()
documents = reader.load_data(urls=["https://example.com"])

# æ•°æ®åº“åŠ è½½å™¨
from llama_index.readers.database import DatabaseReader

reader = DatabaseReader(
    uri="postgresql://user:password@localhost/dbname"
)
documents = reader.load_data(
    query="SELECT id, content FROM documents"
)
```

---

## 2.2 èŠ‚ç‚¹è§£æå™¨ï¼ˆNode Parserï¼‰

### 2.2.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ

```mermaid
graph LR
    A[é•¿æ–‡æ¡£<br>10000å­—] --> B{ç›´æ¥è¾“å…¥LLM?}
    B -- âŒ --> C[è¶…å‡ºä¸Šä¸‹æ–‡çª—å£]
    B -- âŒ --> D[ä¿¡æ¯å™ªéŸ³å¤ªå¤š]
    B -- âœ… åˆ†å— --> E[å¤šä¸ªå°å—<br>æ¯å—500å­—]
    E --> F[ç²¾ç¡®æ£€ç´¢]
    F --> G[é«˜è´¨é‡å›ç­”]
```

**åˆ†å—çš„å¥½å¤„**ï¼š
- âœ… é€‚åº”æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£
- âœ… æé«˜æ£€ç´¢ç²¾ç¡®åº¦
- âœ… é™ä½æˆæœ¬ï¼ˆåªå¤„ç†ç›¸å…³ç‰‡æ®µï¼‰
- âœ… ä¿æŒè¯­ä¹‰å®Œæ•´æ€§

### 2.2.2 SentenceSplitter - æ™ºèƒ½å¥å­åˆ†å‰²

```python
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import Document

# åˆ›å»ºåˆ†å‰²å™¨
splitter = SentenceSplitter(
    chunk_size=512,          # æ¯å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    chunk_overlap=50,        # å—ä¹‹é—´é‡å ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
    separator=" "            # åˆ†éš”ç¬¦
)

# åˆ†å‰²æ–‡æ¡£
doc = Document(text="å¾ˆé•¿çš„æ–‡æœ¬å†…å®¹...")
nodes = splitter.get_nodes_from_documents([doc])

# æŸ¥çœ‹ç»“æœ
for i, node in enumerate(nodes[:3], 1):
    print(f"\nèŠ‚ç‚¹ {i}:")
    print(f"  å†…å®¹: {node.text[:100]}...")
    print(f"  é•¿åº¦: {len(node.text)}")
    print(f"  å…ƒæ•°æ®: {node.metadata}")
```

### 2.2.3 SemanticSplitter - è¯­ä¹‰åˆ†å—

```python
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

# è¯­ä¹‰åˆ†å—å™¨ï¼ˆæ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—ï¼‰
semantic_splitter = SemanticSplitterNodeParser(
    buffer_size=1,                      # ç¼“å†²åŒºå¤§å°
    breakpoint_percentile_threshold=95, # è¯­ä¹‰æ–­ç‚¹é˜ˆå€¼
    embed_model=OpenAIEmbedding()       # ä½¿ç”¨çš„embeddingæ¨¡å‹
)

# åˆ†å‰²
nodes = semantic_splitter.get_nodes_from_documents(documents)

print(f"âœ‚ï¸  è¯­ä¹‰åˆ†å—åˆ›å»ºäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
```

**è¯­ä¹‰åˆ†å—çš„ä¼˜åŠ¿**ï¼š
- âœ… ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- âœ… è‡ªé€‚åº”å—å¤§å°
- âœ… æ›´å¥½çš„æ£€ç´¢æ•ˆæœ

**ä½•æ—¶ä½¿ç”¨è¯­ä¹‰åˆ†å—**ï¼š
- é•¿æ–‡æ¡£ï¼ˆ> 5000å­—ï¼‰
- å¤æ‚ç»“æ„ï¼ˆå­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ï¼‰
- é«˜è´¨é‡è¦æ±‚ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

### 2.2.4 åˆ†å—ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| **SentenceSplitter** | å¿«é€Ÿã€ç®€å• | å¯èƒ½åˆ‡æ–­è¯­ä¹‰ | é€šç”¨æ–‡æ¡£ã€å¿«é€ŸåŸå‹ |
| **SemanticSplitter** | è¯­ä¹‰å®Œæ•´æ€§æœ€ä½³ | è®¡ç®—å¼€é”€å¤§ | å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ |
| **CodeSplitter** | ä¿æŒä»£ç ç»“æ„ | ä»…é™ä»£ç  | ä»£ç æ–‡æ¡£ |
| **MarkdownNodeParser** | ä¿æŒMarkdownç»“æ„ | ä»…é™Markdown | æ–‡æ¡£ã€æ•™ç¨‹ |

---

# ç¬¬ 3 ç« ï¼šç´¢å¼•ä¸æŸ¥è¯¢

## 3.1 ç´¢å¼•ç±»å‹

### 3.1.1 VectorStoreIndex - å‘é‡ç´¢å¼•ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºå‘é‡ç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æŸ¥è¯¢
query_engine = index.as_query_engine(
    similarity_top_k=3  # è¿”å›æœ€ç›¸ä¼¼çš„3ä¸ªèŠ‚ç‚¹
)
response = query_engine.query("ä»€ä¹ˆæ˜¯RAG?")
print(response)
```

**å·¥ä½œåŸç†**ï¼š
```mermaid
graph LR
    A[æ–‡æ¡£] --> B[Embedding]
    B --> C[å‘é‡å­˜å‚¨]
    D[æŸ¥è¯¢] --> E[Embedding]
    E --> F[å‘é‡ç›¸ä¼¼åº¦æœç´¢]
    C --> F
    F --> G[Top-KèŠ‚ç‚¹]
    G --> H[LLMç”Ÿæˆç­”æ¡ˆ]
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… è¯­ä¹‰æœç´¢
- âœ… é—®ç­”ç³»ç»Ÿ
- âœ… æ–‡æ¡£æ£€ç´¢

### 3.1.2 SummaryIndex - æ‘˜è¦ç´¢å¼•

```python
from llama_index.core import SummaryIndex

# åˆ›å»ºæ‘˜è¦ç´¢å¼•
summary_index = SummaryIndex.from_documents(documents)

# æŸ¥è¯¢ï¼ˆä¼šéå†æ‰€æœ‰æ–‡æ¡£ï¼‰
query_engine = summary_index.as_query_engine()
response = query_engine.query("æ€»ç»“æ‰€æœ‰æ–‡æ¡£çš„è¦ç‚¹")
print(response)
```

**ç‰¹ç‚¹**ï¼š
- éå†æ‰€æœ‰èŠ‚ç‚¹
- é€‚åˆæ‘˜è¦ç±»ä»»åŠ¡
- è®¡ç®—æˆæœ¬é«˜

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… æ–‡æ¡£æ‘˜è¦
- âœ… å…¨é¢åˆ†æ
- âœ… å°æ•°æ®é›†

### 3.1.3 TreeIndex - æ ‘å½¢ç´¢å¼•

```python
from llama_index.core import TreeIndex

# åˆ›å»ºæ ‘å½¢ç´¢å¼•
tree_index = TreeIndex.from_documents(documents)

# æŸ¥è¯¢
query_engine = tree_index.as_query_engine()
response = query_engine.query("åˆ†å±‚æ¬¡æ€»ç»“æ–‡æ¡£")
print(response)
```

**ç‰¹ç‚¹**ï¼š
- å±‚æ¬¡åŒ–ç»“æ„
- è‡ªåº•å‘ä¸Šæ‘˜è¦
- é€‚åˆå¤§æ–‡æ¡£

### 3.1.4 KeywordTableIndex - å…³é”®è¯ç´¢å¼•

```python
from llama_index.core import KeywordTableIndex

# åˆ›å»ºå…³é”®è¯ç´¢å¼•
keyword_index = KeywordTableIndex.from_documents(documents)

# æŸ¥è¯¢
query_engine = keyword_index.as_query_engine()
response = query_engine.query("Pythonç¼–ç¨‹")
print(response)
```

**ç‰¹ç‚¹**ï¼š
- åŸºäºå…³é”®è¯åŒ¹é…
- é€Ÿåº¦å¿«
- ç²¾ç¡®åŒ¹é…

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… ç²¾ç¡®å…³é”®è¯æœç´¢
- âœ… ç»“æ„åŒ–æ–‡æ¡£
- âœ… ä»£ç æœç´¢

---

## 3.2 å‘é‡å­˜å‚¨

### 3.2.1 å†…ç½®å‘é‡å­˜å‚¨

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()

# é»˜è®¤ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼ˆSimpleVectorStoreï¼‰
index = VectorStoreIndex.from_documents(documents)

# æŒä¹…åŒ–åˆ°ç£ç›˜
index.storage_context.persist(persist_dir="./storage")

# ä»ç£ç›˜åŠ è½½
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### 3.2.2 é›†æˆChromaå‘é‡æ•°æ®åº“

```python
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader
import chromadb

# åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("my_collection")

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# åŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)

# æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("ä»€ä¹ˆæ˜¯LlamaIndex?")
print(response)
```

### 3.2.3 å‘é‡æ•°æ®åº“é€‰æ‹©æŒ‡å—

| æ•°æ®åº“ | ç±»å‹ | æ€§èƒ½ | éƒ¨ç½²éš¾åº¦ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|---------|---------|
| **SimpleVectorStore** | å†…å­˜ | ä½ | â­ | å¼€å‘æµ‹è¯•ã€å°æ•°æ®é›† |
| **Chroma** | åµŒå…¥å¼ | ä¸­ | â­â­ | ä¸­å°å‹åº”ç”¨ã€å¿«é€Ÿå¼€å‘ |
| **Pinecone** | äº‘æœåŠ¡ | é«˜ | â­ | äº‘åŸç”Ÿã€æ— éœ€è¿ç»´ |
| **Qdrant** | æœåŠ¡ | é«˜ | â­â­â­ | ç”Ÿäº§ç¯å¢ƒã€åˆ†å¸ƒå¼ |
| **Weaviate** | æœåŠ¡ | é«˜ | â­â­â­ | ä¼ä¸šçº§ã€GraphRAG |

---

## 3.3 æŸ¥è¯¢å¼•æ“

### 3.3.1 åŸºç¡€æŸ¥è¯¢å¼•æ“

```python
from llama_index.core import VectorStoreIndex

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    similarity_top_k=3,           # Top-Kæ£€ç´¢
    response_mode="compact",      # å“åº”æ¨¡å¼
    verbose=True                  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
)

# æŸ¥è¯¢
response = query_engine.query("ä»€ä¹ˆæ˜¯LlamaIndex?")
print(response)

# æŸ¥çœ‹æ¥æº
print("\næ¥æºèŠ‚ç‚¹:")
for node in response.source_nodes:
    print(f"- {node.text[:100]}...")
    print(f"  å¾—åˆ†: {node.score:.4f}")
```

### 3.3.2 å“åº”æ¨¡å¼ï¼ˆResponse Modeï¼‰

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **refine** | é€ä¸ªèŠ‚ç‚¹ç²¾ç‚¼ç­”æ¡ˆï¼ˆé»˜è®¤ï¼‰ | é«˜è´¨é‡ç­”æ¡ˆ |
| **compact** | åˆå¹¶èŠ‚ç‚¹åä¸€æ¬¡ç”Ÿæˆ | å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ |
| **tree_summarize** | æ ‘å½¢æ±‡æ€» | å¤§é‡æ–‡æ¡£ |
| **simple_summarize** | ç®€å•åˆå¹¶ | å¿«é€Ÿæ‘˜è¦ |
| **no_text** | åªè¿”å›èŠ‚ç‚¹ï¼Œä¸ç”Ÿæˆ | æ£€ç´¢æµ‹è¯• |

```python
# ä¸åŒå“åº”æ¨¡å¼å¯¹æ¯”
query_engine_refine = index.as_query_engine(response_mode="refine")
query_engine_compact = index.as_query_engine(response_mode="compact")

query = "ä»€ä¹ˆæ˜¯RAG?"
response1 = query_engine_refine.query(query)  # æ›´é«˜è´¨é‡
response2 = query_engine_compact.query(query)  # æ›´å¿«é€Ÿåº¦
```

### 3.3.3 æµå¼è¾“å‡º

```python
# å¯ç”¨æµå¼è¾“å‡º
query_engine = index.as_query_engine(streaming=True)

response = query_engine.query("è¯¦ç»†è§£é‡ŠLlamaIndexçš„å·¥ä½œåŸç†")

# æµå¼æ‰“å°
print("å›ç­”: ", end="")
for text in response.response_gen:
    print(text, end="", flush=True)
print()
```

### 3.3.4 è‡ªå®šä¹‰Prompt

```python
from llama_index.core import PromptTemplate

# è‡ªå®šä¹‰QAæ¨¡æ¿
qa_prompt_tmpl = PromptTemplate(
    "ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
    "{context_str}\n"
    "æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¸è¦ä½¿ç”¨å…ˆéªŒçŸ¥è¯†ï¼‰ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n"
    "{query_str}\n"
    "ç­”æ¡ˆï¼š"
)

# åº”ç”¨è‡ªå®šä¹‰Prompt
query_engine = index.as_query_engine(
    text_qa_template=qa_prompt_tmpl
)

response = query_engine.query("ä»€ä¹ˆæ˜¯å‘é‡ç´¢å¼•ï¼Ÿ")
print(response)
```

---

## 3.4 æ£€ç´¢å™¨ï¼ˆRetrieversï¼‰

### 3.4.1 åŸºç¡€æ£€ç´¢å™¨

```python
from llama_index.core import VectorStoreIndex

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = index.as_retriever(
    similarity_top_k=5,  # è¿”å›Top-5
    vector_store_query_mode="default"
)

# æ£€ç´¢
nodes = retriever.retrieve("ä»€ä¹ˆæ˜¯å‘é‡ç´¢å¼•?")

for i, node in enumerate(nodes, 1):
    print(f"\nèŠ‚ç‚¹ {i} (å¾—åˆ†: {node.score:.4f}):")
    print(node.text[:200])
```

### 3.4.2 è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
from llama_index.core.retrievers import VectorIndexRetriever

# å‘é‡æ£€ç´¢å™¨
vector_retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=3
)

# æ£€ç´¢
nodes = vector_retriever.retrieve("æŸ¥è¯¢æ–‡æœ¬")
for node in nodes:
    print(f"- {node.text[:100]}... (å¾—åˆ†: {node.score:.4f})")
```

---

# ç¬¬ 4 ç« ï¼šé…ç½®ä¸ä¼˜åŒ–

## 4.1 å…¨å±€é…ç½®ï¼ˆSettingsï¼‰

### 4.1.1 é…ç½®LLMå’ŒEmbedding

```python
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# é…ç½®LLM
Settings.llm = OpenAI(
    model="gpt-4-turbo-preview",
    temperature=0.1,
    api_key="your-api-key"
)

# é…ç½®Embeddingæ¨¡å‹
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-large",
    api_key="your-api-key"
)

# é…ç½®åˆ†å—å‚æ•°
Settings.chunk_size = 512
Settings.chunk_overlap = 50

# ç°åœ¨æ‰€æœ‰åç»­æ“ä½œéƒ½ä¼šä½¿ç”¨è¿™äº›é…ç½®
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)
```

### 4.1.2 é…ç½®æœ¬åœ°æ¨¡å‹

```python
from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
Settings.llm = Ollama(
    model="llama2",
    base_url="http://localhost:11434"
)

# ä½¿ç”¨HuggingFace Embedding
Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-zh-v1.5"
)
```

---

## 4.2 æ€§èƒ½ä¼˜åŒ–

### 4.2.1 åˆ†å—ä¼˜åŒ–

```python
from llama_index.core.node_parser import SentenceSplitter

# åœºæ™¯1: çŸ­æ–‡æœ¬é—®ç­”ï¼ˆå¦‚FAQï¼‰
short_splitter = SentenceSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# åœºæ™¯2: é•¿æ–‡æ¡£åˆ†æï¼ˆå¦‚æŠ€æœ¯æ–‡æ¡£ï¼‰
long_splitter = SentenceSplitter(
    chunk_size=2000,
    chunk_overlap=400
)

# åœºæ™¯3: ä¸­æ–‡æ–‡æ¡£
chinese_splitter = SentenceSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separator="ã€‚"  # ä½¿ç”¨ä¸­æ–‡å¥å·
)
```

### 4.2.2 ç¼“å­˜ä¼˜åŒ–

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤embedding
documents = SimpleDirectoryReader("./data").load_data()

# ç¬¬ä¸€æ¬¡åˆ›å»ºç´¢å¼•ï¼ˆä¼šè¿›è¡Œembeddingï¼‰
index = VectorStoreIndex.from_documents(documents)

# æŒä¹…åŒ–
index.storage_context.persist(persist_dir="./storage")

# åç»­åŠ è½½ï¼ˆä¸éœ€è¦é‡æ–°embeddingï¼‰
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

---

# ç¬¬ 5 ç« ï¼šå®Œæ•´åº”ç”¨å®æˆ˜

## 5.1 ç”Ÿäº§çº§RAGåº”ç”¨

```python
"""
ç”Ÿäº§çº§RAGåº”ç”¨ - LlamaIndexç‰ˆæœ¬
"""
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import os
from pathlib import Path

class LlamaIndexRAG:
    def __init__(self, data_dir="./data", persist_dir="./storage"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        self.index = None

        # é…ç½®å…¨å±€è®¾ç½®
        Settings.llm = OpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.1
        )
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-large"
        )
        Settings.chunk_size = 512
        Settings.chunk_overlap = 50

    def build_index(self, force_rebuild=False):
        """æ„å»ºæˆ–åŠ è½½ç´¢å¼•"""
        if not force_rebuild and Path(self.persist_dir).exists():
            print("ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•...")
            try:
                storage_context = StorageContext.from_defaults(
                    persist_dir=self.persist_dir
                )
                self.index = load_index_from_storage(storage_context)
                print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
                return
            except:
                print("âš ï¸  åŠ è½½å¤±è´¥ï¼Œé‡æ–°æ„å»ºç´¢å¼•...")

        print("ğŸ“„ 1. åŠ è½½æ–‡æ¡£...")
        documents = SimpleDirectoryReader(self.data_dir).load_data()
        print(f"   âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        print("âœ‚ï¸  2. æ–‡æ¡£åˆ†å—...")
        parser = SentenceSplitter(
            chunk_size=Settings.chunk_size,
            chunk_overlap=Settings.chunk_overlap
        )
        nodes = parser.get_nodes_from_documents(documents)
        print(f"   âœ… åˆ›å»ºäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")

        print("ğŸ”¨ 3. åˆ›å»ºå‘é‡ç´¢å¼•...")
        self.index = VectorStoreIndex(nodes)
        print("   âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")

        print("ğŸ’¾ 4. æŒä¹…åŒ–ç´¢å¼•...")
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print("   âœ… ç´¢å¼•å·²ä¿å­˜")

    def query(self, question, top_k=3, response_mode="compact", show_sources=True):
        """æŸ¥è¯¢"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ build_index()")

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            similarity_top_k=top_k,
            response_mode=response_mode
        )

        print(f"\nâ“ é—®é¢˜: {question}")
        response = query_engine.query(question)

        print(f"\nğŸ’¡ å›ç­”:\n{response}\n")

        if show_sources:
            print("ğŸ“š æ¥æº:")
            for i, node in enumerate(response.source_nodes, 1):
                print(f"  {i}. {node.text[:100]}... (å¾—åˆ†: {node.score:.4f})")
                if node.metadata:
                    print(f"     å…ƒæ•°æ®: {node.metadata}")

        return response

    def query_stream(self, question, top_k=3):
        """æµå¼æŸ¥è¯¢"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªåˆå§‹åŒ–")

        query_engine = self.index.as_query_engine(
            similarity_top_k=top_k,
            streaming=True
        )

        print(f"\nâ“ é—®é¢˜: {question}")
        print("ğŸ’¡ å›ç­”: ", end="")

        response = query_engine.query(question)
        for text in response.response_gen:
            print(text, end="", flush=True)
        print("\n")

        return response

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®¾ç½®API Key
    os.environ["OPENAI_API_KEY"] = "sk-your-key"

    # åˆå§‹åŒ–RAGåº”ç”¨
    rag = LlamaIndexRAG(data_dir="./data", persist_dir="./storage")

    # æ„å»ºç´¢å¼•
    rag.build_index()

    # æŸ¥è¯¢
    questions = [
        "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æœ‰å“ªäº›å…³é”®æ¦‚å¿µï¼Ÿ",
        "å¦‚ä½•å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ"
    ]

    for q in questions:
        rag.query(q, top_k=3, response_mode="compact")
        print("-" * 80)

    # æµå¼æŸ¥è¯¢
    rag.query_stream("è¯¦ç»†è§£é‡ŠLlamaIndexçš„æ¶æ„")
```

---

## 5.2 é«˜çº§åŠŸèƒ½

### 5.2.1 ä½¿ç”¨ä¸åŒç´¢å¼•ç±»å‹

```python
from llama_index.core import (
    VectorStoreIndex,
    SummaryIndex,
    TreeIndex,
    KeywordTableIndex
)

# å‘é‡ç´¢å¼•ï¼ˆè¯­ä¹‰æœç´¢ï¼‰
vector_index = VectorStoreIndex.from_documents(documents)

# æ‘˜è¦ç´¢å¼•ï¼ˆæ–‡æ¡£æ‘˜è¦ï¼‰
summary_index = SummaryIndex.from_documents(documents)

# æ ‘å½¢ç´¢å¼•ï¼ˆå±‚æ¬¡åŒ–æ‘˜è¦ï¼‰
tree_index = TreeIndex.from_documents(documents)

# å…³é”®è¯ç´¢å¼•ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
keyword_index = KeywordTableIndex.from_documents(documents)

# æ ¹æ®ä¸åŒä»»åŠ¡é€‰æ‹©ä¸åŒç´¢å¼•
def intelligent_query(query, task_type="search"):
    if task_type == "search":
        engine = vector_index.as_query_engine()
    elif task_type == "summarize":
        engine = summary_index.as_query_engine()
    elif task_type == "hierarchical":
        engine = tree_index.as_query_engine()
    elif task_type == "keyword":
        engine = keyword_index.as_query_engine()
    else:
        engine = vector_index.as_query_engine()

    return engine.query(query)

# ä½¿ç”¨
response1 = intelligent_query("ä»€ä¹ˆæ˜¯RAG?", task_type="search")
response2 = intelligent_query("æ€»ç»“æ‰€æœ‰æ–‡æ¡£", task_type="summarize")
```

### 5.2.2 å¤šæ¨¡æ€RAG

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.core.indices import MultiModalVectorStoreIndex
from llama_index.multi_modal_llms.openai import OpenAIMultiModal

# åŠ è½½åŒ…å«å›¾ç‰‡çš„æ–‡æ¡£
documents = SimpleDirectoryReader(
    "./data",
    required_exts=[".jpg", ".png", ".txt", ".pdf"]
).load_data()

# åˆ›å»ºå¤šæ¨¡æ€ç´¢å¼•
multimodal_index = MultiModalVectorStoreIndex.from_documents(documents)

# ä½¿ç”¨å¤šæ¨¡æ€LLM
multimodal_llm = OpenAIMultiModal(model="gpt-4-vision-preview")

# æŸ¥è¯¢
query_engine = multimodal_index.as_query_engine(
    multi_modal_llm=multimodal_llm
)
response = query_engine.query("å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

---

# ç¬¬ 6 ç« ï¼šä¸LangChainé›†æˆ

## 6.1 LlamaIndexä½œä¸ºLangChainå·¥å…·

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from langchain_core.tools import tool

# 1. åˆ›å»ºLlamaIndexç´¢å¼•
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# 2. å°è£…ä¸ºLangChainå·¥å…·
@tool
def search_documents(query: str) -> str:
    """æœç´¢æ–‡æ¡£åº“ï¼Œå›ç­”å…³äºæ–‡æ¡£çš„é—®é¢˜ã€‚"""
    response = query_engine.query(query)
    return str(response)

# 3. åœ¨LangChain Agentä¸­ä½¿ç”¨
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

agent = create_react_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search_documents]
)

# 4. è¿è¡Œ
result = agent.invoke({
    "messages": [("user", "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›å…³é”®æ¦‚å¿µï¼Ÿ")]
})
print(result["messages"][-1].content)
```

---

## æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å®Œæ•´å­¦ä¹ äº†LlamaIndexçš„RAGåŸºç¡€ï¼š

**ç¬¬1ç« å›é¡¾**ï¼š
- âœ… LlamaIndexè®¾è®¡å“²å­¦
- âœ… æ ¸å¿ƒä¼˜åŠ¿
- âœ… 5è¡Œä»£ç å¿«é€Ÿå¼€å§‹
- âœ… æ ¸å¿ƒç»„ä»¶ï¼ˆDocumentã€Nodeã€Indexï¼‰

**ç¬¬2ç« å›é¡¾**ï¼š
- âœ… æ–‡æ¡£åŠ è½½å™¨ï¼ˆSimpleDirectoryReaderã€ä¸“ç”¨åŠ è½½å™¨ï¼‰
- âœ… èŠ‚ç‚¹è§£æå™¨ï¼ˆSentenceSplitterã€SemanticSplitterï¼‰
- âœ… åˆ†å—ç­–ç•¥é€‰æ‹©

**ç¬¬3ç« å›é¡¾**ï¼š
- âœ… ç´¢å¼•ç±»å‹ï¼ˆVectorã€Summaryã€Treeã€Keywordï¼‰
- âœ… å‘é‡å­˜å‚¨ï¼ˆå†…ç½®ã€Chromaé›†æˆï¼‰
- âœ… æŸ¥è¯¢å¼•æ“ï¼ˆå“åº”æ¨¡å¼ã€æµå¼è¾“å‡ºã€è‡ªå®šä¹‰Promptï¼‰
- âœ… æ£€ç´¢å™¨

**ç¬¬4ç« å›é¡¾**ï¼š
- âœ… å…¨å±€é…ç½®ï¼ˆSettingsï¼‰
- âœ… æ€§èƒ½ä¼˜åŒ–

**ç¬¬5ç« å›é¡¾**ï¼š
- âœ… ç”Ÿäº§çº§RAGåº”ç”¨
- âœ… é«˜çº§åŠŸèƒ½ï¼ˆå¤šç´¢å¼•ã€å¤šæ¨¡æ€ï¼‰

**ç¬¬6ç« å›é¡¾**ï¼š
- âœ… ä¸LangChainé›†æˆ

---

## æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹ 1**ï¼šä½¿ç”¨LlamaIndexæ„å»ºä¸€ä¸ªæœ¬åœ°æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
2. **ç»ƒä¹ 2**ï¼šå¯¹æ¯”ä¸åŒç´¢å¼•ç±»å‹çš„æ•ˆæœ
3. **ç»ƒä¹ 3**ï¼šå®ç°ä¸€ä¸ªä½¿ç”¨è¯­ä¹‰åˆ†å—çš„é«˜è´¨é‡RAGç³»ç»Ÿ
4. **ç»ƒä¹ 4**ï¼šå°†LlamaIndexé›†æˆåˆ°LangChain Agentä¸­

---

## å‚è€ƒèµ„æº

- [LlamaIndexå®˜æ–¹æ–‡æ¡£](https://developers.llamaindex.ai/)
- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)
- [LlamaIndex Examples](https://github.com/run-llama/llama_index/tree/main/docs/examples)

---

**ç‰ˆæœ¬ä¿¡æ¯**ï¼š
- LlamaIndex: 0.11.0+
- llama-index-core: 0.11.0+
- llama-index-llms-openai: 0.2.0+
- llama-index-embeddings-openai: 0.2.0+
- æœ€åæ›´æ–°: 2025-11-23
