# ç¬¬ä¹ç¯‡ é«˜çº§åº”ç”¨ä¸å¤šAgentæ¶æ„

> **ç›®æ ‡**: æŒæ¡å‰æ²¿æŠ€æœ¯å’Œå¤æ‚åœºæ™¯

æœ¬ç¯‡æ·±å…¥ LangChain ç”Ÿæ€çš„é«˜çº§èƒ½åŠ›ï¼šå¤š Agent åä½œã€å¤šæ¨¡æ€å¤„ç†ã€MCP é›†æˆç­‰å‰æ²¿æŠ€æœ¯ï¼Œè®©ä½ èƒ½å¤Ÿæ„å»ºæ›´å¤æ‚ã€æ›´å¼ºå¤§çš„ AI åº”ç”¨ã€‚

---

## ç¬¬1ç« ï¼š å¤š Agent åä½œ

> **å…³æ³¨ç‚¹**ï¼šç†è§£ä¸åŒçš„åä½œæ¨¡å¼ï¼ŒæŒæ¡ LangGraph çš„å¤š Agent æ¶æ„è®¾è®¡ã€‚

### 1.1 åä½œæ¨¡å¼

#### 1.1.1 Supervisor-Worker æ¨¡å¼

**æ¶æ„ç‰¹ç‚¹**ï¼š

- **ä¸­å¤®ç›‘ç£è€…ï¼ˆSupervisorï¼‰**ï¼šè´Ÿè´£æ¥æ”¶ç”¨æˆ·è¾“å…¥ã€åˆ†è§£ä»»åŠ¡ã€åˆ†é…å·¥ä½œã€æ±‡æ€»ç»“æœ
- **ä¸“é—¨åŒ–å·¥ä½œè€…ï¼ˆWorkersï¼‰**ï¼šæ¯ä¸ªå·¥ä½œè€…ä¸“æ³¨ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚æœç´¢ã€åˆ†æã€ä»£ç ç”Ÿæˆï¼‰
- **å•ç‚¹æ§åˆ¶**ï¼šæ‰€æœ‰é€šä¿¡å¿…é¡»ç»è¿‡ Supervisorï¼ŒWorkers ä¹‹é—´ä¸ç›´æ¥é€šä¿¡

**ä¼˜åŠ¿**ï¼šæ§åˆ¶æµæ¸…æ™°ã€æ˜“äºè°ƒè¯•ã€è´£ä»»æ˜ç¡®

**å®ç°ç¤ºä¾‹**ï¼š

```python
from langgraph.graph import StateGraph, MessagesState, END
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from typing import Literal, TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage
import operator

# å®šä¹‰å…±äº«çŠ¶æ€
class SupervisorState(MessagesState):
    """å¤š Agent å…±äº«çŠ¶æ€"""
    next_agent: str  # ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ Agent

# åˆ›å»ºå·¥ä½œè€… Agents
def create_search_agent():
    """æœç´¢ä¸“å®¶ Agent"""
    return create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [search_tool],
        prompt="ä½ æ˜¯æœç´¢ä¸“å®¶ï¼Œè´Ÿè´£æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚"
    )

def create_analyst_agent():
    """åˆ†æä¸“å®¶ Agent"""
    return create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [analyze_tool],
        prompt="ä½ æ˜¯æ•°æ®åˆ†æä¸“å®¶ï¼Œè´Ÿè´£åˆ†æå’Œæ€»ç»“ä¿¡æ¯ã€‚"
    )

def create_writer_agent():
    """å†™ä½œä¸“å®¶ Agent"""
    return create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [],
        prompt="ä½ æ˜¯å†™ä½œä¸“å®¶ï¼Œè´Ÿè´£ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æ¡£ã€‚"
    )

# åˆ›å»º Supervisor
class Supervisor:
    def __init__(self):
        self.model = ChatOpenAI(model="gpt-4o")
        self.workers = ["search", "analyst", "writer"]

    def __call__(self, state: SupervisorState) -> SupervisorState:
        """å†³å®šä¸‹ä¸€ä¸ªæ‰§è¡Œçš„ Agent"""
        messages = state["messages"]

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯å›¢é˜Ÿç›‘ç£è€…ã€‚

å¯ç”¨çš„å·¥ä½œè€…ï¼š
- search: æœç´¢ä¿¡æ¯
- analyst: åˆ†ææ•°æ®
- writer: æ’°å†™æŠ¥å‘Š

æ ¹æ®å½“å‰ä»»åŠ¡çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€ä¸ªåº”è¯¥æ‰§è¡Œçš„å·¥ä½œè€…ã€‚
å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œè¿”å› "FINISH"ã€‚
"""

        # è®© LLM å†³å®šä¸‹ä¸€æ­¥
        response = self.model.invoke([
            {"role": "system", "content": system_prompt},
            *messages
        ])

        # è§£æå†³ç­–ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„è§£æï¼‰
        next_agent = response.content.strip()

        return {"messages": messages + [response], "next_agent": next_agent}

# æ„å»ºå·¥ä½œæµå›¾
def build_supervisor_workflow():
    """æ„å»º Supervisor-Worker å·¥ä½œæµ"""
    workflow = StateGraph(SupervisorState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("supervisor", Supervisor())
    workflow.add_node("search", create_search_agent())
    workflow.add_node("analyst", create_analyst_agent())
    workflow.add_node("writer", create_writer_agent())

    # å®šä¹‰è·¯ç”±é€»è¾‘
    def route_next(state: SupervisorState) -> Literal["search", "analyst", "writer", "FINISH"]:
        """æ ¹æ® next_agent å†³å®šè·¯ç”±"""
        return state.get("next_agent", "FINISH")

    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "supervisor",
        route_next,
        {
            "search": "search",
            "analyst": "analyst",
            "writer": "writer",
            "FINISH": END
        }
    )

    # Workers å®Œæˆåè¿”å› Supervisor
    workflow.add_edge("search", "supervisor")
    workflow.add_edge("analyst", "supervisor")
    workflow.add_edge("writer", "supervisor")

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("supervisor")

    return workflow.compile()

# ä½¿ç”¨
app = build_supervisor_workflow()
result = app.invoke({
    "messages": [("user", "ç ”ç©¶ LangChain 1.0 çš„æ–°ç‰¹æ€§å¹¶å†™ä¸€ä»½æŠ¥å‘Š")]
})
```

#### 1.1.2 Router æ¨¡å¼

**æ¶æ„ç‰¹ç‚¹**ï¼š

- **æ™ºèƒ½è·¯ç”±å™¨**ï¼šæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€é€‰æ‹©åˆé€‚çš„ Agent
- **æ— ä¸­å¤®æ§åˆ¶**ï¼šç›´æ¥è·¯ç”±åˆ°ç›®æ ‡ Agentï¼Œå‡å°‘ä¸­é—´å±‚
- **é€‚åˆåœºæ™¯æ˜ç¡®çš„ä»»åŠ¡**ï¼šå¦‚æ„å›¾åˆ†ç±»åçš„ä¸“é—¨å¤„ç†

**å®ç°ç¤ºä¾‹**ï¼š

```python
from langgraph.graph import StateGraph, MessagesState, END
from typing import Literal

class RouterState(MessagesState):
    """è·¯ç”±çŠ¶æ€"""
    intent: str  # æ„å›¾åˆ†ç±»

def classify_intent(state: RouterState) -> RouterState:
    """æ„å›¾åˆ†ç±»è·¯ç”±å™¨"""
    last_message = state["messages"][-1].content.lower()

    # ç®€å•çš„è§„åˆ™åˆ†ç±»ï¼ˆå®é™…åº”ç”¨ä¸­ä½¿ç”¨ LLM æˆ–åˆ†ç±»å™¨ï¼‰
    if "code" in last_message or "ç¼–ç¨‹" in last_message:
        intent = "coding"
    elif "search" in last_message or "æŸ¥æ‰¾" in last_message:
        intent = "search"
    elif "translate" in last_message or "ç¿»è¯‘" in last_message:
        intent = "translation"
    else:
        intent = "general"

    return {"messages": state["messages"], "intent": intent}

def build_router_workflow():
    """æ„å»ºè·¯ç”±æ¨¡å¼å·¥ä½œæµ"""
    workflow = StateGraph(RouterState)

    # æ·»åŠ è·¯ç”±å™¨
    workflow.add_node("router", classify_intent)

    # æ·»åŠ ä¸“é—¨åŒ– Agents
    workflow.add_node("coding_agent", create_coding_agent())
    workflow.add_node("search_agent", create_search_agent())
    workflow.add_node("translation_agent", create_translation_agent())
    workflow.add_node("general_agent", create_general_agent())

    # è·¯ç”±é€»è¾‘
    def route(state: RouterState) -> str:
        intent_map = {
            "coding": "coding_agent",
            "search": "search_agent",
            "translation": "translation_agent",
            "general": "general_agent"
        }
        return intent_map.get(state["intent"], "general_agent")

    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges("router", route)

    # è®¾ç½®å…¥å£å’Œå‡ºå£
    workflow.set_entry_point("router")

    # æ‰€æœ‰ Agents éƒ½è¿æ¥åˆ° END
    for agent in ["coding_agent", "search_agent", "translation_agent", "general_agent"]:
        workflow.add_edge(agent, END)

    return workflow.compile()
```

#### 1.1.3 Hierarchical æ¨¡å¼

**æ¶æ„ç‰¹ç‚¹**ï¼š

- **å¤šå±‚çº§ç®¡ç†**ï¼šå›¢é˜Ÿé¢†å¯¼ç®¡ç†å­å›¢é˜Ÿï¼Œå½¢æˆæ ‘çŠ¶ç»“æ„
- **è´£ä»»é“¾**ï¼šä»»åŠ¡é€çº§åˆ†è§£ï¼Œç»“æœé€çº§æ±‡æ€»
- **é€‚åˆå¤æ‚é¡¹ç›®**ï¼šå¦‚å¤§å‹ç ”ç©¶ã€è½¯ä»¶å¼€å‘é¡¹ç›®

**å®ç°ç¤ºä¾‹**ï¼š

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.tools import tool
from typing import Dict, List, Literal
import json

# æ³¨æ„ï¼šcreate_team_supervisor å·²ç§»è‡³ç‹¬ç«‹åŒ… langgraph-supervisor
# å®˜æ–¹ç°åœ¨æ¨èä½¿ç”¨å·¥å…·è°ƒç”¨æ–¹å¼å®ç° supervisor
# å¦‚éœ€ä½¿ç”¨ï¼špip install langgraph-supervisor

def create_supervisor_with_tools(workers: List[str], system_prompt: str):
    """ä½¿ç”¨å·¥å…·è°ƒç”¨æ–¹å¼åˆ›å»º Supervisorï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰"""

    # åˆ›å»ºè·¯ç”±å·¥å…·
    @tool
    def route_to_worker(next_worker: Literal["search", "analyst", "writer", "FINISH"]) -> str:
        """è·¯ç”±åˆ°æŒ‡å®šçš„å·¥ä½œè€…æˆ–ç»“æŸ

        Args:
            next_worker: ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„å·¥ä½œè€…ï¼Œæˆ– FINISH è¡¨ç¤ºå®Œæˆ
        """
        return next_worker

    # åˆ›å»º supervisor agent
    supervisor = create_agent(
        ChatOpenAI(model="gpt-4o"),
        [route_to_worker],
        prompt=system_prompt
    )

    return supervisor

# å®šä¹‰å›¢é˜Ÿç»“æ„
class ResearchTeam:
    """ç ”ç©¶å›¢é˜Ÿ"""
    def __init__(self):
        self.lead = create_agent(
            ChatOpenAI(model="gpt-4o"),
            [],
            prompt="ä½ æ˜¯ç ”ç©¶å›¢é˜Ÿè´Ÿè´£äººï¼Œåè°ƒæ•°æ®å’Œå¸‚åœºç ”ç©¶å‘˜ã€‚"
        )

        self.data_researcher = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [data_analysis_tool],
            prompt="ä½ æ˜¯æ•°æ®ç ”ç©¶å‘˜ï¼Œè´Ÿè´£æ•°æ®æ”¶é›†å’Œåˆ†æã€‚"
        )

        self.market_researcher = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [market_research_tool],
            prompt="ä½ æ˜¯å¸‚åœºç ”ç©¶å‘˜ï¼Œè´Ÿè´£å¸‚åœºè¶‹åŠ¿åˆ†æã€‚"
        )

    def create_subgraph(self):
        """åˆ›å»ºç ”ç©¶å›¢é˜Ÿçš„å­å›¾"""
        graph = StateGraph(MessagesState)

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("team_lead", self.lead)
        graph.add_node("data_researcher", self.data_researcher)
        graph.add_node("market_researcher", self.market_researcher)

        # å›¢é˜Ÿè´Ÿè´£äººå†³å®šä»»åŠ¡åˆ†é…
        graph.add_conditional_edges(
            "team_lead",
            lambda x: x.get("next_worker"),
            {
                "data": "data_researcher",
                "market": "market_researcher",
                "done": END
            }
        )

        # ç ”ç©¶å‘˜å®Œæˆåå›åˆ°è´Ÿè´£äºº
        graph.add_edge("data_researcher", "team_lead")
        graph.add_edge("market_researcher", "team_lead")

        graph.set_entry_point("team_lead")

        return graph.compile()

class DevelopmentTeam:
    """å¼€å‘å›¢é˜Ÿ"""
    def __init__(self):
        self.lead = create_agent(
            ChatOpenAI(model="gpt-4o"),
            [],
            prompt="ä½ æ˜¯å¼€å‘å›¢é˜Ÿè´Ÿè´£äººï¼Œåè°ƒå‰ç«¯å’Œåç«¯å¼€å‘ã€‚"
        )

        self.frontend_dev = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [code_generation_tool],
            prompt="ä½ æ˜¯å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆã€‚"
        )

        self.backend_dev = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [code_generation_tool, database_tool],
            prompt="ä½ æ˜¯åç«¯å¼€å‘å·¥ç¨‹å¸ˆã€‚"
        )

    def create_subgraph(self):
        """åˆ›å»ºå¼€å‘å›¢é˜Ÿçš„å­å›¾"""
        # ç±»ä¼¼ç ”ç©¶å›¢é˜Ÿçš„å®ç°
        pass

def build_hierarchical_system():
    """æ„å»ºå±‚çº§åŒ–å¤š Agent ç³»ç»Ÿ"""
    main_graph = StateGraph(MessagesState)

    # CEO Agent
    ceo = create_agent(
        ChatOpenAI(model="gpt-4o"),
        [],
        prompt="""ä½ æ˜¯é¡¹ç›® CEOï¼Œè´Ÿè´£æ•´ä½“åè°ƒã€‚

ä½ ç®¡ç†ä¸¤ä¸ªå›¢é˜Ÿï¼š
- ç ”ç©¶å›¢é˜Ÿï¼šè´Ÿè´£å¸‚åœºå’Œæ•°æ®ç ”ç©¶
- å¼€å‘å›¢é˜Ÿï¼šè´Ÿè´£äº§å“å¼€å‘

æ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼Œåˆ†é…ç»™åˆé€‚çš„å›¢é˜Ÿã€‚"""
    )

    # åˆ›å»ºå›¢é˜Ÿå­å›¾
    research_team = ResearchTeam().create_subgraph()
    dev_team = DevelopmentTeam().create_subgraph()

    # æ·»åŠ åˆ°ä¸»å›¾
    main_graph.add_node("ceo", ceo)
    main_graph.add_node("research_team", research_team)
    main_graph.add_node("dev_team", dev_team)

    # CEO å†³å®šåˆ†é…ç»™å“ªä¸ªå›¢é˜Ÿ
    def route_to_team(state):
        # æ ¹æ®çŠ¶æ€å†³å®šè·¯ç”±
        if "research" in state.get("task_type", ""):
            return "research_team"
        elif "develop" in state.get("task_type", ""):
            return "dev_team"
        else:
            return END

    main_graph.add_conditional_edges("ceo", route_to_team)

    # å›¢é˜Ÿå®Œæˆåè¿”å› CEO
    main_graph.add_edge("research_team", "ceo")
    main_graph.add_edge("dev_team", "ceo")

    main_graph.set_entry_point("ceo")

    return main_graph.compile()
```

#### 1.1.4 æ¨¡å¼é€‰æ‹©æŒ‡å—

| æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|---------|------|------|
| **Supervisor-Worker** | ä»»åŠ¡éœ€è¦å¤šæ­¥éª¤åè°ƒ | æ§åˆ¶æµæ¸…æ™°ã€æ˜“è°ƒè¯• | ä¸­å¤®ç“¶é¢ˆã€å»¶è¿Ÿè¾ƒé«˜ |
| **Router** | ä»»åŠ¡ç±»å‹æ˜ç¡®ã€ç‹¬ç«‹ | ä½å»¶è¿Ÿã€ç›´æ¥å¤„ç† | ç¼ºå°‘åè°ƒã€ä¸é€‚åˆå¤æ‚ä»»åŠ¡ |
| **Hierarchical** | å¤§å‹å¤æ‚é¡¹ç›® | å¯æ‰©å±•ã€è´£ä»»æ˜ç¡® | å¤æ‚åº¦é«˜ã€è°ƒè¯•å›°éš¾ |
| **Hybrid** | æ··åˆéœ€æ±‚ | çµæ´»ã€å¯æ ¹æ®éœ€æ±‚ç»„åˆ | éœ€è¦æ›´å¤šè®¾è®¡è€ƒè™‘ |

**é€‰æ‹©å†³ç­–æ ‘**ï¼š

```python
def choose_pattern(task_complexity: str, coordination_needed: bool, team_size: int) -> str:
    """é€‰æ‹©åˆé€‚çš„å¤š Agent æ¨¡å¼"""

    if task_complexity == "simple":
        if coordination_needed:
            return "Supervisor-Workerï¼ˆç®€å•ç‰ˆï¼‰"
        else:
            return "Router"

    elif task_complexity == "medium":
        if team_size <= 5:
            return "Supervisor-Worker"
        else:
            return "Hierarchicalï¼ˆä¸¤å±‚ï¼‰"

    else:  # complex
        if team_size > 10:
            return "Hierarchicalï¼ˆå¤šå±‚ï¼‰"
        else:
            return "Supervisor-Worker + Router æ··åˆ"
```

---

### 1.2 é€šä¿¡ä¸åè°ƒ

#### 1.2.1 æ¶ˆæ¯ä¼ é€’æœºåˆ¶

**LangGraph ä¸­çš„æ¶ˆæ¯ä¼ é€’**ï¼š

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.graph import MessagesState
from typing import List, TypedDict, Annotated
import operator

# æ–¹å¼ 1ï¼šä½¿ç”¨å†…ç½® MessagesState
class TeamState(MessagesState):
    """ä½¿ç”¨å†…ç½®æ¶ˆæ¯çŠ¶æ€"""
    task_status: str
    results: List[dict]

# æ–¹å¼ 2ï¼šè‡ªå®šä¹‰æ¶ˆæ¯ä¼ é€’
class CustomState(TypedDict):
    """è‡ªå®šä¹‰çŠ¶æ€"""
    messages: Annotated[List[BaseMessage], operator.add]  # æ¶ˆæ¯è¿½åŠ 
    shared_memory: dict  # å…±äº«å†…å­˜
    agent_outputs: dict  # å„ Agent è¾“å‡º

def agent_a(state: CustomState) -> CustomState:
    """Agent A å¤„ç†å¹¶ä¼ é€’æ¶ˆæ¯"""
    # è¯»å–æ¶ˆæ¯
    last_message = state["messages"][-1]

    # å¤„ç†
    result = process_message(last_message)

    # æ›´æ–°å…±äº«å†…å­˜
    state["shared_memory"]["agent_a_result"] = result

    # æ·»åŠ æ¶ˆæ¯ç»™ä¸‹ä¸€ä¸ª Agent
    new_message = AIMessage(
        content=f"Agent A å®Œæˆå¤„ç†ï¼š{result}",
        metadata={"agent": "agent_a", "timestamp": datetime.now()}
    )

    return {
        "messages": [new_message],
        "agent_outputs": {"agent_a": result}
    }
```

**æ¶ˆæ¯æ ¼å¼æ ‡å‡†åŒ–**ï¼š

```python
from pydantic import BaseModel
from datetime import datetime

class AgentMessage(BaseModel):
    """æ ‡å‡†åŒ–çš„ Agent æ¶ˆæ¯"""
    sender: str          # å‘é€è€… Agent ID
    receiver: str        # æ¥æ”¶è€… Agent IDï¼ˆ"all" è¡¨ç¤ºå¹¿æ’­ï¼‰
    content: str         # æ¶ˆæ¯å†…å®¹
    message_type: str    # "task", "result", "error", "info"
    timestamp: datetime
    metadata: dict = {}

def create_message(sender: str, receiver: str, content: str, msg_type: str = "info"):
    """åˆ›å»ºæ ‡å‡†æ¶ˆæ¯"""
    return AgentMessage(
        sender=sender,
        receiver=receiver,
        content=content,
        message_type=msg_type,
        timestamp=datetime.now()
    )

# ä½¿ç”¨ç¤ºä¾‹
message = create_message(
    sender="supervisor",
    receiver="search_agent",
    content="æœç´¢ LangChain 1.0 æ–°ç‰¹æ€§",
    msg_type="task"
)
```

#### 1.2.2 å…±äº«çŠ¶æ€ç®¡ç†

**çŠ¶æ€åŒæ­¥ç­–ç•¥**ï¼š

```python
from typing import Any
import threading
from collections import defaultdict

class SharedStateManager:
    """å…±äº«çŠ¶æ€ç®¡ç†å™¨"""

    def __init__(self):
        self._state = {}
        self._locks = defaultdict(threading.Lock)
        self._version = defaultdict(int)

    def get(self, key: str) -> Any:
        """è¯»å–çŠ¶æ€"""
        with self._locks[key]:
            return self._state.get(key)

    def set(self, key: str, value: Any) -> int:
        """è®¾ç½®çŠ¶æ€"""
        with self._locks[key]:
            self._state[key] = value
            self._version[key] += 1
            return self._version[key]

    def update(self, key: str, updater: callable) -> Any:
        """åŸå­æ›´æ–°"""
        with self._locks[key]:
            old_value = self._state.get(key)
            new_value = updater(old_value)
            self._state[key] = new_value
            self._version[key] += 1
            return new_value

    def get_version(self, key: str) -> int:
        """è·å–ç‰ˆæœ¬å·ï¼ˆç”¨äºæ£€æµ‹å˜åŒ–ï¼‰"""
        return self._version[key]

# åœ¨ LangGraph ä¸­ä½¿ç”¨
class MultiAgentState(TypedDict):
    """å¤š Agent å…±äº«çŠ¶æ€"""
    messages: List[BaseMessage]
    shared_data: SharedStateManager
    agent_status: dict  # {agent_id: "idle" | "working" | "done"}

def worker_agent(state: MultiAgentState) -> MultiAgentState:
    """å·¥ä½œè€… Agent"""
    shared = state["shared_data"]

    # è¯»å–å…±äº«æ•°æ®
    task_queue = shared.get("task_queue") or []

    if task_queue:
        # åŸå­æ“ä½œï¼šå–å‡ºä»»åŠ¡
        task = shared.update(
            "task_queue",
            lambda queue: (queue.pop(0), queue)[0] if queue else None
        )

        # å¤„ç†ä»»åŠ¡
        result = process_task(task)

        # æ›´æ–°ç»“æœ
        shared.update(
            "results",
            lambda results: (results or []) + [result]
        )

    return state
```

#### 1.2.3 ä»»åŠ¡åˆ†è§£ä¸ç»“æœèšåˆ

**ä»»åŠ¡åˆ†è§£ç­–ç•¥**ï¼š

```python
from typing import List, Dict

class TaskDecomposer:
    """ä»»åŠ¡åˆ†è§£å™¨"""

    def __init__(self, model: ChatOpenAI):
        self.model = model

    def decompose(self, task: str) -> List[Dict]:
        """å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡"""

        prompt = f"""å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸ºå¯ç‹¬ç«‹æ‰§è¡Œçš„å­ä»»åŠ¡ï¼š

ä»»åŠ¡ï¼š{task}

è¦æ±‚ï¼š
1. æ¯ä¸ªå­ä»»åŠ¡åº”è¯¥ç‹¬ç«‹å¯æ‰§è¡Œ
2. æ ‡æ˜å­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»
3. ä¼°è®¡æ¯ä¸ªå­ä»»åŠ¡çš„å¤æ‚åº¦ï¼ˆç®€å•/ä¸­ç­‰/å¤æ‚ï¼‰

è¿”å› JSON æ ¼å¼ï¼š
[
    {{
        "id": "task_1",
        "description": "å­ä»»åŠ¡æè¿°",
        "dependencies": [],
        "complexity": "simple",
        "assigned_to": null
    }}
]
"""

        response = self.model.invoke(prompt)
        # è§£æ JSONï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„è§£æï¼‰
        subtasks = json.loads(response.content)

        return subtasks

# ç»“æœèšåˆ
class ResultAggregator:
    """ç»“æœèšåˆå™¨"""

    def __init__(self):
        self.results = {}
        self.dependencies = {}

    def add_result(self, task_id: str, result: Any):
        """æ·»åŠ å­ä»»åŠ¡ç»“æœ"""
        self.results[task_id] = result

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥è§¦å‘ä¾èµ–ä»»åŠ¡
        self._check_dependencies(task_id)

    def _check_dependencies(self, completed_task_id: str):
        """æ£€æŸ¥å¹¶è§¦å‘ä¾èµ–ä»»åŠ¡"""
        for task_id, deps in self.dependencies.items():
            if completed_task_id in deps:
                deps.remove(completed_task_id)

                if not deps:  # æ‰€æœ‰ä¾èµ–éƒ½æ»¡è¶³
                    print(f"Task {task_id} ready to execute")

    def aggregate(self) -> Dict:
        """èšåˆæ‰€æœ‰ç»“æœ"""
        # ç®€å•èšåˆ
        aggregated = {
            "total_tasks": len(self.results),
            "results": self.results,
            "summary": self._generate_summary()
        }

        return aggregated

    def _generate_summary(self) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        # å¯ä»¥ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        return f"å®Œæˆ {len(self.results)} ä¸ªä»»åŠ¡"

# ä½¿ç”¨ç¤ºä¾‹
decomposer = TaskDecomposer(ChatOpenAI(model="gpt-4o"))
aggregator = ResultAggregator()

# åˆ†è§£ä»»åŠ¡
subtasks = decomposer.decompose("æ„å»ºä¸€ä¸ªç”µå•†ç½‘ç«™")

# åˆ†é…ç»™ä¸åŒ Agents å¹¶æ”¶é›†ç»“æœ
for task in subtasks:
    # åˆ†é…ä»»åŠ¡
    agent = assign_agent(task)
    result = agent.execute(task)

    # æ”¶é›†ç»“æœ
    aggregator.add_result(task["id"], result)

# æœ€ç»ˆèšåˆ
final_result = aggregator.aggregate()
```

#### 1.2.4 å†²çªè§£å†³ç­–ç•¥

**å†²çªç±»å‹ä¸è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from enum import Enum

class ConflictType(Enum):
    """å†²çªç±»å‹"""
    RESOURCE = "resource"      # èµ„æºç«äº‰
    DECISION = "decision"      # å†³ç­–åˆ†æ­§
    PRIORITY = "priority"      # ä¼˜å…ˆçº§å†²çª
    DATA = "data"             # æ•°æ®ä¸ä¸€è‡´

class ConflictResolver:
    """å†²çªè§£å†³å™¨"""

    def __init__(self, arbitrator_model: ChatOpenAI):
        self.arbitrator = arbitrator_model
        self.resolution_history = []

    def resolve(self, conflict_type: ConflictType, parties: List[str], context: dict) -> dict:
        """è§£å†³å†²çª"""

        if conflict_type == ConflictType.RESOURCE:
            return self._resolve_resource_conflict(parties, context)

        elif conflict_type == ConflictType.DECISION:
            return self._resolve_decision_conflict(parties, context)

        elif conflict_type == ConflictType.PRIORITY:
            return self._resolve_priority_conflict(parties, context)

        else:  # DATA
            return self._resolve_data_conflict(parties, context)

    def _resolve_resource_conflict(self, parties: List[str], context: dict) -> dict:
        """è§£å†³èµ„æºç«äº‰"""
        # ç­–ç•¥ï¼šåŸºäºä¼˜å…ˆçº§å’Œç­‰å¾…æ—¶é—´
        priorities = context.get("priorities", {})
        wait_times = context.get("wait_times", {})

        # è®¡ç®—å¾—åˆ†
        scores = {}
        for party in parties:
            priority_score = priorities.get(party, 0) * 10
            wait_score = wait_times.get(party, 0)
            scores[party] = priority_score + wait_score

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        winner = max(scores, key=scores.get)

        resolution = {
            "winner": winner,
            "reason": f"åŸºäºä¼˜å…ˆçº§å’Œç­‰å¾…æ—¶é—´",
            "scores": scores
        }

        self.resolution_history.append(resolution)
        return resolution

    def _resolve_decision_conflict(self, parties: List[str], context: dict) -> dict:
        """è§£å†³å†³ç­–åˆ†æ­§ï¼ˆä½¿ç”¨ LLM ä»²è£ï¼‰"""

        proposals = context.get("proposals", {})

        prompt = f"""ä½œä¸ºä»²è£è€…ï¼Œè¯·ä»ä»¥ä¸‹ææ¡ˆä¸­é€‰æ‹©æœ€ä½³æ–¹æ¡ˆï¼š

{json.dumps(proposals, indent=2)}

è€ƒè™‘å› ç´ ï¼š
1. å¯è¡Œæ€§
2. æ•ˆç‡
3. æˆæœ¬
4. é£é™©

è¿”å›é€‰æ‹©çš„æ–¹æ¡ˆ ID å’Œç†ç”±ã€‚"""

        response = self.arbitrator.invoke(prompt)

        return {
            "decision": response.content,
            "arbitrator": "LLM",
            "timestamp": datetime.now()
        }

    def _resolve_priority_conflict(self, parties: List[str], context: dict) -> dict:
        """è§£å†³ä¼˜å…ˆçº§å†²çª"""
        # ç­–ç•¥ï¼šä½¿ç”¨é¢„å®šä¹‰çš„ä¼˜å…ˆçº§è§„åˆ™
        priority_rules = {
            "safety": 100,
            "user_request": 90,
            "performance": 80,
            "cost": 70
        }

        task_types = context.get("task_types", {})

        sorted_parties = sorted(
            parties,
            key=lambda p: priority_rules.get(task_types.get(p, ""), 0),
            reverse=True
        )

        return {
            "order": sorted_parties,
            "method": "rule-based"
        }

    def _resolve_data_conflict(self, parties: List[str], context: dict) -> dict:
        """è§£å†³æ•°æ®ä¸ä¸€è‡´ï¼ˆæŠ•ç¥¨æˆ–æœ€æ–°ä¼˜å…ˆï¼‰"""
        data_versions = context.get("data_versions", {})

        # ç­–ç•¥ 1ï¼šæœ€æ–°ç‰ˆæœ¬ä¼˜å…ˆ
        if "timestamps" in context:
            timestamps = context["timestamps"]
            latest = max(parties, key=lambda p: timestamps.get(p, 0))
            return {
                "selected": latest,
                "method": "latest-wins",
                "data": data_versions[latest]
            }

        # ç­–ç•¥ 2ï¼šå¤šæ•°æŠ•ç¥¨
        from collections import Counter
        values = [str(data_versions[p]) for p in parties]
        most_common = Counter(values).most_common(1)[0][0]

        winner = [p for p in parties if str(data_versions[p]) == most_common][0]

        return {
            "selected": winner,
            "method": "majority-vote",
            "data": data_versions[winner]
        }

# ä½¿ç”¨ç¤ºä¾‹
resolver = ConflictResolver(ChatOpenAI(model="gpt-4o"))

# èµ„æºå†²çª
resource_conflict = resolver.resolve(
    ConflictType.RESOURCE,
    parties=["agent_a", "agent_b"],
    context={
        "priorities": {"agent_a": 2, "agent_b": 1},
        "wait_times": {"agent_a": 5, "agent_b": 10}
    }
)

print(f"èµ„æºåˆ†é…ç»™: {resource_conflict['winner']}")
```

---

### 1.3 å®æˆ˜æ¡ˆä¾‹

#### 1.3.1 æ•°æ®åˆ†æç³»ç»Ÿ

**å®Œæ•´çš„å¤š Agent æ•°æ®åˆ†æç³»ç»Ÿ**ï¼š

```python
from langgraph.graph import StateGraph, MessagesState, END
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from typing import TypedDict, List
import pandas as pd

class DataAnalysisState(MessagesState):
    """æ•°æ®åˆ†æç³»ç»ŸçŠ¶æ€"""
    data_source: str
    raw_data: pd.DataFrame
    cleaned_data: pd.DataFrame
    analysis_results: dict
    visualization_urls: List[str]
    report: str

# æ•°æ®æ”¶é›† Agent
def data_collector_agent(state: DataAnalysisState) -> DataAnalysisState:
    """æ”¶é›†æ•°æ®"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [sql_query_tool, api_fetch_tool, file_reader_tool],
        prompt="ä½ æ˜¯æ•°æ®æ”¶é›†ä¸“å®¶ï¼Œè´Ÿè´£ä»å„ç§æºè·å–æ•°æ®ã€‚"
    )

    result = agent.invoke({"messages": state["messages"]})

    # å‡è®¾è¿”å›äº†æ•°æ®
    raw_data = pd.DataFrame({
        "date": pd.date_range("2024-01-01", periods=100),
        "sales": np.random.randint(100, 1000, 100),
        "category": np.random.choice(["A", "B", "C"], 100)
    })

    return {
        "messages": state["messages"] + [result["messages"][-1]],
        "raw_data": raw_data
    }

# æ•°æ®æ¸…æ´— Agent
def data_cleaner_agent(state: DataAnalysisState) -> DataAnalysisState:
    """æ¸…æ´—æ•°æ®"""
    raw_data = state["raw_data"]

    # æ¸…æ´—é€»è¾‘
    cleaned_data = raw_data.dropna()
    cleaned_data = cleaned_data[cleaned_data["sales"] > 0]

    # æ·»åŠ è¡ç”Ÿç‰¹å¾
    cleaned_data["month"] = pd.to_datetime(cleaned_data["date"]).dt.month
    cleaned_data["quarter"] = pd.to_datetime(cleaned_data["date"]).dt.quarter

    message = AIMessage(content=f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(cleaned_data)} æ¡æœ‰æ•ˆè®°å½•")

    return {
        "messages": state["messages"] + [message],
        "cleaned_data": cleaned_data
    }

# ç»Ÿè®¡åˆ†æ Agent
def statistical_analyst_agent(state: DataAnalysisState) -> DataAnalysisState:
    """ç»Ÿè®¡åˆ†æ"""
    data = state["cleaned_data"]

    analysis_results = {
        "summary_stats": data.describe().to_dict(),
        "correlation_matrix": data.corr().to_dict(),
        "category_performance": data.groupby("category")["sales"].agg(["mean", "sum", "count"]).to_dict(),
        "trend_analysis": {
            "monthly_avg": data.groupby("month")["sales"].mean().to_dict(),
            "growth_rate": calculate_growth_rate(data)
        }
    }

    message = AIMessage(content=f"ç»Ÿè®¡åˆ†æå®Œæˆï¼Œå‘ç° {len(analysis_results)} é¡¹å…³é”®æ´å¯Ÿ")

    return {
        "messages": state["messages"] + [message],
        "analysis_results": analysis_results
    }

# å¯è§†åŒ– Agent
def visualization_agent(state: DataAnalysisState) -> DataAnalysisState:
    """ç”Ÿæˆå¯è§†åŒ–"""
    import matplotlib.pyplot as plt
    import seaborn as sns

    data = state["cleaned_data"]
    results = state["analysis_results"]

    urls = []

    # ç”Ÿæˆå›¾è¡¨
    # 1. é”€å”®è¶‹åŠ¿å›¾
    plt.figure(figsize=(10, 6))
    data.groupby("date")["sales"].sum().plot()
    plt.title("Sales Trend")
    plt.savefig("/tmp/sales_trend.png")
    urls.append("/tmp/sales_trend.png")

    # 2. ç±»åˆ«å¯¹æ¯”
    plt.figure(figsize=(8, 6))
    data.groupby("category")["sales"].mean().plot(kind="bar")
    plt.title("Average Sales by Category")
    plt.savefig("/tmp/category_comparison.png")
    urls.append("/tmp/category_comparison.png")

    message = AIMessage(content=f"ç”Ÿæˆ {len(urls)} ä¸ªå¯è§†åŒ–å›¾è¡¨")

    return {
        "messages": state["messages"] + [message],
        "visualization_urls": urls
    }

# æŠ¥å‘Šç”Ÿæˆ Agent
def report_writer_agent(state: DataAnalysisState) -> DataAnalysisState:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o"),
        [],
        prompt="""ä½ æ˜¯æ•°æ®åˆ†ææŠ¥å‘Šä¸“å®¶ã€‚

æ ¹æ®æä¾›çš„åˆ†æç»“æœå’Œå›¾è¡¨ï¼Œç”Ÿæˆä¸“ä¸šçš„æ•°æ®åˆ†ææŠ¥å‘Šã€‚

æŠ¥å‘Šåº”åŒ…å«ï¼š
1. æ‰§è¡Œæ‘˜è¦
2. æ•°æ®æ¦‚è§ˆ
3. å…³é”®å‘ç°
4. è¶‹åŠ¿åˆ†æ
5. å»ºè®®
"""
    )

    # æ„å»ºä¸Šä¸‹æ–‡
    context = f"""
åˆ†æç»“æœï¼š
{json.dumps(state["analysis_results"], indent=2)}

å¯è§†åŒ–å›¾è¡¨ï¼š
{state["visualization_urls"]}
"""

    result = agent.invoke({
        "messages": state["messages"] + [HumanMessage(content=context)]
    })

    report = result["messages"][-1].content

    return {
        "messages": state["messages"] + [result["messages"][-1]],
        "report": report
    }

def build_data_analysis_system():
    """æ„å»ºæ•°æ®åˆ†æç³»ç»Ÿ"""
    workflow = StateGraph(DataAnalysisState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("collector", data_collector_agent)
    workflow.add_node("cleaner", data_cleaner_agent)
    workflow.add_node("analyst", statistical_analyst_agent)
    workflow.add_node("visualizer", visualization_agent)
    workflow.add_node("reporter", report_writer_agent)

    # å®šä¹‰æµç¨‹
    workflow.add_edge("collector", "cleaner")
    workflow.add_edge("cleaner", "analyst")
    workflow.add_edge("analyst", "visualizer")
    workflow.add_edge("visualizer", "reporter")
    workflow.add_edge("reporter", END)

    workflow.set_entry_point("collector")

    return workflow.compile()

# ä½¿ç”¨
system = build_data_analysis_system()
result = system.invoke({
    "messages": [HumanMessage(content="åˆ†ææœ€è¿‘ä¸‰ä¸ªæœˆçš„é”€å”®æ•°æ®")],
    "data_source": "sales_database"
})

print(result["report"])
```

#### 1.3.2 æ™ºèƒ½å®¢æœç³»ç»Ÿ

**å¤šå±‚çº§å®¢æœç³»ç»Ÿå®ç°**ï¼š

```python
class CustomerServiceSystem:
    """æ™ºèƒ½å®¢æœç³»ç»Ÿ"""

    def __init__(self):
        from langchain.agents import create_agent

        # ä¸€çº¿å®¢æœï¼ˆå¤„ç†å¸¸è§é—®é¢˜ï¼‰
        self.tier1_agent = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [faq_search_tool, order_status_tool],
            prompt="""ä½ æ˜¯ä¸€çº¿å®¢æœä»£è¡¨ã€‚

èŒè´£ï¼š
1. å›ç­”å¸¸è§é—®é¢˜
2. æŸ¥è¯¢è®¢å•çŠ¶æ€
3. æ”¶é›†ç”¨æˆ·ä¿¡æ¯

å¦‚æœé‡åˆ°å¤æ‚é—®é¢˜ï¼Œå‡çº§ç»™äºŒçº¿å®¢æœã€‚"""
        )

        # äºŒçº¿å®¢æœï¼ˆå¤„ç†å¤æ‚é—®é¢˜ï¼‰
        self.tier2_agent = create_agent(
            ChatOpenAI(model="gpt-4o"),
            [refund_tool, account_modify_tool, technical_support_tool],
            prompt="""ä½ æ˜¯äºŒçº¿å®¢æœä¸“å®¶ã€‚

èŒè´£ï¼š
1. å¤„ç†é€€æ¬¾è¯·æ±‚
2. è´¦æˆ·é—®é¢˜è§£å†³
3. æŠ€æœ¯æ”¯æŒ

å¦‚æœéœ€è¦äººå·¥ä»‹å…¥ï¼Œå‡çº§ç»™äººå·¥å®¢æœã€‚"""
        )

        # æƒ…æ„Ÿåˆ†æ Agent
        self.sentiment_analyzer = create_agent(
            ChatOpenAI(model="gpt-4o-mini"),
            [],
            prompt="""åˆ†æç”¨æˆ·æƒ…ç»ªã€‚

è¿”å›ï¼š
- æƒ…ç»ªçŠ¶æ€ï¼šç§¯æ/ä¸­æ€§/æ¶ˆæ/æ„¤æ€’
- ç´§æ€¥ç¨‹åº¦ï¼šä½/ä¸­/é«˜
- æ˜¯å¦éœ€è¦ç«‹å³äººå·¥ä»‹å…¥"""
        )

    def route_request(self, user_message: str) -> str:
        """æ™ºèƒ½è·¯ç”±è¯·æ±‚"""
        # å…ˆè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        sentiment = self.analyze_sentiment(user_message)

        if sentiment["urgent"] or sentiment["emotion"] == "æ„¤æ€’":
            return "human"  # ç›´æ¥è½¬äººå·¥

        # æ„å›¾è¯†åˆ«
        intent = self.classify_intent(user_message)

        if intent in ["order_query", "faq", "product_info"]:
            return "tier1"
        elif intent in ["refund", "complaint", "technical"]:
            return "tier2"
        else:
            return "tier1"  # é»˜è®¤ä¸€çº¿

    def handle_conversation(self, messages: List[BaseMessage]) -> dict:
        """å¤„ç†å®Œæ•´å¯¹è¯"""
        state = {
            "messages": messages,
            "escalation_count": 0,
            "resolved": False
        }

        current_tier = "tier1"

        while not state["resolved"] and state["escalation_count"] < 3:
            if current_tier == "tier1":
                response = self.tier1_agent.invoke({"messages": state["messages"]})

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§
                if "å‡çº§" in response["messages"][-1].content:
                    current_tier = "tier2"
                    state["escalation_count"] += 1
                    continue

            elif current_tier == "tier2":
                response = self.tier2_agent.invoke({"messages": state["messages"]})

                # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥
                if "äººå·¥" in response["messages"][-1].content:
                    current_tier = "human"
                    state["escalation_count"] += 1
                    continue

            else:  # human
                response = self.transfer_to_human(state["messages"])

            state["messages"].extend(response["messages"])
            state["resolved"] = True

        return state
```

#### 1.3.3 ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ

**å®ç°å®Œæ•´çš„ç ”ç©¶åŠ©æ‰‹**ï¼š

```python
from langgraph.graph import StateGraph, END
from typing import List, Dict
from langchain.agents import create_agent

class ResearchAssistantSystem:
    """ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ"""

    def __init__(self):
        self.workflow = self._build_workflow()

    def _build_workflow(self):
        """æ„å»ºç ”ç©¶å·¥ä½œæµ"""
        workflow = StateGraph(ResearchState)

        # æ·»åŠ ä¸“é—¨åŒ–ç ”ç©¶ Agents
        workflow.add_node("topic_explorer", self.topic_explorer_agent())
        workflow.add_node("literature_reviewer", self.literature_review_agent())
        workflow.add_node("data_analyst", self.data_analysis_agent())
        workflow.add_node("hypothesis_generator", self.hypothesis_agent())
        workflow.add_node("experiment_designer", self.experiment_design_agent())
        workflow.add_node("paper_writer", self.paper_writing_agent())

        # Supervisor åè°ƒ
        workflow.add_node("supervisor", self.research_supervisor())

        # å®šä¹‰æµç¨‹
        workflow.set_entry_point("supervisor")

        # Supervisor å†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "supervisor",
            lambda x: x.get("next_phase"),
            {
                "explore": "topic_explorer",
                "review": "literature_reviewer",
                "analyze": "data_analyst",
                "hypothesize": "hypothesis_generator",
                "design": "experiment_designer",
                "write": "paper_writer",
                "complete": END
            }
        )

        # æ‰€æœ‰ Agents è¿”å› Supervisor
        for node in ["topic_explorer", "literature_reviewer", "data_analyst",
                    "hypothesis_generator", "experiment_designer", "paper_writer"]:
            workflow.add_edge(node, "supervisor")

        return workflow.compile()

    def topic_explorer_agent(self):
        """ä¸»é¢˜æ¢ç´¢ Agent"""
        return create_agent(
            ChatOpenAI(model="gpt-4o"),
            [web_search_tool, arxiv_search_tool],
            prompt="""ä½ æ˜¯ç ”ç©¶ä¸»é¢˜æ¢ç´¢ä¸“å®¶ã€‚

ä»»åŠ¡ï¼š
1. è¯†åˆ«ç ”ç©¶é¢†åŸŸçš„å…³é”®é—®é¢˜
2. å‘ç°ç ”ç©¶ç©ºç™½
3. è¯„ä¼°ç ”ç©¶ä»·å€¼å’Œå¯è¡Œæ€§"""
        )

    def literature_review_agent(self):
        """æ–‡çŒ®ç»¼è¿° Agent"""
        return create_agent(
            ChatOpenAI(model="gpt-4o"),
            [paper_search_tool, citation_tool, summarize_tool],
            prompt="""ä½ æ˜¯æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚

ä»»åŠ¡ï¼š
1. æœç´¢ç›¸å…³æ–‡çŒ®
2. æ€»ç»“å…³é”®å‘ç°
3. è¯†åˆ«ç ”ç©¶è¶‹åŠ¿
4. æ„å»ºçŸ¥è¯†å›¾è°±"""
        )

    def research_supervisor(self):
        """ç ”ç©¶ç›‘ç£è€…"""
        return create_agent(
            ChatOpenAI(model="gpt-4o"),
            [],
            prompt="""ä½ æ˜¯ç ”ç©¶é¡¹ç›®æ€»ç›‘ã€‚

æ ¹æ®ç ”ç©¶è¿›å±•ï¼Œå†³å®šä¸‹ä¸€ä¸ªé˜¶æ®µï¼š
- explore: åˆæ­¥æ¢ç´¢
- review: æ–‡çŒ®ç»¼è¿°
- analyze: æ•°æ®åˆ†æ
- hypothesize: å‡è®¾ç”Ÿæˆ
- design: å®éªŒè®¾è®¡
- write: è®ºæ–‡æ’°å†™
- complete: å®Œæˆ

ç¡®ä¿ç ”ç©¶çš„ç§‘å­¦æ€§å’Œå®Œæ•´æ€§ã€‚"""
        )

    def conduct_research(self, topic: str) -> Dict:
        """æ‰§è¡Œå®Œæ•´ç ”ç©¶"""
        initial_state = {
            "messages": [HumanMessage(content=f"ç ”ç©¶ä¸»é¢˜ï¼š{topic}")],
            "research_topic": topic,
            "phase": "explore",
            "findings": {},
            "papers": []
        }

        result = self.workflow.invoke(initial_state)

        return {
            "topic": topic,
            "findings": result.get("findings"),
            "papers": result.get("papers"),
            "final_report": result.get("report")
        }

# ä½¿ç”¨ç¤ºä¾‹
research_system = ResearchAssistantSystem()
result = research_system.conduct_research("LLM Agent çš„æœªæ¥å‘å±•æ–¹å‘")
print(result["final_report"])
```

#### 1.3.4 è°ƒè¯•ä¸ä¼˜åŒ–

**å¤š Agent ç³»ç»Ÿè°ƒè¯•æŠ€å·§**ï¼š

```python
from langsmith import Client
from langchain_core.callbacks import LangChainTracer
import logging

class MultiAgentDebugger:
    """å¤š Agent è°ƒè¯•å™¨"""

    def __init__(self, project_name: str):
        self.client = Client()
        self.project_name = project_name
        self.tracer = LangChainTracer(project_name=project_name)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.DEBUG)
        self.logger = logging.getLogger(__name__)

    def trace_agent_communication(self, workflow: StateGraph):
        """è¿½è¸ª Agent é—´é€šä¿¡"""

        # æ³¨å…¥è¿½è¸ªå›è°ƒ
        for node in workflow.nodes:
            if hasattr(node, "invoke"):
                original_invoke = node.invoke

                def traced_invoke(self, *args, **kwargs):
                    self.logger.debug(f"Agent {node.name} invoked with: {args}")
                    result = original_invoke(*args, **kwargs)
                    self.logger.debug(f"Agent {node.name} returned: {result}")
                    return result

                node.invoke = traced_invoke

    def profile_performance(self, workflow: StateGraph, test_cases: List[dict]):
        """æ€§èƒ½åˆ†æ"""
        results = []

        for case in test_cases:
            start_time = time.time()

            # è¿è¡Œå·¥ä½œæµ
            result = workflow.invoke(case)

            duration = time.time() - start_time

            # æ”¶é›†æŒ‡æ ‡
            metrics = {
                "case_id": case.get("id"),
                "duration": duration,
                "agent_calls": self._count_agent_calls(result),
                "token_usage": self._calculate_tokens(result),
                "cost": self._estimate_cost(result)
            }

            results.append(metrics)

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_performance_report(results)

        return results

    def detect_bottlenecks(self, trace_data: dict):
        """æ£€æµ‹æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # åˆ†ææ¯ä¸ª Agent çš„æ‰§è¡Œæ—¶é—´
        agent_times = {}
        for run in trace_data["runs"]:
            agent_name = run["name"]
            duration = run["end_time"] - run["start_time"]

            if agent_name not in agent_times:
                agent_times[agent_name] = []
            agent_times[agent_name].append(duration)

        # æ‰¾å‡ºæœ€æ…¢çš„ Agents
        avg_times = {
            agent: sum(times) / len(times)
            for agent, times in agent_times.items()
        }

        slowest = sorted(avg_times.items(), key=lambda x: x[1], reverse=True)[:3]

        for agent, avg_time in slowest:
            if avg_time > 5.0:  # è¶…è¿‡ 5 ç§’
                bottlenecks.append({
                    "agent": agent,
                    "avg_time": avg_time,
                    "severity": "high" if avg_time > 10 else "medium"
                })

        return bottlenecks

    def optimize_suggestions(self, bottlenecks: List[dict]) -> List[str]:
        """ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        for bottleneck in bottlenecks:
            agent = bottleneck["agent"]
            avg_time = bottleneck["avg_time"]

            if "search" in agent.lower():
                suggestions.append(f"ä¸º {agent} æ·»åŠ ç¼“å­˜æœºåˆ¶")
                suggestions.append(f"è€ƒè™‘å¹¶è¡ŒåŒ– {agent} çš„æœç´¢è¯·æ±‚")

            elif "model" in agent.lower():
                suggestions.append(f"è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æ›¿ä»£ {agent}")
                suggestions.append(f"å‡å°‘ {agent} çš„ä¸Šä¸‹æ–‡é•¿åº¦")

            if avg_time > 10:
                suggestions.append(f"è€ƒè™‘å°† {agent} çš„ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡")

        return suggestions

# ä½¿ç”¨ç¤ºä¾‹
debugger = MultiAgentDebugger("multi_agent_debug")

# è¿½è¸ªé€šä¿¡
debugger.trace_agent_communication(workflow)

# æ€§èƒ½åˆ†æ
test_cases = [
    {"messages": [HumanMessage(content="æµ‹è¯•æŸ¥è¯¢ 1")]},
    {"messages": [HumanMessage(content="æµ‹è¯•æŸ¥è¯¢ 2")]},
]
metrics = debugger.profile_performance(workflow, test_cases)

# æ£€æµ‹ç“¶é¢ˆ
bottlenecks = debugger.detect_bottlenecks(trace_data)
suggestions = debugger.optimize_suggestions(bottlenecks)

for suggestion in suggestions:
    print(f"ğŸ’¡ {suggestion}")
```

### 1.4 äººæœºååŒä¸ä¸­æ–­æœºåˆ¶

#### 1.4.1 ä¸­æ–­ç‚¹è®¾ç½®

**LangGraph æä¾›çš„äººæœºååŒèƒ½åŠ›**:

åœ¨å®é™…åº”ç”¨ä¸­,æŸäº›å…³é”®å†³ç­–ç‚¹éœ€è¦äººå·¥ä»‹å…¥å®¡æ ¸æˆ–ç¡®è®¤ã€‚LangGraph é€šè¿‡ `interrupt_before` å’Œ `interrupt_after` å®ç°ä¸­æ–­æœºåˆ¶ã€‚

```python
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

# å®šä¹‰çŠ¶æ€
class ApprovalState(MessagesState):
    """éœ€è¦å®¡æ‰¹çš„çŠ¶æ€"""
    draft_response: str = ""
    approved: bool = False
    feedback: str = ""

# åˆ›å»ºèŠ‚ç‚¹
def generate_draft(state: ApprovalState) -> dict:
    """ç”Ÿæˆè‰ç¨¿å›å¤"""
    from langchain.agents import create_agent

    agent = create_agent(
        ChatOpenAI(model="gpt-4o"),
        [],
        prompt="ç”Ÿæˆä¸“ä¸šçš„å®¢æˆ·å›å¤è‰ç¨¿"
    )

    result = agent.invoke({"messages": state["messages"]})
    draft = result["messages"][-1].content

    return {"draft_response": draft}

def human_review(state: ApprovalState) -> dict:
    """äººå·¥å®¡æ ¸èŠ‚ç‚¹(è¿™é‡Œä¸æ‰§è¡Œ,ç”±å¤–éƒ¨å¤„ç†)"""
    # è¿™ä¸ªèŠ‚ç‚¹åœ¨ interrupt_before æ—¶ä¸ä¼šæ‰§è¡Œ
    # å¤–éƒ¨é€šè¿‡ update_state æ›´æ–°çŠ¶æ€
    return {}

def send_response(state: ApprovalState) -> dict:
    """å‘é€æœ€ç»ˆå›å¤"""
    if state.get("approved"):
        final_response = state["draft_response"]
        if state.get("feedback"):
            final_response += f"\n\nè¡¥å……: {state['feedback']}"

        print(f"å‘é€å›å¤: {final_response}")
        return {"messages": [AIMessage(content=final_response)]}
    else:
        return {"messages": [AIMessage(content="å›å¤å·²è¢«æ‹’ç»")]}

# æ„å»ºå›¾
builder = StateGraph(ApprovalState)
builder.add_node("generate_draft", generate_draft)
builder.add_node("human_review", human_review)
builder.add_node("send_response", send_response)

builder.set_entry_point("generate_draft")
builder.add_edge("generate_draft", "human_review")
builder.add_edge("human_review", "send_response")
builder.add_edge("send_response", END)

# å…³é”®: è®¾ç½®ä¸­æ–­ç‚¹
checkpointer = MemorySaver()
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=["human_review"]  # åœ¨äººå·¥å®¡æ ¸å‰ä¸­æ–­
)
```

#### 1.4.2 çŠ¶æ€æ£€æŸ¥ä¸æ›´æ–°

**å®Œæ•´çš„ä¸­æ–­-å®¡æ ¸-æ¢å¤æµç¨‹**:

```python
# 1. è¿è¡Œåˆ°ä¸­æ–­ç‚¹
config = {"configurable": {"thread_id": "approval-001"}}
result = graph.invoke(
    {"messages": [HumanMessage(content="æˆ‘è¦æŠ•è¯‰ä½ ä»¬çš„æœåŠ¡")]},
    config
)
# æ‰§è¡Œä¼šåœ¨ human_review å‰æš‚åœ

# 2. è·å–å½“å‰çŠ¶æ€
state = graph.get_state(config)

print(f"å½“å‰çŠ¶æ€å€¼: {state.values}")
print(f"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {state.next}")  # è¾“å‡º: ('human_review',)
print(f"checkpoint_id: {state.config['configurable']['checkpoint_id']}")

# StateSnapshot å¯¹è±¡åŒ…å«:
# - values: dict - å½“å‰çŠ¶æ€çš„æ‰€æœ‰å€¼
# - next: tuple[str] - ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹(ä¸­æ–­ç‚¹)
# - config: dict - åŒ…å« checkpoint_id çš„é…ç½®
# - metadata: dict - å…ƒæ•°æ®
# - created_at: str - åˆ›å»ºæ—¶é—´
# - parent_config: dict - çˆ¶çŠ¶æ€é…ç½®

# æŸ¥çœ‹è‰ç¨¿å†…å®¹
draft = state.values.get("draft_response")
print(f"\nå¾…å®¡æ ¸è‰ç¨¿:\n{draft}")

# 3. äººå·¥å®¡æ ¸å¹¶æ›´æ–°çŠ¶æ€
# åœºæ™¯A: æ‰¹å‡†å¹¶æ·»åŠ è¡¥å……
graph.update_state(
    config,
    {
        "approved": True,
        "feedback": "è¯·é¢å¤–æä¾›è¡¥å¿æ–¹æ¡ˆ"
    }
)

# åœºæ™¯B: æ‹’ç»å¹¶è¦æ±‚é‡å†™
# graph.update_state(
#     config,
#     {
#         "approved": False,
#         "feedback": "è¯­æ°”å¤ªå¼ºç¡¬,éœ€è¦æ›´æ¸©å’Œçš„è¡¨è¾¾"
#     },
#     as_node="generate_draft"  # å›é€€åˆ°è‰ç¨¿ç”ŸæˆèŠ‚ç‚¹
# )

# 4. æ¢å¤æ‰§è¡Œ
# ä»ä¸­æ–­ç‚¹ç»§ç»­(None è¡¨ç¤ºä¸æ·»åŠ æ–°è¾“å…¥)
final_result = graph.invoke(None, config)
print(f"\næœ€ç»ˆç»“æœ: {final_result}")
```

#### 1.4.3 interrupt_after ç”¨æ³•

**åœ¨èŠ‚ç‚¹æ‰§è¡Œåä¸­æ–­**:

```python
# ä½¿ç”¨ interrupt_after
graph_after = builder.compile(
    checkpointer=MemorySaver(),
    interrupt_after=["generate_draft"]  # åœ¨è‰ç¨¿ç”Ÿæˆåä¸­æ–­
)

config = {"configurable": {"thread_id": "after-001"}}

# è¿è¡Œ
result = graph_after.invoke(
    {"messages": [HumanMessage(content="å’¨è¯¢äº§å“ä¿¡æ¯")]},
    config
)

# generate_draft å·²æ‰§è¡Œå®Œ,ä½†åœ¨ä¸‹ä¸€ä¸ªèŠ‚ç‚¹å‰ä¸­æ–­
state = graph_after.get_state(config)
print(f"å·²ç”Ÿæˆè‰ç¨¿: {state.values['draft_response']}")
print(f"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {state.next}")  # ('human_review',)

# æ£€æŸ¥è‰ç¨¿åå†³å®šæ˜¯å¦ç»§ç»­
if "æ•æ„Ÿè¯" in state.values['draft_response']:
    # ä¿®æ”¹è‰ç¨¿
    graph_after.update_state(
        config,
        {"draft_response": "ä¿®æ”¹åçš„è‰ç¨¿"}
    )

# ç»§ç»­æ‰§è¡Œ
graph_after.invoke(None, config)
```

#### 1.4.4 å¤šä¸­æ–­ç‚¹å·¥ä½œæµ

**å¤æ‚å®¡æ‰¹æµç¨‹**:

```python
from typing import TypedDict

class MultiApprovalState(MessagesState):
    """å¤šçº§å®¡æ‰¹çŠ¶æ€"""
    task: str = ""
    budget_estimate: float = 0.0
    manager_approved: bool = False
    director_approved: bool = False
    cfo_approved: bool = False

def estimate_budget(state: MultiApprovalState) -> dict:
    """é¢„ç®—è¯„ä¼°"""
    # æ¨¡æ‹Ÿé¢„ç®—è®¡ç®—
    return {"budget_estimate": 50000.0}

def manager_review(state: MultiApprovalState) -> dict:
    """ç»ç†å®¡æ‰¹"""
    return {}

def director_review(state: MultiApprovalState) -> dict:
    """æ€»ç›‘å®¡æ‰¹"""
    return {}

def cfo_review(state: MultiApprovalState) -> dict:
    """CFOå®¡æ‰¹"""
    return {}

def execute_task(state: MultiApprovalState) -> dict:
    """æ‰§è¡Œä»»åŠ¡"""
    print(f"æ‰§è¡Œä»»åŠ¡: {state['task']}, é¢„ç®—: {state['budget_estimate']}")
    return {}

# æ„å»ºå¤šçº§å®¡æ‰¹æµç¨‹
builder = StateGraph(MultiApprovalState)
builder.add_node("estimate", estimate_budget)
builder.add_node("manager", manager_review)
builder.add_node("director", director_review)
builder.add_node("cfo", cfo_review)
builder.add_node("execute", execute_task)

builder.set_entry_point("estimate")
builder.add_edge("estimate", "manager")
builder.add_edge("manager", "director")
builder.add_edge("director", "cfo")
builder.add_edge("cfo", "execute")
builder.add_edge("execute", END)

# è®¾ç½®å¤šä¸ªä¸­æ–­ç‚¹
graph = builder.compile(
    checkpointer=MemorySaver(),
    interrupt_before=["manager", "director", "cfo"]  # æ¯ä¸ªå®¡æ‰¹ç¯èŠ‚éƒ½ä¸­æ–­
)

# æ‰§è¡Œæµç¨‹
config = {"configurable": {"thread_id": "multi-approval-001"}}

# æ­¥éª¤1: è¿è¡Œåˆ°ç»ç†å®¡æ‰¹
result = graph.invoke({"task": "è´­ä¹°æ–°æœåŠ¡å™¨"}, config)
state = graph.get_state(config)
print(f"é¢„ç®—: {state.values['budget_estimate']}")
print(f"ç­‰å¾…å®¡æ‰¹: {state.next}")  # ('manager',)

# ç»ç†æ‰¹å‡†
graph.update_state(config, {"manager_approved": True})
result = graph.invoke(None, config)

# æ­¥éª¤2: è¿è¡Œåˆ°æ€»ç›‘å®¡æ‰¹
state = graph.get_state(config)
print(f"ç­‰å¾…å®¡æ‰¹: {state.next}")  # ('director',)

# æ€»ç›‘æ‰¹å‡†
graph.update_state(config, {"director_approved": True})
result = graph.invoke(None, config)

# æ­¥éª¤3: è¿è¡Œåˆ°CFOå®¡æ‰¹
state = graph.get_state(config)
print(f"ç­‰å¾…å®¡æ‰¹: {state.next}")  # ('cfo',)

# CFOæ‰¹å‡†
graph.update_state(config, {"cfo_approved": True})
final_result = graph.invoke(None, config)

print("æ‰€æœ‰å®¡æ‰¹å®Œæˆ,ä»»åŠ¡æ‰§è¡Œ!")
```

#### 1.4.5 æœ€ä½³å®è·µ

**ä¸­æ–­æœºåˆ¶ä½¿ç”¨å»ºè®®**:

1. **é€‰æ‹©åˆé€‚çš„ä¸­æ–­ç‚¹**:
   - `interrupt_before`: é€‚åˆéœ€è¦é¢„å®¡çš„åœºæ™¯(å¦‚å‘é€é‚®ä»¶å‰)
   - `interrupt_after`: é€‚åˆéœ€è¦æ£€æŸ¥ç»“æœçš„åœºæ™¯(å¦‚æ•°æ®å¤„ç†å)

2. **çŠ¶æ€æ›´æ–°ç­–ç•¥**:
   ```python
   # å®Œå…¨æ›¿æ¢çŠ¶æ€å­—æ®µ
   graph.update_state(config, {"field": "new_value"})

   # æŒ‡å®šä»å“ªä¸ªèŠ‚ç‚¹ç»§ç»­(å›é€€)
   graph.update_state(
       config,
       {"field": "value"},
       as_node="previous_node"
   )
   ```

3. **è¶…æ—¶å¤„ç†**:
   ```python
   from datetime import datetime, timedelta

   state = graph.get_state(config)
   created_time = datetime.fromisoformat(state.created_at)

   if datetime.now() - created_time > timedelta(hours=24):
       # è¶…æ—¶è‡ªåŠ¨æ‹’ç»æˆ–æé†’
       graph.update_state(config, {"approved": False, "reason": "è¶…æ—¶"})
   ```

4. **å®¡è®¡æ—¥å¿—**:
   ```python
   # è®°å½•å®¡æ‰¹å†å²
   graph.update_state(
       config,
       {
           "approved": True,
           "audit_log": {
               "reviewer": "user@example.com",
               "timestamp": datetime.now().isoformat(),
               "action": "approved",
               "comment": "é€šè¿‡å®¡æ ¸"
           }
       }
   )
   ```

---

## ç¬¬2ç« ï¼š é«˜çº§ç‰¹æ€§ä¸é›†æˆ

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ LangChain çš„å‰æ²¿åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ã€æµå¼å¤„ç†ã€MCP é›†æˆç­‰ã€‚

### 2.1 å¤šæ¨¡æ€åº”ç”¨

#### 2.1.1 å›¾åƒå¤„ç†ï¼ˆOCRã€å›¾åƒç†è§£ï¼‰

**å›¾åƒç†è§£ä¸å·¥å…·è°ƒç”¨**ï¼š

```python
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
import base64

def encode_image(image_path: str) -> str:
    """å°†å›¾åƒç¼–ç ä¸º base64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# åˆ›å»ºæ”¯æŒè§†è§‰çš„æ¨¡å‹
vision_model = ChatOpenAI(model="gpt-4o")

# å‘é€å›¾åƒè¿›è¡Œç†è§£
image_base64 = encode_image("chart.png")

message = HumanMessage(
    content=[
        {"type": "text", "text": "åˆ†æè¿™ä¸ªå›¾è¡¨ä¸­çš„æ•°æ®è¶‹åŠ¿"},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{image_base64}"}
        }
    ]
)

response = vision_model.invoke([message])
print(response.content)

# ç»“åˆå·¥å…·è°ƒç”¨
from langchain.agents import create_agent
from langchain_core.tools import tool

@tool
def save_analysis(analysis: str, filename: str) -> str:
    """ä¿å­˜åˆ†æç»“æœ"""
    with open(filename, "w") as f:
        f.write(analysis)
    return f"åˆ†æå·²ä¿å­˜åˆ° {filename}"

# åˆ›å»ºå¤šæ¨¡æ€ Agent
multimodal_agent = create_agent(
    vision_model,
    [save_analysis],
    prompt="ä½ æ˜¯å›¾åƒåˆ†æä¸“å®¶ï¼Œå¯ä»¥ç†è§£å›¾åƒå¹¶ä½¿ç”¨å·¥å…·ä¿å­˜åˆ†æç»“æœã€‚"
)

# åˆ†æå›¾åƒå¹¶ä¿å­˜
result = multimodal_agent.invoke({
    "messages": [
        HumanMessage(content=[
            {"type": "text", "text": "åˆ†æè¿™ä¸ªå›¾è¡¨å¹¶ä¿å­˜ç»“æœåˆ° analysis.txt"},
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
        ])
    ]
})
```

**OCR ä¸æ–‡æ¡£å¤„ç†**ï¼š

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import pytesseract
from PIL import Image

class OCRProcessor:
    """OCR å¤„ç†å™¨"""

    def __init__(self, model: ChatOpenAI):
        self.model = model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )

    def extract_text_from_image(self, image_path: str) -> str:
        """ä»å›¾åƒæå–æ–‡æœ¬ï¼ˆOCRï¼‰"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='chi_sim+eng')
        return text

    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:
        """ä» PDF æå–æ–‡æœ¬"""
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # åˆ†å‰²æ–‡æœ¬
        texts = self.text_splitter.split_documents(documents)

        return [doc.page_content for doc in texts]

    def understand_document_with_vision(self, image_path: str) -> dict:
        """ä½¿ç”¨è§†è§‰æ¨¡å‹ç†è§£æ–‡æ¡£"""
        image_base64 = encode_image(image_path)

        response = self.model.invoke([
            HumanMessage(content=[
                {"type": "text", "text": "æå–å¹¶ç»“æ„åŒ–è¿™ä¸ªæ–‡æ¡£ä¸­çš„ä¿¡æ¯"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
            ])
        ])

        # è§£æå“åº”ï¼ˆå¯ä»¥è¦æ±‚è¿”å› JSONï¼‰
        return {
            "extracted_text": response.content,
            "structured_data": self._parse_structured_data(response.content)
        }

    def _parse_structured_data(self, text: str) -> dict:
        """è§£æç»“æ„åŒ–æ•°æ®"""
        # ä½¿ç”¨ LLM æå–ç»“æ„åŒ–ä¿¡æ¯
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆJSON æ ¼å¼ï¼‰ï¼š

{text}

è¿”å›æ ¼å¼ï¼š
{{
    "title": "...",
    "date": "...",
    "key_points": [...],
    "numbers": {{...}}
}}"""

        response = self.model.invoke(prompt)
        # è§£æ JSONï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„è§£æï¼‰
        import json
        try:
            return json.loads(response.content)
        except:
            return {"raw_text": response.content}

# ä½¿ç”¨ç¤ºä¾‹
processor = OCRProcessor(ChatOpenAI(model="gpt-4o"))

# OCR æå–
text = processor.extract_text_from_image("scanned_document.png")

# è§†è§‰ç†è§£
understanding = processor.understand_document_with_vision("complex_chart.png")
print(understanding["structured_data"])
```

#### 2.1.2 éŸ³é¢‘å¤„ç†ï¼ˆè¯­éŸ³è¯†åˆ«ã€åˆæˆï¼‰

**éŸ³é¢‘å¤„ç†é›†æˆ**ï¼š

```python
from openai import OpenAI
import io
from pydub import AudioSegment
from typing import BinaryIO

class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å™¨"""

    def __init__(self):
        self.client = OpenAI()

    def transcribe_audio(self, audio_file: BinaryIO) -> str:
        """è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆä½¿ç”¨ Whisperï¼‰"""
        transcript = self.client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
        return transcript.text

    def text_to_speech(self, text: str, voice: str = "alloy") -> bytes:
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        response = self.client.audio.speech.create(
            model="tts-1",
            voice=voice,  # alloy, echo, fable, onyx, nova, shimmer
            input=text
        )

        return response.content

    def process_audio_with_agent(self, audio_path: str):
        """ä½¿ç”¨ Agent å¤„ç†éŸ³é¢‘"""
        from langchain.agents import create_agent

        # è½¬å½•éŸ³é¢‘
        with open(audio_path, "rb") as audio_file:
            transcript = self.transcribe_audio(audio_file)

        # ä½¿ç”¨ Agent å¤„ç†è½¬å½•æ–‡æœ¬
        agent = create_agent(
            ChatOpenAI(model="gpt-4o"),
            [search_tool, summarize_tool],
            prompt="å¤„ç†éŸ³é¢‘è½¬å½•å¹¶å›ç­”é—®é¢˜ã€‚"
        )

        response = agent.invoke({
            "messages": [HumanMessage(content=f"éŸ³é¢‘å†…å®¹ï¼š{transcript}\n\nè¯·æ€»ç»“è¦ç‚¹")]
        })

        summary = response["messages"][-1].content

        # å°†æ‘˜è¦è½¬ä¸ºè¯­éŸ³
        audio_response = self.text_to_speech(summary)

        return {
            "transcript": transcript,
            "summary": summary,
            "audio_response": audio_response
        }

# å®æ—¶éŸ³é¢‘æµå¤„ç†
class AudioStreamProcessor:
    """å®æ—¶éŸ³é¢‘æµå¤„ç†"""

    def __init__(self, agent):
        self.agent = agent
        self.audio_buffer = io.BytesIO()

    async def process_audio_stream(self, audio_chunk: bytes):
        """å¤„ç†éŸ³é¢‘æµå—"""
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.write(audio_chunk)

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³é¢‘ï¼ˆä¾‹å¦‚ 3 ç§’ï¼‰
        if self.audio_buffer.tell() > 48000:  # 16kHz * 3s
            # å¤„ç†ç¼“å†²åŒºä¸­çš„éŸ³é¢‘
            self.audio_buffer.seek(0)
            transcript = self.transcribe_audio(self.audio_buffer)

            # æ¸…ç©ºç¼“å†²åŒº
            self.audio_buffer = io.BytesIO()

            # ä½¿ç”¨ Agent å¤„ç†
            response = await self.agent.ainvoke({
                "messages": [HumanMessage(content=transcript)]
            })

            return response["messages"][-1].content

        return None
```

**å®Œæ•´çš„å›¾æ–‡æ··åˆç¤ºä¾‹**:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import base64

# ========== ç¤ºä¾‹1: å›¾ç‰‡ç†è§£(æœ¬åœ°æ–‡ä»¶) ==========
def analyze_local_image():
    """åˆ†ææœ¬åœ°å›¾ç‰‡"""
    model = ChatOpenAI(model="gpt-4o")

    # æ–¹æ³•1: ä½¿ç”¨ base64 ç¼–ç 
    with open("product_image.jpg", "rb") as image_file:
        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

    message = HumanMessage(
        content=[
            {"type": "text", "text": "è¯·æè¿°è¿™ä¸ªäº§å“å›¾ç‰‡,åŒ…æ‹¬é¢œè‰²ã€æè´¨å’Œå¯èƒ½çš„ç”¨é€”"},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
            }
        ]
    )

    response = model.invoke([message])
    print("å›¾ç‰‡åˆ†æ:", response.content)

# ========== ç¤ºä¾‹2: åœ¨çº¿å›¾ç‰‡URL ==========
def analyze_url_image():
    """åˆ†æåœ¨çº¿å›¾ç‰‡"""
    model = ChatOpenAI(model="gpt-4o")

    message = HumanMessage(
        content=[
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆ?"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://example.com/image.jpg",
                    "detail": "high"  # "low", "high", or "auto"
                }
            }
        ]
    )

    response = model.invoke([message])
    return response.content

# ========== ç¤ºä¾‹3: å¤šå›¾ç‰‡å¯¹æ¯” ==========
def compare_images():
    """å¯¹æ¯”å¤šå¼ å›¾ç‰‡"""
    model = ChatOpenAI(model="gpt-4o")

    # ç¼–ç ä¸¤å¼ å›¾ç‰‡
    with open("before.jpg", "rb") as f:
        image1_b64 = base64.b64encode(f.read()).decode('utf-8')

    with open("after.jpg", "rb") as f:
        image2_b64 = base64.b64encode(f.read()).decode('utf-8')

    message = HumanMessage(
        content=[
            {"type": "text", "text": "å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡çš„å·®å¼‚:"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image1_b64}"}},
            {"type": "text", "text": "å’Œ"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image2_b64}"}},
            {"type": "text", "text": "åˆ—å‡ºæ‰€æœ‰ä¸åŒä¹‹å¤„"}
        ]
    )

    response = model.invoke([message])
    print("å¯¹æ¯”ç»“æœ:", response.content)

# ========== ç¤ºä¾‹4: å›¾è¡¨æ•°æ®æå– ==========
def extract_chart_data():
    """ä»å›¾è¡¨ä¸­æå–æ•°æ®"""
    model = ChatOpenAI(model="gpt-4o")

    with open("sales_chart.png", "rb") as f:
        chart_b64 = base64.b64encode(f.read()).decode('utf-8')

    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": """æå–è¿™ä¸ªé”€å”®å›¾è¡¨ä¸­çš„æ•°æ®,è¿”å›JSONæ ¼å¼:
                {
                    "months": ["1æœˆ", "2æœˆ", ...],
                    "sales": [1000, 1200, ...],
                    "trend": "ä¸Šå‡/ä¸‹é™/ç¨³å®š"
                }"""
            },
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{chart_b64}"}}
        ]
    )

    response = model.invoke([message])

    # è§£æJSON
    import json
    try:
        data = json.loads(response.content)
        return data
    except json.JSONDecodeError:
        # å¦‚æœLLMè¿”å›çš„ä¸æ˜¯çº¯JSON,å°è¯•æå–
        print("åŸå§‹å“åº”:", response.content)
        return None

# ========== ç¤ºä¾‹5: è§†è§‰é—®ç­”(VQA) ==========
def visual_question_answering():
    """è§†è§‰é—®ç­”"""
    model = ChatOpenAI(model="gpt-4o")

    with open("scene.jpg", "rb") as f:
        scene_b64 = base64.b64encode(f.read()).decode('utf-8')

    questions = [
        "å›¾ç‰‡ä¸­æœ‰å¤šå°‘äºº?",
        "ä»–ä»¬åœ¨åšä»€ä¹ˆ?",
        "åœºæ™¯æ˜¯å®¤å†…è¿˜æ˜¯å®¤å¤–?",
        "å¤©æ°”å¦‚ä½•?",
        "å¤§æ¦‚æ˜¯ä»€ä¹ˆæ—¶é—´?"
    ]

    results = {}
    for question in questions:
        message = HumanMessage(
            content=[
                {"type": "text", "text": question},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{scene_b64}"}}
            ]
        )

        response = model.invoke([message])
        results[question] = response.content

    return results

# ========== ç¤ºä¾‹6: å›¾ç‰‡+æ–‡æœ¬æ¨ç† ==========
def image_text_reasoning():
    """å›¾æ–‡æ¨ç†"""
    model = ChatOpenAI(model="gpt-4o")

    with open("receipt.jpg", "rb") as f:
        receipt_b64 = base64.b64encode(f.read()).decode('utf-8')

    message = HumanMessage(
        content=[
            {
                "type": "text",
                "text": """åˆ†æè¿™å¼ æ”¶æ®:
                1. åˆ—å‡ºæ‰€æœ‰å•†å“å’Œä»·æ ¼
                2. è®¡ç®—æ€»é‡‘é¢æ˜¯å¦æ­£ç¡®
                3. æ£€æŸ¥æ˜¯å¦æœ‰æŠ˜æ‰£
                4. æå–æ—¥æœŸå’Œå•†åº—ä¿¡æ¯
                5. åˆ¤æ–­è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æ¶ˆè´¹(é¤é¥®/è´­ç‰©/æœåŠ¡)"""
            },
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{receipt_b64}"}}
        ]
    )

    response = model.invoke([message])
    return response.content

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    # é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹
    analyze_local_image()
    compare_images()
    chart_data = extract_chart_data()
    print("æå–çš„æ•°æ®:", chart_data)

    vqa_results = visual_question_answering()
    for q, a in vqa_results.items():
        print(f"Q: {q}\nA: {a}\n")

    receipt_analysis = image_text_reasoning()
    print("æ”¶æ®åˆ†æ:", receipt_analysis)
```

#### 2.1.3 æ··åˆåº”ç”¨æ¡ˆä¾‹

**å¤šæ¨¡æ€åŠ©æ‰‹å®Œæ•´å®ç°**ï¼š

```python
from typing import Union, List
from langchain_core.messages import BaseMessage

class MultiModalAssistant:
    """å¤šæ¨¡æ€åŠ©æ‰‹"""

    def __init__(self):
        self.vision_model = ChatOpenAI(model="gpt-4o")
        self.audio_processor = AudioProcessor()
        self.agent = self._create_react_agent()

    def _create_react_agent(self):
        """åˆ›å»ºå¤šæ¨¡æ€ Agent"""
        from langchain.agents import create_agent

        @tool
        def process_image(image_path: str) -> str:
            """å¤„ç†å›¾åƒ"""
            image_base64 = encode_image(image_path)
            response = self.vision_model.invoke([
                HumanMessage(content=[
                    {"type": "text", "text": "æè¿°è¿™ä¸ªå›¾åƒ"},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
                ])
            ])
            return response.content

        @tool
        def process_audio(audio_path: str) -> str:
            """å¤„ç†éŸ³é¢‘"""
            with open(audio_path, "rb") as f:
                transcript = self.audio_processor.transcribe_audio(f)
            return transcript

        @tool
        def generate_audio(text: str, output_path: str) -> str:
            """ç”ŸæˆéŸ³é¢‘"""
            audio_content = self.audio_processor.text_to_speech(text)
            with open(output_path, "wb") as f:
                f.write(audio_content)
            return f"éŸ³é¢‘å·²ä¿å­˜åˆ° {output_path}"

        return create_agent(
            ChatOpenAI(model="gpt-4o"),
            [process_image, process_audio, generate_audio],
            prompt="""ä½ æ˜¯å¤šæ¨¡æ€åŠ©æ‰‹ã€‚

èƒ½åŠ›ï¼š
1. ç†è§£å’Œæè¿°å›¾åƒ
2. è½¬å½•å’Œç†è§£éŸ³é¢‘
3. ç”Ÿæˆè¯­éŸ³å›å¤

æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œçµæ´»ä½¿ç”¨å„ç§æ¨¡æ€ã€‚"""
        )

    def process(self, input_data: Union[str, dict]) -> dict:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""

        if isinstance(input_data, str):
            # çº¯æ–‡æœ¬
            messages = [HumanMessage(content=input_data)]

        elif isinstance(input_data, dict):
            # å¤šæ¨¡æ€è¾“å…¥
            content = []

            if "text" in input_data:
                content.append({"type": "text", "text": input_data["text"]})

            if "image" in input_data:
                image_base64 = encode_image(input_data["image"])
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{image_base64}"}
                })

            if "audio" in input_data:
                # å…ˆè½¬å½•éŸ³é¢‘
                with open(input_data["audio"], "rb") as f:
                    transcript = self.audio_processor.transcribe_audio(f)
                content.append({"type": "text", "text": f"éŸ³é¢‘è½¬å½•ï¼š{transcript}"})

            messages = [HumanMessage(content=content)]

        # å¤„ç†
        response = self.agent.invoke({"messages": messages})

        return {
            "text_response": response["messages"][-1].content,
            "messages": response["messages"]
        }

# ä½¿ç”¨ç¤ºä¾‹
assistant = MultiModalAssistant()

# å¤„ç†å›¾æ–‡æ··åˆ
result = assistant.process({
    "text": "è¿™ä¸ªå›¾è¡¨æ˜¾ç¤ºäº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿè¯·ç”Ÿæˆè¯­éŸ³è§£é‡Šã€‚",
    "image": "sales_chart.png"
})

# å¤„ç†éŸ³é¢‘
result = assistant.process({
    "audio": "question.mp3",
    "text": "å›ç­”éŸ³é¢‘ä¸­çš„é—®é¢˜"
})
```

---

### 2.2 æµå¼ä¸å¼‚æ­¥ç¼–ç¨‹

#### 2.2.1 æµå¼è¾“å‡ºï¼ˆstreamã€astreamï¼‰

**åŸºç¡€æµå¼è¾“å‡º**ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# åŒæ­¥æµå¼
model = ChatOpenAI(model="gpt-4o", streaming=True)

# stream æ–¹æ³•
for chunk in model.stream([HumanMessage(content="è®²ä¸ªé•¿æ•…äº‹")]):
    print(chunk.content, end="", flush=True)

# å¼‚æ­¥æµå¼
import asyncio

async def async_stream_example():
    """å¼‚æ­¥æµå¼ç¤ºä¾‹"""
    model = ChatOpenAI(model="gpt-4o", streaming=True)

    async for chunk in model.astream([HumanMessage(content="è§£é‡Šé‡å­è®¡ç®—")]):
        print(chunk.content, end="", flush=True)
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¶ä»–å¼‚æ­¥æ“ä½œ
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†

# è¿è¡Œ
asyncio.run(async_stream_example())
```

**Agent æµå¼è¾“å‡º**ï¼š

```python
from langchain.agents import create_agent
from langchain_core.messages import AIMessageChunk

async def stream_agent_response():
    """æµå¼ Agent å“åº”"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool],
        prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
    )

    # ä½¿ç”¨ astream_events è·å–æ‰€æœ‰äº‹ä»¶
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æœç´¢å¹¶æ€»ç»“ LangChain æ–°ç‰¹æ€§")]},
        version="v1"
    ):
        kind = event["event"]

        if kind == "on_chat_model_stream":
            # æ¨¡å‹è¾“å‡ºæµ
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)

        elif kind == "on_tool_start":
            # å·¥å…·å¼€å§‹æ‰§è¡Œ
            print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {event['name']}")

        elif kind == "on_tool_end":
            # å·¥å…·æ‰§è¡Œå®Œæˆ
            print(f"\nâœ… å·¥å…·å®Œæˆ: {event['name']}")

# æ›´ç²¾ç»†çš„æµå¼æ§åˆ¶
async def advanced_streaming():
    """é«˜çº§æµå¼å¤„ç†"""
    from langchain.agents import create_agent

    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool, analyze_tool],
        prompt="ä½ æ˜¯ç ”ç©¶åŠ©æ‰‹"
    )

    # æ”¶é›†ä¸åŒç±»å‹çš„è¾“å‡º
    tokens = []
    tool_calls = []

    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="ç ”ç©¶ AI å‘å±•è¶‹åŠ¿")]},
        version="v2"
    ):
        if event["event"] == "on_chat_model_stream":
            chunk = event["data"]["chunk"]

            # æ”¶é›† tokens
            if hasattr(chunk, "content") and chunk.content:
                tokens.append(chunk.content)
                # å®æ—¶æ˜¾ç¤º
                print(chunk.content, end="", flush=True)

            # æ”¶é›†å·¥å…·è°ƒç”¨
            if hasattr(chunk, "tool_calls") and chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        elif event["event"] == "on_chat_model_end":
            # æ¨¡å‹è¾“å‡ºç»“æŸ
            full_response = "".join(tokens)
            print(f"\n\nå®Œæ•´å“åº”é•¿åº¦: {len(full_response)}")
            print(f"å·¥å…·è°ƒç”¨æ•°: {len(tool_calls)}")

asyncio.run(advanced_streaming())
```

**astream_events ç‰ˆæœ¬å·®å¼‚è¯¦è§£**:

LangChain æä¾› `v1` å’Œ `v2` ä¸¤ä¸ªç‰ˆæœ¬çš„ astream_events API,æ¨èä½¿ç”¨ `v2`(æˆ–çœç•¥versionå‚æ•°,é»˜è®¤v2)ã€‚

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

# ========== v1 (æ—§ç‰ˆ,ä¸æ¨è) ==========
async def use_v1():
    """v1 ç‰ˆæœ¬ç¤ºä¾‹"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool],
        prompt="ä½ æ˜¯åŠ©æ‰‹"
    )

    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æŸ¥è¯¢å¤©æ°”")]},
        version="v1"  # æ˜ç¡®æŒ‡å®š v1
    ):
        # v1 äº‹ä»¶æ ¼å¼è¾ƒç®€å•
        kind = event["event"]

        if kind == "on_chat_model_stream":
            # åŸºç¡€å­—æ®µ
            chunk = event["data"]["chunk"]
            print(chunk.content, end="")

        # v1 ç¼ºå°‘éƒ¨åˆ†å…ƒæ•°æ®,äº‹ä»¶ç±»å‹è¾ƒå°‘

# ========== v2 (æ¨è,é»˜è®¤) ==========
async def use_v2():
    """v2 ç‰ˆæœ¬ç¤ºä¾‹(æ¨è)"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool],
        prompt="ä½ æ˜¯åŠ©æ‰‹"
    )

    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æŸ¥è¯¢å¤©æ°”")]},
        version="v2"  # æˆ–çœç•¥æ­¤å‚æ•°,é»˜è®¤v2
    ):
        # v2 äº‹ä»¶æ ¼å¼æ›´å®Œæ•´
        # äº‹ä»¶å¯¹è±¡ç»“æ„:
        # {
        #   "event": "on_chain_start" | "on_chain_stream" | "on_chain_end" | ...,
        #   "name": "component_name",
        #   "run_id": "uuid",
        #   "parent_ids": ["parent_uuid"],
        #   "tags": ["tag1", "tag2"],
        #   "metadata": {...},
        #   "data": {...}
        # }

        event_type = event["event"]
        event_name = event.get("name", "")
        run_id = event.get("run_id", "")

        if event_type == "on_chat_model_stream":
            # LLM æµå¼è¾“å‡º
            chunk = event["data"]["chunk"]
            if chunk.content:
                print(f"[{run_id[:8]}] {chunk.content}", end="")

        elif event_type == "on_tool_start":
            # å·¥å…·å¼€å§‹
            print(f"\nğŸ”§ [{event_name}] å¼€å§‹æ‰§è¡Œ")
            print(f"   è¾“å…¥: {event['data'].get('input')}")

        elif event_type == "on_tool_end":
            # å·¥å…·ç»“æŸ
            print(f"\nâœ… [{event_name}] æ‰§è¡Œå®Œæˆ")
            print(f"   è¾“å‡º: {event['data'].get('output')}")

        elif event_type == "on_chain_start":
            # Chain å¼€å§‹
            print(f"\nâ–¶ [{event_name}] é“¾å¼€å§‹")

        elif event_type == "on_chain_end":
            # Chain ç»“æŸ
            print(f"\nâ–  [{event_name}] é“¾ç»“æŸ")

# ========== v1 vs v2 ä¸»è¦å·®å¼‚ ==========
"""
å·®å¼‚å¯¹æ¯”:

1. å…ƒæ•°æ®å®Œæ•´æ€§:
   - v1: ä»…åŒ…å«åŸºç¡€å­—æ®µ(event, data)
   - v2: åŒ…å«å®Œæ•´å…ƒæ•°æ®(run_id, parent_ids, tags, metadata)

2. äº‹ä»¶ç±»å‹:
   - v1: äº‹ä»¶ç±»å‹è¾ƒå°‘,ç²’åº¦è¾ƒç²—
   - v2: æ›´ç»†ç²’åº¦çš„äº‹ä»¶ç±»å‹:
     - on_llm_start, on_llm_stream, on_llm_end
     - on_chat_model_start, on_chat_model_stream, on_chat_model_end
     - on_chain_start, on_chain_stream, on_chain_end
     - on_tool_start, on_tool_stream, on_tool_end
     - on_retriever_start, on_retriever_end
     - on_prompt_start, on_prompt_end

3. æ•°æ®ç»“æ„:
   - v1: event["data"]["chunk"]
   - v2: event["data"]["chunk"] + æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯

4. è¿‡æ»¤èƒ½åŠ›:
   - v1: æœ‰é™çš„è¿‡æ»¤é€‰é¡¹
   - v2: æ”¯æŒæ›´ç²¾ç»†çš„è¿‡æ»¤:
     - include_names: åªåŒ…å«ç‰¹å®šç»„ä»¶
     - include_types: åªåŒ…å«ç‰¹å®šç±»å‹
     - include_tags: åªåŒ…å«ç‰¹å®šæ ‡ç­¾
     - exclude_names/exclude_types/exclude_tags: æ’é™¤è§„åˆ™

5. æ€§èƒ½:
   - v2 æä¾›æ›´å¥½çš„æ€§èƒ½ä¼˜åŒ–å’Œäº‹ä»¶å»é‡
"""

# ========== v2 é«˜çº§è¿‡æ»¤ç¤ºä¾‹ ==========
async def v2_filtering():
    """v2 çš„é«˜çº§è¿‡æ»¤åŠŸèƒ½"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool, analyze_tool],
        prompt="ä½ æ˜¯åŠ©æ‰‹"
    )

    # åªç›‘å¬ LLM ç›¸å…³äº‹ä»¶
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="åˆ†ææ•°æ®")]},
        version="v2",
        include_types=["chat_model"]  # åªè¦èŠå¤©æ¨¡å‹äº‹ä»¶
    ):
        if event["event"] == "on_chat_model_stream":
            print(event["data"]["chunk"].content, end="")

    # åªç›‘å¬ç‰¹å®šå·¥å…·
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æœç´¢ä¿¡æ¯")]},
        version="v2",
        include_names=["search_tool"]  # åªè¦ search_tool çš„äº‹ä»¶
    ):
        if event["event"] == "on_tool_end":
            print(f"æœç´¢ç»“æœ: {event['data']['output']}")

    # æ’é™¤æŸäº›ç»„ä»¶
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="å¤„ç†è¯·æ±‚")]},
        version="v2",
        exclude_types=["retriever"]  # æ’é™¤æ£€ç´¢å™¨äº‹ä»¶
    ):
        # å¤„ç†å…¶ä»–æ‰€æœ‰äº‹ä»¶
        pass

# ========== æ¨èä½¿ç”¨æ–¹å¼ ==========
async def recommended_usage():
    """æ¨èçš„ v2 ä½¿ç”¨æ–¹å¼"""
    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool],
        prompt="ä½ æ˜¯åŠ©æ‰‹"
    )

    # æ–¹å¼1: çœç•¥ version,é»˜è®¤ä½¿ç”¨ v2
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æŸ¥è¯¢")]},
        # version å‚æ•°çœç•¥,è‡ªåŠ¨ä½¿ç”¨ v2
    ):
        # å¤„ç†äº‹ä»¶
        pass

    # æ–¹å¼2: æ˜ç¡®æŒ‡å®š v2(æ¨è)
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content="æŸ¥è¯¢")]},
        version="v2"
    ):
        # ä½¿ç”¨ v2 çš„å®Œæ•´åŠŸèƒ½
        event_type = event["event"]

        # åˆ©ç”¨ v2 çš„å…ƒæ•°æ®
        if "metadata" in event:
            custom_data = event["metadata"].get("custom_field")

        # è¿½è¸ªè°ƒç”¨é“¾
        run_id = event.get("run_id")
        parent_ids = event.get("parent_ids", [])
```

**è¿ç§»æŒ‡å—(v1 -> v2)**:

```python
# æ—§ä»£ç (v1)
async for event in agent.astream_events(input, version="v1"):
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        print(chunk.content)

# æ–°ä»£ç (v2) - æ— éœ€ä¿®æ”¹,100%å…¼å®¹
async for event in agent.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        print(chunk.content)

        # v2 é¢å¤–æä¾›çš„ä¿¡æ¯
        run_id = event.get("run_id")
        metadata = event.get("metadata", {})
```

**æ€»ç»“**:
- âœ… **æ–°é¡¹ç›®**: ä½¿ç”¨ `version="v2"` æˆ–çœç•¥å‚æ•°(é»˜è®¤v2)
- âš ï¸ **æ—§é¡¹ç›®**: å¯ä»¥ç»§ç»­ä½¿ç”¨ v1,ä½†å»ºè®®è¿ç§»åˆ° v2
- âŒ **ä¸æ¨è**: åœ¨æ–°ä»£ç ä¸­æ˜¾å¼æŒ‡å®š `version="v1"`

#### 2.2.2 å¼‚æ­¥æ‰§è¡Œä¸å¹¶å‘æ§åˆ¶

**å¼‚æ­¥ Agent ç³»ç»Ÿ**ï¼š

```python
import asyncio
from typing import List
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

class AsyncMultiAgent:
    """å¼‚æ­¥å¤š Agent ç³»ç»Ÿ"""

    def __init__(self, concurrency_limit: int = 5):
        self.semaphore = asyncio.Semaphore(concurrency_limit)
        self.agents = {}

    def register_agent(self, name: str, agent):
        """æ³¨å†Œ Agent"""
        self.agents[name] = agent

    async def run_agent_with_limit(self, name: str, input_data: dict):
        """å¸¦å¹¶å‘é™åˆ¶çš„ Agent æ‰§è¡Œ"""
        async with self.semaphore:
            agent = self.agents[name]
            print(f"ğŸš€ å¯åŠ¨ {name}")

            try:
                result = await agent.ainvoke(input_data)
                print(f"âœ… {name} å®Œæˆ")
                return {"agent": name, "result": result, "status": "success"}

            except Exception as e:
                print(f"âŒ {name} å¤±è´¥: {e}")
                return {"agent": name, "error": str(e), "status": "failed"}

    async def run_parallel(self, tasks: List[dict]):
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡"""
        coroutines = [
            self.run_agent_with_limit(task["agent"], task["input"])
            for task in tasks
        ]

        results = await asyncio.gather(*coroutines, return_exceptions=True)

        return results

    async def run_pipeline(self, stages: List[List[dict]]):
        """æµæ°´çº¿æ‰§è¡Œï¼ˆé˜¶æ®µå†…å¹¶è¡Œï¼Œé˜¶æ®µé—´ä¸²è¡Œï¼‰"""
        all_results = []

        for stage_num, stage_tasks in enumerate(stages):
            print(f"\n=== é˜¶æ®µ {stage_num + 1} ===")

            # å¹¶è¡Œæ‰§è¡Œå½“å‰é˜¶æ®µ
            stage_results = await self.run_parallel(stage_tasks)
            all_results.append(stage_results)

            # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥
            failures = [r for r in stage_results if r.get("status") == "failed"]
            if failures:
                print(f"âš ï¸  é˜¶æ®µ {stage_num + 1} æœ‰ {len(failures)} ä¸ªå¤±è´¥")
                # å¯ä»¥é€‰æ‹©æ˜¯å¦ç»§ç»­

        return all_results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    system = AsyncMultiAgent(concurrency_limit=3)

    # æ³¨å†Œ Agents
    system.register_agent("search", create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [search_tool],
        prompt="æœç´¢ä¸“å®¶"
    ))

    system.register_agent("analyze", create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [analyze_tool],
        prompt="åˆ†æä¸“å®¶"
    ))

    system.register_agent("summarize", create_agent(
        ChatOpenAI(model="gpt-4o-mini"),
        [],
        prompt="æ€»ç»“ä¸“å®¶"
    ))

    # å®šä¹‰ä»»åŠ¡
    tasks = [
        {"agent": "search", "input": {"messages": [HumanMessage(content="æœç´¢ A")]}},
        {"agent": "search", "input": {"messages": [HumanMessage(content="æœç´¢ B")]}},
        {"agent": "search", "input": {"messages": [HumanMessage(content="æœç´¢ C")]}},
    ]

    # å¹¶è¡Œæ‰§è¡Œ
    results = await system.run_parallel(tasks)

    # æµæ°´çº¿æ‰§è¡Œ
    pipeline = [
        # é˜¶æ®µ 1ï¼šå¹¶è¡Œæœç´¢
        [
            {"agent": "search", "input": {"messages": [HumanMessage(content="æœç´¢ X")]}},
            {"agent": "search", "input": {"messages": [HumanMessage(content="æœç´¢ Y")]}}
        ],
        # é˜¶æ®µ 2ï¼šåˆ†æç»“æœ
        [
            {"agent": "analyze", "input": {"messages": [HumanMessage(content="åˆ†ææœç´¢ç»“æœ")]}}
        ],
        # é˜¶æ®µ 3ï¼šæ€»ç»“
        [
            {"agent": "summarize", "input": {"messages": [HumanMessage(content="æ€»ç»“åˆ†æ")]}}
        ]
    ]

    pipeline_results = await system.run_pipeline(pipeline)

asyncio.run(main())
```

#### 2.2.3 å®æ—¶äº¤äº’ï¼ˆWebSocketã€SSEï¼‰

**WebSocket å®æ—¶äº¤äº’**ï¼š

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
import json

app = FastAPI()

class WebSocketAgentHandler:
    """WebSocket Agent å¤„ç†å™¨"""

    def __init__(self):
        from langchain.agents import create_agent

        self.agent = create_agent(
            ChatOpenAI(model="gpt-4o", streaming=True),
            [search_tool],
            prompt="ä½ æ˜¯å®æ—¶åŠ©æ‰‹"
        )
        self.connections = set()

    async def handle_connection(self, websocket: WebSocket):
        """å¤„ç† WebSocket è¿æ¥"""
        await websocket.accept()
        self.connections.add(websocket)

        try:
            while True:
                # æ¥æ”¶æ¶ˆæ¯
                data = await websocket.receive_text()
                message = json.loads(data)

                # æµå¼å¤„ç†å¹¶å‘é€
                await self.stream_response(websocket, message["content"])

        except WebSocketDisconnect:
            self.connections.remove(websocket)

    async def stream_response(self, websocket: WebSocket, user_input: str):
        """æµå¼å‘é€å“åº”"""

        # å‘é€å¼€å§‹ä¿¡å·
        await websocket.send_json({"type": "start"})

        # æµå¼å¤„ç†
        async for event in self.agent.astream_events(
            {"messages": [HumanMessage(content=user_input)]},
            version="v1"
        ):
            if event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if chunk.content:
                    # å‘é€æ–‡æœ¬å—
                    await websocket.send_json({
                        "type": "chunk",
                        "content": chunk.content
                    })

            elif event["event"] == "on_tool_start":
                # å‘é€å·¥å…·è°ƒç”¨é€šçŸ¥
                await websocket.send_json({
                    "type": "tool_start",
                    "tool": event["name"]
                })

            elif event["event"] == "on_tool_end":
                # å‘é€å·¥å…·å®Œæˆé€šçŸ¥
                await websocket.send_json({
                    "type": "tool_end",
                    "tool": event["name"],
                    "output": str(event["data"].get("output", ""))[:100]
                })

        # å‘é€ç»“æŸä¿¡å·
        await websocket.send_json({"type": "end"})

handler = WebSocketAgentHandler()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await handler.handle_connection(websocket)

# HTML å®¢æˆ·ç«¯ç¤ºä¾‹
@app.get("/")
async def get():
    return HTMLResponse("""
<!DOCTYPE html>
<html>
<head>
    <title>å®æ—¶ Agent</title>
</head>
<body>
    <div id="messages"></div>
    <input type="text" id="messageText" placeholder="è¾“å…¥æ¶ˆæ¯">
    <button onclick="sendMessage()">å‘é€</button>

    <script>
        const ws = new WebSocket("ws://localhost:8000/ws");

        ws.onmessage = function(event) {
            const message = JSON.parse(event.data);
            const messagesDiv = document.getElementById('messages');

            if (message.type === 'chunk') {
                messagesDiv.innerHTML += message.content;
            } else if (message.type === 'tool_start') {
                messagesDiv.innerHTML += `<br>ğŸ”§ è°ƒç”¨å·¥å…·: ${message.tool}<br>`;
            } else if (message.type === 'end') {
                messagesDiv.innerHTML += '<br><br>';
            }
        };

        function sendMessage() {
            const input = document.getElementById('messageText');
            ws.send(JSON.stringify({content: input.value}));
            input.value = '';
        }
    </script>
</body>
</html>
    """)
```

**SSEï¼ˆServer-Sent Eventsï¼‰å®ç°**ï¼š

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from typing import AsyncGenerator
import asyncio

app = FastAPI()

async def event_generator(user_input: str) -> AsyncGenerator[str, None]:
    """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
    from langchain.agents import create_agent

    agent = create_agent(
        ChatOpenAI(model="gpt-4o", streaming=True),
        [search_tool],
        prompt="ä½ æ˜¯æµå¼åŠ©æ‰‹"
    )

    # å‘é€è¿æ¥å»ºç«‹äº‹ä»¶
    yield f"data: {json.dumps({'type': 'connected'})}\n\n"

    # æµå¼å¤„ç†
    async for event in agent.astream_events(
        {"messages": [HumanMessage(content=user_input)]},
        version="v1"
    ):
        if event["event"] == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            if chunk.content:
                # å‘é€å†…å®¹å—
                data = json.dumps({
                    "type": "content",
                    "data": chunk.content
                })
                yield f"data: {data}\n\n"

                # å°å»¶è¿Ÿé¿å…è¿‡å¿«
                await asyncio.sleep(0.01)

        elif event["event"] == "on_tool_start":
            # å‘é€å·¥å…·äº‹ä»¶
            data = json.dumps({
                "type": "tool",
                "action": "start",
                "name": event["name"]
            })
            yield f"data: {data}\n\n"

    # å‘é€å®Œæˆäº‹ä»¶
    yield f"data: {json.dumps({'type': 'done'})}\n\n"

@app.get("/stream")
async def stream_response(q: str):
    """SSE ç«¯ç‚¹"""
    return StreamingResponse(
        event_generator(q),
        media_type="text/event-stream"
    )

# JavaScript å®¢æˆ·ç«¯
"""
const eventSource = new EventSource('/stream?q=ä½ çš„é—®é¢˜');

eventSource.onmessage = function(event) {
    const data = JSON.parse(event.data);

    if (data.type === 'content') {
        document.getElementById('output').innerHTML += data.data;
    } else if (data.type === 'done') {
        eventSource.close();
    }
};
"""
```

---

### 2.3 Model Context Protocol (MCP) é›†æˆ

> **è¯´æ˜**: MCP (Model Context Protocol) æ˜¯ Anthropic æ¨å‡ºçš„ç»Ÿä¸€åè®®,ç”¨äºè¿æ¥AIåº”ç”¨ä¸å¤–éƒ¨å·¥å…·/æ•°æ®æºã€‚

#### 2.3.1 MCP å®Œæ•´é›†æˆæŒ‡å—

**MCP åè®®æ¦‚è¿°**:

MCP æä¾›æ ‡å‡†åŒ–çš„æ–¹å¼è®© LLM è®¿é—®:
- **å·¥å…· (Tools)**: å‡½æ•°è°ƒç”¨èƒ½åŠ›(å¦‚è®¡ç®—ã€APIè°ƒç”¨)
- **èµ„æº (Resources)**: æ•°æ®è®¿é—®(å¦‚æ–‡ä»¶ã€æ•°æ®åº“)
- **æç¤ºè¯ (Prompts)**: é¢„å®šä¹‰çš„æç¤ºè¯æ¨¡æ¿
- **é‡‡æ · (Sampling)**: LLM ç”Ÿæˆèƒ½åŠ›

**1. å®‰è£…MCPå®¢æˆ·ç«¯**:

```bash
# å®‰è£… MCP é€‚é…å™¨(ç”¨äº LangChain)
pip install langchain-mcp-adapters

# å®‰è£… MCP Python SDK
pip install mcp

# å¯é€‰: å®‰è£…å®˜æ–¹ MCP æœåŠ¡å™¨(Node.js)
npm install -g @modelcontextprotocol/server-filesystem
npm install -g @modelcontextprotocol/server-github
npm install -g @modelcontextprotocol/server-postgres

# æˆ–ä½¿ç”¨ Python MCP æœåŠ¡å™¨ç¤ºä¾‹
pip install mcp-server-git  # Git æ“ä½œæœåŠ¡å™¨
```

**2. MCP æœåŠ¡å™¨é…ç½®æ–‡ä»¶**:

åˆ›å»º `mcp_config.json` é…ç½®å¤šä¸ªæœåŠ¡å™¨:

```json
{
  "servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/å…è®¸çš„ç›®å½•è·¯å¾„"],
      "env": {}
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_TOKEN": "ghp_your_token_here"
      }
    },
    "postgres": {
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-postgres",
        "postgresql://user:password@localhost:5432/dbname"
      ],
      "env": {}
    },
    "custom-python": {
      "command": "python",
      "args": ["-m", "my_mcp_server"],
      "env": {
        "API_KEY": "your_api_key"
      }
    }
  }
}
```

**3. LangChainé›†æˆæµç¨‹**:

å®Œæ•´çš„MCPé›†æˆåŒ…æ‹¬åˆå§‹åŒ–ã€è·å–å·¥å…·ã€åˆ›å»ºAgentã€æ‰§è¡Œä»»åŠ¡å››ä¸ªæ­¥éª¤:

```python
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# ========== æ­¥éª¤1: åˆå§‹åŒ– MCP å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é…ç½®å­—å…¸ï¼‰==========
async def create_mcp_agent():
    """åˆ›å»ºé›†æˆ MCP å·¥å…·çš„ Agent"""
    # ä½¿ç”¨é…ç½®å­—å…¸åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = MultiServerMCPClient({
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", "/workspace"],
            "transport": "stdio"
        },
        "github": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "transport": "stdio",
            "env": {"GITHUB_TOKEN": "ghp_your_token"}
        }
    })

    # ========== æ­¥éª¤2: è·å–æ‰€æœ‰å·¥å…· ==========
    # get_tools() ä¼šè‡ªåŠ¨ä¸ºæ¯ä¸ªé…ç½®çš„æœåŠ¡å™¨åˆ›å»º session å¹¶è·å–å·¥å…·
    tools = await client.get_tools()
    print(f"æ€»å…±è·å–åˆ° {len(tools)} ä¸ªå·¥å…·")

    # ========== æ­¥éª¤3: åˆ›å»º Agent ==========
    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    agent = create_agent(
        llm,
        tools,
        prompt="""ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹,å¯ä»¥è®¿é—®:
        1. æ–‡ä»¶ç³»ç»Ÿå·¥å…·(è¯»å†™æ–‡ä»¶)
        2. GitHub å·¥å…·(ç®¡ç†ä»“åº“ã€Issueã€PR)

        æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·å®Œæˆä»»åŠ¡ã€‚"""
    )

    return agent

# ========== æ­¥éª¤4: ä½¿ç”¨ Agent æ‰§è¡Œä»»åŠ¡ ==========
async def main():
    """å®Œæ•´ç¤ºä¾‹"""
    # åˆ›å»º Agent
    agent = await create_mcp_agent()

    # ç¤ºä¾‹ä»»åŠ¡1: æ–‡ä»¶æ“ä½œ
    result1 = await agent.ainvoke({
        "messages": [HumanMessage(content="åœ¨ /workspace ä¸‹åˆ›å»ºä¸€ä¸ªREADME.mdæ–‡ä»¶")]
    })
    print("ä»»åŠ¡1ç»“æœ:", result1["messages"][-1].content)

    # ç¤ºä¾‹ä»»åŠ¡2: GitHub æ“ä½œ
    result2 = await agent.ainvoke({
        "messages": [HumanMessage(content="åˆ—å‡ºæˆ‘çš„GitHubä»“åº“")]
    })
    print("ä»»åŠ¡2ç»“æœ:", result2["messages"][-1].content)

    # ç¤ºä¾‹ä»»åŠ¡3: æ··åˆæ“ä½œ
    result3 = await agent.ainvoke({
        "messages": [HumanMessage(
            content="æ£€æŸ¥æˆ‘çš„é¡¹ç›®ä»“åº“æ˜¯å¦æœ‰æ–°çš„Issue,å¹¶å°†Issueæ ‡é¢˜å†™å…¥æ–‡ä»¶"
        )]
    })
    print("ä»»åŠ¡3ç»“æœ:", result3["messages"][-1].content)

# è¿è¡Œ
asyncio.run(main())
```

**4. ä½¿ç”¨é…ç½®æ–‡ä»¶çš„æ–¹å¼**:

```python
import json
from pathlib import Path

async def init_from_config(config_path: str):
    """ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ– MCP å®¢æˆ·ç«¯"""
    # è¯»å–é…ç½®
    with open(config_path) as f:
        config = json.load(f)

    client = MultiServerMCPClient()
    server_params = {}

    # è§£ææ¯ä¸ªæœåŠ¡å™¨é…ç½®
    for server_name, server_config in config["servers"].items():
        params = StdioServerParameters(
            command=server_config["command"],
            args=server_config["args"],
            env=server_config.get("env", None)
        )
        server_params[server_name] = params

    return client, server_params

# ä½¿ç”¨
client, params = await init_from_config("mcp_config.json")
```

**5. é”™è¯¯å¤„ç†ä¸é‡è¯•**:

```python
import asyncio
from typing import Optional

async def robust_mcp_call(client, server_name, params, max_retries=3):
    """å¸¦é‡è¯•çš„ MCP è°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            async with client.session(server_name, params) as session:
                tools = client.get_tools()
                return tools
        except Exception as e:
            print(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                raise

# ä½¿ç”¨
tools = await robust_mcp_call(client, "filesystem", fs_params)
```

**6. ç›‘æ§å’Œæ—¥å¿—**:

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("mcp_integration")

async def init_mcp_with_logging(client, server_params):
    """å¸¦æ—¥å¿—çš„ MCP åˆå§‹åŒ–"""
    for server_name, params in server_params.items():
        logger.info(f"è¿æ¥åˆ° MCP æœåŠ¡å™¨: {server_name}")

        try:
            async with client.session(server_name, params) as session:
                tools = client.get_tools()
                logger.info(f"âœ… {server_name}: è·å– {len(tools)} ä¸ªå·¥å…·")

                # è®°å½•å·¥å…·è¯¦æƒ…
                for tool in tools:
                    logger.debug(f"  - {tool.name}: {tool.description}")

        except Exception as e:
            logger.error(f"âŒ {server_name} è¿æ¥å¤±è´¥: {e}")
            raise
```

**7. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ**:

```python
# é—®é¢˜1: æœåŠ¡å™¨å¯åŠ¨å¤±è´¥
# è§£å†³: æ£€æŸ¥å‘½ä»¤å’Œå‚æ•°æ˜¯å¦æ­£ç¡®
try:
    async with client.session("server", params):
        pass
except FileNotFoundError:
    print("å‘½ä»¤ä¸å­˜åœ¨,è¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¯¹åº”çš„ MCP æœåŠ¡å™¨")
except PermissionError:
    print("æƒé™ä¸è¶³,è¯·æ£€æŸ¥ç›®å½•è®¿é—®æƒé™")

# é—®é¢˜2: å·¥å…·è°ƒç”¨å¤±è´¥
# è§£å†³: éªŒè¯å·¥å…·å‚æ•°
tool_schema = tool.inputSchema  # è·å–å·¥å…·çš„å‚æ•°æ¨¡å¼
print(f"å·¥å…· {tool.name} éœ€è¦å‚æ•°: {tool_schema}")

# é—®é¢˜3: ä¼šè¯è¶…æ—¶
# è§£å†³: è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
from mcp.client.stdio import StdioServerParameters

params = StdioServerParameters(
    command="npx",
    args=["..."],
    env=None,
    timeout=60.0  # 60ç§’è¶…æ—¶
)
```

**MCP åŸºç¡€é›†æˆ**ï¼š

```python
from langchain_mcp_adapters.client import MultiServerMCPClient
from mcp.client.stdio import StdioServerParameters
from mcp.client.sse import create_mcp_http_client
import httpx

# åˆ›å»º MCP å®¢æˆ·ç«¯
mcp_client = MultiServerMCPClient()

# ========== æ­£ç¡®æ–¹å¼ 1ï¼šä½¿ç”¨é…ç½®å­—å…¸ï¼ˆæ¨èï¼‰==========
async def setup_mcp_with_config():
    """ä½¿ç”¨é…ç½®å­—å…¸åˆå§‹åŒ–ï¼ˆæ¨èæ–¹å¼ï¼‰"""
    # ä½¿ç”¨é…ç½®å­—å…¸åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = MultiServerMCPClient({
        "math_server": {
            "command": "python",
            "args": ["-m", "mcp_servers.math"],
            "transport": "stdio"
        }
    })

    # ç›´æ¥è·å–æ‰€æœ‰å·¥å…·
    tools = await client.get_tools()
    print(f"å¯ç”¨å·¥å…·: {len(tools)} ä¸ª")
    return tools

# ========== æ­£ç¡®æ–¹å¼ 1bï¼šä½¿ç”¨ session æ–¹æ³•ï¼ˆæ˜¾å¼ç®¡ç†ï¼‰==========
from langchain_mcp_adapters.tools import load_mcp_tools

async def setup_mcp_with_session():
    """ä½¿ç”¨ session æ–¹æ³•æ˜¾å¼ç®¡ç†è¿æ¥"""
    mcp_client = MultiServerMCPClient()

    # åˆ›å»º stdio å‚æ•°
    stdio_params = StdioServerParameters(
        command="python",
        args=["-m", "mcp_servers.math"],
        env=None
    )

    # ä½¿ç”¨ session ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    async with mcp_client.session("math_server", stdio_params) as session:
        # ä½¿ç”¨ load_mcp_tools ä» session è·å–å·¥å…·
        tools = await load_mcp_tools(session)
        print(f"å¯ç”¨å·¥å…·: {len(tools)} ä¸ª")
        return tools

# ========== æ­£ç¡®æ–¹å¼ 2ï¼šåˆ›å»º HTTP å®¢æˆ·ç«¯ ==========
def create_http_client_correct():
    """æ­£ç¡®åˆ›å»º HTTP å®¢æˆ·ç«¯"""

    # create_mcp_http_client ä¸æ¥å— url å‚æ•°
    # å®ƒè¿”å›ä¸€ä¸ª httpx.AsyncClient
    http_client = create_mcp_http_client(
        headers={"Authorization": "Bearer token"},
        timeout=httpx.Timeout(30.0),
        auth=None
    )

    # URL åœ¨å®é™…è¯·æ±‚æ—¶æŒ‡å®š
    # ä½¿ç”¨ç¤ºä¾‹ï¼š
    # response = await http_client.get("http://localhost:8000/mcp")

    return http_client

# ========== ä½¿ç”¨ LangGraph åˆ›å»º Agentï¼ˆæ¨èï¼‰==========
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

async def create_agent_with_mcp():
    """åˆ›å»ºé›†æˆ MCP å·¥å…·çš„ Agentï¼ˆæ¨èé…ç½®å­—å…¸æ–¹å¼ï¼‰"""
    # ä½¿ç”¨é…ç½®å­—å…¸åˆå§‹åŒ–
    client = MultiServerMCPClient({
        "math": {
            "command": "python",
            "args": ["-m", "mcp_servers.math"],
            "transport": "stdio"
        }
    })

    # è·å–å·¥å…·
    tools = await client.get_tools()

    # åˆ›å»º LLM
    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    # ä½¿ç”¨ create_agent
    agent = create_agent(
        llm,
        tools,
        prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·å¸®åŠ©ç”¨æˆ·ã€‚"
    )

    # è°ƒç”¨ Agent
    result = await agent.ainvoke({
        "messages": [HumanMessage(content="è®¡ç®— sqrt(144)")]
    })

    return result

# ========== å¤‡é€‰æ–¹æ¡ˆï¼šåˆ›å»ºæ¨¡æ‹Ÿå·¥å…· ==========
from langchain_core.tools import Tool

def create_mock_tools():
    """åˆ›å»ºæ¨¡æ‹Ÿå·¥å…·ï¼ˆä¸éœ€è¦ MCP æœåŠ¡å™¨ï¼‰"""

    def calculate(expression: str) -> str:
        """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
        import math
        safe_dict = {
            "__builtins__": {},
            "sqrt": math.sqrt,
            "log": math.log,
            "pi": math.pi
        }
        try:
            result = eval(expression, safe_dict)
            return f"ç»“æœ: {result}"
        except Exception as e:
            return f"é”™è¯¯: {e}"

    def query_weather(city: str) -> str:
        """æŸ¥è¯¢å¤©æ°”ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return f"{city} çš„å¤©æ°”: æ™´å¤© 22Â°C"

    tools = [
        Tool(name="calculate", func=calculate, description="è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"),
        Tool(name="query_weather", func=query_weather, description="æŸ¥è¯¢å¤©æ°”")
    ]

    return tools

# ä½¿ç”¨æ¨¡æ‹Ÿå·¥å…·åˆ›å»º Agent
tools = create_mock_tools()
llm = ChatOpenAI(model="gpt-4o")
agent = create_agent(llm, tools)
```

#### 2.3.2 stdio ä¸ HTTP ä¼ è¾“

**è‡ªå®šä¹‰ MCP æœåŠ¡å™¨**ï¼š

```python
# mcp_server.py
import json
import sys
from typing import Any, Dict

class CustomMCPServer:
    """è‡ªå®šä¹‰ MCP æœåŠ¡å™¨"""

    def __init__(self):
        self.tools = {
            "calculate": self.calculate,
            "database_query": self.database_query
        }

    def calculate(self, expression: str) -> float:
        """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
        # å®‰å…¨è¯„ä¼°
        return eval(expression, {"__builtins__": {}}, {})

    def database_query(self, query: str) -> List[dict]:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢"""
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        return [{"id": 1, "name": "example"}]

    def handle_request(self, request: dict) -> dict:
        """å¤„ç† MCP è¯·æ±‚"""
        method = request.get("method")

        if method == "tools/list":
            # è¿”å›å·¥å…·åˆ—è¡¨
            return {
                "tools": [
                    {
                        "name": name,
                        "description": func.__doc__,
                        "inputSchema": self._get_schema(func)
                    }
                    for name, func in self.tools.items()
                ]
            }

        elif method == "tools/call":
            # è°ƒç”¨å·¥å…·
            tool_name = request["params"]["name"]
            args = request["params"]["arguments"]

            if tool_name in self.tools:
                result = self.tools[tool_name](**args)
                return {"content": [{"type": "text", "text": str(result)}]}

            else:
                return {"error": f"Unknown tool: {tool_name}"}

    def run_stdio(self):
        """è¿è¡Œ stdio æ¨¡å¼"""
        while True:
            line = sys.stdin.readline()
            if not line:
                break

            request = json.loads(line)
            response = self.handle_request(request)

            sys.stdout.write(json.dumps(response) + "\n")
            sys.stdout.flush()

# HTTP æ¨¡å¼
from fastapi import FastAPI

app = FastAPI()
server = CustomMCPServer()

@app.post("/mcp")
async def mcp_endpoint(request: dict):
    """MCP HTTP ç«¯ç‚¹"""
    return server.handle_request(request)

# ä½¿ç”¨è‡ªå®šä¹‰æœåŠ¡å™¨
mcp_client = MultiServerMCPClient()
mcp_client.add_server(
    "custom",
    StdioMCPTransport(
        command="python",
        args=["mcp_server.py"]
    )
)
```

#### 2.3.3 å·¥å…·æœåŠ¡å™¨é›†æˆ

**å®Œæ•´çš„ MCP å·¥å…·æœåŠ¡å™¨**ï¼š

```python
from typing import List, Dict, Any
import asyncio

class MCPToolServer:
    """MCP å·¥å…·æœåŠ¡å™¨"""

    def __init__(self):
        self.sessions = {}  # ä¼šè¯ç®¡ç†
        self._register_tools()

    def _register_tools(self):
        """æ³¨å†Œå·¥å…·"""
        self.tools = {
            "github": GitHubTool(),
            "docker": DockerTool(),
            "slack": SlackTool(),
            "database": DatabaseTool()
        }

    async def initialize(self, session_id: str) -> dict:
        """åˆå§‹åŒ–ä¼šè¯"""
        self.sessions[session_id] = {
            "created_at": datetime.now(),
            "context": {}
        }

        return {
            "protocolVersion": "2024-11-01",
            "capabilities": {
                "tools": True,
                "prompts": False,
                "resources": False,
                "sampling": False
            },
            "serverInfo": {
                "name": "custom-mcp-server",
                "version": "1.0.0"
            }
        }

    async def list_tools(self) -> List[dict]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·"""
        tools_list = []

        for name, tool in self.tools.items():
            tools_list.append({
                "name": name,
                "description": tool.description,
                "inputSchema": tool.get_schema()
            })

        return {"tools": tools_list}

    async def call_tool(self, tool_name: str, arguments: dict) -> dict:
        """è°ƒç”¨å·¥å…·"""
        if tool_name not in self.tools:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Error: Unknown tool {tool_name}"
                }],
                "isError": True
            }

        try:
            result = await self.tools[tool_name].execute(**arguments)

            return {
                "content": [{
                    "type": "text",
                    "text": json.dumps(result, indent=2)
                }]
            }

        except Exception as e:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Error executing {tool_name}: {str(e)}"
                }],
                "isError": True
            }

class GitHubTool:
    """GitHub å·¥å…·"""

    description = "ä¸ GitHub äº¤äº’"

    def get_schema(self):
        return {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "enum": ["list_repos", "create_issue", "get_pr"]
                },
                "params": {"type": "object"}
            }
        }

    async def execute(self, action: str, params: dict) -> dict:
        """æ‰§è¡Œ GitHub æ“ä½œ"""
        if action == "list_repos":
            # åˆ—å‡ºä»“åº“
            return {"repos": ["repo1", "repo2"]}

        elif action == "create_issue":
            # åˆ›å»º Issue
            return {"issue_id": "123", "url": "https://github.com/..."}

        # æ›´å¤šæ“ä½œ...

# é›†æˆåˆ° LangChain
class MCPToolAdapter:
    """MCP å·¥å…·é€‚é…å™¨"""

    def __init__(self, mcp_server: MCPToolServer):
        self.server = mcp_server

    async def to_langchain_tools(self) -> List:
        """è½¬æ¢ä¸º LangChain å·¥å…·"""
        mcp_tools = await self.server.list_tools()

        langchain_tools = []
        for tool in mcp_tools["tools"]:

            @tool
            def langchain_tool(**kwargs):
                f"""MCP Tool: {tool['name']}

                {tool['description']}
                """
                return asyncio.run(
                    self.server.call_tool(tool["name"], kwargs)
                )

            langchain_tool.__name__ = tool["name"]
            langchain_tools.append(langchain_tool)

        return langchain_tools
```

#### 2.3.4 å®æˆ˜æ¡ˆä¾‹

**å®Œæ•´çš„ MCP é›†æˆåº”ç”¨**ï¼š

```python
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
import os

class MCPIntegratedSystem:
    """MCP é›†æˆç³»ç»Ÿ"""

    def __init__(self):
        self.mcp_client = self._setup_mcp()

    def _setup_mcp(self) -> MultiServerMCPClient:
        """è®¾ç½® MCP å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é…ç½®å­—å…¸ï¼‰"""
        # ä½¿ç”¨é…ç½®å­—å…¸åˆå§‹åŒ–å¤šä¸ªæœåŠ¡å™¨
        client = MultiServerMCPClient({
            "database": {
                "command": "npx",
                "args": [
                    "@modelcontextprotocol/server-postgres",
                    os.getenv("DATABASE_URL")
                ],
                "transport": "stdio"
            },
            "github": {
                "command": "npx",
                "args": [
                    "@modelcontextprotocol/server-github",
                    "--token", os.getenv("GITHUB_TOKEN")
                ],
                "transport": "stdio"
            },
            "slack": {
                "url": "http://localhost:3000/mcp",
                "transport": "streamable_http",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('SLACK_TOKEN')}"
                }
            },
            "filesystem": {
                "command": "npx",
                "args": [
                    "@modelcontextprotocol/server-filesystem",
                    "--root", "/workspace"
                ],
                "transport": "stdio"
            }
        })

        return client

    async def create_react_agent(self):
        """åˆ›å»ºé›†æˆ Agent"""
        # è·å–æ‰€æœ‰ MCP å·¥å…·
        tools = await self.mcp_client.get_tools()

        return create_agent(
            ChatOpenAI(model="gpt-4o"),
            tools,
            prompt="""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ï¼š

1. æŸ¥è¯¢å’Œæ“ä½œæ•°æ®åº“
2. ç®¡ç† GitHub ä»“åº“å’Œ Issues
3. å‘é€ Slack æ¶ˆæ¯
4. æ“ä½œæ–‡ä»¶ç³»ç»Ÿ

æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œçµæ´»ä½¿ç”¨å„ç§å·¥å…·å®Œæˆä»»åŠ¡ã€‚"""
        )

    async def execute_workflow(self, task: str):
        """æ‰§è¡Œå·¥ä½œæµ"""
        # åˆ›å»º Agent
        agent = await self.create_react_agent()

        # ä½¿ç”¨ Agent æ‰§è¡Œä»»åŠ¡
        result = await agent.ainvoke({
            "messages": [("user", f"æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š{task}")]
        })

        return result["messages"][-1].content

# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
async def main():
    """è¿è¡Œç¤ºä¾‹"""
    system = MCPIntegratedSystem()

    # æ‰§è¡Œå¤šä¸ªä»»åŠ¡
    result1 = await system.execute_workflow("åœ¨ myproject ä»“åº“åˆ›å»ºä¸€ä¸ªå…³äºç”¨æˆ·è®¤è¯çš„ Issue")
    print(f"ä»»åŠ¡1ç»“æœ: {result1}")

    result2 = await system.execute_workflow("æŸ¥è¯¢æ•°æ®åº“ä¸­çŠ¶æ€ä¸ºpendingçš„ä»»åŠ¡")
    print(f"ä»»åŠ¡2ç»“æœ: {result2}")

    result3 = await system.execute_workflow("åœ¨Slack #devé¢‘é“å‘é€æ¶ˆæ¯ï¼šå¼€å§‹æ–°çš„å¼€å‘è¿­ä»£")
    print(f"ä»»åŠ¡3ç»“æœ: {result3}")

# è¿è¡Œ
import asyncio
asyncio.run(main())
```

---

### 2.4 è‡ªå®šä¹‰é›†æˆ

#### 2.4.0 å¸¸ç”¨å·¥å…·å¯¼å…¥è·¯å¾„å‚è€ƒ

**LangChain ç”Ÿæ€å¸¸ç”¨å¯¼å…¥éªŒè¯**:

åœ¨ä½¿ç”¨ LangChain é«˜çº§åŠŸèƒ½æ—¶,æ­£ç¡®çš„å¯¼å…¥è·¯å¾„è‡³å…³é‡è¦ã€‚ä»¥ä¸‹æ˜¯ç»è¿‡éªŒè¯çš„å¸¸ç”¨å¯¼å…¥è·¯å¾„(é€‚ç”¨äº LangChain 1.0+)ã€‚

```python
# ========== æ ¸å¿ƒç»„ä»¶ ==========
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage,
    BaseMessage,
    AIMessageChunk  # æµå¼è¾“å‡ºæ—¶ä½¿ç”¨
)

from langchain_core.tools import tool  # è£…é¥°å™¨åˆ›å»ºå·¥å…·(æ¨è)
from langchain_core.tools import Tool, StructuredTool  # ç±»åˆ›å»ºå·¥å…·

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import Runnable, RunnablePassthrough

# ========== LLM/Chat æ¨¡å‹ ==========
from langchain_openai import ChatOpenAI, OpenAI
from langchain_openai import OpenAIEmbeddings

from langchain_anthropic import ChatAnthropic
from langchain_anthropic import AnthropicEmbeddings

# ========== ç¤¾åŒºé›†æˆå·¥å…· ==========
# æœç´¢å·¥å…·
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_community.tools import WikipediaQueryRun 

# æ–‡æ¡£åŠ è½½å™¨
from langchain_community.document_loaders import (
    PyPDFLoader,         
    TextLoader,          
    WebBaseLoader,       
    DirectoryLoader      
)

# å‘é‡æ•°æ®åº“
from langchain_community.vectorstores import (
    Chroma,             
    FAISS,              
    Qdrant              
)

# ========== æ–‡æœ¬åˆ†å‰² ==========
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,(æ–°è·¯å¾„)
    CharacterTextSplitter           (æ–°è·¯å¾„)
)

# æ—§è·¯å¾„(å·²å¼ƒç”¨,ä½†ä»å…¼å®¹)
# from langchain.text_splitter import RecursiveCharacterTextSplitter  # âš ï¸ æ—§è·¯å¾„

# ========== LangGraph ç»„ä»¶ ==========
from langgraph.graph import StateGraph, MessagesState, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver

# ========== Agent åˆ›å»º ==========
from langchain.agents import create_agent  # âœ… æ¨è(LangChain 1.0+)

# ========== Retriever ==========
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document

# ========== å›è°ƒä¸è¿½è¸ª ==========
from langchain_core.callbacks import (
    CallbackManagerForLLMRun,
    AsyncCallbackManagerForLLMRun
)

# LangSmith (éœ€è¦å•ç‹¬å®‰è£… langsmith)
from langsmith import Client
from langchain_core.callbacks.tracers import LangChainTracer

# ========== MCP é›†æˆ ==========
from langchain_mcp_adapters.client import MultiServerMCPClient
from mcp.client.stdio import StdioServerParameters

# ========== Pydantic æ¨¡å‹(ç”¨äºå·¥å…·å®šä¹‰) ==========
# LangChain 1.0+ ä½¿ç”¨å†…ç½®çš„ pydantic v1 å…¼å®¹å±‚
from langchain.pydantic_v1 import BaseModel, Field

# å¦‚æœç›´æ¥ä½¿ç”¨ Pydantic v2
from pydantic import BaseModel, Field  # âœ… ä¹Ÿå¯ä»¥,ä½†æ³¨æ„å…¼å®¹æ€§

# ========== è‡ªå®šä¹‰åŸºç±» ==========
from langchain_core.language_models import BaseChatModel
from langchain_core.retrievers import BaseRetriever
from langchain.tools import BaseTool

# ========== å®Œæ•´ç¤ºä¾‹ï¼šéªŒè¯å¯¼å…¥ ==========
def verify_imports():
    """éªŒè¯å¸¸ç”¨å¯¼å…¥æ˜¯å¦æ­£ç¡®"""
    try:
        # æ ¸å¿ƒç»„ä»¶
        from langchain_core.messages import HumanMessage, AIMessage
        from langchain_core.tools import tool, Tool

        # LLM
        from langchain_openai import ChatOpenAI

        # ç¤¾åŒºå·¥å…·
        from langchain_community.tools import DuckDuckGoSearchRun
        from langchain_community.tools import WikipediaQueryRun

        # æ–‡æ¡£å¤„ç†
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter

        # å‘é‡å­˜å‚¨
        from langchain_community.vectorstores import Chroma

        # LangGraph
        from langgraph.graph import StateGraph, MessagesState, END
        from langgraph.checkpoint.memory import MemorySaver

        # Agent
        from langchain.agents import create_agent

        # MCP
        from langchain_mcp_adapters.client import MultiServerMCPClient

        print("âœ… æ‰€æœ‰å¯¼å…¥éªŒè¯é€šè¿‡!")
        return True

    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å®‰è£…äº†ç›¸åº”çš„åŒ…:")
        print("  pip install langchain langchain-openai langchain-community")
        print("  pip install langchain-text-splitters langgraph")
        print("  pip install langchain-mcp-adapters")
        return False

# è¿è¡ŒéªŒè¯
verify_imports()
```

**å¸¸è§å¯¼å…¥é”™è¯¯ä¸ä¿®å¤**:

```python
# âŒ é”™è¯¯1: ä½¿ç”¨å·²å¼ƒç”¨çš„ text_splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter

# âœ… ä¿®å¤: ä½¿ç”¨æ–°çš„ langchain_text_splitters åŒ…
from langchain_text_splitters import RecursiveCharacterTextSplitter

# âŒ é”™è¯¯2: ä½¿ç”¨å·²åºŸå¼ƒçš„ create_react_agent (LangGraph 1.0+ å·²åºŸå¼ƒ)
from langgraph.prebuilt import create_react_agent  # å·²åºŸå¼ƒ

# âœ… ä¿®å¤: ä½¿ç”¨ LangChain 1.0+ çš„ create_agent
from langchain.agents import create_agent  # LangChain 1.0+ æ¨è

# âŒ é”™è¯¯3: å·¥å…·å®šä¹‰ä½¿ç”¨é”™è¯¯çš„ Pydantic ç‰ˆæœ¬
from pydantic import BaseModel, Field  # å¯èƒ½å¯¼è‡´å…¼å®¹æ€§é—®é¢˜

# âœ… ä¿®å¤: ä½¿ç”¨ LangChain çš„ pydantic_v1 å…¼å®¹å±‚
from langchain.pydantic_v1 import BaseModel, Field

# âŒ é”™è¯¯4: ç¤¾åŒºå·¥å…·å¯¼å…¥è·¯å¾„é”™è¯¯
from langchain.tools import DuckDuckGoSearchRun  # æ—§è·¯å¾„

# âœ… ä¿®å¤: ä½¿ç”¨ langchain_community
from langchain_community.tools import DuckDuckGoSearchRun

# âŒ é”™è¯¯5: æ¶ˆæ¯ç±»å‹å¯¼å…¥è·¯å¾„é”™è¯¯
from langchain.schema import HumanMessage  # æ—§è·¯å¾„

# âœ… ä¿®å¤: ä½¿ç”¨ langchain_core.messages
from langchain_core.messages import HumanMessage
```

**åŒ…å®‰è£…æŒ‡å—**:

```bash
# æ ¸å¿ƒåŒ…
pip install langchain langchain-core

# OpenAI é›†æˆ
pip install langchain-openai

# Anthropic é›†æˆ
pip install langchain-anthropic

# ç¤¾åŒºé›†æˆ(å·¥å…·ã€åŠ è½½å™¨ã€å‘é‡æ•°æ®åº“)
pip install langchain-community

# æ–‡æœ¬åˆ†å‰²å™¨(ç‹¬ç«‹åŒ…)
pip install langchain-text-splitters

# LangGraph(Agent å·¥ä½œæµ)
pip install langgraph

# MCP é€‚é…å™¨
pip install langchain-mcp-adapters

# å¯é€‰: ç‰¹å®šå‘é‡æ•°æ®åº“
pip install chromadb  # Chroma
pip install faiss-cpu  # FAISS
pip install qdrant-client  # Qdrant

# å¯é€‰: æ–‡æ¡£å¤„ç†
pip install pypdf  # PDF æ”¯æŒ
pip install beautifulsoup4  # Web å†…å®¹è§£æ

# å¯é€‰: LangSmith è¿½è¸ª
pip install langsmith
```

**å¯¼å…¥è·¯å¾„æ¼”è¿›å†å²**:

```python
# LangChain 0.x (æ—§ç‰ˆ - å·²åºŸå¼ƒ)
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.agents import AgentExecutor  # âŒ å·²ç§»é™¤ï¼ŒLangChain 1.0+ ä¸å†å­˜åœ¨

# LangChain 1.0+ (å½“å‰ - æ¨è)
from langchain_openai import OpenAI, ChatOpenAI  # ç‹¬ç«‹åŒ…
from langchain_core.messages import HumanMessage  # æ ¸å¿ƒæ¨¡å—
from langchain.agents import create_agent  # âœ… ç®€åŒ– APIï¼Œç›´æ¥è¿”å›å¯æ‰§è¡Œçš„ agent
```

---

#### 2.4.1 è‡ªå®šä¹‰ LLMï¼ˆBaseChatModelï¼‰

```python
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage
from typing import List, Optional, Any
import requests

class CustomLLM(BaseChatModel):
    """è‡ªå®šä¹‰ LLM å®ç°"""

    api_key: str
    api_url: str
    model_name: str = "custom-model"
    temperature: float = 0.7

    class Config:
        """Pydantic é…ç½®"""
        extra = "forbid"

    @property
    def _llm_type(self) -> str:
        """è¿”å› LLM ç±»å‹"""
        return "custom"

    def _call(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any
    ) -> str:
        """åŒæ­¥è°ƒç”¨"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._format_messages(messages)

        # è°ƒç”¨ API
        response = requests.post(
            self.api_url,
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "model": self.model_name,
                "messages": formatted_messages,
                "temperature": self.temperature,
                "stop": stop
            }
        )

        result = response.json()
        return result["choices"][0]["message"]["content"]

    async def _acall(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨"""
        import aiohttp

        formatted_messages = self._format_messages(messages)

        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.api_url,
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "model": self.model_name,
                    "messages": formatted_messages,
                    "temperature": self.temperature
                }
            ) as response:
                result = await response.json()
                return result["choices"][0]["message"]["content"]

    def _format_messages(self, messages: List[BaseMessage]) -> List[dict]:
        """æ ¼å¼åŒ–æ¶ˆæ¯"""
        formatted = []
        for msg in messages:
            formatted.append({
                "role": msg.type,
                "content": msg.content
            })
        return formatted

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any
    ):
        """æµå¼è¾“å‡º"""
        formatted_messages = self._format_messages(messages)

        response = requests.post(
            self.api_url,
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "model": self.model_name,
                "messages": formatted_messages,
                "stream": True
            },
            stream=True
        )

        for line in response.iter_lines():
            if line:
                # è§£æ SSE æ ¼å¼
                chunk = self._parse_sse(line)
                if chunk:
                    yield AIMessageChunk(content=chunk)

# ä½¿ç”¨è‡ªå®šä¹‰ LLM
custom_llm = CustomLLM(
    api_key="your-api-key",
    api_url="https://api.custom-llm.com/v1/chat",
    model_name="custom-gpt"
)

# å¯ä»¥åƒä½¿ç”¨å…¶ä»– LLM ä¸€æ ·ä½¿ç”¨
from langchain.agents import create_agent

agent = create_agent(
    custom_llm,
    [],
    prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
)
```

#### 2.4.2 è‡ªå®šä¹‰ Retriever ä¸ Tool

```python
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List

class CustomRetriever(BaseRetriever):
    """è‡ªå®šä¹‰æ£€ç´¢å™¨"""

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[Any] = None
    ) -> List[Document]:
        """æ£€ç´¢æ–‡æ¡£"""
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        results = self.search_backend(query)

        documents = []
        for result in results:
            doc = Document(
                page_content=result["content"],
                metadata={
                    "source": result["source"],
                    "score": result["score"],
                    "timestamp": result["timestamp"]
                }
            )
            documents.append(doc)

        return documents

    async def _aget_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[Any] = None
    ) -> List[Document]:
        """å¼‚æ­¥æ£€ç´¢"""
        # å¼‚æ­¥å®ç°
        results = await self.async_search_backend(query)
        # ... è½¬æ¢ä¸º Documents
        return documents

# è‡ªå®šä¹‰å·¥å…·
from langchain.pydantic_v1 import BaseModel, Field
from langchain.tools import BaseTool

class CustomToolInput(BaseModel):
    """å·¥å…·è¾“å…¥æ¨¡å¼"""
    query: str = Field(description="æŸ¥è¯¢å­—ç¬¦ä¸²")
    filters: dict = Field(default={}, description="è¿‡æ»¤æ¡ä»¶")

class CustomTool(BaseTool):
    """è‡ªå®šä¹‰å·¥å…·"""
    name = "custom_search"
    description = "è‡ªå®šä¹‰æœç´¢å·¥å…·"
    args_schema = CustomToolInput

    def _run(
        self,
        query: str,
        filters: dict = {},
        run_manager: Optional[Any] = None
    ) -> str:
        """æ‰§è¡Œå·¥å…·"""
        # å®ç°å·¥å…·é€»è¾‘
        results = self.perform_search(query, filters)
        return json.dumps(results)

    async def _arun(
        self,
        query: str,
        filters: dict = {},
        run_manager: Optional[Any] = None
    ) -> str:
        """å¼‚æ­¥æ‰§è¡Œ"""
        results = await self.async_perform_search(query, filters)
        return json.dumps(results)
```

---

### æœ¬ç« å°ç»“

1. **å¤š Agent åä½œ**ï¼šæŒæ¡ Supervisor-Workerã€Routerã€Hierarchical ä¸‰ç§æ¨¡å¼
2. **å¤šæ¨¡æ€å¤„ç†**ï¼šå›¾åƒç†è§£ã€OCRã€éŸ³é¢‘å¤„ç†ã€æ··åˆåº”ç”¨
3. **æµå¼ä¸å¼‚æ­¥**ï¼šstream/astreamã€å¹¶å‘æ§åˆ¶ã€WebSocket/SSE å®æ—¶äº¤äº’
4. **MCP é›†æˆ**ï¼šè¿æ¥å¤–éƒ¨æœåŠ¡ã€å·¥å…·æœåŠ¡å™¨ã€å¤šåè®®æ”¯æŒ
5. **è‡ªå®šä¹‰é›†æˆ**ï¼šå®ç°è‡ªå®šä¹‰ LLMã€Retrieverã€Tool

---

## ç¬¬3ç« ï¼šLong-term Memory æ¶æ„è®¾è®¡

> **ç›®æ ‡**ï¼šæŒæ¡é•¿æœŸè®°å¿†ç³»ç»Ÿè®¾è®¡ï¼Œå®ç°è·¨ä¼šè¯çš„çŸ¥è¯†ç§¯ç´¯å’Œä¸ªæ€§åŒ–ä½“éªŒ

çŸ­æœŸè®°å¿†(Checkpointer)åªä¿å­˜å•ä¸ªä¼šè¯çš„å†å²ï¼Œæ— æ³•å®ç°è·¨ä¼šè¯çš„çŸ¥è¯†ç§¯ç´¯ã€‚Long-term Memory ç³»ç»Ÿé€šè¿‡å‘é‡æ•°æ®åº“ã€çŸ¥è¯†å›¾è°±ç­‰æŠ€æœ¯ï¼Œè®© Agent æ‹¥æœ‰æŒä¹…åŒ–çš„"é•¿æœŸè®°å¿†"èƒ½åŠ›ã€‚

---

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦ Long-term Memory

#### 3.1.1 çŸ­æœŸè®°å¿†çš„å±€é™

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

# âœ… çŸ­æœŸè®°å¿†ï¼šå•ä¼šè¯å†…æœ‰æ•ˆ
agent = create_agent(
    ChatOpenAI(model="gpt-4o"),
    [],
    checkpointer=MemorySaver()
)

# ä¼šè¯1
config1 = {"configurable": {"thread_id": "session-1"}}
agent.invoke({"messages": [("user", "æˆ‘å–œæ¬¢Python")]}, config=config1)
agent.invoke({"messages": [("user", "æˆ‘å–œæ¬¢ä»€ä¹ˆ?")]}, config=config1)
# è¾“å‡ºï¼šä½ å–œæ¬¢Python âœ…

# ä¼šè¯2ï¼ˆæ–°ä¼šè¯ï¼‰
config2 = {"configurable": {"thread_id": "session-2"}}
agent.invoke({"messages": [("user", "æˆ‘å–œæ¬¢ä»€ä¹ˆ?")]}, config=config2)
# è¾“å‡ºï¼šæˆ‘ä¸çŸ¥é“ âŒ çŸ­æœŸè®°å¿†æ— æ³•è·¨ä¼šè¯!
```

**Long-term Memory è§£å†³çš„é—®é¢˜**ï¼š
1. âŒ ç”¨æˆ·åå¥½æ— æ³•è·¨ä¼šè¯ä¿ç•™
2. âŒ å†å²äº¤äº’çŸ¥è¯†æ— æ³•å¤ç”¨
3. âŒ æ— æ³•å®ç°ä¸ªæ€§åŒ–æ¨è
4. âŒ æ— æ³•ç§¯ç´¯é¢†åŸŸçŸ¥è¯†

---

### 3.2 Long-term Memory æ¶æ„

#### 3.2.1 æ··åˆè®°å¿†æ¶æ„

```mermaid
graph TB
    A[ç”¨æˆ·è¾“å…¥] --> B{è®°å¿†ç³»ç»Ÿ}

    B --> C[çŸ­æœŸè®°å¿†<br/>Checkpointer]
    B --> D[é•¿æœŸè®°å¿†<br/>VectorStore]

    C --> E[ä¼šè¯å†å²<br/>æœ€è¿‘10è½®å¯¹è¯]
    D --> F[ç”¨æˆ·ç”»åƒ<br/>åå¥½/ä¹ æƒ¯]
    D --> G[çŸ¥è¯†åº“<br/>FAQ/æ–‡æ¡£]
    D --> H[äº¤äº’å†å²<br/>æ‰€æœ‰å¯¹è¯æ‘˜è¦]

    E --> I[Agent]
    F --> I
    G --> I
    H --> I

    style C fill:#FFF9C4
    style D fill:#C8E6C9
```

#### 3.2.2 åŸºç¡€å®ç°

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.messages import HumanMessage, AIMessage

class LongTermMemory:
    """é•¿æœŸè®°å¿†ç³»ç»Ÿ"""

    def __init__(self, user_id: str):
        self.user_id = user_id
        self.embeddings = OpenAIEmbeddings()

        # å‘é‡æ•°æ®åº“(å­˜å‚¨é•¿æœŸè®°å¿†)
        self.vectorstore = Chroma(
            collection_name=f"memory_{user_id}",
            embedding_function=self.embeddings
        )

    def add_interaction(self, user_message: str, ai_response: str):
        """ä¿å­˜ä¸€æ¬¡äº¤äº’åˆ°é•¿æœŸè®°å¿†"""
        # åˆ›å»ºè®°å¿†æ–‡æœ¬
        memory_text = f"ç”¨æˆ·: {user_message}\nAI: {ai_response}"

        # å­˜å…¥å‘é‡æ•°æ®åº“
        self.vectorstore.add_texts(
            texts=[memory_text],
            metadatas=[{
                "user_id": self.user_id,
                "timestamp": datetime.now().isoformat(),
                "type": "interaction"
            }]
        )

    def recall(self, query: str, k: int = 3) -> list[str]:
        """æ ¹æ®æŸ¥è¯¢å¬å›ç›¸å…³è®°å¿†"""
        docs = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in docs]

    def get_user_profile(self) -> dict:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        # ä»æ‰€æœ‰äº¤äº’ä¸­æå–ç”¨æˆ·åå¥½
        all_memories = self.vectorstore.similarity_search("", k=100)

        # ç®€åŒ–å®ç°ï¼šç»Ÿè®¡å…³é”®è¯
        preferences = {}
        for memory in all_memories:
            # åˆ†æç”¨æˆ·åå¥½...
            pass

        return preferences
```

**å¢å¼ºç‰ˆé•¿æœŸè®°å¿†ç³»ç»Ÿ**:

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import uuid

class AdvancedLongTermMemory:
    """å¢å¼ºç‰ˆé•¿æœŸè®°å¿†ç³»ç»Ÿ"""

    def __init__(self, user_id: str, persist_directory: str = "./chroma_db"):
        self.user_id = user_id
        self.embeddings = OpenAIEmbeddings()

        # åˆ†å±‚å‘é‡å­˜å‚¨
        self.interaction_store = Chroma(
            collection_name=f"interactions_{user_id}",
            embedding_function=self.embeddings,
            persist_directory=f"{persist_directory}/interactions"
        )

        self.knowledge_store = Chroma(
            collection_name=f"knowledge_{user_id}",
            embedding_function=self.embeddings,
            persist_directory=f"{persist_directory}/knowledge"
        )

        self.preference_store = Chroma(
            collection_name=f"preferences_{user_id}",
            embedding_function=self.embeddings,
            persist_directory=f"{persist_directory}/preferences"
        )

    def add_interaction(
        self,
        user_message: str,
        ai_response: str,
        metadata: Optional[Dict] = None
    ):
        """ä¿å­˜äº¤äº’"""
        interaction_id = str(uuid.uuid4())

        full_metadata = {
            "user_id": self.user_id,
            "timestamp": datetime.now().isoformat(),
            "type": "interaction",
            "interaction_id": interaction_id,
            **(metadata or {})
        }

        memory_text = f"ç”¨æˆ·: {user_message}\nAI: {ai_response}"

        self.interaction_store.add_texts(
            texts=[memory_text],
            metadatas=[full_metadata],
            ids=[interaction_id]
        )

    def add_knowledge(
        self,
        knowledge: str,
        source: str = "user",
        importance: int = 1
    ):
        """æ·»åŠ çŸ¥è¯†ç‚¹"""
        knowledge_id = str(uuid.uuid4())

        self.knowledge_store.add_texts(
            texts=[knowledge],
            metadatas=[{
                "user_id": self.user_id,
                "timestamp": datetime.now().isoformat(),
                "source": source,
                "importance": importance,
                "type": "knowledge"
            }],
            ids=[knowledge_id]
        )

    def add_preference(
        self,
        preference_key: str,
        preference_value: str,
        confidence: float = 1.0
    ):
        """æ·»åŠ ç”¨æˆ·åå¥½"""
        pref_id = str(uuid.uuid4())

        pref_text = f"{preference_key}: {preference_value}"

        self.preference_store.add_texts(
            texts=[pref_text],
            metadatas=[{
                "user_id": self.user_id,
                "timestamp": datetime.now().isoformat(),
                "key": preference_key,
                "value": preference_value,
                "confidence": confidence,
                "type": "preference"
            }],
            ids=[pref_id]
        )

    def recall_interactions(
        self,
        query: str,
        k: int = 3,
        time_range: Optional[timedelta] = None
    ) -> List[Dict]:
        """å¬å›ç›¸å…³äº¤äº’"""
        results = self.interaction_store.similarity_search_with_score(
            query, k=k*2
        )

        # æ—¶é—´è¿‡æ»¤
        if time_range:
            cutoff_time = datetime.now() - time_range
            results = [
                (doc, score) for doc, score in results
                if datetime.fromisoformat(doc.metadata["timestamp"]) > cutoff_time
            ]

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top k
        results.sort(key=lambda x: x[1])
        return [
            {
                "content": doc.page_content,
                "score": score,
                "metadata": doc.metadata
            }
            for doc, score in results[:k]
        ]

    def recall_knowledge(self, query: str, k: int = 5) -> List[Dict]:
        """å¬å›ç›¸å…³çŸ¥è¯†"""
        results = self.knowledge_store.similarity_search_with_score(query, k=k)

        # æŒ‰é‡è¦æ€§å’Œç›¸ä¼¼åº¦ç»¼åˆæ’åº
        sorted_results = sorted(
            results,
            key=lambda x: (
                x[0].metadata.get("importance", 1) * 10 + (1 - x[1])
            ),
            reverse=True
        )

        return [
            {
                "content": doc.page_content,
                "importance": doc.metadata.get("importance", 1),
                "source": doc.metadata.get("source"),
                "score": score
            }
            for doc, score in sorted_results
        ]

    def get_preferences(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰åå¥½"""
        all_prefs = self.preference_store.get()

        preferences = {}
        if all_prefs and all_prefs["metadatas"]:
            for metadata in all_prefs["metadatas"]:
                key = metadata.get("key")
                value = metadata.get("value")
                if key and value:
                    preferences[key] = value

        return preferences

    def build_user_context(self, query: str) -> str:
        """æ„å»ºç”¨æˆ·ä¸Šä¸‹æ–‡"""
        # å¬å›å„ç±»è®°å¿†
        interactions = self.recall_interactions(query, k=2)
        knowledge = self.recall_knowledge(query, k=3)
        preferences = self.get_preferences()

        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        context_parts = []

        if preferences:
            prefs_str = "\n".join([f"- {k}: {v}" for k, v in preferences.items()])
            context_parts.append(f"ç”¨æˆ·åå¥½:\n{prefs_str}")

        if knowledge:
            knowledge_str = "\n".join([
                f"- {k['content']} (é‡è¦æ€§: {k['importance']})"
                for k in knowledge
            ])
            context_parts.append(f"\nç›¸å…³çŸ¥è¯†:\n{knowledge_str}")

        if interactions:
            interactions_str = "\n".join([
                f"- {i['content']}" for i in interactions[:2]
            ])
            context_parts.append(f"\nç›¸å…³å†å²:\n{interactions_str}")

        return "\n".join(context_parts)

    def extract_and_store_knowledge(
        self,
        conversation: List[Dict],
        llm: ChatOpenAI
    ):
        """ä»å¯¹è¯ä¸­æå–å¹¶å­˜å‚¨çŸ¥è¯†"""
        # ä½¿ç”¨LLMæå–çŸ¥è¯†ç‚¹
        conversation_text = "\n".join([
            f"{msg['role']}: {msg['content']}" for msg in conversation
        ])

        extraction_prompt = f"""ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ç”¨æˆ·çš„å…³é”®ä¿¡æ¯ã€åå¥½å’ŒçŸ¥è¯†ç‚¹:

{conversation_text}

è¿”å›JSONæ ¼å¼:
{{
    "knowledge": ["çŸ¥è¯†ç‚¹1", "çŸ¥è¯†ç‚¹2"],
    "preferences": {{"åå¥½é”®": "åå¥½å€¼"}},
    "importance": 1-5
}}"""

        response = llm.invoke([HumanMessage(content=extraction_prompt)])

        # è§£æå¹¶å­˜å‚¨
        import json
        try:
            extracted = json.loads(response.content)

            # å­˜å‚¨çŸ¥è¯†
            for knowledge in extracted.get("knowledge", []):
                self.add_knowledge(
                    knowledge,
                    source="conversation",
                    importance=extracted.get("importance", 1)
                )

            # å­˜å‚¨åå¥½
            for key, value in extracted.get("preferences", {}).items():
                self.add_preference(key, value, confidence=0.8)

        except json.JSONDecodeError:
            pass  # æå–å¤±è´¥,é™é»˜å¤„ç†
```

---

### 3.3 å®æˆ˜ï¼šä¸ªæ€§åŒ–åŠ©æ‰‹

#### 3.3.1 é›†æˆé•¿æœŸè®°å¿†åˆ°Agent

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

class PersonalizedAgent:
    """å¸¦é•¿æœŸè®°å¿†çš„ä¸ªæ€§åŒ–Agent"""

    def __init__(self, user_id: str):
        self.user_id = user_id
        self.long_term_memory = LongTermMemory(user_id)

        # Agent(å¸¦çŸ­æœŸè®°å¿†)
        self.agent = create_agent(
            ChatOpenAI(model="gpt-4o"),
            [],
            checkpointer=MemorySaver(),
            prompt=self._build_system_prompt()
        )

    def _build_system_prompt(self) -> str:
        """æ„å»ºåŒ…å«ç”¨æˆ·ç”»åƒçš„ç³»ç»Ÿæç¤º"""
        profile = self.long_term_memory.get_user_profile()

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸ªæ€§åŒ–åŠ©æ‰‹ã€‚

ç”¨æˆ·ç”»åƒï¼š
- ç”¨æˆ·ID: {self.user_id}
- åå¥½: {profile.get('preferences', 'æœªçŸ¥')}
- ä¹ æƒ¯: {profile.get('habits', 'æœªçŸ¥')}

è¯·æ ¹æ®ç”¨æˆ·ç”»åƒæä¾›ä¸ªæ€§åŒ–æœåŠ¡ã€‚"""

    def invoke(self, user_message: str, thread_id: str):
        """è°ƒç”¨Agent"""
        # 1. ä»é•¿æœŸè®°å¿†å¬å›ç›¸å…³å†…å®¹
        relevant_memories = self.long_term_memory.recall(user_message, k=3)

        # 2. å¢å¼ºæç¤ºè¯
        enhanced_message = f"""å½“å‰é—®é¢˜: {user_message}

ç›¸å…³å†å²è®°å¿†:
{chr(10).join(f"- {mem}" for mem in relevant_memories)}

è¯·åŸºäºå†å²è®°å¿†æä¾›ä¸ªæ€§åŒ–å›ç­”ã€‚"""

        # 3. è°ƒç”¨Agent
        config = {"configurable": {"thread_id": thread_id}}
        result = self.agent.invoke(
            {"messages": [("user", enhanced_message)]},
            config=config
        )

        ai_response = result["messages"][-1].content

        # 4. ä¿å­˜åˆ°é•¿æœŸè®°å¿†
        self.long_term_memory.add_interaction(user_message, ai_response)

        return ai_response

# ä½¿ç”¨ç¤ºä¾‹
agent = PersonalizedAgent(user_id="alice")

# ä¼šè¯1
response1 = agent.invoke("æˆ‘å–œæ¬¢Pythonç¼–ç¨‹", thread_id="session-1")

# ä¼šè¯2ï¼ˆæ–°ä¼šè¯ï¼Œä½†èƒ½å¬å›é•¿æœŸè®°å¿†ï¼‰
response2 = agent.invoke("æ¨èä¸€æœ¬ä¹¦ç»™æˆ‘", thread_id="session-2")
# Agentä¼šåŸºäº"å–œæ¬¢Python"æ¨èPythonç›¸å…³ä¹¦ç± âœ…
```

---

### 3.4 é«˜çº§ç‰¹æ€§

#### 3.4.1 è®°å¿†ä¼˜å…ˆçº§

```python
class PrioritizedMemory:
    """å¸¦ä¼˜å…ˆçº§çš„è®°å¿†ç³»ç»Ÿ"""

    def __init__(self):
        self.vectorstore = Chroma(...)

    def add_with_priority(self, text: str, priority: str):
        """æ·»åŠ å¸¦ä¼˜å…ˆçº§çš„è®°å¿†"""
        self.vectorstore.add_texts(
            texts=[text],
            metadatas=[{
                "priority": priority,  # "high", "medium", "low"
                "timestamp": datetime.now().isoformat()
            }]
        )

    def recall_prioritized(self, query: str, k: int = 5):
        """å¬å›æ—¶ä¼˜å…ˆè¿”å›é«˜ä¼˜å…ˆçº§è®°å¿†"""
        all_results = self.vectorstore.similarity_search(query, k=k*2)

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        prioritized = sorted(
            all_results,
            key=lambda x: {"high": 3, "medium": 2, "low": 1}.get(
                x.metadata.get("priority", "low"), 0
            ),
            reverse=True
        )

        return prioritized[:k]
```

#### 3.4.2 è®°å¿†é—å¿˜æœºåˆ¶

```python
from datetime import datetime, timedelta

class ForgettableMemory:
    """å¸¦é—å¿˜æœºåˆ¶çš„è®°å¿†"""

    def recall_with_decay(self, query: str, k: int = 5):
        """å¬å›è®°å¿†ï¼Œä½†æ—§è®°å¿†ä¼šè¡°å‡"""
        results = self.vectorstore.similarity_search(query, k=k*2)

        scored_results = []
        for doc in results:
            # æ—¶é—´è¡°å‡å› å­
            timestamp = datetime.fromisoformat(doc.metadata["timestamp"])
            age_days = (datetime.now() - timestamp).days

            decay_factor = 1.0 / (1 + age_days / 30)  # 30å¤©è¡°å‡ä¸€åŠ

            # ç›¸ä¼¼åº¦ * è¡°å‡å› å­
            score = doc.metadata.get("similarity_score", 1.0) * decay_factor

            scored_results.append((doc, score))

        # æŒ‰åˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x[1], reverse=True)

        return [doc for doc, score in scored_results[:k]]
```

---

### 3.5 æœ€ä½³å®è·µ

#### 3.5.1 è®°å¿†ç±»å‹åˆ†å±‚

```python
# æ¨èçš„è®°å¿†åˆ†å±‚æ¶æ„
memory_system = {
    "çŸ­æœŸè®°å¿†": {
        "å®ç°": "Checkpointer",
        "èŒƒå›´": "å½“å‰ä¼šè¯",
        "å®¹é‡": "æœ€è¿‘10-20è½®å¯¹è¯",
        "ç”¨é€”": "ä¸Šä¸‹æ–‡ç†è§£"
    },

    "å·¥ä½œè®°å¿†": {
        "å®ç°": "VectorStore (ä¸´æ—¶)",
        "èŒƒå›´": "å½“å‰ä»»åŠ¡",
        "å®¹é‡": "ä»»åŠ¡ç›¸å…³çŸ¥è¯†",
        "ç”¨é€”": "ä»»åŠ¡æ‰§è¡Œè¾…åŠ©"
    },

    "é•¿æœŸè®°å¿†": {
        "å®ç°": "VectorStore (æŒä¹…)",
        "èŒƒå›´": "æ‰€æœ‰å†å²",
        "å®¹é‡": "æ— é™åˆ¶",
        "ç”¨é€”": "ä¸ªæ€§åŒ–ã€çŸ¥è¯†ç§¯ç´¯"
    }
}
```

#### 3.5.2 æ€§èƒ½ä¼˜åŒ–

**ç¼“å­˜ç­–ç•¥**:
```python
from functools import lru_cache

class OptimizedMemory:
    @lru_cache(maxsize=100)
    def recall_cached(self, query: str, k: int = 3):
        """å¸¦ç¼“å­˜çš„å¬å›"""
        return tuple(self.vectorstore.similarity_search(query, k=k))
```

---

### 3.6 æ€»ç»“

#### 3.6.1 æ ¸å¿ƒè¦ç‚¹

1. **é•¿æœŸè®°å¿†**ï¼šè·¨ä¼šè¯çš„çŸ¥è¯†ç§¯ç´¯
2. **å‘é‡å¬å›**ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢
3. **ç”¨æˆ·ç”»åƒ**ï¼šä»å†å²äº¤äº’æå–åå¥½
4. **è®°å¿†ç®¡ç†**ï¼šä¼˜å…ˆçº§ã€é—å¿˜ã€åˆ†å±‚

#### 3.6.2 å®æ–½æ¸…å•

- [ ] ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„è®°å¿†ç©ºé—´
- [ ] å®ç°äº¤äº’è‡ªåŠ¨ä¿å­˜æœºåˆ¶
- [ ] é…ç½®åˆç†çš„å¬å›æ•°é‡(k=3-5)
- [ ] å®šæœŸæ¸…ç†è¿‡æœŸ/ä½ä»·å€¼è®°å¿†
- [ ] ç›‘æ§å‘é‡æ•°æ®åº“æ€§èƒ½

---

###æ€è€ƒä¸ç»ƒä¹ 

1. **æ€è€ƒ**ï¼šåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹© Hierarchical è€Œä¸æ˜¯ Supervisor-Worker æ¨¡å¼ï¼Ÿ

2. **ç»ƒä¹ **ï¼šå®ç°ä¸€ä¸ªæ”¯æŒå›¾åƒè¾“å…¥çš„å¤šæ¨¡æ€ RAG ç³»ç»Ÿã€‚

3. **æ€è€ƒ**ï¼šå¦‚ä½•åœ¨ WebSocket å®æ—¶äº¤äº’ä¸­å¤„ç†è¿æ¥æ–­å¼€å’Œé‡è¿ï¼Ÿ

---

## ç¬¬4ç« ï¼šAgent äº¤äº’åè®®

> **å…³æ³¨ç‚¹**ï¼šç†è§£ Agent ç”Ÿæ€çš„ä¸‰å¤§åè®®æ ‡å‡†ï¼ŒæŒæ¡ AG-UI å’Œ A2A çš„å®æˆ˜åº”ç”¨ã€‚

éšç€ AI Agent ç”Ÿæ€çš„æˆç†Ÿï¼Œä¸‰å¤§äº¤äº’åè®®é€æ¸æˆä¸ºè¡Œä¸šæ ‡å‡†ï¼š

- **MCP (Model Context Protocol)**: Agent â†” å·¥å…·/æ•°æ®ï¼ˆå·²åœ¨ 2.3 èŠ‚è¯¦è§£ï¼‰
- **AG-UI (Agent-User Interaction Protocol)**: Agent â†” ç”¨æˆ·ç•Œé¢
- **A2A (Agent-to-Agent Protocol)**: Agent â†” Agent è·¨ç³»ç»Ÿåä½œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Agent åè®®ç”Ÿæ€                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚   ç”¨æˆ·ç•Œé¢    â”‚â—„â”€â”€â”€â”€â”€â”€ AG-UI â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  (Web/App)   â”‚      (äº‹ä»¶æµ)        â”‚    Agent     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   (ä¸»ä½“)     â”‚    â”‚
â”‚                                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                â”‚             â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                         â”‚                      â”‚          â”‚ â”‚
â”‚                         â–¼                      â–¼          â–¼ â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                  â”‚  MCP Server  â”‚       â”‚ Other Agent  â”‚    â”‚
â”‚                  â”‚  (å·¥å…·/æ•°æ®)  â”‚â—„â”€MCPâ”€â–ºâ”‚   (åä½œ)     â”‚    â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                â–²            â”‚
â”‚                                                â”‚            â”‚
â”‚                                            A2A â”‚            â”‚
â”‚                                         (JSON-RPC)          â”‚
â”‚                                                â”‚            â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                      â”‚  External Agent  â”‚   â”‚
â”‚                                      â”‚  (è·¨ç³»ç»Ÿ/è·¨å…¬å¸)  â”‚   â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.1 åè®®å¯¹æ¯”ä¸é€‰å‹

| ç‰¹æ€§ | MCP (Anthropic) | AG-UI | A2A (Google) |
|------|-----------------|-------|--------------|
| **è¿æ¥æ–¹å‘** | Agent â†’ å·¥å…·/æ•°æ® | Agent â†” ç”¨æˆ·ç•Œé¢ | Agent â†” Agent |
| **ä¸»è¦ç”¨é€”** | å·¥å…·è°ƒç”¨ã€æ•°æ®è®¿é—® | å®æ—¶UIäº¤äº’ | è·¨ç³»ç»ŸAgentåä½œ |
| **é€šä¿¡åè®®** | JSON-RPC | äº‹ä»¶æµ (SSE) | JSON-RPC 2.0 |
| **çŠ¶æ€ç®¡ç†** | æ— çŠ¶æ€ | å…±äº«çŠ¶æ€ | ä»»åŠ¡çŠ¶æ€ |
| **æµå¼æ”¯æŒ** | âœ… | âœ… åŸç”Ÿ | âœ… (SSE) |
| **äººæœºäº¤äº’** | å·¥å…·çº§åˆ« | åŸç”Ÿæ”¯æŒ | é€šè¿‡ä»»åŠ¡ |
| **å¤šæ¨¡æ€** | å·¥å…·è¿”å› | åŸç”Ÿæ”¯æŒ | æ–‡ä»¶/åª’ä½“ |

**é€‰å‹å»ºè®®**ï¼š

- **æ„å»ºèŠå¤©UI**ï¼šä½¿ç”¨ AG-UI
- **è°ƒç”¨å¤–éƒ¨å·¥å…·**ï¼šä½¿ç”¨ MCP
- **å¤šAgentåä½œ**ï¼šä½¿ç”¨ A2A

---

### 4.2 AG-UI åè®®è¯¦è§£

AG-UI (Agent-User Interaction Protocol) æ˜¯ä¸€ä¸ªå¼€æ”¾ã€è½»é‡çº§ã€åŸºäºäº‹ä»¶çš„åè®®ï¼Œæ ‡å‡†åŒ– AI Agent ä¸ç”¨æˆ·ç•Œé¢ä¹‹é—´çš„è¿æ¥ã€‚

#### 4.2.1 æ ¸å¿ƒæ¦‚å¿µ

**äº‹ä»¶ç±»å‹**ï¼š

| ç±»åˆ« | äº‹ä»¶ | ç”¨é€” |
|------|------|------|
| **ç”Ÿå‘½å‘¨æœŸ** | RunStarted, RunFinished, RunError | ç›‘æ§Agentè¿è¡ŒçŠ¶æ€ |
| **æ–‡æœ¬æ¶ˆæ¯** | TextMessageStart â†’ Content â†’ End | æµå¼æ–‡æœ¬ä¼ è¾“ |
| **å·¥å…·è°ƒç”¨** | ToolCallStart â†’ Args â†’ End â†’ Result | å·¥å…·æ‰§è¡Œè¿½è¸ª |
| **çŠ¶æ€ç®¡ç†** | StateSnapshot, StateDelta | å‰åç«¯çŠ¶æ€åŒæ­¥ |

**æµå¼æ¨¡å¼**ï¼š

```
# æ–‡æœ¬æ¶ˆæ¯æµå¼ä¼ è¾“
TextMessageStart(messageId, role)
    â†’ TextMessageContent(delta="Hello")
    â†’ TextMessageContent(delta=" World")
    â†’ TextMessageEnd()

# å·¥å…·è°ƒç”¨æµå¼ä¼ è¾“
ToolCallStart(toolCallId, toolCallName)
    â†’ ToolCallArgs(delta='{"query":')
    â†’ ToolCallArgs(delta='"weather"}')
    â†’ ToolCallEnd()
    â†’ ToolCallResult(content="æ™´å¤©, 25Â°C")
```

#### 4.2.2 Python SDK ä½¿ç”¨

**å®‰è£…**ï¼š

```bash
pip install ag-ui-core
```

**åŸºç¡€äº‹ä»¶å®šä¹‰**ï¼š

```python
from dataclasses import dataclass
from typing import Optional, Literal
from enum import Enum

class EventType(str, Enum):
    """AG-UI äº‹ä»¶ç±»å‹"""
    RUN_STARTED = "RUN_STARTED"
    RUN_FINISHED = "RUN_FINISHED"
    RUN_ERROR = "RUN_ERROR"
    TEXT_MESSAGE_START = "TEXT_MESSAGE_START"
    TEXT_MESSAGE_CONTENT = "TEXT_MESSAGE_CONTENT"
    TEXT_MESSAGE_END = "TEXT_MESSAGE_END"
    TOOL_CALL_START = "TOOL_CALL_START"
    TOOL_CALL_ARGS = "TOOL_CALL_ARGS"
    TOOL_CALL_END = "TOOL_CALL_END"
    TOOL_CALL_RESULT = "TOOL_CALL_RESULT"
    STATE_SNAPSHOT = "STATE_SNAPSHOT"
    STATE_DELTA = "STATE_DELTA"

@dataclass
class BaseEvent:
    """åŸºç¡€äº‹ä»¶"""
    type: EventType
    timestamp: Optional[str] = None

@dataclass
class TextMessageContentEvent(BaseEvent):
    """æ–‡æœ¬å†…å®¹äº‹ä»¶"""
    message_id: str
    delta: str  # å¢é‡æ–‡æœ¬

@dataclass
class ToolCallStartEvent(BaseEvent):
    """å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶"""
    tool_call_id: str
    tool_call_name: str
```

#### 4.2.3 ä¸ LangGraph é›†æˆ

**åˆ›å»º AG-UI å…¼å®¹çš„ Agent æœåŠ¡å™¨**ï¼š

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langgraph.graph import StateGraph, MessagesState
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
import json
import asyncio
from typing import AsyncGenerator

app = FastAPI()

# åˆ›å»º LangGraph Agent
def create_chat_agent():
    model = ChatOpenAI(model="gpt-4o-mini")

    def chat_node(state: MessagesState):
        response = model.invoke(state["messages"])
        return {"messages": [response]}

    builder = StateGraph(MessagesState)
    builder.add_node("chat", chat_node)
    builder.set_entry_point("chat")
    builder.set_finish_point("chat")

    return builder.compile()

agent = create_chat_agent()

async def generate_agui_events(
    user_message: str,
    thread_id: str
) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆ AG-UI å…¼å®¹çš„äº‹ä»¶æµ"""
    import uuid

    run_id = str(uuid.uuid4())
    message_id = str(uuid.uuid4())

    # 1. å‘é€ RUN_STARTED
    yield f"data: {json.dumps({
        'type': 'RUN_STARTED',
        'threadId': thread_id,
        'runId': run_id
    })}\n\n"

    # 2. å‘é€ TEXT_MESSAGE_START
    yield f"data: {json.dumps({
        'type': 'TEXT_MESSAGE_START',
        'messageId': message_id,
        'role': 'assistant'
    })}\n\n"

    # 3. æµå¼è¾“å‡ºå†…å®¹
    config = {"configurable": {"thread_id": thread_id}}

    async for event in agent.astream_events(
        {"messages": [{"role": "user", "content": user_message}]},
        config=config,
        version="v2"
    ):
        if event["event"] == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            if hasattr(chunk, "content") and chunk.content:
                yield f"data: {json.dumps({
                    'type': 'TEXT_MESSAGE_CONTENT',
                    'messageId': message_id,
                    'delta': chunk.content
                })}\n\n"
                await asyncio.sleep(0.01)  # æ§åˆ¶æµé€Ÿ

    # 4. å‘é€ TEXT_MESSAGE_END
    yield f"data: {json.dumps({
        'type': 'TEXT_MESSAGE_END',
        'messageId': message_id
    })}\n\n"

    # 5. å‘é€ RUN_FINISHED
    yield f"data: {json.dumps({
        'type': 'RUN_FINISHED',
        'threadId': thread_id,
        'runId': run_id
    })}\n\n"

@app.post("/chat")
async def chat(request: dict):
    """AG-UI å…¼å®¹çš„èŠå¤©ç«¯ç‚¹"""
    user_message = request.get("message", "")
    thread_id = request.get("threadId", "default")

    return StreamingResponse(
        generate_agui_events(user_message, thread_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

**å‰ç«¯æ¶ˆè´¹ç¤ºä¾‹ (JavaScript)**ï¼š

```javascript
async function chat(message, threadId) {
    const response = await fetch('/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message, threadId })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const lines = decoder.decode(value).split('\n');
        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const event = JSON.parse(line.slice(6));
                handleAGUIEvent(event);
            }
        }
    }
}

function handleAGUIEvent(event) {
    switch (event.type) {
        case 'RUN_STARTED':
            console.log('Agent å¼€å§‹è¿è¡Œ:', event.runId);
            break;
        case 'TEXT_MESSAGE_CONTENT':
            // è¿½åŠ æ–‡æœ¬åˆ° UI
            appendText(event.delta);
            break;
        case 'TOOL_CALL_START':
            showToolCallIndicator(event.toolCallName);
            break;
        case 'RUN_FINISHED':
            console.log('Agent è¿è¡Œå®Œæˆ');
            break;
        case 'RUN_ERROR':
            showError(event.message);
            break;
    }
}
```

---

### 4.3 A2A åè®®è¯¦è§£

A2A (Agent-to-Agent Protocol) æ˜¯ Google è´¡çŒ®çš„å¼€æ”¾åè®®ï¼Œç°ä¸º Linux åŸºé‡‘ä¼šé¡¹ç›®ï¼Œå®ç°ä¸åŒ AI Agent ä¹‹é—´çš„é€šä¿¡å’Œäº’æ“ä½œã€‚

#### 4.3.1 æ ¸å¿ƒæ¦‚å¿µ

**è®¾è®¡ç›®æ ‡**ï¼š

1. **æ‰“ç ´å­¤å²›**ï¼šè¿æ¥ä¸åŒç”Ÿæ€ç³»ç»Ÿä¸­çš„ Agent
2. **å¯ç”¨å¤æ‚åä½œ**ï¼šä¸“ä¸šåŒ– Agent ååŒå¤„ç†å¤æ‚ä»»åŠ¡
3. **ä¿æŒä¸é€æ˜æ€§**ï¼šåä½œæ—¶æ— éœ€æš´éœ²å†…éƒ¨çŠ¶æ€å’Œå®ç°

**æ ¸å¿ƒç»„ä»¶**ï¼š

| ç»„ä»¶ | æè¿° |
|------|------|
| **Agent Card** | æè¿° Agent èƒ½åŠ›å’Œè¿æ¥ä¿¡æ¯çš„å…ƒæ•°æ® |
| **Task** | ä¸€æ¬¡å®Œæ•´çš„ Agent æ‰§è¡Œå•å…ƒ |
| **Context** | è·¨å¤šä¸ª Task çš„å¯¹è¯çº¿ç¨‹ |
| **Message** | Agent é—´äº¤æ¢çš„æ¶ˆæ¯ |

**Agent Card ç¤ºä¾‹**ï¼š

```json
{
    "name": "Weather Agent",
    "description": "æä¾›å…¨çƒå¤©æ°”æŸ¥è¯¢æœåŠ¡",
    "version": "1.0.0",
    "capabilities": ["weather_query", "forecast"],
    "endpoint": "https://weather-agent.example.com/a2a",
    "authentication": {
        "type": "bearer",
        "token_url": "https://auth.example.com/token"
    },
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"},
            "date": {"type": "string", "format": "date"}
        }
    }
}
```

#### 4.3.2 FastA2A + Pydantic AI å®ç°

Pydantic AI é€šè¿‡ FastA2A åº“æä¾›äº† A2A åè®®çš„å®Œæ•´å®ç°ï¼Œè¿™æ˜¯ç›®å‰æœ€ç®€æ´çš„ A2A å®ç°æ–¹å¼ã€‚

**å®‰è£…**ï¼š

```bash
# æ–¹å¼1: å•ç‹¬å®‰è£… FastA2A
pip install fasta2a

# æ–¹å¼2: é€šè¿‡ Pydantic AI å®‰è£…
pip install 'pydantic-ai-slim[a2a]'
```

**æœ€ç®€å®ç° - ä¸€è¡Œä»£ç æš´éœ² A2A æœåŠ¡**ï¼š

```python
from pydantic_ai import Agent

# å®šä¹‰ Agent
agent = Agent(
    'openai:gpt-4o',
    instructions='ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚'
)

# ä¸€è¡Œä»£ç è½¬æ¢ä¸º A2A æœåŠ¡å™¨
app = agent.to_a2a()

# è¿è¡Œ: uvicorn main:app --host 0.0.0.0 --port 8000
```

**å®Œæ•´ç¤ºä¾‹ - å¸¦å·¥å…·çš„ A2A Agent**ï¼š

```python
from pydantic_ai import Agent, RunContext
from pydantic import BaseModel
from datetime import date
import httpx

# å®šä¹‰è¾“å‡ºç»“æ„
class WeatherResponse(BaseModel):
    location: str
    temperature: float
    condition: str
    humidity: int

# åˆ›å»º Agent
weather_agent = Agent(
    'openai:gpt-4o',
    instructions='''ä½ æ˜¯å¤©æ°”æŸ¥è¯¢åŠ©æ‰‹ã€‚
    å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ï¼Œä½¿ç”¨ get_weather å·¥å…·è·å–ä¿¡æ¯ã€‚
    å§‹ç»ˆç”¨ä¸­æ–‡å›å¤ã€‚''',
    output_type=WeatherResponse
)

@weather_agent.tool
async def get_weather(ctx: RunContext, location: str) -> dict:
    """è·å–æŒ‡å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯

    Args:
        location: åŸå¸‚åç§°ï¼Œå¦‚ "åŒ—äº¬"ã€"ä¸Šæµ·"
    """
    # å®é™…åº”ç”¨ä¸­è°ƒç”¨å¤©æ°” API
    # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    weather_data = {
        "åŒ—äº¬": {"temp": 22, "condition": "æ™´", "humidity": 45},
        "ä¸Šæµ·": {"temp": 26, "condition": "å¤šäº‘", "humidity": 65},
        "å¹¿å·": {"temp": 30, "condition": "é˜µé›¨", "humidity": 80},
    }

    data = weather_data.get(location, {"temp": 20, "condition": "æœªçŸ¥", "humidity": 50})
    return {
        "location": location,
        "temperature": data["temp"],
        "condition": data["condition"],
        "humidity": data["humidity"]
    }

# è½¬æ¢ä¸º A2A åº”ç”¨
app = weather_agent.to_a2a()
```

**è¿è¡Œå’Œæµ‹è¯•**ï¼š

```bash
# å¯åŠ¨æœåŠ¡
uvicorn weather_agent:app --host 0.0.0.0 --port 8000

# æµ‹è¯• Agent Card ç«¯ç‚¹
curl http://localhost:8000/.well-known/agent.json

# å‘é€ä»»åŠ¡è¯·æ±‚
curl -X POST http://localhost:8000/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "message": {
      "role": "user",
      "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    }
  }'
```

#### 4.3.3 è‡ªå®šä¹‰ FastA2A é…ç½®

**é…ç½®å­˜å‚¨å’Œä»»åŠ¡é˜Ÿåˆ—**ï¼š

```python
from fasta2a import FastA2A, InMemoryStorage, InMemoryBroker
from pydantic_ai import Agent

agent = Agent('openai:gpt-4o', instructions='ä½ æ˜¯åŠ©æ‰‹')

# è‡ªå®šä¹‰é…ç½®
storage = InMemoryStorage()  # ç”Ÿäº§ç¯å¢ƒå¯ç”¨ PostgresStorage, RedisStorage
broker = InMemoryBroker()    # ç”Ÿäº§ç¯å¢ƒå¯ç”¨ RedisBroker

# åˆ›å»º FastA2A åº”ç”¨
a2a_app = FastA2A(
    agent=agent,
    storage=storage,
    broker=broker,
    name="My Custom Agent",
    description="è‡ªå®šä¹‰ A2A Agent æœåŠ¡",
    version="1.0.0"
)

app = a2a_app.app
```

**å¤š Agent A2A åä½œ**ï¼š

```python
from pydantic_ai import Agent
from httpx import AsyncClient

# Agent 1: ç ”ç©¶åŠ©æ‰‹
research_agent = Agent(
    'openai:gpt-4o',
    instructions='ä½ æ˜¯ç ”ç©¶åŠ©æ‰‹ï¼Œè´Ÿè´£æ”¶é›†å’Œæ•´ç†ä¿¡æ¯ã€‚'
)

# Agent 2: å†™ä½œåŠ©æ‰‹
writing_agent = Agent(
    'openai:gpt-4o',
    instructions='ä½ æ˜¯å†™ä½œåŠ©æ‰‹ï¼Œæ ¹æ®ç ”ç©¶ç»“æœæ’°å†™å†…å®¹ã€‚'
)

@writing_agent.tool
async def consult_research_agent(ctx: RunContext, query: str) -> str:
    """å’¨è¯¢ç ”ç©¶åŠ©æ‰‹è·å–ä¿¡æ¯

    Args:
        query: éœ€è¦ç ”ç©¶çš„é—®é¢˜
    """
    async with AsyncClient() as client:
        # è°ƒç”¨ç ”ç©¶ Agent çš„ A2A ç«¯ç‚¹
        response = await client.post(
            "http://research-agent:8001/tasks",
            json={
                "message": {
                    "role": "user",
                    "content": query
                }
            }
        )
        result = response.json()
        return result.get("output", "æ— æ³•è·å–ç ”ç©¶ç»“æœ")

# å„è‡ªæš´éœ²ä¸º A2A æœåŠ¡
research_app = research_agent.to_a2a()
writing_app = writing_agent.to_a2a()
```

#### 4.3.4 A2A ä¸ LangGraph é›†æˆ

**å°† LangGraph Agent æš´éœ²ä¸º A2A æœåŠ¡**ï¼š

```python
from fastapi import FastAPI
from pydantic import BaseModel
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.memory import InMemorySaver
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from typing import Optional
import uuid

app = FastAPI()

# åˆ›å»º LangGraph Agent
def build_agent():
    model = ChatOpenAI(model="gpt-4o-mini")

    def agent_node(state: MessagesState):
        response = model.invoke(state["messages"])
        return {"messages": [response]}

    builder = StateGraph(MessagesState)
    builder.add_node("agent", agent_node)
    builder.set_entry_point("agent")
    builder.set_finish_point("agent")

    return builder.compile(checkpointer=InMemorySaver())

agent = build_agent()

# A2A Agent Card
@app.get("/.well-known/agent.json")
async def get_agent_card():
    return {
        "name": "LangGraph Assistant",
        "description": "åŸºäº LangGraph æ„å»ºçš„æ™ºèƒ½åŠ©æ‰‹",
        "version": "1.0.0",
        "capabilities": ["chat", "reasoning"],
        "endpoint": "/tasks"
    }

# A2A ä»»åŠ¡è¯·æ±‚æ¨¡å‹
class TaskRequest(BaseModel):
    message: dict
    context_id: Optional[str] = None

class TaskResponse(BaseModel):
    task_id: str
    context_id: str
    output: str
    status: str

# A2A ä»»åŠ¡ç«¯ç‚¹
@app.post("/tasks", response_model=TaskResponse)
async def create_task(request: TaskRequest):
    task_id = str(uuid.uuid4())
    context_id = request.context_id or str(uuid.uuid4())

    # æ‰§è¡Œ LangGraph Agent
    config = {"configurable": {"thread_id": context_id}}

    result = await agent.ainvoke(
        {"messages": [request.message]},
        config=config
    )

    # æå–è¾“å‡º
    last_message = result["messages"][-1]
    output = last_message.content if hasattr(last_message, "content") else str(last_message)

    return TaskResponse(
        task_id=task_id,
        context_id=context_id,
        output=output,
        status="completed"
    )
```

---

## ç¬¬5ç« ï¼šå¤š Agent é«˜çº§æ¨¡å¼

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ Handoffs å’Œ Skills ä¸¤ç§é«˜çº§å¤š Agent åä½œæ¨¡å¼ã€‚

æœ¬ç« è¡¥å……ç¬¬1ç« æœªè¦†ç›–çš„ä¸¤ç§é‡è¦å¤š Agent æ¨¡å¼ï¼Œä¸å·²æœ‰çš„ Supervisor-Worker æ¨¡å¼å½¢æˆå®Œæ•´çš„æ¨¡å¼ä½“ç³»ã€‚

### 5.1 æ¨¡å¼å¯¹æ¯”ä¸é€‰å‹

| æ¨¡å¼ | æ§åˆ¶æ–¹å¼ | çŠ¶æ€å…±äº« | å¹¶è¡Œèƒ½åŠ› | ç”¨æˆ·äº¤äº’ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|---------|---------|
| **Supervisor-Worker** | é›†ä¸­å¼ | ç»ç›‘ç£è€… | å¯å¹¶è¡Œ | ä½ | ä»»åŠ¡åˆ†è§£ã€æ‰¹é‡å¤„ç† |
| **Subagents** | å·¥å…·è°ƒç”¨ | éš”ç¦» | é«˜å¹¶è¡Œ | ä½ | ç‹¬ç«‹å­ä»»åŠ¡ã€æ¨¡å—åŒ– |
| **Handoffs** | çŠ¶æ€è½¬ç§» | ç›´æ¥ä¼ é€’ | é¡ºåº | é«˜ | ä¸“ä¸šå‡çº§ã€å®¢æœç³»ç»Ÿ |
| **Skills** | å•Agent | å…¨å±€ | ä¸­ç­‰ | é«˜ | å¤šé¢†åŸŸé—®ç­”ã€çŸ¥è¯†åˆ‡æ¢ |
| **Router** | åˆ†ç±»è·¯ç”± | éš”ç¦» | é«˜å¹¶è¡Œ | ä¸­ç­‰ | æ„å›¾åˆ†ç±»ã€è´Ÿè½½åˆ†å‘ |

**é€‰å‹å†³ç­–æ ‘**ï¼š

```
éœ€è¦å¤š Agent åä½œï¼Ÿ
â”œâ”€â”€ å¦ â†’ å• Agent + å·¥å…·
â””â”€â”€ æ˜¯ â†’ Agent é—´éœ€è¦å…±äº«å®Œæ•´ä¸Šä¸‹æ–‡ï¼Ÿ
    â”œâ”€â”€ å¦ â†’ ä»»åŠ¡å¯å¹¶è¡Œï¼Ÿ
    â”‚   â”œâ”€â”€ æ˜¯ â†’ Subagents æˆ– Router
    â”‚   â””â”€â”€ å¦ â†’ Supervisor-Worker
    â””â”€â”€ æ˜¯ â†’ éœ€è¦ä¸“ä¸šå‡çº§/è½¬æ¥ï¼Ÿ
        â”œâ”€â”€ æ˜¯ â†’ Handoffs
        â””â”€â”€ å¦ â†’ Skills
```

---

### 5.2 Handoffs æ¨¡å¼è¯¦è§£

Handoffsï¼ˆäº¤æ¥ï¼‰æ¨¡å¼é€šè¿‡çŠ¶æ€è½¬ç§»å®ç° Agent é—´çš„æ§åˆ¶æƒä¼ é€’ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦"ä¸“ä¸šå‡çº§"çš„åœºæ™¯ã€‚

#### 5.2.1 æ ¸å¿ƒæ¦‚å¿µ

**å·¥ä½œåŸç†**ï¼š

```
ç”¨æˆ· â†’ Agent A (é€šç”¨) â†’ æ£€æµ‹åˆ°ä¸“ä¸šé—®é¢˜ â†’ Handoff â†’ Agent B (ä¸“å®¶)
                                                      â†“
ç”¨æˆ· â† æœ€ç»ˆå›å¤ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ Supervisor çš„åŒºåˆ«**ï¼š

| ç‰¹æ€§ | Supervisor | Handoffs |
|------|-----------|----------|
| æ§åˆ¶æµ | ä¸­å¤®é›†ä¸­ | åˆ†å¸ƒå¼ä¼ é€’ |
| çŠ¶æ€ | ç»ç›‘ç£è€… | ç›´æ¥ä¼ é€’ |
| å›é€€ | å›åˆ°ç›‘ç£è€… | å¯å›é€€åˆ°å‰ä¸€ä¸ª |
| é€‚ç”¨ | ä»»åŠ¡åˆ†è§£ | ä¸“ä¸šå‡çº§ |

#### 5.2.2 LangGraph å®ç°

```python
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.types import Command
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from typing import Literal, TypedDict
from dataclasses import dataclass

# å®šä¹‰çŠ¶æ€
class HandoffState(MessagesState):
    current_agent: str
    handoff_reason: str | None

# å®šä¹‰ Handoff å‘½ä»¤
@dataclass
class Handoff:
    target: str
    reason: str

# é€šç”¨å®¢æœ Agent
def general_support_agent(state: HandoffState) -> Command[Literal["technical", "billing", "__end__"]]:
    """é€šç”¨å®¢æœ Agentï¼Œå¤„ç†ç®€å•é—®é¢˜æˆ–è½¬æ¥ä¸“å®¶"""
    model = ChatOpenAI(model="gpt-4o-mini")

    system_prompt = """ä½ æ˜¯é€šç”¨å®¢æœåŠ©æ‰‹ã€‚

    èŒè´£ï¼š
    1. å¤„ç†ç®€å•çš„é—®å€™å’Œé€šç”¨é—®é¢˜
    2. è¯†åˆ«éœ€è¦ä¸“ä¸šå¸®åŠ©çš„é—®é¢˜å¹¶è½¬æ¥

    è½¬æ¥è§„åˆ™ï¼š
    - æŠ€æœ¯é—®é¢˜ï¼ˆä»£ç ã€APIã€é”™è¯¯ï¼‰â†’ å›å¤ [HANDOFF:technical]
    - è´¦å•é—®é¢˜ï¼ˆä»˜æ¬¾ã€é€€æ¬¾ã€è®¢é˜…ï¼‰â†’ å›å¤ [HANDOFF:billing]
    - å…¶ä»–é—®é¢˜ â†’ ç›´æ¥å›ç­”
    """

    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    response = model.invoke(messages)
    content = response.content

    # æ£€æµ‹æ˜¯å¦éœ€è¦è½¬æ¥
    if "[HANDOFF:technical]" in content:
        return Command(
            goto="technical",
            update={
                "messages": [AIMessage(content="æ­£åœ¨ä¸ºæ‚¨è½¬æ¥æŠ€æœ¯ä¸“å®¶...")],
                "current_agent": "technical",
                "handoff_reason": "æŠ€æœ¯é—®é¢˜"
            }
        )
    elif "[HANDOFF:billing]" in content:
        return Command(
            goto="billing",
            update={
                "messages": [AIMessage(content="æ­£åœ¨ä¸ºæ‚¨è½¬æ¥è´¦åŠ¡ä¸“å®¶...")],
                "current_agent": "billing",
                "handoff_reason": "è´¦åŠ¡é—®é¢˜"
            }
        )
    else:
        return Command(
            goto="__end__",
            update={
                "messages": [response],
                "current_agent": "general"
            }
        )

# æŠ€æœ¯æ”¯æŒ Agent
def technical_support_agent(state: HandoffState) -> Command[Literal["general", "__end__"]]:
    """æŠ€æœ¯æ”¯æŒä¸“å®¶"""
    model = ChatOpenAI(model="gpt-4o")

    system_prompt = """ä½ æ˜¯æŠ€æœ¯æ”¯æŒä¸“å®¶ï¼Œæ“…é•¿ï¼š
    - ä»£ç è°ƒè¯•å’Œé”™è¯¯æ’æŸ¥
    - API ä½¿ç”¨æŒ‡å¯¼
    - ç³»ç»Ÿæ¶æ„å»ºè®®

    å¦‚æœé—®é¢˜ä¸å±äºæŠ€æœ¯èŒƒç•´ï¼Œå›å¤ [HANDOFF:general] è½¬å›é€šç”¨å®¢æœã€‚
    """

    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    response = model.invoke(messages)

    if "[HANDOFF:general]" in response.content:
        return Command(
            goto="general",
            update={
                "messages": [AIMessage(content="è¿™ä¸ªé—®é¢˜æˆ‘æ¥å¸®æ‚¨è½¬æ¥é€šç”¨å®¢æœ...")],
                "current_agent": "general"
            }
        )

    return Command(
        goto="__end__",
        update={"messages": [response]}
    )

# è´¦åŠ¡æ”¯æŒ Agent
def billing_support_agent(state: HandoffState) -> Command[Literal["general", "__end__"]]:
    """è´¦åŠ¡æ”¯æŒä¸“å®¶"""
    model = ChatOpenAI(model="gpt-4o")

    system_prompt = """ä½ æ˜¯è´¦åŠ¡æ”¯æŒä¸“å®¶ï¼Œæ“…é•¿ï¼š
    - è´¦å•æŸ¥è¯¢å’Œè§£é‡Š
    - é€€æ¬¾å¤„ç†æµç¨‹
    - è®¢é˜…ç®¡ç†

    å¦‚æœé—®é¢˜ä¸å±äºè´¦åŠ¡èŒƒç•´ï¼Œå›å¤ [HANDOFF:general] è½¬å›é€šç”¨å®¢æœã€‚
    """

    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    response = model.invoke(messages)

    if "[HANDOFF:general]" in response.content:
        return Command(
            goto="general",
            update={
                "messages": [AIMessage(content="è¿™ä¸ªé—®é¢˜æˆ‘æ¥å¸®æ‚¨è½¬æ¥é€šç”¨å®¢æœ...")],
                "current_agent": "general"
            }
        )

    return Command(
        goto="__end__",
        update={"messages": [response]}
    )

# æ„å»ºå·¥ä½œæµ
def build_handoff_workflow():
    builder = StateGraph(HandoffState)

    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("general", general_support_agent)
    builder.add_node("technical", technical_support_agent)
    builder.add_node("billing", billing_support_agent)

    # è®¾ç½®å…¥å£
    builder.set_entry_point("general")

    return builder.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    workflow = build_handoff_workflow()

    # æµ‹è¯•æŠ€æœ¯é—®é¢˜
    result = await workflow.ainvoke({
        "messages": [HumanMessage(content="æˆ‘çš„ API è°ƒç”¨è¿”å› 401 é”™è¯¯ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ")],
        "current_agent": "general",
        "handoff_reason": None
    })

    for msg in result["messages"]:
        print(f"{msg.__class__.__name__}: {msg.content}")
```

---

### 5.3 Skills æ¨¡å¼è¯¦è§£

Skillsï¼ˆæŠ€èƒ½ï¼‰æ¨¡å¼è®©å•ä¸ª Agent åŠ¨æ€åŠ è½½ä¸åŒçš„ä¸“ä¸šèƒ½åŠ›ï¼Œæ— éœ€åˆ›å»ºå¤šä¸ª Agent å®ä¾‹ã€‚

#### 5.3.1 æ ¸å¿ƒæ¦‚å¿µ

**å·¥ä½œåŸç†**ï¼š

```
ç”¨æˆ·é—®é¢˜ â†’ Agent â†’ è¯†åˆ«é¢†åŸŸ â†’ åŠ è½½å¯¹åº” Skill â†’ ä½¿ç”¨ä¸“ä¸š prompt/å·¥å…· â†’ å›å¤
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
              â†“       â†“       â†“
           SQLæŠ€èƒ½  PythonæŠ€èƒ½  é€šç”¨æŠ€èƒ½
```

**ä¼˜åŠ¿**ï¼š

- å‡å°‘ Agent å®ä¾‹å’Œé€šä¿¡å¼€é”€
- ä¸Šä¸‹æ–‡å®Œå…¨å…±äº«
- åˆ‡æ¢é€Ÿåº¦å¿«

#### 5.3.2 å®ç°æ–¹å¼

```python
from langgraph.graph import StateGraph, MessagesState, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from typing import TypedDict, Callable
from dataclasses import dataclass

# å®šä¹‰æŠ€èƒ½
@dataclass
class Skill:
    name: str
    description: str
    system_prompt: str
    tools: list[Callable]

# SQL æŠ€èƒ½
@tool
def execute_sql(query: str) -> str:
    """æ‰§è¡Œ SQL æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…åº”ç”¨ä¸­è¿æ¥æ•°æ®åº“
    return f"æ‰§è¡ŒæŸ¥è¯¢: {query}\nç»“æœ: [æ¨¡æ‹Ÿæ•°æ®]"

sql_skill = Skill(
    name="sql",
    description="SQL æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æ",
    system_prompt="""ä½ æ˜¯ SQL ä¸“å®¶ã€‚
    - å¸®åŠ©ç”¨æˆ·ç¼–å†™å’Œä¼˜åŒ– SQL æŸ¥è¯¢
    - è§£é‡ŠæŸ¥è¯¢ç»“æœ
    - ä½¿ç”¨ execute_sql å·¥å…·æ‰§è¡ŒæŸ¥è¯¢""",
    tools=[execute_sql]
)

# Python æŠ€èƒ½
@tool
def execute_python(code: str) -> str:
    """æ‰§è¡Œ Python ä»£ç ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    # å®é™…åº”ç”¨ä¸­ä½¿ç”¨æ²™ç®±æ‰§è¡Œ
    return f"æ‰§è¡Œä»£ç :\n{code}\nè¾“å‡º: [æ¨¡æ‹Ÿç»“æœ]"

python_skill = Skill(
    name="python",
    description="Python ç¼–ç¨‹å’Œæ•°æ®å¤„ç†",
    system_prompt="""ä½ æ˜¯ Python ä¸“å®¶ã€‚
    - å¸®åŠ©ç”¨æˆ·ç¼–å†™ Python ä»£ç 
    - è§£é‡Šä»£ç é€»è¾‘
    - ä½¿ç”¨ execute_python å·¥å…·è¿è¡Œä»£ç """,
    tools=[execute_python]
)

# é€šç”¨æŠ€èƒ½
general_skill = Skill(
    name="general",
    description="é€šç”¨é—®ç­”å’Œé—²èŠ",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€‚",
    tools=[]
)

# æŠ€èƒ½æ³¨å†Œè¡¨
SKILLS = {
    "sql": sql_skill,
    "python": python_skill,
    "general": general_skill
}

# çŠ¶æ€å®šä¹‰
class SkillState(MessagesState):
    current_skill: str
    skill_history: list[str]

# æŠ€èƒ½è·¯ç”±å™¨
def skill_router(state: SkillState) -> str:
    """æ ¹æ®ç”¨æˆ·è¾“å…¥é€‰æ‹©åˆé€‚çš„æŠ€èƒ½"""
    model = ChatOpenAI(model="gpt-4o-mini")

    last_message = state["messages"][-1].content

    router_prompt = f"""åˆ†æç”¨æˆ·é—®é¢˜ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æŠ€èƒ½ã€‚

å¯ç”¨æŠ€èƒ½ï¼š
- sql: SQL æ•°æ®åº“ç›¸å…³é—®é¢˜
- python: Python ç¼–ç¨‹ç›¸å…³é—®é¢˜
- general: å…¶ä»–é€šç”¨é—®é¢˜

ç”¨æˆ·é—®é¢˜: {last_message}

åªå›å¤æŠ€èƒ½åç§°ï¼ˆsql/python/generalï¼‰ï¼š"""

    response = model.invoke([{"role": "user", "content": router_prompt}])
    skill_name = response.content.strip().lower()

    return skill_name if skill_name in SKILLS else "general"

# æŠ€èƒ½æ‰§è¡Œå™¨
def skill_executor(state: SkillState) -> dict:
    """ä½¿ç”¨å½“å‰æŠ€èƒ½å¤„ç†ç”¨æˆ·è¯·æ±‚"""
    skill = SKILLS[state["current_skill"]]

    # åˆ›å»ºå¸¦å·¥å…·çš„æ¨¡å‹
    model = ChatOpenAI(model="gpt-4o-mini")
    if skill.tools:
        model = model.bind_tools(skill.tools)

    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=skill.system_prompt),
        *state["messages"]
    ]

    # æ‰§è¡Œ
    response = model.invoke(messages)

    # å¤„ç†å·¥å…·è°ƒç”¨
    if hasattr(response, "tool_calls") and response.tool_calls:
        tool_results = []
        for tool_call in response.tool_calls:
            tool_fn = next(t for t in skill.tools if t.name == tool_call["name"])
            result = tool_fn.invoke(tool_call["args"])
            tool_results.append(f"å·¥å…· {tool_call['name']} ç»“æœ: {result}")

        # å†æ¬¡è°ƒç”¨è·å–æœ€ç»ˆå›å¤
        messages.append(response)
        messages.append(HumanMessage(content="\n".join(tool_results)))
        response = model.invoke(messages)

    return {
        "messages": [response],
        "skill_history": state.get("skill_history", []) + [state["current_skill"]]
    }

# æ„å»ºå·¥ä½œæµ
def build_skill_workflow():
    builder = StateGraph(SkillState)

    # è·¯ç”±èŠ‚ç‚¹
    def route_and_update(state: SkillState) -> dict:
        selected_skill = skill_router(state)
        return {"current_skill": selected_skill}

    builder.add_node("router", route_and_update)
    builder.add_node("executor", skill_executor)

    # è¾¹
    builder.set_entry_point("router")
    builder.add_edge("router", "executor")
    builder.add_edge("executor", END)

    return builder.compile()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    workflow = build_skill_workflow()

    # æµ‹è¯• SQL é—®é¢˜
    result = await workflow.ainvoke({
        "messages": [HumanMessage(content="å¦‚ä½•æŸ¥è¯¢ç”¨æˆ·è¡¨ä¸­å¹´é¾„å¤§äº 25 çš„è®°å½•ï¼Ÿ")],
        "current_skill": "",
        "skill_history": []
    })

    print(f"ä½¿ç”¨æŠ€èƒ½: {result['current_skill']}")
    print(f"å›å¤: {result['messages'][-1].content}")

    # æµ‹è¯• Python é—®é¢˜
    result = await workflow.ainvoke({
        "messages": [HumanMessage(content="ç”¨ Python å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•")],
        "current_skill": "",
        "skill_history": []
    })

    print(f"ä½¿ç”¨æŠ€èƒ½: {result['current_skill']}")
    print(f"å›å¤: {result['messages'][-1].content}")
```

#### 5.3.3 é«˜çº§ Skills æ¨¡å¼ï¼šåŠ¨æ€æŠ€èƒ½æ³¨å†Œ

```python
from typing import Protocol, runtime_checkable
from abc import abstractmethod

@runtime_checkable
class SkillProtocol(Protocol):
    """æŠ€èƒ½åè®®"""
    name: str
    description: str

    @abstractmethod
    def get_system_prompt(self) -> str: ...

    @abstractmethod
    def get_tools(self) -> list: ...

class SkillRegistry:
    """æŠ€èƒ½æ³¨å†Œè¡¨"""

    def __init__(self):
        self._skills: dict[str, SkillProtocol] = {}

    def register(self, skill: SkillProtocol) -> None:
        """æ³¨å†ŒæŠ€èƒ½"""
        self._skills[skill.name] = skill

    def unregister(self, name: str) -> None:
        """æ³¨é”€æŠ€èƒ½"""
        self._skills.pop(name, None)

    def get(self, name: str) -> SkillProtocol | None:
        """è·å–æŠ€èƒ½"""
        return self._skills.get(name)

    def list_skills(self) -> list[dict]:
        """åˆ—å‡ºæ‰€æœ‰æŠ€èƒ½"""
        return [
            {"name": s.name, "description": s.description}
            for s in self._skills.values()
        ]

    def create_router_prompt(self) -> str:
        """ç”Ÿæˆè·¯ç”±æç¤ºè¯"""
        skills_desc = "\n".join(
            f"- {s.name}: {s.description}"
            for s in self._skills.values()
        )
        return f"""æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©æœ€åˆé€‚çš„æŠ€èƒ½ã€‚

å¯ç”¨æŠ€èƒ½ï¼š
{skills_desc}

åªå›å¤æŠ€èƒ½åç§°ï¼š"""

# å…¨å±€æ³¨å†Œè¡¨
skill_registry = SkillRegistry()

# æ³¨å†Œé»˜è®¤æŠ€èƒ½
skill_registry.register(sql_skill)
skill_registry.register(python_skill)
skill_registry.register(general_skill)
```

---

### æœ¬ç« å°ç»“

#### 5.4.1 æ ¸å¿ƒè¦ç‚¹

1. **AG-UI åè®®**ï¼šæ ‡å‡†åŒ– Agent ä¸ UI çš„äº¤äº’ï¼Œæ”¯æŒæµå¼ä¼ è¾“
2. **A2A åè®®**ï¼šå®ç°è·¨ç³»ç»Ÿ Agent åä½œï¼Œä¿æŠ¤å†…éƒ¨å®ç°
3. **Handoffs æ¨¡å¼**ï¼šçŠ¶æ€é©±åŠ¨çš„ä¸“ä¸šå‡çº§è½¬æ¥
4. **Skills æ¨¡å¼**ï¼šå• Agent åŠ¨æ€èƒ½åŠ›åˆ‡æ¢

#### 5.4.2 é€‰å‹å»ºè®®

| éœ€æ±‚ | æ¨èæ–¹æ¡ˆ |
|------|---------|
| æ„å»ºèŠå¤© UI | AG-UI + LangGraph |
| è·¨ç³»ç»Ÿ Agent è°ƒç”¨ | A2A (FastA2A) |
| å®¢æœä¸“ä¸šè½¬æ¥ | Handoffs |
| å¤šé¢†åŸŸé—®ç­” | Skills |
| ä»»åŠ¡å¹¶è¡Œå¤„ç† | Subagents æˆ– Router |

#### 5.4.3 å®æ–½æ¸…å•

- [ ] è¯„ä¼°æ˜¯å¦éœ€è¦è·¨ç³»ç»Ÿåä½œï¼ˆâ†’ A2Aï¼‰
- [ ] ç¡®å®š UI äº¤äº’éœ€æ±‚ï¼ˆâ†’ AG-UIï¼‰
- [ ] é€‰æ‹©åˆé€‚çš„å¤š Agent æ¨¡å¼
- [ ] å®ç°åè®®é€‚é…å±‚
- [ ] æ·»åŠ ç›‘æ§å’Œé”™è¯¯å¤„ç†

---

## æ€»ç»“

æœ¬ç¯‡æ¶µç›–äº† LangChain ç”Ÿæ€çš„é«˜çº§åº”ç”¨ï¼š

- **å¤š Agent ç³»ç»Ÿè®¾è®¡æ¨¡å¼**ï¼ˆSupervisor-Workerã€Subagentsã€Handoffsã€Skillsã€Routerï¼‰
- **å¤šæ¨¡æ€ AI åº”ç”¨å¼€å‘**
- **å®æ—¶æµå¼äº¤äº’å®ç°**
- **MCP åè®®é›†æˆ**ï¼ˆAgent â†” å·¥å…·/æ•°æ®ï¼‰
- **AG-UI åè®®é›†æˆ**ï¼ˆAgent â†” ç”¨æˆ·ç•Œé¢ï¼‰
- **A2A åè®®é›†æˆ**ï¼ˆAgent â†” Agent è·¨ç³»ç»Ÿåä½œï¼‰
- **è‡ªå®šä¹‰ç»„ä»¶å¼€å‘**

è¿™äº›é«˜çº§ç‰¹æ€§è®©ä½ èƒ½å¤Ÿæ„å»ºæ›´å¤æ‚ã€æ›´å¼ºå¤§çš„ AI åº”ç”¨ç³»ç»Ÿã€‚

---

**å‚è€ƒèµ„æº**ï¼š

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [MCP åè®®è§„èŒƒ](https://modelcontextprotocol.io)
- [AG-UI åè®®æ–‡æ¡£](https://docs.ag-ui.com)
- [A2A åè®®è§„èŒƒ](https://a2a-protocol.org)
- [Pydantic AI æ–‡æ¡£](https://ai.pydantic.dev)
- [FastA2A æ–‡æ¡£](https://ai.pydantic.dev/a2a/)
- [LangChain å¤šæ¨¡æ€æŒ‡å—](https://python.langchain.com/docs/use_cases/multimodal)