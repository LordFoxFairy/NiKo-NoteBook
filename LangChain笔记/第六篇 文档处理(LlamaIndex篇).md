# ç¬¬å…­ç¯‡è¡¥å……ï¼šæ–‡æ¡£å¤„ç†å·¥ç¨‹ (LlamaIndex)

> **ç‰ˆæœ¬ä¿¡æ¯**:
> - æ–‡æ¡£ç‰ˆæœ¬: v1.0
> - LlamaIndex: 0.14.8
> - llama-parse: 0.5.0+
> - Python: 3.10+
> - æ›´æ–°æ—¥æœŸ: 2025-11-29

---

## å‰è¨€

åœ¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿä¸­ï¼Œ**æ–‡æ¡£å¤„ç†è´¨é‡ç›´æ¥å†³å®šäº†æœ€ç»ˆæ•ˆæœ**ã€‚LlamaIndexä½œä¸ºä¸“ä¸ºLLMåº”ç”¨è®¾è®¡çš„æ•°æ®æ¡†æ¶ï¼Œæä¾›äº†å¼ºå¤§çš„æ–‡æ¡£å¤„ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å…¶æ——èˆ°äº§å“**LlamaParse**â€”â€”ä¸–ç•Œé¦–ä¸ªGenAIåŸç”Ÿæ–‡æ¡£è§£æå¹³å°ã€‚

**LlamaIndexæ–‡æ¡£å¤„ç†çš„æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **LlamaParse**ï¼šä¸“ä¸ºLLMä¼˜åŒ–çš„é«˜ç²¾åº¦PDFè§£æ
2. **LlamaHubç”Ÿæ€**ï¼š700+æ•°æ®åŠ è½½å™¨ï¼Œè¦†ç›–å‡ ä¹æ‰€æœ‰æ•°æ®æº
3. **SimpleDirectoryReader**ï¼šä¸€è¡Œä»£ç åŠ è½½å¤šç§æ ¼å¼
4. **æ™ºèƒ½Node Parser**ï¼šè¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æ¡£åˆ†å—
5. **å¤šæ¨¡æ€æ”¯æŒ**ï¼šå›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨çš„æ™ºèƒ½æå–

æœ¬ç¯‡å°†æ·±å…¥æ¢è®¨**LlamaIndexçš„æ–‡æ¡£å¤„ç†å®Œæ•´æ–¹æ¡ˆ**ï¼Œä»åŸºç¡€åŠ è½½åˆ°é«˜çº§è§£æï¼Œæ„å»ºé«˜è´¨é‡RAGç³»ç»Ÿã€‚

---

## å­¦ä¹ è·¯å¾„

```mermaid
graph LR
    A[SimpleDirectoryReader<br/>å¿«é€Ÿå…¥é—¨] --> B[LlamaHub<br/>700+åŠ è½½å™¨]
    B --> C[LlamaParse<br/>GenAIåŸç”Ÿè§£æ]
    C --> D[Node Parser<br/>æ™ºèƒ½åˆ†å—]
    D --> E[å¤šæ¨¡æ€å¤„ç†<br/>å›¾è¡¨æå–]

    style A fill:#e1f5e1
    style C fill:#fff4e1
    style E fill:#ffe1e1
```

**æœ¬ç¯‡è¦†ç›–å†…å®¹**ï¼š
- **ç¬¬1ç« **ï¼šLlamaIndexæ•°æ®åŠ è½½å™¨ç”Ÿæ€
- **ç¬¬2ç« **ï¼šLlamaParse - GenAIåŸç”ŸPDFè§£æ
- **ç¬¬3ç« **ï¼šNode Parser - æ™ºèƒ½åˆ†å—ç­–ç•¥
- **ç¬¬4ç« **ï¼šå¤šæ¨¡æ€æ–‡æ¡£å¤„ç†
- **ç¬¬5ç« **ï¼šç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline

---

# ç¬¬1ç« ï¼šLlamaIndexæ•°æ®åŠ è½½å™¨ç”Ÿæ€

## 1.1 SimpleDirectoryReader - å¿«é€Ÿå…¥é—¨

### 1.1.1 åŸºç¡€ä½¿ç”¨

**SimpleDirectoryReader** æ˜¯LlamaIndexæœ€ç®€å•çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒ20+ç§æ–‡ä»¶æ ¼å¼ï¼š

```python
from llama_index.core import SimpleDirectoryReader

# ä¸€è¡Œä»£ç åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

print(f"åŠ è½½äº†{len(documents)}ä¸ªæ–‡æ¡£")
for doc in documents[:2]:
    print(f"\næ–‡ä»¶ï¼š{doc.metadata.get('file_name', 'unknown')}")
    print(f"å†…å®¹ï¼š{doc.text[:200]}...")
```

**æ”¯æŒçš„æ ¼å¼**ï¼š
- æ–‡æ¡£ï¼šPDF, DOCX, DOC, TXT, MD
- æ•°æ®ï¼šCSV, JSON, XML
- ç½‘é¡µï¼šHTML, MHTML
- ä»£ç ï¼šPY, JS, JAVA, CPP
- å…¶ä»–ï¼šEPUB, RTF, PPTX

---

### 1.1.2 é«˜çº§é…ç½®

```python
from llama_index.core import SimpleDirectoryReader

# é«˜çº§é…ç½®
reader = SimpleDirectoryReader(
    input_dir="./data",
    required_exts=[".pdf", ".docx"],  # åªåŠ è½½ç‰¹å®šæ ¼å¼
    recursive=True,                    # é€’å½’å­ç›®å½•
    exclude_hidden=True,               # æ’é™¤éšè—æ–‡ä»¶
    exclude_empty=True,               # æ’é™¤ç©ºæ–‡ä»¶ (0.14.8+æ–°å¢)
    filename_as_id=True,              # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºID
    num_files_limit=100               # é™åˆ¶æ–‡ä»¶æ•°é‡
)

documents = reader.load_data()

# æŸ¥çœ‹å…ƒæ•°æ®
for doc in documents[:3]:
    print(f"æ–‡ä»¶ï¼š{doc.metadata['file_name']}")
    print(f"è·¯å¾„ï¼š{doc.metadata['file_path']}")
    print(f"å¤§å°ï¼š{doc.metadata.get('file_size', 'N/A')} bytes")
    print(f"IDï¼š{doc.doc_id}\n")
```

**å®Œæ•´å‚æ•°åˆ—è¡¨** (LlamaIndex 0.14.8):

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `input_dir` | str/Path | None | è¾“å…¥ç›®å½•è·¯å¾„ |
| `input_files` | list | None | æŒ‡å®šæ–‡ä»¶åˆ—è¡¨ |
| `exclude` | list | None | æ’é™¤çš„æ–‡ä»¶åˆ—è¡¨ |
| `exclude_hidden` | bool | True | æ’é™¤éšè—æ–‡ä»¶ |
| `exclude_empty` | bool | False | æ’é™¤ç©ºæ–‡ä»¶ **(0.14.8+æ–°å¢)** |
| `errors` | str | "ignore" | é”™è¯¯å¤„ç†ç­–ç•¥ |
| `recursive` | bool | False | é€’å½’è¯»å–å­ç›®å½• |
| `encoding` | str | "utf-8" | æ–‡ä»¶ç¼–ç  |
| `filename_as_id` | bool | False | ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ–‡æ¡£ID |
| `required_exts` | list[str] | None | åªåŠ è½½æŒ‡å®šæ‰©å±•åæ–‡ä»¶ |
| `file_extractor` | dict | None | è‡ªå®šä¹‰æ–‡ä»¶åŠ è½½å™¨æ˜ å°„ |
| `num_files_limit` | int | None | é™åˆ¶åŠ è½½æ–‡ä»¶æ•°é‡ |
| `file_metadata` | Callable | None | è‡ªå®šä¹‰å…ƒæ•°æ®æå–å‡½æ•° |
| `raise_on_error` | bool | False | é‡åˆ°é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸ |
| `fs` | fsspec.AbstractFileSystem | None | è‡ªå®šä¹‰æ–‡ä»¶ç³»ç»Ÿ(æ”¯æŒS3/GCSç­‰) **(0.14.8+æ–°å¢)** |

**æ–°å‚æ•°ä½¿ç”¨ç¤ºä¾‹**:

```python
# exclude_empty: è‡ªåŠ¨è¿‡æ»¤ç©ºæ–‡ä»¶
reader = SimpleDirectoryReader(
    input_dir="./data",
    exclude_empty=True  # è·³è¿‡ç©ºæ–‡ä»¶,é¿å…æ— æ•ˆæ–‡æ¡£
)
documents = reader.load_data()

# fs: ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶ç³»ç»Ÿ(å¦‚S3)
import fsspec
s3_fs = fsspec.filesystem('s3', anon=False)
reader = SimpleDirectoryReader(
    input_dir="s3://my-bucket/documents/",
    fs=s3_fs  # ç›´æ¥ä»S3è¯»å–æ–‡æ¡£
)
documents = reader.load_data()
```

---

### 1.1.3 è‡ªå®šä¹‰æ–‡ä»¶åŠ è½½å™¨

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import PDFReader

# ä¸ºç‰¹å®šæ ¼å¼æŒ‡å®šè‡ªå®šä¹‰åŠ è½½å™¨
file_extractor = {
    ".pdf": PDFReader(),  # ä½¿ç”¨ä¸“ç”¨PDFReader
}

reader = SimpleDirectoryReader(
    input_dir="./data",
    file_extractor=file_extractor
)

documents = reader.load_data()
```

---

## 1.2 LlamaHub - 700+æ•°æ®åŠ è½½å™¨

### 1.2.1 LlamaHubæ¦‚è¿°

**LlamaHub**æ˜¯LlamaIndexçš„æ•°æ®åŠ è½½å™¨æ³¨å†Œä¸­å¿ƒï¼Œæä¾›ï¼š
- 700+é¢„æ„å»ºçš„æ•°æ®åŠ è½½å™¨
- è¦†ç›–æ–‡æ¡£ã€æ•°æ®åº“ã€APIã€ç½‘é¡µç­‰
- ç¤¾åŒºé©±åŠ¨ï¼ŒæŒç»­æ›´æ–°

**æµè§ˆå™¨è®¿é—®**ï¼šhttps://llamahub.ai/

---

### 1.2.2 å¸¸ç”¨åŠ è½½å™¨

**PDFåŠ è½½å™¨**ï¼š
```python
from llama_index.readers.file import PDFReader

# åŸºç¡€PDFåŠ è½½
reader = PDFReader()
documents = reader.load_data(file="document.pdf")

print(f"åŠ è½½äº†{len(documents)}é¡µ")
```

**DOCXåŠ è½½å™¨**ï¼š
```python
from llama_index.readers.file import DocxReader

reader = DocxReader()
documents = reader.load_data(file="document.docx")
```

**HTMLåŠ è½½å™¨**ï¼š
```python
from llama_index.readers.file import HTMLTagReader

# æŒ‰HTMLæ ‡ç­¾æå–
reader = HTMLTagReader(
    tag="article",  # åªæå–<article>æ ‡ç­¾å†…å®¹
    ignore_no_id=False
)
documents = reader.load_data(file="webpage.html")
```

---

### 1.2.3 æ•°æ®åº“åŠ è½½å™¨

```python
# MongoDBåŠ è½½å™¨
from llama_index.readers.mongodb import SimpleMongoReader

reader = SimpleMongoReader(
    host="localhost",
    port=27017
)
documents = reader.load_data(
    db_name="mydb",
    collection_name="documents"
)

# SQLæ•°æ®åº“åŠ è½½å™¨
from llama_index.readers.database import DatabaseReader

reader = DatabaseReader(
    sql_database="postgresql://user:pass@localhost/db"
)
documents = reader.load_data(
    query="SELECT * FROM articles WHERE category='tech'"
)
```

---

### 1.2.4 APIåŠ è½½å™¨

```python
# GitHubåŠ è½½å™¨
from llama_index.readers.github import GithubRepositoryReader, GithubClient

github_client = GithubClient(github_token="your-token")
reader = GithubRepositoryReader(
    github_client=github_client,
    owner="langchain-ai",
    repo="langchain",
    filter_file_extensions=[".py", ".md"]
)
documents = reader.load_data(branch="main")

# NotionåŠ è½½å™¨
from llama_index.readers.notion import NotionPageReader

reader = NotionPageReader(integration_token="your-token")
documents = reader.load_data(page_ids=["page-id-1", "page-id-2"])
```

---

## 1.3 ä¸“ç”¨PDFåŠ è½½å™¨å¯¹æ¯”

### 1.3.1 å·¥å…·å¯¹æ¯”çŸ©é˜µ

| å·¥å…· | é€Ÿåº¦ | å‡†ç¡®ç‡ | è¡¨æ ¼æ”¯æŒ | å›¾ç‰‡æå– | é€‚ç”¨åœºæ™¯ |
|------|------|--------|---------|---------|---------|
| **PDFReader** | â­â­â­â­ å¿« | â­â­â­ ä¸­ç­‰ | âŒ å·® | âŒ ä¸æ”¯æŒ | ç®€å•PDF |
| **PyMuPDFReader** | â­â­â­â­â­ æœ€å¿« | â­â­â­â­ å¥½ | â­â­ ä¸€èˆ¬ | âœ… æ”¯æŒ | æ‰¹é‡å¤„ç† |
| **PDFPlumberReader** | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¥½ | âœ… ä¼˜ç§€ | âŒ ä¸æ”¯æŒ | è¡¨æ ¼å¯†é›† |
| **LlamaParse** | â­â­ æ…¢ | â­â­â­â­â­ æœ€å¥½ | âœ… ä¼˜ç§€ | âœ… ä¼˜ç§€ | å¤æ‚æ–‡æ¡£ |

---

### 1.3.2 å¯¹æ¯”ç¤ºä¾‹

```python
from llama_index.readers.file import PDFReader, PyMuPDFReader
import time

pdf_path = "complex_document.pdf"

# Test 1: PDFReaderï¼ˆåŸºç¡€ï¼‰
start = time.time()
reader1 = PDFReader()
docs1 = reader1.load_data(file=pdf_path)
time1 = time.time() - start
print(f"PDFReader: {len(docs1)}é¡µ, {time1:.2f}s")

# Test 2: PyMuPDFReaderï¼ˆå¿«é€Ÿï¼‰
start = time.time()
reader2 = PyMuPDFReader()
docs2 = reader2.load_data(file=pdf_path)
time2 = time.time() - start
print(f"PyMuPDFReader: {len(docs2)}é¡µ, {time2:.2f}s")

# è´¨é‡å¯¹æ¯”
print(f"\nPDFReaderå¹³å‡æ–‡æœ¬é•¿åº¦ï¼š{sum(len(d.text) for d in docs1)/len(docs1):.0f}")
print(f"PyMuPDFReaderå¹³å‡æ–‡æœ¬é•¿åº¦ï¼š{sum(len(d.text) for d in docs2)/len(docs2):.0f}")
```

---

## å°ç»“

**ç¬¬1ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **SimpleDirectoryReader**ï¼š
   - ä¸€è¡Œä»£ç åŠ è½½å¤šç§æ ¼å¼
   - æ”¯æŒé€’å½’ã€è¿‡æ»¤ã€è‡ªå®šä¹‰åŠ è½½å™¨
   - æœ€å¿«çš„å…¥é—¨æ–¹å¼

2. **LlamaHubç”Ÿæ€**ï¼š
   - 700+é¢„æ„å»ºåŠ è½½å™¨
   - è¦†ç›–æ–‡æ¡£ã€æ•°æ®åº“ã€API
   - æŒç»­æ›´æ–°çš„ç¤¾åŒºèµ„æº

3. **PDFå·¥å…·é€‰æ‹©**ï¼š
   - ç®€å•æ–‡æ¡£ â†’ PDFReader
   - æ‰¹é‡å¤„ç† â†’ PyMuPDFReader
   - è¡¨æ ¼å¯†é›† â†’ PDFPlumberReader
   - å¤æ‚æ–‡æ¡£ â†’ LlamaParseï¼ˆä¸‹ä¸€ç« ï¼‰

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬2ç« å°†æ·±å…¥æ¢è®¨**LlamaParse**â€”â€”ä¸–ç•Œé¦–ä¸ªGenAIåŸç”ŸPDFè§£æå¹³å°ã€‚

---

# ç¬¬2ç« ï¼šLlamaParse - GenAIåŸç”ŸPDFè§£æ

## 2.1 LlamaParseç®€ä»‹

### 2.1.1 ä»€ä¹ˆæ˜¯LlamaParse

**LlamaParse** æ˜¯LlamaIndexæ¨å‡ºçš„å•†ä¸šçº§PDFè§£ææœåŠ¡ï¼Œæ ¸å¿ƒç‰¹ç‚¹ï¼š

**GenAIåŸç”Ÿ**ï¼š
- ä½¿ç”¨LLMæ„å»ºï¼Œä¸“ä¸ºLLMåº”ç”¨ä¼˜åŒ–
- ç†è§£æ–‡æ¡£è¯­ä¹‰ï¼Œè€Œéç®€å•OCR
- ä¿ç•™ä¸Šä¸‹æ–‡å…³ç³»

**ä¸–ç•Œçº§è§£æèƒ½åŠ›**ï¼š
- å¤æ‚è¡¨æ ¼è¯†åˆ«ï¼ˆè·¨é¡µã€åµŒå¥—ï¼‰
- æ•°å­¦å…¬å¼æå–
- å¤šæ å¸ƒå±€å¤„ç†
- å›¾è¡¨ä¸æ–‡å­—å…³è”
- å¤šè¯­è¨€æ”¯æŒ

**ä¸ºRAGä¼˜åŒ–**ï¼š
- è¾“å‡ºMarkdownæ ¼å¼ï¼ˆLLMå‹å¥½ï¼‰
- è‡ªåŠ¨åˆ†å—å»ºè®®
- ä¿ç•™è¯­ä¹‰ç»“æ„

---

### 2.1.2 ä¸ºä»€ä¹ˆéœ€è¦LlamaParse

**ä¼ ç»Ÿå·¥å…·çš„å±€é™**ï¼š
```python
# PyPDF2/PDFMiner - åŸºç¡€å·¥å…·
âŒ å¤æ‚è¡¨æ ¼è¯†åˆ«å·®
âŒ å¤šæ å¸ƒå±€æ··ä¹±
âŒ å…¬å¼ä¸¢å¤±
âŒ æ— è¯­ä¹‰ç†è§£

# OCRå·¥å…·ï¼ˆTesseract/PaddleOCRï¼‰
âŒ åªè¯†åˆ«æ–‡å­—ï¼Œæ— ç»“æ„
âŒ è¡¨æ ¼è¿˜åŸå›°éš¾
âŒ ä¸Šä¸‹æ–‡å…³ç³»ä¸¢å¤±
```

**LlamaParseçš„ä¼˜åŠ¿**ï¼š
```python
âœ… å®Œæ•´ä¿ç•™è¡¨æ ¼ç»“æ„ï¼ˆMarkdownæ ¼å¼ï¼‰
âœ… å¤šæ å¸ƒå±€æ­£ç¡®è¿˜åŸ
âœ… æ•°å­¦å…¬å¼è¯†åˆ«
âœ… å›¾è¡¨ä¸è¯´æ˜å…³è”
âœ… è¯­ä¹‰æ„ŸçŸ¥åˆ†å—
```

---

## 2.2 LlamaParseå¿«é€Ÿå…¥é—¨

### 2.2.1 å®‰è£…ä¸é…ç½®

```bash
# å®‰è£…llama-parse
pip install llama-parse

# å®‰è£…llama-indexï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
pip install llama-index
```

**è·å–API Key**ï¼š
1. è®¿é—® https://cloud.llamaindex.ai/
2. æ³¨å†Œè´¦å·
3. è·å–API Key

```python
# è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ["LLAMA_CLOUD_API_KEY"] = "llx-your-api-key"
```

---

### 2.2.2 åŸºç¡€ä½¿ç”¨

```python
from llama_parse import LlamaParse

# åˆ›å»ºè§£æå™¨
parser = LlamaParse(
    api_key="llx-your-api-key",  # æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–
    result_type="markdown",       # è¾“å‡ºæ ¼å¼ï¼šmarkdownæˆ–text
    verbose=True
)

# è§£æPDF
documents = parser.load_data("complex_document.pdf")

print(f"è§£æäº†{len(documents)}ä¸ªæ–‡æ¡£å—")
print("\nè§£æç»“æœï¼ˆMarkdownæ ¼å¼ï¼‰ï¼š")
print(documents[0].text[:500])
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```markdown
# ç¬¬ä¸€ç« ï¼šå¼•è¨€

æœ¬æ–‡æ¡£ä»‹ç»äº†...

## 1.1 èƒŒæ™¯

åœ¨è¿‡å»çš„åå¹´ä¸­...

### è¡¨æ ¼1ï¼šå¹´åº¦æ•°æ®å¯¹æ¯”

| å¹´ä»½ | æ”¶å…¥ï¼ˆä¸‡å…ƒï¼‰ | å¢é•¿ç‡ |
|------|-------------|--------|
| 2021 | 1000        | 10%    |
| 2022 | 1200        | 20%    |
| 2023 | 1500        | 25%    |
```

---

### 2.2.3 é«˜çº§é…ç½®

```python
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",

    # è§£æé€‰é¡¹
    verbose=True,
    language="zh",                    # è¯­è¨€ï¼ˆzhä¸­æ–‡ï¼Œenè‹±æ–‡ï¼‰
    num_workers=4,                    # å¹¶è¡Œå¤„ç†æ•°

    # é«˜çº§ç‰¹æ€§
    parsing_instruction="""
    è¯·ç‰¹åˆ«æ³¨æ„ï¼š
    1. ä¿ç•™æ‰€æœ‰è¡¨æ ¼çš„å®Œæ•´ç»“æ„
    2. æ•°å­¦å…¬å¼ä½¿ç”¨LaTeXæ ¼å¼
    3. å›¾è¡¨è¯´æ˜ä¸å›¾è¡¨å…³è”
    """,                             # è‡ªå®šä¹‰è§£ææŒ‡ä»¤

    invalidate_cache=False,          # ä½¿ç”¨ç¼“å­˜ï¼ˆåŠ é€Ÿé‡å¤è§£æï¼‰
    do_not_cache=False,

    # è¾“å‡ºæ§åˆ¶
    gpt4o_mode=True,                 # ä½¿ç”¨GPT-4oå¢å¼ºï¼ˆæ›´å‡†ç¡®ä½†æ…¢ï¼‰
    gpt4o_api_key="your-openai-key"  # å¦‚æœä½¿ç”¨gpt4o_mode
)

documents = parser.load_data("academic_paper.pdf")
```

---

## 2.3 LlamaParseé«˜çº§ç‰¹æ€§

### 2.3.1 è‡ªå®šä¹‰è§£ææŒ‡ä»¤

**åœºæ™¯1ï¼šå­¦æœ¯è®ºæ–‡**
```python
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    è¿™æ˜¯ä¸€ç¯‡å­¦æœ¯è®ºæ–‡ï¼Œè¯·ï¼š
    1. ä¿ç•™æ‰€æœ‰æ•°å­¦å…¬å¼ï¼ˆLaTeXæ ¼å¼ï¼‰
    2. è¡¨æ ¼å®Œæ•´æå–ä¸ºMarkdown
    3. å›¾è¡¨ç¼–å·ä¸è¯´æ˜å…³è”
    4. å‚è€ƒæ–‡çŒ®å•ç‹¬åˆ—å‡º
    """
)

documents = parser.load_data("research_paper.pdf")
```

**åœºæ™¯2ï¼šè´¢åŠ¡æŠ¥è¡¨**
```python
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    è¿™æ˜¯è´¢åŠ¡æŠ¥è¡¨ï¼Œè¯·ï¼š
    1. é‡ç‚¹æå–æ‰€æœ‰è¡¨æ ¼ï¼ˆèµ„äº§è´Ÿå€ºè¡¨ã€åˆ©æ¶¦è¡¨ï¼‰
    2. ä¿ç•™æ•°å­—ç²¾åº¦ï¼ˆä¸è¦å››èˆäº”å…¥ï¼‰
    3. è¡¨æ ¼æ ‡é¢˜ä¸å†…å®¹å…³è”
    4. æ³¨é‡Šä¸å¯¹åº”è¡¨æ ¼å…³è”
    """
)

documents = parser.load_data("financial_report.pdf")
```

**åœºæ™¯3ï¼šåˆåŒæ–‡æ¡£**
```python
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    è¿™æ˜¯æ³•å¾‹åˆåŒï¼Œè¯·ï¼š
    1. ä¿ç•™æ¡æ¬¾ç¼–å·ç»“æ„
    2. é‡ç‚¹æ ‡è®°é‡‘é¢ã€æ—¥æœŸ
    3. ç”²ä¹™åŒæ–¹ä¿¡æ¯å‡†ç¡®æå–
    4. é™„ä»¶æ¸…å•å•ç‹¬åˆ—å‡º
    """
)

documents = parser.load_data("contract.pdf")
```

---

### 2.3.2 è¡¨æ ¼å¤„ç†

**LlamaParseçš„è¡¨æ ¼å¤„ç†ä¼˜åŠ¿**ï¼š

```python
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    é‡ç‚¹å¤„ç†è¡¨æ ¼ï¼š
    1. è·¨é¡µè¡¨æ ¼åˆå¹¶
    2. åµŒå¥—è¡¨æ ¼å±•å¹³
    3. åˆå¹¶å•å…ƒæ ¼æ­£ç¡®å¤„ç†
    """
)

documents = parser.load_data("tables_heavy.pdf")

# æå–æ‰€æœ‰è¡¨æ ¼
tables = []
for doc in documents:
    # LlamaParseè¾“å‡ºçš„Markdownä¸­ï¼Œè¡¨æ ¼ç”¨ | åˆ†éš”
    if "|" in doc.text and "---" in doc.text:
        tables.append(doc.text)

print(f"æå–åˆ°{len(tables)}ä¸ªè¡¨æ ¼")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```markdown
## å­£åº¦è´¢åŠ¡æ•°æ®

| å­£åº¦ | æ”¶å…¥ï¼ˆä¸‡å…ƒï¼‰ | æˆæœ¬ï¼ˆä¸‡å…ƒï¼‰ | åˆ©æ¶¦ï¼ˆä¸‡å…ƒï¼‰ | åˆ©æ¶¦ç‡ |
|------|-------------|-------------|-------------|--------|
| Q1   | 1000        | 700         | 300         | 30%    |
| Q2   | 1200        | 800         | 400         | 33%    |
| Q3   | 1500        | 900         | 600         | 40%    |
| Q4   | 1800        | 1000        | 800         | 44%    |
| åˆè®¡ | 5500        | 3400        | 2100        | 38%    |

**æ³¨**ï¼šåˆ©æ¶¦ç‡ = åˆ©æ¶¦ Ã· æ”¶å…¥
```

---

### 2.3.3 å¤šæ¨¡æ€æ”¯æŒ

**å›¾ç‰‡æå–ä¸æè¿°**ï¼š
```python
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    gpt4o_mode=True,  # å¯ç”¨GPT-4oè¿›è¡Œå›¾ç‰‡ç†è§£
    parsing_instruction="""
    å¯¹äºå›¾ç‰‡å’Œå›¾è¡¨ï¼š
    1. æå–å›¾ç‰‡å¹¶ç”Ÿæˆæè¿°
    2. å°†å›¾è¡¨æ•°æ®è½¬æ¢ä¸ºè¡¨æ ¼
    3. å›¾ç‰‡ç¼–å·ä¸æ­£æ–‡å…³è”
    """
)

documents = parser.load_data("illustrated_document.pdf")

# LlamaParseä¼šåœ¨Markdownä¸­åŒ…å«å›¾ç‰‡æè¿°
# ç¤ºä¾‹è¾“å‡ºï¼š
# ![å›¾1ï¼šç³»ç»Ÿæ¶æ„å›¾](image_description)
# æè¿°ï¼šè¯¥å›¾å±•ç¤ºäº†ç³»ç»Ÿçš„ä¸‰å±‚æ¶æ„...
```

---

## 2.4 LlamaParse vs ä¼ ç»Ÿå·¥å…·

### 2.4.1 å®æˆ˜å¯¹æ¯”

```python
import time
from llama_parse import LlamaParse
from llama_index.readers.file import PDFReader

pdf_path = "complex_academic_paper.pdf"

# æµ‹è¯•1: ä¼ ç»ŸPDFReader
start = time.time()
reader = PDFReader()
docs_traditional = reader.load_data(file=pdf_path)
time_traditional = time.time() - start

# æµ‹è¯•2: LlamaParse
start = time.time()
parser = LlamaParse(api_key="llx-your-api-key")
docs_llamaparse = parser.load_data(pdf_path)
time_llamaparse = time.time() - start

# å¯¹æ¯”
print("=== æ€§èƒ½å¯¹æ¯” ===")
print(f"ä¼ ç»Ÿå·¥å…·ï¼š{len(docs_traditional)}é¡µ, {time_traditional:.2f}s")
print(f"LlamaParseï¼š{len(docs_llamaparse)}é¡µ, {time_llamaparse:.2f}s")

print("\n=== è´¨é‡å¯¹æ¯” ===")
# æ£€æŸ¥è¡¨æ ¼æå–
traditional_tables = sum(1 for d in docs_traditional if "|" in d.text)
llamaparse_tables = sum(1 for d in docs_llamaparse if "|" in d.text)
print(f"ä¼ ç»Ÿå·¥å…·æå–è¡¨æ ¼ï¼š{traditional_tables}ä¸ª")
print(f"LlamaParseæå–è¡¨æ ¼ï¼š{llamaparse_tables}ä¸ª")

# å¹³å‡æ–‡æœ¬é•¿åº¦
avg_traditional = sum(len(d.text) for d in docs_traditional) / len(docs_traditional)
avg_llamaparse = sum(len(d.text) for d in docs_llamaparse) / len(docs_llamaparse)
print(f"ä¼ ç»Ÿå·¥å…·å¹³å‡æ–‡æœ¬é•¿åº¦ï¼š{avg_traditional:.0f}å­—ç¬¦")
print(f"LlamaParseå¹³å‡æ–‡æœ¬é•¿åº¦ï¼š{avg_llamaparse:.0f}å­—ç¬¦")
```

**å…¸å‹ç»“æœ**ï¼š
```
=== æ€§èƒ½å¯¹æ¯” ===
ä¼ ç»Ÿå·¥å…·ï¼š20é¡µ, 2.5s
LlamaParseï¼š20é¡µ, 45s

=== è´¨é‡å¯¹æ¯” ===
ä¼ ç»Ÿå·¥å…·æå–è¡¨æ ¼ï¼š2ä¸ªï¼ˆæ ¼å¼æ··ä¹±ï¼‰
LlamaParseæå–è¡¨æ ¼ï¼š15ä¸ªï¼ˆå®Œç¾Markdownï¼‰

ä¼ ç»Ÿå·¥å…·å¹³å‡æ–‡æœ¬é•¿åº¦ï¼š800å­—ç¬¦
LlamaParseå¹³å‡æ–‡æœ¬é•¿åº¦ï¼š1500å­—ç¬¦ï¼ˆåŒ…å«å®Œæ•´è¡¨æ ¼å’Œç»“æ„ï¼‰
```

---

### 2.4.2 æˆæœ¬è€ƒé‡

**LlamaParseå®šä»·**ï¼ˆ2025å¹´æ•°æ®ï¼‰ï¼š
- å…è´¹é¢åº¦ï¼š1000é¡µ/æœˆ
- Pay-as-you-goï¼šçº¦$0.003/é¡µ
- ä½¿ç”¨gpt4o_modeï¼šçº¦$0.01/é¡µï¼ˆæ›´å‡†ç¡®ï¼‰

**æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š
1. **åˆ†å±‚å¤„ç†**ï¼š
   - ç®€å•PDF â†’ ä¼ ç»Ÿå·¥å…·ï¼ˆå…è´¹ï¼‰
   - å¤æ‚PDF â†’ LlamaParseï¼ˆä»˜è´¹ï¼‰

2. **ç¼“å­˜åˆ©ç”¨**ï¼š
   ```python
   parser = LlamaParse(
       api_key="llx-your-api-key",
       invalidate_cache=False  # é‡å¤è§£æä½¿ç”¨ç¼“å­˜
   )
   ```

3. **æ‰¹é‡å¤„ç†**ï¼š
   ```python
   # ä¸€æ¬¡è§£æå¤šä¸ªæ–‡ä»¶ï¼ˆå…±äº«åˆå§‹åŒ–æˆæœ¬ï¼‰
   parser = LlamaParse(api_key="llx-your-api-key")

   files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
   all_docs = []
   for file in files:
       docs = parser.load_data(file)
       all_docs.extend(docs)
   ```

---

## 2.5 é›†æˆåˆ°RAGç³»ç»Ÿ

### 2.5.1 å®Œæ•´ç¤ºä¾‹

```python
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# æ­¥éª¤1: ä½¿ç”¨LlamaParseè§£æå¤æ‚PDF
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    è¿™æ˜¯ä¸€ä»½æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·ï¼š
    1. ä¿ç•™æ‰€æœ‰è¡¨æ ¼
    2. ä»£ç å—ä½¿ç”¨```æ ‡è®°
    3. ç« èŠ‚ç»“æ„æ¸…æ™°
    """
)

documents = parser.load_data("technical_manual.pdf")

# æ­¥éª¤2: ä½¿ç”¨MarkdownNodeParseråˆ†å—ï¼ˆä¿ç•™Markdownç»“æ„ï¼‰
node_parser = MarkdownNodeParser()
nodes = node_parser.get_nodes_from_documents(documents)

print(f"è§£æäº†{len(documents)}ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ{len(nodes)}ä¸ªèŠ‚ç‚¹")

# æ­¥éª¤3: æ„å»ºç´¢å¼•
index = VectorStoreIndex(
    nodes=nodes,
    embed_model=OpenAIEmbedding()
)

# æ­¥éª¤4: åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    llm=OpenAI(model="gpt-4"),
    similarity_top_k=5
)

# æ­¥éª¤5: æŸ¥è¯¢
response = query_engine.query("ç³»ç»Ÿæ¶æ„ä¸­çš„å„ä¸ªç»„ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response.response)

# æŸ¥çœ‹æ£€ç´¢åˆ°çš„æºèŠ‚ç‚¹
for i, node in enumerate(response.source_nodes, 1):
    print(f"\næ¥æº{i}ï¼š")
    print(node.text[:200])
```

---

---

## 2.6 MinerU vs LlamaParse - æ·±åº¦å¯¹æ¯”

### 2.6.1 æ ¸å¿ƒå®šä½å¯¹æ¯”

**LlamaParse - GenAIåŸç”Ÿå•†ä¸šè§£æ**ï¼š
- **å®šä½**ï¼šä¸–ç•Œé¦–ä¸ªGenAIåŸç”Ÿæ–‡æ¡£è§£æå¹³å°
- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šä½¿ç”¨LLMç†è§£æ–‡æ¡£è¯­ä¹‰ï¼Œä¸ºLLMåº”ç”¨ä¼˜åŒ–
- **æŠ€æœ¯è·¯çº¿**ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½è§£æ
- **å•†ä¸šæ¨¡å¼**ï¼šä»˜è´¹æœåŠ¡ï¼ˆå…è´¹é¢åº¦ + Pay-as-you-goï¼‰
- **å®˜æ–¹æ–‡æ¡£**ï¼šhttps://developers.llamaindex.ai/python/framework/llama_cloud/llama_parse/

**MinerU - å­¦æœ¯å¼€æºé«˜æ€§èƒ½è§£æ**ï¼š
- **å®šä½**ï¼šå°†å¤æ‚æ–‡æ¡£è½¬æ¢ä¸ºLLMå°±ç»ªçš„Markdown/JSON
- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šSOTAæ€§èƒ½ï¼ˆåœ¨OmniDocBenchä¸Šè¶…è¶ŠGPT-4oå’ŒGemini 2.5 Proï¼‰
- **æŠ€æœ¯è·¯çº¿**ï¼šä¼ ç»ŸCV/OCRï¼ˆpipelineï¼‰+ æœ€æ–°VLMæ¨¡å‹MinerU2.5ï¼ˆ1.2Bå‚æ•°ï¼‰
- **å¼€æºåè®®**ï¼šAGPL-3.0ï¼ˆå®Œå…¨å¼€æºï¼ŒGitHub 49.3k+ starsï¼‰
- **å®˜æ–¹ä»“åº“**ï¼šhttps://github.com/opendatalab/MinerU

---

### 2.6.2 åŠŸèƒ½å¯¹æ¯”çŸ©é˜µ

| åŠŸèƒ½ç»´åº¦ | LlamaParse | MinerU | è¯´æ˜ |
|---------|-----------|--------|------|
| **æ–‡æœ¬æå–** | â­â­â­â­â­ ä¼˜ç§€ | â­â­â­â­â­ 84è¯­è¨€ | ä¸¤è€…å‡æ”¯æŒå¤šè¯­è¨€æ–‡æœ¬æå– |
| **è¡¨æ ¼è¯†åˆ«** | â­â­â­â­â­ Markdownå®Œç¾ | â­â­â­â­â­ æ—‹è½¬/æ— è¾¹æ¡†/è·¨é¡µ | **å¹³æ‰‹**ï¼ˆLlamaParseæ›´LLMå‹å¥½ï¼ŒMinerUæ›´å…¨é¢ï¼‰ |
| **å…¬å¼è¯†åˆ«** | â­â­â­â­ LaTeXæ ¼å¼ | â­â­â­â­â­ UniMERï¼ˆSOTAï¼‰ | **MinerUä¼˜åŠ¿**ï¼ˆæé€Ÿ1400%ï¼Œå‡†ç¡®ç‡98%+ï¼‰ |
| **å¤šæ¨¡æ€æ”¯æŒ** | â­â­â­â­â­ GPT-4oå›¾ç‰‡ç†è§£ | â­â­â­â­ OCR+VLM | **LlamaParseä¼˜åŠ¿**ï¼ˆè¯­ä¹‰ç†è§£æ›´å¼ºï¼‰ |
| **å¤æ‚å¸ƒå±€** | â­â­â­â­â­ å¤šæ /åµŒå¥— | â­â­â­â­â­ DocLayout-YOLO | **å¹³æ‰‹**ï¼ˆä¸åŒæŠ€æœ¯è·¯å¾„ï¼‰ |
| **å¤„ç†é€Ÿåº¦** | â­â­â­ è¾ƒæ…¢ï¼ˆAPIï¼‰ | â­â­â­â­â­ 10,000+ tokens/s | **MinerUä¼˜åŠ¿**ï¼ˆæœ¬åœ°GPUæ¨ç†ï¼‰ |
| **æˆæœ¬** | â­â­â­ ä»˜è´¹ï¼ˆçº¦$0.003/é¡µï¼‰ | â­â­â­â­â­ å®Œå…¨å…è´¹ | **MinerUä¼˜åŠ¿**ï¼ˆå¼€æºï¼‰ |
| **éƒ¨ç½²å¤æ‚åº¦** | â­â­â­â­â­ APIè°ƒç”¨ï¼ˆé›¶éƒ¨ç½²ï¼‰ | â­â­â­ éœ€GPUï¼ˆ6-8GBæ˜¾å­˜ï¼‰ | **LlamaParseä¼˜åŠ¿**ï¼ˆäº‘æœåŠ¡ï¼‰ |
| **æ•°æ®éšç§** | â­â­â­ æ•°æ®ä¸Šä¼ åˆ°äº‘ç«¯ | â­â­â­â­â­ æœ¬åœ°å¤„ç† | **MinerUä¼˜åŠ¿**ï¼ˆä¼ä¸šå‹å¥½ï¼‰ |

---

### 2.6.3 æ€§èƒ½åŸºå‡†å¯¹æ¯”

**è§£æå‡†ç¡®ç‡ï¼ˆåŸºäºOmniDocBenchåŸºå‡†æµ‹è¯•ï¼‰**ï¼š

| æ¨¡å‹ | ç»¼åˆå¾—åˆ† | è¡¨æ ¼å‡†ç¡®ç‡ | å…¬å¼å‡†ç¡®ç‡ | å¸ƒå±€å‡†ç¡®ç‡ | æ•°æ®æ¥æº |
|------|---------|-----------|-----------|-----------|----------|
| **MinerU2.5** | SOTA | 95%+ | 98%+ | 97%+ | [MinerU GitHub](https://github.com/opendatalab/MinerU) |
| **GPT-4o** | ä¼˜ç§€ | 93% | 95% | 95% | OmniDocBench |
| **LlamaParse** | ä¼˜ç§€ | 96%+ | 94% | 96%+ | LlamaIndexå®˜æ–¹æ•°æ® |
| **ä¼ ç»ŸOCR** | ä¸€èˆ¬ | 70-80% | 60-70% | 75-85% | è¡Œä¸šå¹³å‡æ°´å¹³ |

**å¤„ç†é€Ÿåº¦å¯¹æ¯”**ï¼š

```python
# æµ‹è¯•æ–‡æ¡£ï¼š100é¡µå­¦æœ¯è®ºæ–‡ï¼ˆå«50ä¸ªè¡¨æ ¼ã€30ä¸ªå…¬å¼ã€20å¼ å›¾ç‰‡ï¼‰

# LlamaParse
â± å¤„ç†æ—¶é—´ï¼š~150-300ç§’ï¼ˆå–å†³äºAPIè´Ÿè½½ï¼‰
ğŸ’° æˆæœ¬ï¼š$0.30ï¼ˆ100é¡µ Ã— $0.003/é¡µï¼‰
âœ… ä¼˜åŠ¿ï¼šæ— éœ€æœ¬åœ°èµ„æºï¼Œé›¶éƒ¨ç½²
âŒ åŠ£åŠ¿ï¼šä¾èµ–ç½‘ç»œï¼ŒæŒ‰é‡è®¡è´¹

# MinerU (NVIDIA 4090)
â± å¤„ç†æ—¶é—´ï¼š~30-60ç§’ï¼ˆæœ¬åœ°GPUï¼‰
ğŸ’° æˆæœ¬ï¼š$0ï¼ˆç¡¬ä»¶æˆæœ¬å·²æ‘Šé”€ï¼‰
âœ… ä¼˜åŠ¿ï¼šæé€Ÿã€ç¦»çº¿ã€éšç§ã€å…è´¹
âŒ åŠ£åŠ¿ï¼šéœ€è¦GPUç¡¬ä»¶ï¼ˆ6-8GBæ˜¾å­˜ï¼‰

# MinerU (Apple M3 Max with MLX)
â± å¤„ç†æ—¶é—´ï¼š~60-120ç§’ï¼ˆMLXåŠ é€Ÿï¼‰
ğŸ’° æˆæœ¬ï¼š$0
âœ… ä¼˜åŠ¿ï¼šAppleèŠ¯ç‰‡ä¼˜åŒ–ï¼ˆ100-200%æé€Ÿï¼‰
âŒ åŠ£åŠ¿ï¼šéœ€è¦Macè®¾å¤‡
```

---

### 2.6.4 é€‚ç”¨åœºæ™¯å†³ç­–æ ‘

```mermaid
graph TD
    Start[æ–‡æ¡£è§£æéœ€æ±‚] --> Q1{æ˜¯å¦æœ‰GPU?}
    Q1 -->|æ— GPU| Q2{é¢„ç®—å¦‚ä½•?}
    Q1 -->|æœ‰GPU| Q3{æ•°æ®æ•æ„Ÿ?}

    Q2 -->|é¢„ç®—å……è¶³| LlamaParse[âœ… LlamaParse<br/>äº‘ç«¯APIï¼Œé›¶éƒ¨ç½²]
    Q2 -->|é¢„ç®—æœ‰é™| Traditional[ä¼ ç»Ÿå·¥å…·<br/>PyMuPDF/PDFPlumber]

    Q3 -->|é«˜åº¦æ•æ„Ÿ| MinerU[âœ… MinerU<br/>æœ¬åœ°å¤„ç†ï¼Œéšç§å®‰å…¨]
    Q3 -->|å¯æ¥å—äº‘ç«¯| Q4{æ–‡æ¡£å¤æ‚åº¦?}

    Q4 -->|æé«˜| LlamaParse2[âœ… LlamaParse<br/>è¯­ä¹‰ç†è§£å¼º]
    Q4 -->|ä¸€èˆ¬| MinerU2[âœ… MinerU<br/>é€Ÿåº¦å¿«ï¼Œå…è´¹]

    style LlamaParse fill:#fff4e1
    style LlamaParse2 fill:#fff4e1
    style MinerU fill:#e1f5e1
    style MinerU2 fill:#e1f5e1
```

**å†³ç­–æŒ‡å—**ï¼š

1. **é€‰æ‹©LlamaParseçš„åœºæ™¯**ï¼š
   - âœ… æ— GPUèµ„æºï¼ˆäº‘ç«¯éƒ¨ç½²ï¼‰
   - âœ… éœ€è¦æœ€å¼ºè¯­ä¹‰ç†è§£ï¼ˆå¤æ‚åˆåŒã€å­¦æœ¯è®ºæ–‡ï¼‰
   - âœ… å¿«é€ŸåŸå‹å¼€å‘ï¼ˆé›¶éƒ¨ç½²æˆæœ¬ï¼‰
   - âœ… å·²æœ‰LlamaCloudç”Ÿæ€ï¼ˆä¸LlamaIndexæ— ç¼é›†æˆï¼‰
   - âœ… å¶å°”ä½¿ç”¨ï¼ˆ<1000é¡µ/æœˆï¼Œæˆæœ¬å¯æ§ï¼‰
   - âŒ é¢„ç®—æœ‰é™ï¼ˆå¤§è§„æ¨¡ä½¿ç”¨ï¼‰
   - âŒ é«˜åº¦æ•æ„Ÿæ•°æ®ï¼ˆä¸èƒ½ä¸Šä¼ äº‘ç«¯ï¼‰

2. **é€‰æ‹©MinerUçš„åœºæ™¯**ï¼š
   - âœ… æœ‰GPUï¼ˆ6-8GBæ˜¾å­˜ï¼‰æˆ–Apple Silicon
   - âœ… å¤§è§„æ¨¡æ‰¹é‡å¤„ç†ï¼ˆèŠ‚çœæˆæœ¬ï¼‰
   - âœ… æ•°æ®éšç§è¦æ±‚ï¼ˆæœ¬åœ°å¤„ç†ï¼‰
   - âœ… å…¬å¼å¯†é›†æ–‡æ¡£ï¼ˆSOTAå…¬å¼è¯†åˆ«ï¼Œæé€Ÿ1400%ï¼‰
   - âœ… é«˜ååé‡éœ€æ±‚ï¼ˆ10,000+ tokens/sï¼‰
   - âœ… ç¦»çº¿ç¯å¢ƒï¼ˆæ— éœ€ç½‘ç»œï¼‰
   - âŒ ä¸æƒ³ç®¡ç†ç¡¬ä»¶
   - âŒ æ— æŠ€æœ¯å›¢é˜Ÿ

---

### 2.6.5 æ··åˆä½¿ç”¨ç­–ç•¥

**ç­–ç•¥1ï¼šæ™ºèƒ½è·¯ç”±ï¼ˆåŸºäºæ–‡æ¡£ç‰¹å¾ï¼‰**

```python
from pathlib import Path
from llama_parse import LlamaParse
from llama_index.readers.file import PyMuPDFReader

class HybridDocumentParser:
    """æ··åˆä½¿ç”¨LlamaParseå’Œä¼ ç»Ÿå·¥å…·çš„æ™ºèƒ½è·¯ç”±"""

    def __init__(
        self,
        llamaparse_api_key: str,
        monthly_budget: float = 50.0
    ):
        self.llamaparse = LlamaParse(api_key=llamaparse_api_key)
        self.pymupdf = PyMuPDFReader()
        self.monthly_budget = monthly_budget
        self.llamaparse_usage = 0.0

    def analyze_document_complexity(self, pdf_path: str) -> dict:
        """åˆ†ææ–‡æ¡£å¤æ‚åº¦"""
        import fitz  # PyMuPDF

        doc = fitz.open(pdf_path)
        stats = {
            'pages': len(doc),
            'has_images': False,
            'has_tables': False,
            'has_formulas': False,
            'text_density': 0
        }

        # é‡‡æ ·åˆ†æï¼ˆå‰3é¡µï¼‰
        for page in doc[:min(3, len(doc))]:
            text = page.get_text()
            stats['text_density'] += len(text)

            # ç®€å•å¯å‘å¼æ£€æµ‹
            if 'âˆ«' in text or 'âˆ‘' in text or '\\' in text:
                stats['has_formulas'] = True
            if page.get_images():
                stats['has_images'] = True

        stats['text_density'] /= min(3, len(doc))
        doc.close()

        # å¤æ‚åº¦è¯„åˆ†
        complexity_score = 0
        if stats['pages'] > 50:
            complexity_score += 20
        if stats['has_formulas']:
            complexity_score += 30
        if stats['has_images']:
            complexity_score += 20
        if stats['text_density'] < 500:  # ä½æ–‡æœ¬å¯†åº¦ï¼ˆå¯èƒ½å¾ˆå¤šè¡¨æ ¼/å›¾ç‰‡ï¼‰
            complexity_score += 30

        stats['complexity_score'] = complexity_score
        stats['complexity_level'] = (
            'high' if complexity_score >= 60 else
            'medium' if complexity_score >= 30 else
            'low'
        )

        return stats

    def parse(self, pdf_path: str) -> list:
        """æ™ºèƒ½é€‰æ‹©è§£æå·¥å…·"""
        stats = self.analyze_document_complexity(pdf_path)

        print(f"æ–‡æ¡£åˆ†æï¼š{stats['pages']}é¡µ, å¤æ‚åº¦={stats['complexity_level']}")
        print(f"  å…¬å¼: {stats['has_formulas']}, å›¾ç‰‡: {stats['has_images']}")

        # å†³ç­–é€»è¾‘
        estimated_cost = stats['pages'] * 0.003

        # è§„åˆ™1: é¢„ç®—ä¸è¶³ â†’ ä¼ ç»Ÿå·¥å…·ï¼ˆå…è´¹ï¼‰
        if self.llamaparse_usage + estimated_cost > self.monthly_budget:
            print(f"â†’ é€‰æ‹©PyMuPDFï¼ˆé¢„ç®—ä¸è¶³ï¼š${self.llamaparse_usage:.2f}/{self.monthly_budget}ï¼‰")
            return self.pymupdf.load_data(file=pdf_path)

        # è§„åˆ™2: é«˜å¤æ‚åº¦ä¸”æœ‰é¢„ç®— â†’ LlamaParseï¼ˆè¯­ä¹‰ç†è§£ï¼‰
        if stats['complexity_level'] == 'high':
            print(f"â†’ é€‰æ‹©LlamaParseï¼ˆå¤æ‚æ–‡æ¡£ï¼Œæˆæœ¬${estimated_cost:.2f}ï¼‰")
            self.llamaparse_usage += estimated_cost
            return self.llamaparse.load_data(pdf_path)

        # è§„åˆ™3: é»˜è®¤ â†’ ä¼ ç»Ÿå·¥å…·ï¼ˆé€Ÿåº¦å¿«ï¼Œå…è´¹ï¼‰
        print("â†’ é€‰æ‹©PyMuPDFï¼ˆé»˜è®¤ç­–ç•¥ï¼‰")
        return self.pymupdf.load_data(file=pdf_path)


# ä½¿ç”¨ç¤ºä¾‹
parser = HybridDocumentParser(
    llamaparse_api_key="llx-your-api-key",
    monthly_budget=50.0
)

# æ‰¹é‡å¤„ç†
for pdf_file in Path("./documents").glob("*.pdf"):
    print(f"\nå¤„ç†ï¼š{pdf_file.name}")
    documents = parser.parse(str(pdf_file))
    print(f"âœ… æå–{len(documents)}ä¸ªæ–‡æ¡£å—")

print(f"\næœ¬æœˆLlamaParseä½¿ç”¨ï¼š${parser.llamaparse_usage:.2f}/{parser.monthly_budget}")
```

---

**ç­–ç•¥2ï¼šæˆæœ¬ä¼˜åŒ–ï¼ˆåˆ†å±‚å¤„ç†ï¼‰**

```python
from llama_index.readers.file import PyMuPDFReader, PDFReader
from llama_parse import LlamaParse

class CostOptimizedStrategy:
    """æˆæœ¬ä¼˜åŒ–çš„åˆ†å±‚ç­–ç•¥"""

    def __init__(self):
        self.tier_1_tool = PyMuPDFReader()  # å…è´¹ï¼Œå¿«é€Ÿ
        self.tier_2_tool = LlamaParse()     # ä»˜è´¹ï¼Œæœ€é«˜è´¨é‡

    def parse_with_fallback(self, pdf_path: str):
        """åˆ†å±‚é™çº§ç­–ç•¥"""

        # Tier 1: å°è¯•å¿«é€Ÿå…è´¹å·¥å…·
        try:
            print("å°è¯• Tier 1: PyMuPDFReaderï¼ˆå…è´¹ï¼Œå¿«é€Ÿï¼‰")
            docs = self.tier_1_tool.load_data(file=pdf_path)

            # è´¨é‡æ£€æŸ¥
            quality_score = self._check_quality(docs)
            print(f"è´¨é‡è¯„åˆ†ï¼š{quality_score:.2f}")

            if quality_score >= 0.8:
                print("âœ… Tier 1 æˆåŠŸï¼Œè´¨é‡åˆæ ¼")
                return docs
            else:
                print("âš ï¸ Tier 1 è´¨é‡ä¸è¶³ï¼Œå‡çº§åˆ°LlamaParse...")
        except Exception as e:
            print(f"âŒ Tier 1 å¤±è´¥: {e}")

        # Tier 2: å•†ä¸šçº§å·¥å…·ï¼ˆæœ€åæ‰‹æ®µï¼‰
        print("ä½¿ç”¨ Tier 2: LlamaParseï¼ˆä»˜è´¹ï¼Œæœ€é«˜è´¨é‡ï¼‰")
        return self.tier_2_tool.load_data(pdf_path)

    def _check_quality(self, docs: list) -> float:
        """è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼‰"""
        if not docs:
            return 0.0

        total_text = "".join([d.text for d in docs])

        score = 0.0
        # æœ‰è¶³å¤Ÿå†…å®¹
        if len(total_text) > 500:
            score += 0.4
        # æœ‰è¡¨æ ¼ï¼ˆæ£€æµ‹Markdownè¡¨æ ¼ï¼‰
        if "|" in total_text and "---" in total_text:
            score += 0.3
        # ç»“æ„å®Œæ•´ï¼ˆæœ‰æ¢è¡Œï¼‰
        if total_text.count("\n") > 10:
            score += 0.3

        return score


# ä½¿ç”¨
strategy = CostOptimizedStrategy()
docs = strategy.parse_with_fallback("complex_document.pdf")
```

---

**ç­–ç•¥3ï¼šé™çº§æ–¹æ¡ˆï¼ˆå®¹é”™å¤„ç†ï¼‰**

```python
from llama_parse import LlamaParse
from llama_index.readers.file import PyMuPDFReader, PDFReader

def robust_parse_with_retry(pdf_path: str) -> list:
    """å¸¦é‡è¯•çš„é²æ£’è§£æ"""
    strategies = [
        ("LlamaParse", lambda: LlamaParse().load_data(pdf_path)),
        ("PyMuPDF", lambda: PyMuPDFReader().load_data(file=pdf_path)),
        ("PDFReader", lambda: PDFReader().load_data(file=pdf_path)),
    ]

    for name, parser_func in strategies:
        try:
            print(f"å°è¯•ï¼š{name}")
            docs = parser_func()
            if docs and len(docs) > 0:
                print(f"âœ… {name} æˆåŠŸ")
                return docs
        except Exception as e:
            print(f"âŒ {name} å¤±è´¥: {e}ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")

    raise Exception("æ‰€æœ‰è§£æç­–ç•¥å‡å¤±è´¥")


# ä½¿ç”¨
docs = robust_parse_with_retry("problematic_document.pdf")
```

---

### 2.6.6 æœ€ä½³å®è·µå»ºè®®

**1. ç»„åˆä½¿ç”¨åœºæ™¯**ï¼š

```python
# åœºæ™¯1ï¼šå­¦æœ¯è®ºæ–‡ï¼ˆå…¬å¼å¯†é›†ï¼‰
# æ¨èï¼šMinerUï¼ˆå…¬å¼è¯†åˆ«SOTAï¼Œå…è´¹ï¼‰
# æ³¨ï¼šMinerUéœ€å•ç‹¬å®‰è£…ï¼Œè¿™é‡Œä½¿ç”¨LlamaParseä½œä¸ºæ›¿ä»£
parser = LlamaParse(
    api_key="llx-your-api-key",
    parsing_instruction="é‡ç‚¹è¯†åˆ«æ•°å­¦å…¬å¼ï¼ˆLaTeXæ ¼å¼ï¼‰"
)

# åœºæ™¯2ï¼šè´¢åŠ¡æŠ¥è¡¨ï¼ˆå¤æ‚è¡¨æ ¼ + è¯­ä¹‰ç†è§£ï¼‰
# æ¨èï¼šLlamaParseï¼ˆç†è§£è¡¨æ ¼å…³ç³»ï¼ŒMarkdownè¾“å‡ºï¼‰
parser = LlamaParse(
    api_key="llx-your-api-key",
    parsing_instruction="é‡ç‚¹æå–è´¢åŠ¡è¡¨æ ¼ï¼Œä¿ç•™æ•°å­—ç²¾åº¦"
)

# åœºæ™¯3ï¼šæ³•å¾‹åˆåŒï¼ˆç»“æ„åŒ–æ¡æ¬¾ï¼‰
# æ¨èï¼šLlamaParseï¼ˆè¯­ä¹‰ç†è§£å¼ºï¼‰
parser = LlamaParse(
    api_key="llx-your-api-key",
    parsing_instruction="ä¿ç•™æ¡æ¬¾ç¼–å·ï¼Œæ ‡è®°é‡‘é¢å’Œæ—¥æœŸ"
)

# åœºæ™¯4ï¼šé€šç”¨æŠ€æœ¯æ–‡æ¡£ï¼ˆæˆæœ¬æ•æ„Ÿï¼‰
# æ¨èï¼šä¼ ç»Ÿå·¥å…·ï¼ˆé€Ÿåº¦å¿«ï¼Œå…è´¹ï¼‰
from llama_index.readers.file import PyMuPDFReader
parser = PyMuPDFReader()
```

---

**2. æˆæœ¬æ”¶ç›Šåˆ†æ**ï¼š

| åœºæ™¯ | æ–‡æ¡£é‡ | LlamaParseæˆæœ¬ | MinerUæˆæœ¬ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|------|-------|---------------|-----------|---------|------|
| **åŸå‹å¼€å‘** | <100é¡µ | <$0.3 | $0 | **LlamaParse** | å¿«é€Ÿè¿­ä»£ï¼Œé›¶éƒ¨ç½² |
| **å°è§„æ¨¡ç”Ÿäº§** | 1000é¡µ/æœˆ | ~$3/æœˆ | $0 | **MinerU** | æˆæœ¬ä¼˜åŠ¿æ˜æ˜¾ |
| **å¤§è§„æ¨¡ç”Ÿäº§** | 100,000é¡µ/æœˆ | ~$300/æœˆ | $0ï¼ˆéœ€GPUï¼‰ | **MinerU** | æ˜¾è‘—èŠ‚çœï¼ˆéœ€æŠ•èµ„GPUï¼‰ |
| **éšç§æ•æ„Ÿ** | ä»»æ„ | ä¸é€‚ç”¨ | $0 | **MinerU** | æ•°æ®å®‰å…¨ï¼Œæœ¬åœ°å¤„ç† |

---

**3. æŠ€æœ¯é€‰å‹å†³ç­–**ï¼š

```python
# âœ… æ¨èï¼šLlamaParse
if (
    æ— GPUèµ„æº or
    å¿«é€ŸåŸå‹å¼€å‘ or
    å¶å°”ä½¿ç”¨ï¼ˆ<1000é¡µ/æœˆï¼‰ or
    éœ€è¦æœ€ä½³è¯­ä¹‰ç†è§£
):
    use_llamaparse()

# âœ… æ¨èï¼šMinerU
if (
    æœ‰GPUï¼ˆ6-8GB+ï¼‰or Apple Silicon or
    å¤§è§„æ¨¡æ‰¹é‡å¤„ç† or
    æ•°æ®éšç§è¦æ±‚ or
    å…¬å¼å¯†é›†æ–‡æ¡£ or
    é¢„ç®—æœ‰é™
):
    use_mineru()

# âœ… æ¨èï¼šæ··åˆç­–ç•¥
if (
    æ–‡æ¡£ç±»å‹å¤šæ · and
    æˆæœ¬æ•æ„Ÿ and
    æœ‰æŠ€æœ¯èƒ½åŠ›
):
    use_hybrid_strategy()
```

---

**4. å®é™…ç”Ÿäº§ç»éªŒ**ï¼š

**æ¡ˆä¾‹1ï¼šæŸé‡‘èå…¬å¸ï¼ˆ10ä¸‡+é¡µè´¢æŠ¥/å¹´ï¼‰**
- **åˆæœŸæ–¹æ¡ˆ**ï¼šå…¨éƒ¨ä½¿ç”¨LlamaParse
  - æˆæœ¬ï¼š$300/æœˆ
  - é—®é¢˜ï¼šæˆæœ¬è¿‡é«˜

- **ä¼˜åŒ–åæ–¹æ¡ˆ**ï¼š
  - ç®€å•è´¢æŠ¥ï¼ˆ70%ï¼‰ â†’ PyMuPDFï¼ˆå…è´¹ï¼‰
  - å¤æ‚è´¢æŠ¥ï¼ˆ25%ï¼‰ â†’ MinerUï¼ˆå…è´¹ï¼Œæœ¬åœ°GPUï¼‰
  - æå¤æ‚æ–‡æ¡£ï¼ˆ5%ï¼‰ â†’ LlamaParseï¼ˆä»˜è´¹ï¼‰

- **æˆæœ**ï¼š
  - æˆæœ¬ä»$300/æœˆé™è‡³$15/æœˆ
  - èŠ‚çœæˆæœ¬ï¼š**95%**
  - è´¨é‡æ— æ˜æ˜¾ä¸‹é™

**æ¡ˆä¾‹2ï¼šæŸå­¦æœ¯æœºæ„ï¼ˆå­¦æœ¯è®ºæ–‡è§£æï¼‰**
- **æ–¹æ¡ˆ**ï¼š100% MinerU
- **åŸå› **ï¼š
  - å…¬å¼å¯†é›†ï¼ˆMinerUå…¬å¼è¯†åˆ«SOTAï¼‰
  - å…è´¹å¼€æºï¼ˆé¢„ç®—æœ‰é™ï¼‰
  - æ•°æ®éšç§ï¼ˆå­¦æœ¯æ•æ„Ÿï¼‰
- **ç¡¬ä»¶**ï¼šNVIDIA A100ï¼ˆå·²æœ‰ï¼‰
- **æˆæœ**ï¼šæ¯æœˆå¤„ç†10ä¸‡é¡µï¼Œæˆæœ¬$0

**æ¡ˆä¾‹3ï¼šæŸåˆ›ä¸šå…¬å¸ï¼ˆRAGåŸå‹ï¼‰**
- **æ–¹æ¡ˆ**ï¼š100% LlamaParse
- **åŸå› **ï¼š
  - æ— GPUèµ„æº
  - å¿«é€Ÿè¿­ä»£ï¼ˆ2å‘¨ä¸Šçº¿ï¼‰
  - é›¶éƒ¨ç½²æˆæœ¬
- **æˆæœ¬**ï¼š$5-10/æœˆï¼ˆå…è´¹é¢åº¦å†…ï¼‰
- **æˆæœ**ï¼š2å‘¨å†…å®ŒæˆPOCï¼ŒæˆåŠŸèèµ„åå†è€ƒè™‘æˆæœ¬ä¼˜åŒ–

---
## å°ç»“

**ç¬¬2ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **LlamaParseæ ¸å¿ƒä¼˜åŠ¿**ï¼š
   - GenAIåŸç”Ÿï¼Œè¯­ä¹‰ç†è§£å¼º
   - å®Œç¾è¡¨æ ¼è¯†åˆ«ï¼ˆMarkdownæ ¼å¼ï¼‰
   - å¤šæ å¸ƒå±€ã€å…¬å¼ã€å›¾è¡¨å…¨é¢æ”¯æŒ
   - ä¸“ä¸ºRAGä¼˜åŒ–ï¼Œé›¶éƒ¨ç½²æˆæœ¬

2. **MinerUæ ¸å¿ƒä¼˜åŠ¿**ï¼š
   - SOTAæ€§èƒ½ï¼ˆOmniDocBenchè¶…è¶ŠGPT-4oï¼‰
   - å…¬å¼è¯†åˆ«é¡¶çº§ï¼ˆUniMERï¼Œå‡†ç¡®ç‡98%+ï¼Œæé€Ÿ1400%ï¼‰
   - æé€Ÿå¤„ç†ï¼ˆ10,000+ tokens/sï¼‰
   - å®Œå…¨å…è´¹å¼€æºï¼ˆAGPL-3.0ï¼‰

3. **ä½¿ç”¨åœºæ™¯å¯¹æ¯”**ï¼š
   - å­¦æœ¯è®ºæ–‡ï¼ˆå…¬å¼å¯†é›†ï¼‰ â†’ **MinerUä¼˜åŠ¿**ï¼ˆå…¬å¼SOTAï¼‰
   - è´¢åŠ¡æŠ¥è¡¨ï¼ˆå¤æ‚è¡¨æ ¼ï¼‰ â†’ **å¹³æ‰‹**ï¼ˆçœ‹å…·ä½“éœ€æ±‚ï¼‰
   - æŠ€æœ¯æ–‡æ¡£ï¼ˆè¯­ä¹‰ç†è§£ï¼‰ â†’ **LlamaParseä¼˜åŠ¿**
   - æ³•å¾‹åˆåŒï¼ˆç»“æ„åŒ–æ¡æ¬¾ï¼‰ â†’ **LlamaParseä¼˜åŠ¿**

4. **é€‰å‹å†³ç­–**ï¼š
   - æ— GPU + å¿«é€Ÿå¼€å‘ â†’ **LlamaParse**
   - æœ‰GPU + å¤§è§„æ¨¡ + éšç§ â†’ **MinerU**
   - é¢„ç®—å……è¶³ + è¯­ä¹‰ç†è§£ â†’ **LlamaParse**
   - æˆæœ¬æ•æ„Ÿ + å…¬å¼å¯†é›† â†’ **MinerU**
   - æ–‡æ¡£ç±»å‹å¤šæ · â†’ **æ··åˆç­–ç•¥**

5. **æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š
   - ç®€å•æ–‡æ¡£ â†’ ä¼ ç»Ÿå·¥å…·ï¼ˆå…è´¹ï¼‰
   - å¤æ‚æ–‡æ¡£ â†’ MinerUï¼ˆå…è´¹ä½†éœ€GPUï¼‰
   - æå¤æ‚æ–‡æ¡£ â†’ LlamaParseï¼ˆä»˜è´¹ä½†æœ€å‡†ç¡®ï¼‰
   - åˆ©ç”¨æ™ºèƒ½è·¯ç”±èŠ‚çœ95%+æˆæœ¬
   - åˆ†å±‚é™çº§ç¡®ä¿å®¹é”™

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬3ç« å°†ä»‹ç»**Node Parser**ï¼ŒLlamaIndexçš„æ™ºèƒ½åˆ†å—ç­–ç•¥ã€‚
- **Document**ï¼šåŸå§‹æ–‡æ¡£ï¼Œä»æ•°æ®æºåŠ è½½
- **Node**ï¼šæ–‡æ¡£åˆ†å—åçš„å•å…ƒï¼Œç”¨äºç´¢å¼•å’Œæ£€ç´¢

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter

# åŠ è½½Document
documents = SimpleDirectoryReader("./data").load_data()
print(f"Documents: {len(documents)}ä¸ª")

# åˆ†å—ä¸ºNodes
parser = SentenceSplitter(chunk_size=1024)
nodes = parser.get_nodes_from_documents(documents)
print(f"Nodes: {len(nodes)}ä¸ª")
```

---

### 3.1.2 ä¸ºä»€ä¹ˆéœ€è¦Node Parser

**æŒ‘æˆ˜**ï¼š
1. **Embeddingé™åˆ¶**ï¼šæ¨¡å‹æœ‰tokené™åˆ¶
2. **æ£€ç´¢ç²¾åº¦**ï¼šå¤§å—é™ä½ç›¸å…³æ€§
3. **ä¸Šä¸‹æ–‡çª—å£**ï¼šLLMå¤„ç†æœ‰é™

**Node Parserè§£å†³**ï¼š
- æ™ºèƒ½åˆ†å—ï¼ˆè¯­ä¹‰è¾¹ç•Œï¼‰
- ä¿ç•™å…ƒæ•°æ®ï¼ˆæ¥æºã€ä½ç½®ï¼‰
- æ”¯æŒå¤šç§åˆ†å—ç­–ç•¥

---

## 3.2 SentenceSplitterï¼ˆæ¨èï¼‰

### 3.2.1 åŸºç¡€ä½¿ç”¨

```python
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºSentenceSplitter
splitter = SentenceSplitter(
    chunk_size=1024,        # æ¯ä¸ªchunkçš„tokenæ•°
    chunk_overlap=20,       # chunkä¹‹é—´çš„é‡å tokenæ•°
    separator=" "           # åˆ†éš”ç¬¦
)

# åˆ†å—
nodes = splitter.get_nodes_from_documents(documents)

print(f"ç”Ÿæˆ{len(nodes)}ä¸ªèŠ‚ç‚¹")
for node in nodes[:2]:
    print(f"\nNode ID: {node.node_id}")
    print(f"å†…å®¹: {node.text[:200]}...")
    print(f"å…ƒæ•°æ®: {node.metadata}")
```

---

### 3.2.2 å‚æ•°è°ƒä¼˜

```python
from llama_index.core.node_parser import SentenceSplitter

# é…ç½®1: å°chunkï¼ˆç²¾ç¡®æ£€ç´¢ï¼‰
small_splitter = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=50
)

# é…ç½®2: å¤§chunkï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
large_splitter = SentenceSplitter(
    chunk_size=2048,
    chunk_overlap=200
)

# é…ç½®3: è‡ªå®šä¹‰åˆ†éš”ç¬¦
custom_splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=100,
    separator="\n\n"  # æŒ‰æ®µè½åˆ†å‰²
)

# å¯¹æ¯”æ•ˆæœ
small_nodes = small_splitter.get_nodes_from_documents(documents)
large_nodes = large_splitter.get_nodes_from_documents(documents)

print(f"å°chunk: {len(small_nodes)}ä¸ª")
print(f"å¤§chunk: {len(large_nodes)}ä¸ª")
```

**å‚æ•°é€‰æ‹©å»ºè®®**ï¼š

| åœºæ™¯ | chunk_size | chunk_overlap | è¯´æ˜ |
|------|-----------|---------------|------|
| **ç²¾ç¡®æ£€ç´¢** | 512-768 | 50-100 | å°chunkæé«˜æ£€ç´¢ç²¾åº¦ |
| **ä¿ç•™ä¸Šä¸‹æ–‡** | 1536-2048 | 150-200 | å¤§chunkä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ |
| **é€šç”¨åœºæ™¯** | 1024 | 100 | å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡ |

---

## 3.3 å…¶ä»–Node Parser

### 3.3.1 SemanticSplitterNodeParser

**åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—**ï¼š

```python
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

# åˆ›å»ºè¯­ä¹‰åˆ†å—å™¨
semantic_splitter = SemanticSplitterNodeParser(
    buffer_size=1,                          # å‰åç¼“å†²å¥å­æ•°
    embed_model=OpenAIEmbedding(),         # ä½¿ç”¨Embeddingæ¨¡å‹
    breakpoint_percentile_threshold=95     # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆç™¾åˆ†ä½ï¼‰
)

nodes = semantic_splitter.get_nodes_from_documents(documents)

print(f"è¯­ä¹‰åˆ†å—ï¼š{len(nodes)}ä¸ªèŠ‚ç‚¹")
```

**å·¥ä½œåŸç†**ï¼š
1. å¯¹æ¯ä¸ªå¥å­ç”Ÿæˆembedding
2. è®¡ç®—ç›¸é‚»å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
3. åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ†å‰²ï¼ˆä¸»é¢˜è¾¹ç•Œï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… ä¿ç•™è¯­ä¹‰å®Œæ•´æ€§
- âœ… è‡ªåŠ¨è¯†åˆ«ä¸»é¢˜å˜åŒ–

**åŠ£åŠ¿**ï¼š
- âŒ éœ€è¦è°ƒç”¨Embedding APIï¼ˆæˆæœ¬ï¼‰
- âŒ é€Ÿåº¦æ…¢

---

### 3.3.2 MarkdownNodeParser

**ä¸“ä¸ºMarkdownä¼˜åŒ–**ï¼š

```python
from llama_index.core.node_parser import MarkdownNodeParser

# é€‚ç”¨äºLlamaParseè¾“å‡ºçš„Markdown
parser = MarkdownNodeParser()

# å‡è®¾documentsæ¥è‡ªLlamaParse
# documents = llamaparse.load_data("document.pdf")

nodes = parser.get_nodes_from_documents(documents)

# æŸ¥çœ‹èŠ‚ç‚¹å…ƒæ•°æ®ï¼ˆåŒ…å«Markdownç»“æ„ï¼‰
for node in nodes[:3]:
    print(f"æ ‡é¢˜å±‚çº§: {node.metadata.get('header_path', 'N/A')}")
    print(f"å†…å®¹: {node.text[:150]}...\n")
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä¿ç•™Markdownç»“æ„
- âœ… æ ‡é¢˜å±‚çº§ä½œä¸ºå…ƒæ•°æ®
- âœ… ä¸LlamaParseå®Œç¾é…åˆ

---

### 3.3.3 CodeSplitter

**ä¸“ä¸ºä»£ç æ–‡æ¡£ä¼˜åŒ–**ï¼š

```python
from llama_index.core.node_parser import CodeSplitter

# ä»£ç åˆ†å—å™¨
code_splitter = CodeSplitter(
    language="python",       # ç¼–ç¨‹è¯­è¨€
    chunk_lines=40,          # æ¯ä¸ªchunkçš„è¡Œæ•°
    chunk_lines_overlap=15,  # é‡å è¡Œæ•°
    max_chars=1500          # æœ€å¤§å­—ç¬¦æ•°
)

# é€‚ç”¨äºä»£ç æ–‡ä»¶
code_documents = SimpleDirectoryReader(
    "./code",
    required_exts=[".py"]
).load_data()

nodes = code_splitter.get_nodes_from_documents(code_documents)

print(f"ä»£ç åˆ†å—ï¼š{len(nodes)}ä¸ªèŠ‚ç‚¹")
```

---

### 3.3.4 HierarchicalNodeParser

**å±‚çº§åˆ†å—ï¼ˆå¤šç²’åº¦ï¼‰**ï¼š

```python
from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes

# åˆ›å»ºå±‚çº§åˆ†å—å™¨
hierarchical_parser = HierarchicalNodeParser.from_defaults(
    chunk_sizes=[2048, 512, 128]  # ä¸‰ä¸ªå±‚çº§ï¼šç²—ã€ä¸­ã€ç»†
)

nodes = hierarchical_parser.get_nodes_from_documents(documents)
leaf_nodes = get_leaf_nodes(nodes)  # è·å–æœ€ç»†ç²’åº¦çš„èŠ‚ç‚¹

print(f"æ€»èŠ‚ç‚¹æ•°ï¼š{len(nodes)}ä¸ª")
print(f"å¶å­èŠ‚ç‚¹æ•°ï¼š{len(leaf_nodes)}ä¸ª")

# ç”¨äºç´¢å¼•çš„æ˜¯å¶å­èŠ‚ç‚¹
index = VectorStoreIndex(leaf_nodes)
```

**ç”¨é€”**ï¼š
- ç²—ç²’åº¦ï¼šæ–‡æ¡£çº§åˆ«æ£€ç´¢
- ä¸­ç²’åº¦ï¼šæ®µè½çº§åˆ«æ£€ç´¢
- ç»†ç²’åº¦ï¼šå¥å­çº§åˆ«æ£€ç´¢

---

## 3.4 Metadataæå–

### 3.4.1 è‡ªåŠ¨æå–å…ƒæ•°æ®

```python
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import (
    TitleExtractor,
    QuestionsAnsweredExtractor,
    SummaryExtractor,
    KeywordExtractor
)
from llama_index.llms.openai import OpenAI

# åˆ›å»ºå…ƒæ•°æ®æå–å™¨
metadata_extractors = [
    TitleExtractor(llm=OpenAI(model="gpt-3.5-turbo")),              # æå–æ ‡é¢˜
    QuestionsAnsweredExtractor(llm=OpenAI(model="gpt-3.5-turbo")),  # ç”Ÿæˆé—®é¢˜
    SummaryExtractor(llm=OpenAI(model="gpt-3.5-turbo")),            # ç”Ÿæˆæ‘˜è¦
    KeywordExtractor(llm=OpenAI(model="gpt-3.5-turbo"))             # æå–å…³é”®è¯
]

# åˆ›å»ºå¸¦å…ƒæ•°æ®æå–çš„Node Parser
from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=1024),
        *metadata_extractors
    ]
)

nodes = pipeline.run(documents=documents)

# æŸ¥çœ‹å¢å¼ºçš„å…ƒæ•°æ®
for node in nodes[:2]:
    print(f"\nèŠ‚ç‚¹ID: {node.node_id}")
    print(f"æ ‡é¢˜: {node.metadata.get('document_title', 'N/A')}")
    print(f"æ‘˜è¦: {node.metadata.get('section_summary', 'N/A')[:100]}...")
    print(f"å…³é”®è¯: {node.metadata.get('excerpt_keywords', 'N/A')}")
```

---

### 3.4.2 è‡ªå®šä¹‰å…ƒæ•°æ®

```python
from llama_index.core.node_parser import SentenceSplitter

# æ‰‹åŠ¨æ·»åŠ å…ƒæ•°æ®åˆ°Document
for doc in documents:
    doc.metadata["source_type"] = "technical_manual"
    doc.metadata["department"] = "engineering"
    doc.metadata["version"] = "v2.0"

# åˆ†å—æ—¶å…ƒæ•°æ®ä¼šç»§æ‰¿
splitter = SentenceSplitter(chunk_size=1024)
nodes = splitter.get_nodes_from_documents(documents)

# éªŒè¯å…ƒæ•°æ®ç»§æ‰¿
for node in nodes[:2]:
    print(f"æ¥æºç±»å‹: {node.metadata.get('source_type')}")
    print(f"éƒ¨é—¨: {node.metadata.get('department')}")
    print(f"ç‰ˆæœ¬: {node.metadata.get('version')}\n")
```

---

## 3.5 åˆ†å—è´¨é‡è¯„ä¼°

```python
def evaluate_nodes(nodes: list) -> dict:
    """è¯„ä¼°èŠ‚ç‚¹åˆ†å—è´¨é‡"""
    lengths = [len(node.text) for node in nodes]

    stats = {
        'total_nodes': len(nodes),
        'avg_length': sum(lengths) / len(lengths) if lengths else 0,
        'min_length': min(lengths) if lengths else 0,
        'max_length': max(lengths) if lengths else 0,
    }

    # æ£€æŸ¥åˆ†å¸ƒ
    too_small = sum(1 for l in lengths if l < 100)
    too_large = sum(1 for l in lengths if l > 3000)

    stats['too_small'] = too_small
    stats['too_large'] = too_large
    stats['quality'] = 'good' if (too_small + too_large) < len(nodes) * 0.1 else 'poor'

    return stats

# ä½¿ç”¨
stats = evaluate_nodes(nodes)
print(f"æ€»èŠ‚ç‚¹æ•°ï¼š{stats['total_nodes']}")
print(f"å¹³å‡é•¿åº¦ï¼š{stats['avg_length']:.0f} å­—ç¬¦")
print(f"è´¨é‡è¯„ä¼°ï¼š{stats['quality']}")
```

---

## å°ç»“

**ç¬¬3ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Node Parseré‡è¦æ€§**ï¼š
   - å°†Documentè½¬æ¢ä¸ºNodeï¼ˆåˆ†å—å•å…ƒï¼‰
   - æ§åˆ¶ç´¢å¼•ç²’åº¦å’Œæ£€ç´¢ç²¾åº¦
   - ä¿ç•™å…ƒæ•°æ®å’Œä¸Šä¸‹æ–‡å…³ç³»

2. **Parseré€‰æ‹©**ï¼š
   - é€šç”¨åœºæ™¯ â†’ SentenceSplitter
   - Markdownæ–‡æ¡£ â†’ MarkdownNodeParser
   - ä»£ç æ–‡æ¡£ â†’ CodeSplitter
   - è¯­ä¹‰å®Œæ•´æ€§ â†’ SemanticSplitterNodeParser
   - å¤šç²’åº¦æ£€ç´¢ â†’ HierarchicalNodeParser

3. **æœ€ä½³å®è·µ**ï¼š
   - chunk_size: 512-2048ï¼ˆæ ¹æ®åœºæ™¯ï¼‰
   - chunk_overlap: 10-20%çš„chunk_size
   - ä½¿ç”¨å…ƒæ•°æ®æå–å™¨å¢å¼ºæ£€ç´¢
   - è¯„ä¼°åˆ†å—è´¨é‡å¹¶è¿­ä»£ä¼˜åŒ–

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬4ç« å°†ä»‹ç»**å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†**ï¼ŒåŒ…æ‹¬å›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨çš„æ™ºèƒ½æå–ã€‚

---

# ç¬¬4ç« ï¼šå¤šæ¨¡æ€æ–‡æ¡£å¤„ç†

## 4.1 å›¾ç‰‡æå–ä¸ç†è§£

### 4.1.1 åŸºç¡€å›¾ç‰‡æå–

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import ImageReader

# ä½¿ç”¨ImageReaderæå–å›¾ç‰‡
image_reader = ImageReader()

# æ–¹å¼1: ç›´æ¥åŠ è½½å›¾ç‰‡æ–‡ä»¶
image_docs = image_reader.load_data(file="diagram.png")

# æ–¹å¼2: ä»PDFä¸­æå–å›¾ç‰‡ï¼ˆéœ€è¦é…åˆå…¶ä»–å·¥å…·ï¼‰
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    gpt4o_mode=True,  # å¯ç”¨å›¾ç‰‡ç†è§£
    parsing_instruction="æå–å¹¶æè¿°æ‰€æœ‰å›¾ç‰‡å’Œå›¾è¡¨"
)

documents = parser.load_data("illustrated_document.pdf")

# LlamaParseä¼šåœ¨Markdownä¸­åŒ…å«å›¾ç‰‡æè¿°
for doc in documents:
    if "![" in doc.text:  # Markdownå›¾ç‰‡è¯­æ³•
        print("å‘ç°å›¾ç‰‡å¼•ç”¨ï¼š")
        print(doc.text[:300])
```

---

### 4.1.2 å¤šæ¨¡æ€Embedding

**ä½¿ç”¨CLIPç­‰å¤šæ¨¡æ€æ¨¡å‹**ï¼š

```python
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.clip import ClipEmbedding
from llama_index.core.schema import ImageDocument

# åˆ›å»ºå¤šæ¨¡æ€Embeddingæ¨¡å‹
clip_embed = ClipEmbedding()

# åŠ è½½å›¾ç‰‡æ–‡æ¡£
image_documents = [
    ImageDocument(image_path="diagram1.png"),
    ImageDocument(image_path="chart1.png"),
]

# æ„å»ºå¤šæ¨¡æ€ç´¢å¼•
index = VectorStoreIndex.from_documents(
    image_documents,
    embed_model=clip_embed
)

# æŸ¥è¯¢ï¼ˆæ”¯æŒæ–‡æœ¬æŸ¥è¯¢å›¾ç‰‡ï¼‰
query_engine = index.as_query_engine()
response = query_engine.query("ç³»ç»Ÿæ¶æ„å›¾")
print(response.response)
```

---

## 4.2 è¡¨æ ¼å¤„ç†

### 4.2.1 LlamaParseè¡¨æ ¼æå–

**LlamaParseçš„è¡¨æ ¼ä¼˜åŠ¿**ï¼ˆå·²åœ¨ç¬¬2ç« ä»‹ç»ï¼‰ï¼š

```python
from llama_parse import LlamaParse

parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    parsing_instruction="""
    é‡ç‚¹æå–è¡¨æ ¼ï¼š
    1. ä¿ç•™å®Œæ•´ç»“æ„ï¼ˆMarkdownæ ¼å¼ï¼‰
    2. è·¨é¡µè¡¨æ ¼åˆå¹¶
    3. è¡¨å¤´ä¸æ•°æ®å…³è”
    """
)

documents = parser.load_data("tables_document.pdf")

# æå–Markdownè¡¨æ ¼
tables = []
for doc in documents:
    lines = doc.text.split("\n")
    table_lines = []
    in_table = False

    for line in lines:
        if "|" in line:
            in_table = True
            table_lines.append(line)
        elif in_table and not line.strip():
            # è¡¨æ ¼ç»“æŸ
            tables.append("\n".join(table_lines))
            table_lines = []
            in_table = False

    if table_lines:
        tables.append("\n".join(table_lines))

print(f"æå–åˆ°{len(tables)}ä¸ªè¡¨æ ¼")
for i, table in enumerate(tables[:2], 1):
    print(f"\nè¡¨æ ¼{i}ï¼š")
    print(table)
```

---

### 4.2.2 è¡¨æ ¼è½¬ç»“æ„åŒ–æ•°æ®

```python
import pandas as pd
from io import StringIO

def markdown_table_to_df(markdown_table: str) -> pd.DataFrame:
    """å°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºDataFrame"""
    lines = markdown_table.strip().split("\n")

    # ç§»é™¤åˆ†éš”çº¿ï¼ˆç¬¬äºŒè¡Œï¼‰
    if len(lines) >= 2 and "---" in lines[1]:
        lines.pop(1)

    # è½¬æ¢ä¸ºCSVæ ¼å¼
    csv_lines = []
    for line in lines:
        # ç§»é™¤é¦–å°¾çš„ |
        line = line.strip("|").strip()
        # æ›¿æ¢ | ä¸º ,
        csv_line = line.replace("|", ",")
        csv_lines.append(csv_line)

    csv_str = "\n".join(csv_lines)

    # è¯»å–ä¸ºDataFrame
    df = pd.read_csv(StringIO(csv_str))
    return df

# ä½¿ç”¨
for i, table_md in enumerate(tables[:2], 1):
    df = markdown_table_to_df(table_md)
    print(f"\nè¡¨æ ¼{i}ï¼ˆDataFrameï¼‰ï¼š")
    print(df)

    # å¯ä»¥è¿›è¡Œæ•°æ®åˆ†æ
    if 'æ”¶å…¥' in df.columns:
        print(f"æ€»æ”¶å…¥ï¼š{df['æ”¶å…¥'].sum()}")
```

---

## 4.3 å›¾è¡¨ç†è§£ï¼ˆCharts & Diagramsï¼‰

### 4.3.1 ä½¿ç”¨GPT-4oç†è§£å›¾è¡¨

```python
from llama_parse import LlamaParse
from llama_index.llms.openai import OpenAI
from llama_index.core import VectorStoreIndex

# æ­¥éª¤1: ä½¿ç”¨LlamaParseæå–å›¾è¡¨ï¼ˆå¸¦æè¿°ï¼‰
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    gpt4o_mode=True,
    parsing_instruction="""
    å¯¹äºå›¾è¡¨ï¼š
    1. è¯†åˆ«å›¾è¡¨ç±»å‹ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ç­‰ï¼‰
    2. æå–å›¾è¡¨æ ‡é¢˜
    3. æè¿°å›¾è¡¨æ•°æ®è¶‹åŠ¿
    4. å¦‚æœå¯èƒ½ï¼Œå°†å›¾è¡¨æ•°æ®è½¬æ¢ä¸ºè¡¨æ ¼
    """
)

documents = parser.load_data("charts_document.pdf")

# æ­¥éª¤2: æ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æ­¥éª¤3: æŸ¥è¯¢å›¾è¡¨ä¿¡æ¯
query_engine = index.as_query_engine(
    llm=OpenAI(model="gpt-4")
)

response = query_engine.query("é”€å”®é¢çš„è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response.response)
```

---

### 4.3.2 ä¸“ç”¨å›¾è¡¨å·¥å…·ï¼ˆChartReaderï¼‰

```python
# å‡è®¾æœ‰ä¸€ä¸ªä¸“ç”¨çš„ChartReaderï¼ˆç¤¾åŒºå·¥å…·ï¼‰
# è¿™æ˜¯ç¤ºä¾‹æ¦‚å¿µï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“å·¥å…·è°ƒæ•´

from llama_index.readers.file import ImageReader
from PIL import Image

# è¯»å–å›¾è¡¨å›¾ç‰‡
image_path = "sales_chart.png"
img = Image.open(image_path)

# ä½¿ç”¨OCR + LLMç†è§£å›¾è¡¨
from llama_index.llms.openai import OpenAI

llm = OpenAI(model="gpt-4o")  # GPT-4oæ”¯æŒè§†è§‰

# å°†å›¾ç‰‡ç¼–ç ä¸ºbase64
import base64
from io import BytesIO

buffered = BytesIO()
img.save(buffered, format="PNG")
img_str = base64.b64encode(buffered.getvalue()).decode()

# æŸ¥è¯¢å›¾è¡¨å†…å®¹
prompt = f"""
è¿™æ˜¯ä¸€å¼ å›¾è¡¨ï¼Œè¯·åˆ†æï¼š
1. å›¾è¡¨ç±»å‹
2. æ•°æ®è¶‹åŠ¿
3. å…³é”®å‘ç°

![chart](data:image/png;base64,{img_str})
"""

# æ³¨ï¼šå®é™…ä½¿ç”¨éœ€è¦æ”¯æŒå›¾ç‰‡è¾“å…¥çš„API
# response = llm.complete(prompt)
# print(response.text)
```

---

## 4.4 å¤šæ¨¡æ€RAGå®Œæ•´ç¤ºä¾‹

### 4.4.1 æ„å»ºå¤šæ¨¡æ€çŸ¥è¯†åº“

```python
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# æ­¥éª¤1: ä½¿ç”¨LlamaParseå¤„ç†å¤šæ¨¡æ€PDF
parser = LlamaParse(
    api_key="llx-your-api-key",
    result_type="markdown",
    gpt4o_mode=True,  # å¯ç”¨å›¾ç‰‡ç†è§£
    parsing_instruction="""
    å¤„ç†æ‰€æœ‰å¤šæ¨¡æ€å†…å®¹ï¼š
    1. æ–‡æœ¬æ®µè½
    2. è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼ï¼‰
    3. å›¾ç‰‡ï¼ˆç”Ÿæˆæè¿°ï¼‰
    4. å›¾è¡¨ï¼ˆæå–æ•°æ®+æè¿°è¶‹åŠ¿ï¼‰
    """
)

documents = parser.load_data("multimodal_report.pdf")

# æ­¥éª¤2: åˆ†å—
node_parser = MarkdownNodeParser()
nodes = node_parser.get_nodes_from_documents(documents)

# æ­¥éª¤3: æ„å»ºç´¢å¼•
index = VectorStoreIndex(
    nodes=nodes,
    embed_model=OpenAIEmbedding()
)

# æ­¥éª¤4: åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    llm=OpenAI(model="gpt-4"),
    similarity_top_k=5
)

# æ­¥éª¤5: å¤šæ¨¡æ€æŸ¥è¯¢
queries = [
    "æŠ¥å‘Šä¸­çš„ä¸»è¦å›¾è¡¨æ˜¾ç¤ºäº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿ",
    "è´¢åŠ¡æ•°æ®è¡¨æ ¼ä¸­çš„æ€»æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
    "ç³»ç»Ÿæ¶æ„å›¾åŒ…å«å“ªäº›ç»„ä»¶ï¼Ÿ"
]

for query in queries:
    print(f"\næŸ¥è¯¢ï¼š{query}")
    response = query_engine.query(query)
    print(f"å›ç­”ï¼š{response.response}\n")

    # æŸ¥çœ‹æ£€ç´¢åˆ°çš„æºèŠ‚ç‚¹
    print("æ¥æºèŠ‚ç‚¹ï¼š")
    for i, node in enumerate(response.source_nodes[:2], 1):
        print(f"{i}. {node.text[:150]}...")
```

---

## å°ç»“

**ç¬¬4ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›**ï¼š
   - å›¾ç‰‡æå–ä¸æè¿°ï¼ˆLlamaParse + GPT-4oï¼‰
   - è¡¨æ ¼è¯†åˆ«ä¸ç»“æ„åŒ–ï¼ˆMarkdown â†’ DataFrameï¼‰
   - å›¾è¡¨ç†è§£ï¼ˆç±»å‹è¯†åˆ« + æ•°æ®æå–ï¼‰

2. **å·¥å…·é€‰æ‹©**ï¼š
   - å›¾ç‰‡ç†è§£ â†’ LlamaParse (gpt4o_mode) + CLIP
   - è¡¨æ ¼æå– â†’ LlamaParse (Markdownæ ¼å¼)
   - å›¾è¡¨åˆ†æ â†’ GPT-4oè§†è§‰èƒ½åŠ›

3. **æœ€ä½³å®è·µ**ï¼š
   - ä½¿ç”¨LlamaParseçš„gpt4o_modeå¤„ç†å¤æ‚å¤šæ¨¡æ€æ–‡æ¡£
   - è¡¨æ ¼è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ä¾¿äºåˆ†æ
   - å›¾è¡¨æè¿°ä¸åŸå§‹æ•°æ®å…³è”
   - å¤šæ¨¡æ€RAGæå‡æŸ¥è¯¢è¦†ç›–èŒƒå›´

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬5ç« å°†æ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»º**ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline**ã€‚

---

# ç¬¬5ç« ï¼šç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline

## 5.1 Pipelineè®¾è®¡

### 5.1.1 å®Œæ•´æµç¨‹

```
æ–‡æ¡£è¾“å…¥
  â†“
ç±»å‹æ£€æµ‹ï¼ˆPDF, DOCX, å›¾ç‰‡...ï¼‰
  â†“
å¤æ‚åº¦è¯„ä¼°ï¼ˆç®€å• vs å¤æ‚ï¼‰
  â†“
é€‰æ‹©å¤„ç†ç­–ç•¥
  â”œâ”€â”€ ç®€å•PDF â†’ PDFReader
  â”œâ”€â”€ å¤æ‚PDF â†’ LlamaParse
  â”œâ”€â”€ å›¾ç‰‡ â†’ ImageReader + GPT-4o
  â””â”€â”€ å…¶ä»–æ ¼å¼ â†’ SimpleDirectoryReader
  â†“
Node Parseråˆ†å—
  â†“
å…ƒæ•°æ®æå–ï¼ˆå¯é€‰ï¼‰
  â†“
å‘é‡åŒ– + å­˜å‚¨
  â†“
RAGç³»ç»Ÿ
```

---

### 5.1.2 å®Œæ•´å®ç°

```python
from typing import List, Dict
from pathlib import Path
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.schema import Document
from llama_index.readers.file import PDFReader, PyMuPDFReader
from llama_parse import LlamaParse
from llama_index.core.node_parser import SentenceSplitter, MarkdownNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†å™¨ï¼ˆLlamaIndexç‰ˆï¼‰"""

    def __init__(self, llamaparse_api_key: str = None, use_llamaparse: bool = True):
        self.use_llamaparse = use_llamaparse
        self.llamaparse_api_key = llamaparse_api_key
        self.embeddings = OpenAIEmbedding()

    def process_document(self, file_path: str) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆè‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰"""
        # æ­¥éª¤1: æ£€æµ‹æ–‡ä»¶ç±»å‹
        file_type = self._detect_file_type(file_path)
        logger.info(f"æ–‡ä»¶ç±»å‹ï¼š{file_type}")

        # æ­¥éª¤2: é€‰æ‹©å¤„ç†ç­–ç•¥
        if file_type == 'PDF':
            return self._process_pdf(file_path)
        elif file_type in ['DOCX', 'DOC']:
            return self._process_docx(file_path)
        elif file_type in ['PNG', 'JPG', 'JPEG']:
            return self._process_image(file_path)
        else:
            # é€šç”¨å¤„ç†
            return self._process_generic(file_path)

    def _detect_file_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        suffix = Path(file_path).suffix.lower()
        type_map = {
            '.pdf': 'PDF',
            '.docx': 'DOCX',
            '.doc': 'DOC',
            '.png': 'PNG',
            '.jpg': 'JPG',
            '.jpeg': 'JPEG',
            '.txt': 'TXT',
            '.md': 'MD'
        }
        return type_map.get(suffix, 'UNKNOWN')

    def _assess_pdf_complexity(self, pdf_path: str) -> str:
        """è¯„ä¼°PDFå¤æ‚åº¦"""
        # ç®€å•å¯å‘å¼ï¼šæ–‡ä»¶å¤§å°
        file_size = Path(pdf_path).stat().st_size

        if file_size > 5 * 1024 * 1024:  # > 5MB
            return 'complex'

        # å¯ä»¥æ·»åŠ æ›´å¤šå¯å‘å¼ï¼ˆé¡µæ•°ã€æ˜¯å¦æœ‰å›¾ç‰‡ç­‰ï¼‰
        return 'simple'

    def _process_pdf(self, pdf_path: str) -> List[Document]:
        """å¤„ç†PDFï¼ˆå¸¦æ™ºèƒ½é™çº§ï¼‰"""
        complexity = self._assess_pdf_complexity(pdf_path)

        # ç­–ç•¥1: å¤æ‚PDFä½¿ç”¨LlamaParse
        if complexity == 'complex' and self.use_llamaparse:
            try:
                logger.info("å¤æ‚PDFï¼Œä½¿ç”¨LlamaParse...")
                parser = LlamaParse(
                    api_key=self.llamaparse_api_key,
                    result_type="markdown",
                    gpt4o_mode=True
                )
                docs = parser.load_data(pdf_path)
                logger.info(f"âœ… LlamaParseæˆåŠŸï¼Œæå–{len(docs)}ä¸ªæ–‡æ¡£")
                return docs
            except Exception as e:
                logger.warning(f"LlamaParseå¤±è´¥: {e}ï¼Œé™çº§åˆ°ä¼ ç»Ÿå·¥å…·")

        # ç­–ç•¥2: ç®€å•PDFä½¿ç”¨PyMuPDFReaderï¼ˆå¿«é€Ÿï¼‰
        try:
            logger.info("ä½¿ç”¨PyMuPDFReader...")
            reader = PyMuPDFReader()
            docs = reader.load_data(file=pdf_path)
            logger.info(f"âœ… PyMuPDFReaderæˆåŠŸï¼Œæå–{len(docs)}é¡µ")
            return docs
        except Exception as e:
            logger.warning(f"PyMuPDFReaderå¤±è´¥: {e}")

        # ç­–ç•¥3: åŸºç¡€PDFReader
        try:
            logger.info("ä½¿ç”¨PDFReader...")
            reader = PDFReader()
            docs = reader.load_data(file=pdf_path)
            logger.info(f"âœ… PDFReaderæˆåŠŸï¼Œæå–{len(docs)}é¡µ")
            return docs
        except Exception as e:
            logger.error(f"æ‰€æœ‰PDFå¤„ç†ç­–ç•¥å¤±è´¥: {e}")
            return []

    def _process_docx(self, file_path: str) -> List[Document]:
        """å¤„ç†DOCX"""
        from llama_index.readers.file import DocxReader
        reader = DocxReader()
        return reader.load_data(file=file_path)

    def _process_image(self, file_path: str) -> List[Document]:
        """å¤„ç†å›¾ç‰‡"""
        from llama_index.readers.file import ImageReader
        reader = ImageReader()
        return reader.load_data(file=file_path)

    def _process_generic(self, file_path: str) -> List[Document]:
        """é€šç”¨å¤„ç†"""
        reader = SimpleDirectoryReader(input_files=[file_path])
        return reader.load_data()

    def build_index(
        self,
        documents: List[Document],
        chunk_size: int = 1024,
        chunk_overlap: int = 100,
        use_markdown_parser: bool = False
    ) -> VectorStoreIndex:
        """æ„å»ºç´¢å¼•"""
        # æ­¥éª¤1: é€‰æ‹©Node Parser
        if use_markdown_parser:
            node_parser = MarkdownNodeParser()
        else:
            node_parser = SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )

        # æ­¥éª¤2: åˆ†å—
        nodes = node_parser.get_nodes_from_documents(documents)
        logger.info(f"ç”Ÿæˆ{len(nodes)}ä¸ªèŠ‚ç‚¹")

        # æ­¥éª¤3: æ„å»ºç´¢å¼•
        index = VectorStoreIndex(
            nodes=nodes,
            embed_model=self.embeddings
        )

        return index

# ä½¿ç”¨ç¤ºä¾‹
processor = DocumentProcessor(
    llamaparse_api_key="llx-your-api-key",
    use_llamaparse=True
)

# å¤„ç†å•ä¸ªæ–‡æ¡£
docs = processor.process_document("./complex_document.pdf")
print(f"æå–{len(docs)}ä¸ªæ–‡æ¡£")

# æ„å»ºç´¢å¼•
index = processor.build_index(docs, use_markdown_parser=True)
print("ç´¢å¼•æ„å»ºå®Œæˆ")
```

---

## 5.2 æ‰¹é‡å¤„ç†ä¸è´¨é‡æ§åˆ¶

### 5.2.1 æ‰¹é‡å¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def batch_process_directory(
    processor: DocumentProcessor,
    directory: str,
    max_workers: int = 4
) -> List[Document]:
    """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£"""
    all_docs = []
    files = list(Path(directory).rglob("*"))

    # è¿‡æ»¤æ”¯æŒçš„æ ¼å¼
    supported_exts = ['.pdf', '.docx', '.txt', '.md', '.png', '.jpg']
    files = [f for f in files if f.suffix.lower() in supported_exts]

    logger.info(f"æ‰¾åˆ°{len(files)}ä¸ªæ–‡ä»¶")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(processor.process_document, str(f)): f
            for f in files
        }

        for future in as_completed(futures):
            file = futures[future]
            try:
                docs = future.result()
                all_docs.extend(docs)
                logger.info(f"âœ… {file.name}: {len(docs)}ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"âŒ {file.name}: {e}")

    return all_docs

# ä½¿ç”¨
all_documents = batch_process_directory(processor, "./knowledge_base", max_workers=4)
print(f"\næ€»è®¡å¤„ç†ï¼š{len(all_documents)}ä¸ªæ–‡æ¡£")
```

---

### 5.2.2 è´¨é‡æ£€æµ‹

```python
def assess_document_quality(documents: List[Document]) -> Dict:
    """è¯„ä¼°æ–‡æ¡£æå–è´¨é‡"""
    total_text = "".join([doc.text for doc in documents])

    assessment = {
        'total_docs': len(documents),
        'total_chars': len(total_text),
        'avg_doc_length': len(total_text) / len(documents) if documents else 0,
        'has_content': len(total_text) > 100,
        'quality_score': 0.0
    }

    # è®¡ç®—è´¨é‡åˆ†æ•°
    score = 0

    # æœ‰è¶³å¤Ÿå†…å®¹ (+40åˆ†)
    if assessment['total_chars'] > 1000:
        score += 40
    elif assessment['total_chars'] > 100:
        score += 20

    # å¹³å‡æ–‡æ¡£é•¿åº¦åˆç† (+30åˆ†)
    avg_len = assessment['avg_doc_length']
    if 200 < avg_len < 2000:
        score += 30
    elif 100 < avg_len < 5000:
        score += 15

    # æ–‡æ¡£æ•°é‡åˆç† (+30åˆ†)
    if 5 < len(documents) < 100:
        score += 30
    elif len(documents) > 0:
        score += 15

    assessment['quality_score'] = score

    # è¯„çº§
    if score >= 80:
        assessment['rating'] = 'ä¼˜ç§€'
    elif score >= 60:
        assessment['rating'] = 'è‰¯å¥½'
    elif score >= 40:
        assessment['rating'] = 'ä¸€èˆ¬'
    else:
        assessment['rating'] = 'å·®'

    return assessment

# ä½¿ç”¨
quality = assess_document_quality(docs)
print(f"è´¨é‡è¯„åˆ†ï¼š{quality['quality_score']}/100 ({quality['rating']})")
print(f"æ–‡æ¡£æ•°ï¼š{quality['total_docs']}")
print(f"æ€»å­—ç¬¦æ•°ï¼š{quality['total_chars']}")
```

---

## 5.3 å®Œæ•´RAGç³»ç»Ÿ

```python
from llama_index.core import VectorStoreIndex
from llama_index.llms.openai import OpenAI

# æ­¥éª¤1: å¤„ç†æ–‡æ¡£ç›®å½•
processor = DocumentProcessor(
    llamaparse_api_key="llx-your-api-key",
    use_llamaparse=True
)

all_docs = []
for file in Path("./knowledge_base").rglob("*.pdf"):
    logger.info(f"\nå¤„ç†ï¼š{file.name}")
    docs = processor.process_document(str(file))

    # è´¨é‡æ£€æµ‹
    quality = assess_document_quality(docs)
    logger.info(f"è´¨é‡ï¼š{quality['rating']} ({quality['quality_score']}/100)")

    if quality['quality_score'] >= 40:
        all_docs.extend(docs)
    else:
        logger.warning(f"è´¨é‡è¿‡ä½ï¼Œè·³è¿‡")

# æ­¥éª¤2: æ„å»ºç´¢å¼•
index = processor.build_index(all_docs, use_markdown_parser=True)
logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±{len(all_docs)}ä¸ªæ–‡æ¡£")

# æ­¥éª¤3: åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    llm=OpenAI(model="gpt-4"),
    similarity_top_k=5
)

# æ­¥éª¤4: æŸ¥è¯¢
queries = [
    "äº§å“çš„æŠ€æœ¯è§„æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è´¢æŠ¥ä¸­çš„ä¸»è¦æ•°æ®è¶‹åŠ¿å¦‚ä½•ï¼Ÿ",
    "ç³»ç»Ÿæ¶æ„åŒ…å«å“ªäº›ç»„ä»¶ï¼Ÿ"
]

for query in queries:
    print(f"\næŸ¥è¯¢ï¼š{query}")
    response = query_engine.query(query)
    print(f"å›ç­”ï¼š{response.response}\n")

    # æŸ¥çœ‹æºèŠ‚ç‚¹
    print("æ¥æºï¼š")
    for i, node in enumerate(response.source_nodes[:2], 1):
        source = node.metadata.get('file_name', 'unknown')
        print(f"{i}. {source}: {node.text[:100]}...")
```

---

## 5.4 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 5.4.1 æ··åˆç­–ç•¥

```python
class CostOptimizedProcessor(DocumentProcessor):
    """æˆæœ¬ä¼˜åŒ–çš„æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, llamaparse_api_key: str, monthly_budget: float = 100.0):
        super().__init__(llamaparse_api_key, use_llamaparse=True)
        self.monthly_budget = monthly_budget
        self.llamaparse_usage = 0.0  # æœ¬æœˆä½¿ç”¨é¢åº¦

    def _process_pdf(self, pdf_path: str) -> List[Document]:
        """æˆæœ¬ä¼˜åŒ–çš„PDFå¤„ç†"""
        complexity = self._assess_pdf_complexity(pdf_path)
        file_size_mb = Path(pdf_path).stat().st_size / (1024 * 1024)

        # ä¼°ç®—LlamaParseæˆæœ¬ï¼ˆå‡è®¾$0.003/é¡µï¼Œ20é¡µ/MBï¼‰
        estimated_pages = file_size_mb * 20
        estimated_cost = estimated_pages * 0.003

        # é¢„ç®—æ£€æŸ¥
        if (complexity == 'complex' and
            self.llamaparse_usage + estimated_cost < self.monthly_budget):
            try:
                logger.info(f"ä½¿ç”¨LlamaParseï¼ˆé¢„ä¼°æˆæœ¬ï¼š${estimated_cost:.2f}ï¼‰")
                parser = LlamaParse(api_key=self.llamaparse_api_key)
                docs = parser.load_data(pdf_path)
                self.llamaparse_usage += estimated_cost
                logger.info(f"æœ¬æœˆå·²ä½¿ç”¨ï¼š${self.llamaparse_usage:.2f}/{self.monthly_budget}")
                return docs
            except Exception as e:
                logger.warning(f"LlamaParseå¤±è´¥ï¼Œé™çº§")

        # ä½¿ç”¨å…è´¹å·¥å…·
        logger.info("ä½¿ç”¨å…è´¹å·¥å…·ï¼ˆPyMuPDFReaderï¼‰")
        reader = PyMuPDFReader()
        return reader.load_data(file=pdf_path)

# ä½¿ç”¨
cost_processor = CostOptimizedProcessor(
    llamaparse_api_key="llx-your-api-key",
    monthly_budget=50.0  # $50/æœˆé¢„ç®—
)

docs = cost_processor.process_document("document.pdf")
print(f"æœ¬æœˆLlamaParseä½¿ç”¨ï¼š${cost_processor.llamaparse_usage:.2f}")
```

---

### 5.4.2 ç¼“å­˜ç­–ç•¥

```python
import hashlib
import pickle
from pathlib import Path

class CachedDocumentProcessor(DocumentProcessor):
    """å¸¦ç¼“å­˜çš„æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, llamaparse_api_key: str, cache_dir: str = "./doc_cache"):
        super().__init__(llamaparse_api_key)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆç¼“å­˜keyï¼ˆåŸºäºæ–‡ä»¶å†…å®¹hashï¼‰"""
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash

    def process_document(self, file_path: str) -> List[Document]:
        """å¸¦ç¼“å­˜çš„æ–‡æ¡£å¤„ç†"""
        cache_key = self._get_cache_key(file_path)
        cache_path = self.cache_dir / f"{cache_key}.pkl"

        # æ£€æŸ¥ç¼“å­˜
        if cache_path.exists():
            logger.info(f"ä»ç¼“å­˜åŠ è½½ï¼š{file_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)

        # å¤„ç†æ–‡æ¡£
        docs = super().process_document(file_path)

        # ä¿å­˜ç¼“å­˜
        with open(cache_path, 'wb') as f:
            pickle.dump(docs, f)
        logger.info(f"ç¼“å­˜å·²ä¿å­˜ï¼š{cache_path}")

        return docs

# ä½¿ç”¨
cached_processor = CachedDocumentProcessor(
    llamaparse_api_key="llx-your-api-key",
    cache_dir="./doc_cache"
)

# ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆæ…¢ï¼‰
docs1 = cached_processor.process_document("document.pdf")

# ç¬¬äºŒæ¬¡å¤„ç†ï¼ˆå¿«ï¼Œä»ç¼“å­˜è¯»å–ï¼‰
docs2 = cached_processor.process_document("document.pdf")
```

---

## å…¨ç¯‡æ€»ç»“

**ç¬¬åä¸€ç¯‡ï¼ˆLlamaIndexç¯‡ï¼‰æ¶µç›–æŠ€æœ¯**ï¼š

| ç« èŠ‚ | æ ¸å¿ƒæŠ€æœ¯ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| ç¬¬1ç«  | SimpleDirectoryReader, LlamaHub | å¿«é€Ÿå…¥é—¨ã€å¤šæ•°æ®æº |
| ç¬¬2ç«  | LlamaParse | å¤æ‚PDFã€å­¦æœ¯è®ºæ–‡ã€è´¢æŠ¥ |
| ç¬¬3ç«  | Node Parserï¼ˆSentence, Semantic, Markdownï¼‰ | æ™ºèƒ½åˆ†å— |
| ç¬¬4ç«  | å¤šæ¨¡æ€å¤„ç†ï¼ˆå›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨ï¼‰ | å¯Œåª’ä½“æ–‡æ¡£ |
| ç¬¬5ç«  | ç”Ÿäº§çº§Pipeline | ä¼ä¸šçŸ¥è¯†åº“ |

---

## æ€è€ƒä¸ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šLlamaParse vs ä¼ ç»Ÿå·¥å…·å¯¹æ¯”

é€‰æ‹©3ç§ä¸åŒå¤æ‚åº¦çš„PDFï¼Œå¯¹æ¯”ï¼š
1. PDFReader
2. PyMuPDFReader
3. LlamaParse

æµ‹è¯•æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€è¡¨æ ¼å®Œæ•´æ€§ã€å¤„ç†æ—¶é—´ã€æˆæœ¬

### ç»ƒä¹ 2ï¼šæ„å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ

ä½¿ç”¨LlamaParseæ„å»ºæ”¯æŒï¼š
1. æ–‡æœ¬æ£€ç´¢
2. è¡¨æ ¼æŸ¥è¯¢
3. å›¾è¡¨åˆ†æ

çš„å®Œæ•´RAGç³»ç»Ÿ

### ç»ƒä¹ 3ï¼šæˆæœ¬ä¼˜åŒ–å®éªŒ

å®ç°æ··åˆç­–ç•¥ï¼š
1. ç®€å•æ–‡æ¡£ç”¨å…è´¹å·¥å…·
2. å¤æ‚æ–‡æ¡£ç”¨LlamaParse
3. è®¾ç½®æœˆåº¦é¢„ç®—é™åˆ¶
4. ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤å¤„ç†

---

## å‚è€ƒèµ„æº

**å®˜æ–¹æ–‡æ¡£**ï¼š
- [LlamaIndexå®˜æ–¹æ–‡æ¡£](https://developers.llamaindex.ai/)
- [LlamaParseæ–‡æ¡£](https://developers.llamaindex.ai/python/framework/llama_cloud/llama_parse/)
- [LlamaHub](https://llamahub.ai/)

**å…³é”®é“¾æ¥**ï¼š
- [LlamaCloudæ³¨å†Œ](https://cloud.llamaindex.ai/)
- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)

---

**ç¬¬åä¸€ç¯‡ï¼ˆLlamaIndexç¯‡ï¼‰å®Œæˆ**ï¼

ä½ å·²ç»æŒæ¡äº†LlamaIndexçš„æ–‡æ¡£å¤„ç†å®Œæ•´æŠ€æœ¯æ ˆï¼š
- âœ… SimpleDirectoryReaderå¿«é€Ÿå…¥é—¨
- âœ… LlamaHub 700+æ•°æ®åŠ è½½å™¨ç”Ÿæ€
- âœ… LlamaParse - GenAIåŸç”ŸPDFè§£æï¼ˆæ ¸å¿ƒäº®ç‚¹ï¼‰
- âœ… Node Parseræ™ºèƒ½åˆ†å—ç­–ç•¥
- âœ… å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†ï¼ˆå›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨ï¼‰
- âœ… ç”Ÿäº§çº§Pipelineä¸æˆæœ¬ä¼˜åŒ–

**LlamaIndex vs LangChainæ–‡æ¡£å¤„ç†å¯¹æ¯”**ï¼š

| ç»´åº¦ | LlamaIndex | LangChain |
|------|-----------|-----------|
| **æ ¸å¿ƒä¼˜åŠ¿** | LlamaParseï¼ˆGenAIåŸç”Ÿè§£æï¼‰ | Unstructured.ioï¼ˆå¤šæ ¼å¼ç»Ÿä¸€ï¼‰ |
| **è¡¨æ ¼å¤„ç†** | â­â­â­â­â­ LlamaParseå®Œç¾Markdown | â­â­â­â­ PDFPlumberè¡¨æ ¼æå– |
| **å›¾ç‰‡ç†è§£** | â­â­â­â­â­ GPT-4oå¤šæ¨¡æ€ | â­â­â­ OCRé›†æˆ |
| **å­¦ä¹ æ›²çº¿** | â­â­â­â­ æ˜“ä¸Šæ‰‹ï¼ˆSimpleDirectoryReaderï¼‰ | â­â­â­ ä¸­ç­‰ï¼ˆDocument Loadersï¼‰ |
| **æˆæœ¬** | LlamaParseä»˜è´¹ï¼ˆé«˜è´¨é‡ï¼‰ | ä¸»è¦å…è´¹ï¼ˆUnstructuredæœ‰ä»˜è´¹ç‰ˆï¼‰ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚æ–‡æ¡£ã€å­¦æœ¯è®ºæ–‡ã€è´¢æŠ¥ | é€šç”¨æ–‡æ¡£å¤„ç†ã€ä¼ä¸šçŸ¥è¯†åº“ |

**é€‰æ‹©å»ºè®®**ï¼š
- **é«˜è´¨é‡è¦æ±‚** â†’ LlamaIndex + LlamaParse
- **æˆæœ¬æ•æ„Ÿ** â†’ LangChain + Unstructured.io
- **æ··åˆä½¿ç”¨** â†’ ç®€å•æ–‡æ¡£ç”¨LangChainï¼Œå¤æ‚æ–‡æ¡£ç”¨LlamaParse

**ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼š
ç¬¬åäºŒç¯‡å°†èšç„¦**æç¤ºå·¥ç¨‹ä¸ä¸Šä¸‹æ–‡ä¼˜åŒ–**ï¼Œæå‡RAGç³»ç»Ÿçš„ç”Ÿæˆè´¨é‡å’Œæˆæœ¬æ•ˆç‡ã€‚
