# ç¬¬åç¯‡ ç”Ÿäº§å®è·µä¸ç›‘æ§è¯„ä¼°

> **ç›®æ ‡**: æ„å»ºç”Ÿäº§çº§LLMåº”ç”¨çš„å®Œæ•´ä½“ç³»

ä»ç›‘æ§è¿½è¸ªåˆ°æ¶æ„è®¾è®¡,ä»æ€§èƒ½ä¼˜åŒ–åˆ°å®‰å…¨é˜²æŠ¤,ä»éƒ¨ç½²è¿ç»´åˆ°æ•…éšœæ’æŸ¥,å…¨é¢æŒæ¡ç”Ÿäº§ç¯å¢ƒçš„å…³é”®è¦ç´ ã€‚

---

## ç¬¬1ç« ï¼šLangSmith Tracing ä¸ Evaluation

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ Agent æ‰§è¡Œçš„å…¨é“¾è·¯å¯è§‚æµ‹æ€§ï¼Œå»ºç«‹ç§‘å­¦çš„è¯„ä¼°æ¡†æ¶ã€‚

### 1.1 è¿½è¸ªä½“ç³»

#### 1.1.1 è¿½è¸ªåŸç†ä¸æ•°æ®æ¨¡å‹

**ä»€ä¹ˆæ˜¯è¿½è¸ªï¼ˆTracingï¼‰ï¼Ÿ**

è¿½è¸ªæ˜¯è®°å½•å’Œåˆ†æ Agent æ‰§è¡Œè¿‡ç¨‹çš„å®Œæ•´é“¾è·¯ï¼Œä»ç”¨æˆ·è¾“å…¥å¼€å§‹ï¼Œè®°å½•æ¯ä¸€ä¸ªä¸­é—´æ­¥éª¤ï¼ˆæ¨¡å‹è°ƒç”¨ã€å·¥å…·æ‰§è¡Œã€çŠ¶æ€å˜åŒ–ï¼‰ï¼Œæœ€ç»ˆå¾—åˆ°è¾“å‡ºã€‚LangSmith è¿½è¸ªå½¢æˆä¸€æ£µæ‰§è¡Œæ ‘ï¼š

```
root_run (Agent æ‰§è¡Œ)
â”œâ”€â”€ before_model_hook (Middleware)
â”œâ”€â”€ model_call (æ¨¡å‹è°ƒç”¨)
â”‚   â”œâ”€â”€ system_prompt
â”‚   â”œâ”€â”€ messages
â”‚   â””â”€â”€ tools
â”œâ”€â”€ tool_run (å·¥å…·æ‰§è¡Œ)
â”‚   â”œâ”€â”€ search_tool
â”‚   â””â”€â”€ get_weather_tool
â””â”€â”€ after_model_hook (åå¤„ç†)
```

**è¿½è¸ªçš„æ ¸å¿ƒä½œç”¨**ï¼š

1. **è°ƒè¯•**ï¼šçœ‹åˆ°å®Œæ•´çš„æ‰§è¡Œé“¾ï¼Œå¿«é€Ÿå®šä½é—®é¢˜
2. **ç›‘æ§**ï¼šè¿½è¸ªå»¶è¿Ÿã€Token æˆæœ¬ã€é”™è¯¯ç‡ç­‰æŒ‡æ ‡
3. **ä¼˜åŒ–**ï¼šè¯†åˆ«ç“¶é¢ˆï¼Œæ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„æ€§èƒ½å·®å¼‚
4. **å®¡è®¡**ï¼šè®°å½•è°åšäº†ä»€ä¹ˆï¼Œæ»¡è¶³åˆè§„è¦æ±‚

**æ•°æ®æ¨¡å‹**ï¼š

```python
class Run:
    id: str                          # å”¯ä¸€ ID
    name: str                        # è¿è¡Œåç§°
    run_type: str                    # "agent", "model", "tool", "chain" ç­‰
    parent_run_id: Optional[str]     # çˆ¶ Run IDï¼ˆå½¢æˆæ ‘å…³ç³»ï¼‰

    # è¾“å…¥è¾“å‡º
    inputs: dict[str, Any]           # è¾“å…¥å‚æ•°
    outputs: dict[str, Any]          # è¾“å‡ºç»“æœ

    # æ—¶é—´å’Œæˆæœ¬
    start_time: datetime             # å¼€å§‹æ—¶é—´
    end_time: datetime               # ç»“æŸæ—¶é—´
    duration: float                  # æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰

    # Token å’Œæˆæœ¬
    token_usage: Optional[TokenUsage]
    cost: Optional[float]            # ç¾å…ƒæˆæœ¬

    # çŠ¶æ€å’Œé”™è¯¯
    status: str                      # "success", "error"
    error: Optional[str]             # é”™è¯¯ä¿¡æ¯

    # å…ƒæ•°æ®
    metadata: dict[str, Any]         # è‡ªå®šä¹‰å…ƒæ•°æ®
    tags: list[str]                  # æ ‡ç­¾ï¼ˆç”¨äºç­›é€‰ï¼‰

    # åé¦ˆ
    feedback_records: list[Feedback] # ç”¨æˆ·åé¦ˆ
```

#### 1.1.2 è‡ªåŠ¨è¿½è¸ªï¼šç¯å¢ƒå˜é‡é…ç½®

**æœ€ç®€å•çš„å¼€å¯æ–¹å¼**ï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆLangSmith 1.0+ æœ€æ–°å‘½åï¼‰
export LANGSMITH_API_KEY="lsv2_..."
export LANGCHAIN_PROJECT="my_project"
export LANGSMITH_TRACING="true"
```

> **å‘åå…¼å®¹è¯´æ˜**:
> - æ—§ç‰ˆç¯å¢ƒå˜é‡ `LANGCHAIN_API_KEY` å’Œ `LANGCHAIN_TRACING_V2` ä»ç„¶æ”¯æŒï¼Œä½†å»ºè®®è¿ç§»åˆ°æ–°å‘½å
> - API Key æ ¼å¼ä» `sk-...` å˜æ›´ä¸º `lsv2_...`ï¼ˆLangSmith v2ï¼‰
> - `LANGCHAIN_PROJECT` ä¿æŒä¸å˜ï¼ˆç”Ÿæ€é€šç”¨å˜é‡ï¼‰

**Python ä»£ç ä¸­é…ç½®**ï¼š

```python
import os
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent

# é…ç½® LangSmithï¼ˆæœ€æ–°å‘½åï¼‰
os.environ["LANGSMITH_API_KEY"] = "lsv2_..."
os.environ["LANGCHAIN_PROJECT"] = "my_project"
os.environ["LANGSMITH_TRACING"] = "true"

# åˆ›å»º Agent
model = ChatOpenAI(model="gpt-4o")
agent = create_agent(
    model=model,
    tools=[search, weather],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
)

# âœ… è‡ªåŠ¨è¿½è¸ªï¼šæ‰€æœ‰è°ƒç”¨éƒ½ä¼šè¢«è®°å½•åˆ° LangSmith
result = agent.invoke({"messages": [("user", "ä»Šå¤©å¤©æ°”å¦‚ä½•")]})

# æŸ¥çœ‹è¿½è¸ªï¼š
# 1. æ‰“å¼€ https://smith.langchain.com
# 2. é€‰æ‹©é¡¹ç›® "my_project"
# 3. æŸ¥çœ‹å®æ—¶è¿½è¸ªæ ‘
```

**ç¯å¢ƒå˜é‡é€‰é¡¹**ï¼š

| å˜é‡ | è¯´æ˜ | ç¤ºä¾‹ | çŠ¶æ€ |
|------|------|------|------|
| `LANGSMITH_API_KEY` | LangSmith API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰ | `lsv2_...` | âœ… æ¨è |
| `LANGCHAIN_PROJECT` | é¡¹ç›®åç§°ï¼ˆç”¨äºåˆ†ç»„ï¼‰ | `my_agent` | âœ… é€šç”¨ |
| `LANGSMITH_TRACING` | å¯ç”¨è¿½è¸ªï¼ˆå¿…éœ€ï¼‰ | `true` | âœ… æ¨è |
| `LANGSMITH_WORKSPACE_ID` | å·¥ä½œåŒº IDï¼ˆå›¢é˜Ÿä½¿ç”¨ï¼‰ | `ws-...` | âœ… æ–°å¢ |
| `LANGCHAIN_ENDPOINT` | LangSmith API ç«¯ç‚¹ | `https://api.smith.langchain.com` | âœ… å¯é€‰ |
| `LANGCHAIN_SESSION` | ä¼šè¯åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºå­åˆ†ç»„ï¼‰ | `session-123` | âœ… å¯é€‰ |
| `LANGCHAIN_CALLBACKS_BACKGROUND` | åå°å¼‚æ­¥å‘é€è¿½è¸ªæ•°æ® | `true` | âœ… å¯é€‰ |
| `LANGCHAIN_TRACING_SAMPLING_RATE` | é‡‡æ ·ç‡ï¼ˆ0.0-1.0ï¼Œç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰ | `0.1` | âœ… å¯é€‰ |
| `LANGSMITH_TEST_CACHE` | ç¼“å­˜ API è°ƒç”¨ï¼ˆåŠ é€Ÿè¯„ä¼°ï¼‰ | `true` | âœ… å¯é€‰ |
| `LANGCHAIN_API_KEY` | ï¼ˆæ—§ï¼‰API å¯†é’¥ | `sk-...` | âš ï¸ å·²å¼ƒç”¨ |
| `LANGCHAIN_TRACING_V2` | ï¼ˆæ—§ï¼‰å¯ç”¨è¿½è¸ª | `true` | âš ï¸ å·²å¼ƒç”¨ |

**å®Œæ•´ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹**ï¼š

```python
import os

# åŸºç¡€é…ç½®ï¼ˆå¿…éœ€ï¼‰- LangSmith 1.0+ æœ€æ–°å‘½å
os.environ["LANGSMITH_API_KEY"] = "lsv2_..."
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "my_project"

# å›¢é˜Ÿåä½œé…ç½®ï¼ˆå¯é€‰ï¼‰
os.environ["LANGSMITH_WORKSPACE_ID"] = "ws-..."  # å·¥ä½œåŒº IDï¼ˆå›¢é˜Ÿå…±äº«ï¼‰

# å¯é€‰é…ç½®
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"  # è‡ªå®šä¹‰ç«¯ç‚¹
os.environ["LANGCHAIN_SESSION"] = "user-session-123"  # ä¼šè¯åˆ†ç»„
os.environ["LANGCHAIN_CALLBACKS_BACKGROUND"] = "true"  # åå°å‘é€ï¼Œä¸é˜»å¡ä¸»ç¨‹åº
os.environ["LANGCHAIN_TRACING_SAMPLING_RATE"] = "0.1"  # ç”Ÿäº§ç¯å¢ƒé‡‡æ · 10%

# å¼€å‘/æµ‹è¯•ç¯å¢ƒé…ç½®
os.environ["LANGSMITH_TEST_CACHE"] = "true"  # ç¼“å­˜ API è°ƒç”¨ä»¥åŠ é€Ÿæµ‹è¯•

# å‘åå…¼å®¹ï¼ˆä¸æ¨èï¼Œä»ç„¶æ”¯æŒï¼‰
# os.environ["LANGCHAIN_API_KEY"] = "sk-..."  # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ LANGSMITH_API_KEY
# os.environ["LANGCHAIN_TRACING_V2"] = "true"  # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ LANGSMITH_TRACING
```

**ç¯å¢ƒå˜é‡è¯´æ˜**ï¼š

1. **å¿…éœ€å˜é‡**ï¼ˆLangSmith 1.0+ï¼‰ï¼š
   - `LANGSMITH_API_KEY`ï¼šä» [https://smith.langchain.com](https://smith.langchain.com) è·å–ï¼Œæ ¼å¼ä¸º `lsv2_...`
   - `LANGSMITH_TRACING`ï¼šå¿…é¡»è®¾ä¸º `"true"` å¯ç”¨è¿½è¸ª

2. **é¡¹ç›®ç»„ç»‡**ï¼š
   - `LANGCHAIN_PROJECT`ï¼šé¡¹ç›®åç§°ï¼Œç”¨äºåˆ†ç»„è¿½è¸ªï¼ˆç”Ÿæ€é€šç”¨å˜é‡ï¼Œä¿æŒä¸å˜ï¼‰
   - `LANGCHAIN_SESSION`ï¼šä¼šè¯åç§°ï¼Œç”¨äºæ›´ç»†ç²’åº¦çš„åˆ†ç»„ï¼ˆå¦‚æŒ‰ç”¨æˆ·æˆ–ä»»åŠ¡åˆ†ç»„ï¼‰
   - `LANGSMITH_WORKSPACE_ID`ï¼šå·¥ä½œåŒº IDï¼Œç”¨äºå›¢é˜Ÿåä½œï¼ˆæ ¼å¼ä¸º `ws-...`ï¼‰

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - `LANGCHAIN_CALLBACKS_BACKGROUND`ï¼šè®¾ä¸º `"true"` åï¼Œè¿½è¸ªæ•°æ®å¼‚æ­¥å‘é€ï¼Œä¸é˜»å¡ä¸»ç¨‹åº
   - `LANGCHAIN_TRACING_SAMPLING_RATE`ï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ç½®é‡‡æ ·ç‡ï¼ˆå¦‚ `"0.1"` è¡¨ç¤º 10%ï¼‰ï¼Œé™ä½è¿½è¸ªæˆæœ¬

4. **æµ‹è¯•åŠ é€Ÿ**ï¼š
   - `LANGSMITH_TEST_CACHE`ï¼šåœ¨æµ‹è¯•/è¯„ä¼°æ—¶ç¼“å­˜ API å“åº”ï¼Œé¿å…é‡å¤è°ƒç”¨

5. **å‘åå…¼å®¹**ï¼ˆå·²å¼ƒç”¨ï¼‰ï¼š
   - `LANGCHAIN_API_KEY`ï¼ˆæ—§æ ¼å¼ `sk-...`ï¼‰â†’ è¿ç§»åˆ° `LANGSMITH_API_KEY`ï¼ˆæ–°æ ¼å¼ `lsv2_...`ï¼‰
   - `LANGCHAIN_TRACING_V2` â†’ è¿ç§»åˆ° `LANGSMITH_TRACING`

#### 1.1.3 æ‰‹åŠ¨è¿½è¸ªï¼š@traceable è£…é¥°å™¨

**åœºæ™¯**ï¼šè¿½è¸ªé LangChain ä»£ç ï¼ˆè‡ªå®šä¹‰å‡½æ•°ã€æ•°æ®åº“æ“ä½œç­‰ï¼‰ã€‚

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langsmith.run_helpers import traceable

@traceable(name="my_function", run_type="chain")
def my_custom_function(input_text: str) -> str:
    """è‡ªå®šä¹‰å‡½æ•°ä¼šè‡ªåŠ¨è¢«è¿½è¸ª"""
    # å¤„ç†è¾“å…¥
    result = input_text.upper()
    return result

# è°ƒç”¨æ—¶è‡ªåŠ¨è®°å½•åˆ° LangSmith
output = my_custom_function("hello")
```

**å®Œæ•´å‚æ•°è¯´æ˜**ï¼š

```python
from langsmith import traceable

@traceable(
    # åŸºç¡€å‚æ•°
    name="custom_processing",              # è¿è¡Œåç§°ï¼ˆé»˜è®¤ï¼šå‡½æ•°åï¼‰
    run_type="tool",                       # è¿è¡Œç±»å‹ï¼šllm/chain/tool/retriever/promptï¼ˆé»˜è®¤ï¼šchainï¼‰

    # ç»„ç»‡å’Œæ ‡è®°
    metadata={                             # å…ƒæ•°æ®ï¼ˆä»»æ„é”®å€¼å¯¹ï¼‰
        "version": "1.0",
        "author": "alice",
        "module": "data_processing"
    },
    tags=["production", "v1"],             # æ ‡ç­¾åˆ—è¡¨ï¼ˆç”¨äºç­›é€‰ï¼‰
    project_name="my_project",             # é¡¹ç›®åç§°ï¼ˆè¦†ç›–ç¯å¢ƒå˜é‡ï¼‰

    # é«˜çº§å‚æ•°
    reduce_fn=None,                        # èšåˆå‡½æ•°ï¼ˆç”¨äºç”Ÿæˆå™¨/æµå¼è¾“å‡ºï¼‰
    client=None,                           # è‡ªå®šä¹‰ LangSmith Client
    process_inputs=None,                   # è¾“å…¥é¢„å¤„ç†å‡½æ•°
    process_outputs=None,                  # è¾“å‡ºåå¤„ç†å‡½æ•°
)
def process_data(data: dict) -> dict:
    """å¤„ç†æ•°æ®"""
    result = {**data, "processed": True}
    return result
```

**å‚æ•°è¯¦ç»†è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `name` | `str` | è¿½è¸ªè¿è¡Œçš„æ˜¾ç¤ºåç§° | å‡½æ•°å |
| `run_type` | `str` | è¿è¡Œç±»å‹ï¼ˆ`llm`/`chain`/`tool`/`retriever`/`prompt`ï¼‰ | `"chain"` |
| `metadata` | `dict` | è‡ªå®šä¹‰å…ƒæ•°æ®ï¼ˆé”®å€¼å¯¹ï¼‰ | `None` |
| `tags` | `list[str]` | æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤å’Œåˆ†ç»„ | `None` |
| `project_name` | `str` | é¡¹ç›®åç§°ï¼ˆè¦†ç›– `LANGCHAIN_PROJECT` ç¯å¢ƒå˜é‡ï¼‰ | `None` |
| `reduce_fn` | `Callable` | èšåˆå‡½æ•°ï¼Œç”¨äºå¤„ç†ç”Ÿæˆå™¨/æµå¼è¾“å‡º | `None` |
| `client` | `Client` | è‡ªå®šä¹‰ LangSmith Client å®ä¾‹ | `None` |
| `process_inputs` | `Callable` | è¾“å…¥åºåˆ—åŒ–å‡½æ•°ï¼ˆç”¨äºè‡ªå®šä¹‰è¾“å…¥æ ¼å¼ï¼‰ | `None` |
| `process_outputs` | `Callable` | è¾“å‡ºåºåˆ—åŒ–å‡½æ•°ï¼ˆç”¨äºè‡ªå®šä¹‰è¾“å‡ºæ ¼å¼ï¼‰ | `None` |

**run_type ç±»å‹è¯´æ˜**ï¼š

- `"llm"`ï¼šLLM æ¨¡å‹è°ƒç”¨
- `"chain"`ï¼šé“¾å¼è°ƒç”¨ï¼ˆé»˜è®¤ï¼‰
- `"tool"`ï¼šå·¥å…·æ‰§è¡Œ
- `"retriever"`ï¼šæ£€ç´¢æ“ä½œ
- `"prompt"`ï¼šæç¤ºè¯æ¨¡æ¿
- ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰ç±»å‹ï¼ˆå¦‚ `"database"`, `"api_call"`ï¼‰

**é«˜çº§ç”¨æ³•ç¤ºä¾‹**ï¼š

```python
# 1. å¤„ç†ç”Ÿæˆå™¨è¾“å‡º
@traceable(
    name="stream_processor",
    reduce_fn=lambda outputs: "".join(outputs)  # å°†æµå¼è¾“å‡ºåˆå¹¶ä¸ºå­—ç¬¦ä¸²
)
def stream_data(n: int):
    """ç”Ÿæˆå™¨å‡½æ•°"""
    for i in range(n):
        yield f"chunk-{i}"

# 2. è‡ªå®šä¹‰è¾“å…¥/è¾“å‡ºå¤„ç†
def serialize_inputs(inputs):
    """è‡ªå®šä¹‰è¾“å…¥åºåˆ—åŒ–"""
    return {k: str(v)[:100] for k, v in inputs.items()}  # æˆªæ–­é•¿æ–‡æœ¬

@traceable(
    process_inputs=serialize_inputs,
    process_outputs=lambda o: {"result": str(o)[:200]}
)
def process_large_data(data: dict) -> dict:
    # å¤„ç†å¤§é‡æ•°æ®ï¼Œä½†åªè®°å½•æ‘˜è¦
    return {"result": data}
```

**åµŒå¥—è¿½è¸ª**ï¼ˆè‡ªåŠ¨å½¢æˆæ ‘å…³ç³»ï¼‰ï¼š

```python
@traceable(name="parent_task")
def parent_task(query: str) -> str:
    # è°ƒç”¨å­ä»»åŠ¡
    result1 = search_task(query)
    result2 = analyze_task(result1)
    return result2

@traceable(name="search_task")
def search_task(query: str) -> str:
    # å­ä»»åŠ¡ 1
    return f"æœç´¢ç»“æœ: {query}"

@traceable(name="analyze_task")
def analyze_task(text: str) -> str:
    # å­ä»»åŠ¡ 2
    return f"åˆ†æ: {text}"

# è°ƒç”¨æ—¶è‡ªåŠ¨å½¢æˆæ ‘ï¼š
# parent_task
# â”œâ”€â”€ search_task
# â””â”€â”€ analyze_task
parent_task("LangChain")
```

**è®¿é—®å½“å‰è¿è¡Œä¿¡æ¯**ï¼š

```python
from langsmith.run_helpers import traceable, get_current_run_tree

@traceable
def my_function(x: int) -> int:
    # è·å–å½“å‰è¿è¡Œ
    run = get_current_run_tree()

    if run:
        print(f"å½“å‰è¿è¡Œ ID: {run.id}")
        print(f"è¿è¡Œåç§°: {run.name}")

        # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
        run.metadata = {"user_id": "user-123"}

    return x * 2
```

**å¼‚æ­¥è¿½è¸ªç¤ºä¾‹**ï¼š

`@traceable` è£…é¥°å™¨å®Œå…¨æ”¯æŒå¼‚æ­¥å‡½æ•°ï¼Œè‡ªåŠ¨è¿½è¸ªå¼‚æ­¥æ“ä½œï¼š

```python
import asyncio
from langsmith import traceable
import httpx

@traceable(name="fetch_data", run_type="retriever")
async def fetch_user_data(user_id: str) -> dict:
    """å¼‚æ­¥è·å–ç”¨æˆ·æ•°æ®"""
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.example.com/users/{user_id}")
        return response.json()

@traceable(name="process_user", run_type="chain")
async def process_user(user_id: str) -> dict:
    """å¼‚æ­¥å¤„ç†ç”¨æˆ·ä¿¡æ¯"""
    # å¼‚æ­¥è°ƒç”¨ä¼šè‡ªåŠ¨è¿½è¸ª
    user_data = await fetch_user_data(user_id)

    # å¤„ç†æ•°æ®
    processed = {
        "id": user_data["id"],
        "name": user_data["name"].upper(),
        "processed_at": "2025-01-15"
    }

    return processed

# ä½¿ç”¨å¼‚æ­¥è¿½è¸ª
async def main():
    result = await process_user("user-123")
    print(result)

# æ‰§è¡Œ
asyncio.run(main())

# è¿½è¸ªæ ‘ä¼šæ˜¾ç¤ºï¼š
# process_user (async)
# â””â”€â”€ fetch_user_data (async)
```

**å¼‚æ­¥æ‰¹é‡æ“ä½œè¿½è¸ª**ï¼š

```python
@traceable(name="batch_fetch", run_type="chain")
async def fetch_multiple_users(user_ids: list[str]) -> list[dict]:
    """å¹¶å‘è·å–å¤šä¸ªç”¨æˆ·æ•°æ®"""
    # æ‰€æœ‰å¼‚æ­¥è°ƒç”¨éƒ½ä¼šè¢«è¿½è¸ª
    tasks = [fetch_user_data(uid) for uid in user_ids]
    results = await asyncio.gather(*tasks)
    return results

# ä½¿ç”¨
async def main():
    users = await fetch_multiple_users(["user-1", "user-2", "user-3"])

# è¿½è¸ªæ ‘ä¼šæ˜¾ç¤ºï¼š
# batch_fetch
# â”œâ”€â”€ fetch_user_data (user-1)
# â”œâ”€â”€ fetch_user_data (user-2)
# â””â”€â”€ fetch_user_data (user-3)
```

**é”™è¯¯å¤„ç†ä¸è¿½è¸ª**ï¼š

å¼‚å¸¸ä¼šè‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```python
@traceable(name="safe_operation", run_type="tool")
def safe_operation(value: int) -> int:
    """å¸¦é”™è¯¯å¤„ç†çš„æ“ä½œ"""
    try:
        if value < 0:
            raise ValueError("Value must be non-negative")

        result = 100 / value
        return result

    except ValueError as e:
        # å¼‚å¸¸ä¼šè‡ªåŠ¨è®°å½•åˆ° LangSmith
        print(f"Validation error: {e}")
        raise  # é‡æ–°æŠ›å‡ºä»¥æ ‡è®°ä¸ºå¤±è´¥

    except ZeroDivisionError as e:
        # é™¤é›¶é”™è¯¯ä¹Ÿä¼šè¢«è®°å½•
        print(f"Division error: {e}")
        raise

# è°ƒç”¨å¤±è´¥çš„å‡½æ•°
try:
    safe_operation(0)  # ä¼šåœ¨ LangSmith ä¸­æ ‡è®°ä¸º error
except ZeroDivisionError:
    pass

# LangSmith ä¸­ä¼šæ˜¾ç¤ºï¼š
# - è¿è¡ŒçŠ¶æ€: error
# - é”™è¯¯ç±»å‹: ZeroDivisionError
# - é”™è¯¯æ¶ˆæ¯: division by zero
# - å®Œæ•´å †æ ˆè·Ÿè¸ª
```

**å¼‚æ­¥é”™è¯¯å¤„ç†ç¤ºä¾‹**ï¼š

```python
@traceable(name="async_safe_fetch")
async def async_safe_fetch(url: str) -> dict:
    """å¸¦é”™è¯¯å¤„ç†çš„å¼‚æ­¥è¯·æ±‚"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, timeout=5.0)
            response.raise_for_status()
            return response.json()

    except httpx.TimeoutException as e:
        # è¶…æ—¶é”™è¯¯ä¼šè¢«è®°å½•
        print(f"Request timeout: {e}")
        raise

    except httpx.HTTPStatusError as e:
        # HTTP é”™è¯¯ä¼šè¢«è®°å½•
        print(f"HTTP error: {e.response.status_code}")
        raise

    except Exception as e:
        # æ‰€æœ‰å…¶ä»–é”™è¯¯ä¹Ÿä¼šè¢«è®°å½•
        print(f"Unexpected error: {e}")
        raise

# ä½¿ç”¨
async def main():
    try:
        data = await async_safe_fetch("https://invalid-url.example.com")
    except Exception:
        # LangSmith ä¼šè®°å½•å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
        pass
```

#### 1.1.4 è¿½è¸ªä¿¡æ¯åˆ†æ

**åœ¨ LangSmith UI ä¸­æŸ¥çœ‹è¿½è¸ª**ï¼š

1. **Trace æ ‘è§†å›¾**ï¼šæŸ¥çœ‹å®Œæ•´çš„æ‰§è¡Œé“¾è·¯
   - æ¯ä¸ªèŠ‚ç‚¹æ˜¾ç¤ºè¿è¡Œåç§°ã€çŠ¶æ€ã€è€—æ—¶
   - ç‚¹å‡»èŠ‚ç‚¹æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼ˆè¾“å…¥ã€è¾“å‡ºã€é”™è¯¯ï¼‰

2. **è¾“å…¥è¾“å‡ºå¯¹æ¯”**ï¼š
   - å·¦ä¾§ï¼šè¾“å…¥å‚æ•°
   - å³ä¾§ï¼šè¾“å‡ºç»“æœ
   - å¿«é€Ÿå®šä½æ•°æ®å˜åŒ–

3. **Token æˆæœ¬åˆ†æ**ï¼š
   - æ˜¾ç¤ºæ¯æ¬¡æ¨¡å‹è°ƒç”¨çš„ Token ä½¿ç”¨é‡
   - è®¡ç®—ç´¯è®¡æˆæœ¬ï¼ˆç¾å…ƒï¼‰
   - è¯†åˆ«æˆæœ¬æœ€é«˜çš„æ“ä½œ

4. **å»¶è¿Ÿåˆ†æ**ï¼š
   - æ˜¾ç¤ºæ¯ä¸ªæ“ä½œçš„è€—æ—¶
   - è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
   - æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„æ€§èƒ½

**ç¼–ç¨‹æ–¹å¼åˆ†æè¿½è¸ª**ï¼š

```python
from langsmith import Client

client = Client()

# è·å–æœ€è¿‘çš„è¿½è¸ª
runs = client.list_runs(
    project_name="my_project",
    limit=10  # è·å–æœ€è¿‘ 10 æ¡
)

for run in runs:
    print(f"è¿è¡Œ: {run.name}")
    print(f"çŠ¶æ€: {run.status}")
    print(f"è€—æ—¶: {run.end_time - run.start_time}")

    if run.token_usage:
        print(f"Tokens: {run.token_usage.completion_tokens}")
        print(f"æˆæœ¬: ${run.cost}")

    # è·å–å­è¿è¡Œ
    if run.child_runs:
        for child in run.child_runs:
            print(f"  â””â”€ {child.name}: {child.status}")
```

**ç­›é€‰å’Œç»Ÿè®¡**ï¼š

```python
from datetime import datetime, timedelta

# ç­›é€‰æ¡ä»¶
yesterday = datetime.now() - timedelta(days=1)

# è·å–ç‰¹å®šæ ‡ç­¾çš„è¿è¡Œ
production_runs = client.list_runs(
    project_name="my_project",
    filter='tags:production',
    start_time=yesterday
)

# ç»Ÿè®¡
total_cost = sum(run.cost for run in production_runs if run.cost)
avg_duration = sum(
    (run.end_time - run.start_time).total_seconds()
    for run in production_runs
) / len(production_runs)

error_count = sum(1 for run in production_runs if run.status == "error")

print(f"æ€»æˆæœ¬: ${total_cost:.2f}")
print(f"å¹³å‡è€—æ—¶: {avg_duration:.2f} ç§’")
print(f"é”™è¯¯æ•°: {error_count}")
```

**åé¦ˆæ”¶é›†**ï¼š

```python
# åœ¨åº”ç”¨ä¸­æ”¶é›†ç”¨æˆ·åé¦ˆ
run_id = "run-123"  # ä»è¿½è¸ªä¸­è·å–

# ç”¨æˆ·ç‚¹èµ
client.create_feedback(
    run_id=run_id,
    key="user_rating",
    score=1.0,  # 1.0 = å¥½, 0.0 = ä¸å¥½
    comment="å¾ˆæœ‰ç”¨ï¼"
)

# ä¸“å®¶æ ‡æ³¨
client.create_feedback(
    run_id=run_id,
    key="expert_evaluation",
    score=0.8,
    comment="å¤§éƒ¨åˆ†æ­£ç¡®ï¼Œä½†æœ‰ä¸€å¤„é”™è¯¯"
)

# æŸ¥çœ‹åé¦ˆ
feedbacks = client.list_feedbacks(run_id=run_id)
for feedback in feedbacks:
    print(f"{feedback.key}: {feedback.score} - {feedback.comment}")
```

---

### 1.2 Evaluation è¯„ä¼°æ¡†æ¶

#### 1.2.1 Dataset ç®¡ç†ä¸æµ‹è¯•é›†è®¾è®¡

**ä»€ä¹ˆæ˜¯ Datasetï¼Ÿ**

Dataset æ˜¯è¯„ä¼°æ‰€éœ€çš„æµ‹è¯•æ•°æ®é›†åˆï¼Œæ¯ä¸ª Example åŒ…å«ï¼š
- **input**ï¼šè¾“å…¥å‚æ•°
- **output**ï¼ˆå¯é€‰ï¼‰ï¼šå‚è€ƒè¾“å‡ºï¼ˆç”¨äºå¯¹æ¯”ï¼‰
- **metadata**ï¼ˆå¯é€‰ï¼‰ï¼šè¡¥å……ä¿¡æ¯

**åˆ›å»º Dataset**ï¼š

```python
from langsmith import Client

client = Client()

# æ–¹æ³• 1ï¼šæ‰‹åŠ¨åˆ›å»º
dataset = client.create_dataset(
    dataset_name="qa_benchmark",
    description="QA ä»»åŠ¡åŸºå‡†æ•°æ®é›†"
)

# æ·»åŠ ç¤ºä¾‹
client.create_example(
    dataset_id=dataset.id,
    inputs={"question": "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"},
    outputs={"answer": "LangChain æ˜¯ä¸€ä¸ªæ„å»º LLM åº”ç”¨çš„æ¡†æ¶"}
)

client.create_example(
    dataset_id=dataset.id,
    inputs={"question": "Python 3.13 çš„ä¸»è¦ç‰¹æ€§ï¼Ÿ"},
    outputs={"answer": "Python 3.13 å¼•å…¥äº† JIT ç¼–è¯‘å™¨..."}
)

# æ–¹æ³• 2ï¼šä» CSV å¯¼å…¥
import pandas as pd

df = pd.read_csv("qa_examples.csv")
# å‡è®¾ CSV æœ‰ "question" å’Œ "answer" åˆ—

dataset = client.create_dataset(
    dataset_name="qa_from_csv",
    description="ä» CSV å¯¼å…¥çš„ QA æ•°æ®é›†"
)

for _, row in df.iterrows():
    client.create_example(
        dataset_id=dataset.id,
        inputs={"question": row["question"]},
        outputs={"answer": row["answer"]}
    )

# æ–¹æ³• 3ï¼šä»ç”Ÿäº§è¿½è¸ªåˆ›å»ºï¼ˆæœ€æ¨èï¼‰
# åœ¨ç”Ÿäº§ç¯å¢ƒè¿è¡Œä¸€æ®µæ—¶é—´åï¼Œç­›é€‰é«˜è´¨é‡çš„è¿½è¸ª
client.create_dataset(
    dataset_name="production_examples",
    description="ä»ç”Ÿäº§ç¯å¢ƒé‡‡é›†çš„çœŸå®ç”¨æˆ·æŸ¥è¯¢"
)

# æ‰‹åŠ¨ç­›é€‰å¥½çš„è¿½è¸ªå¹¶æ·»åŠ 
# ï¼ˆé€šå¸¸é€šè¿‡ LangSmith UI å®Œæˆï¼Œæ”¯æŒæ‰¹é‡å¯¼å…¥ï¼‰
```

**æµ‹è¯•é›†è®¾è®¡æœ€ä½³å®è·µ**ï¼š

**1. è¦†ç›–ä»£è¡¨åœºæ™¯**

```python
# âœ… å¥½ï¼šè¦†ç›–å¤šç§åœºæ™¯
examples = [
    # ç®€å•æŸ¥è¯¢
    {"question": "å¤©æ°”å¦‚ä½•ï¼Ÿ", "answer": "..."},

    # å¤æ‚æŸ¥è¯¢
    {"question": "æ¯”è¾ƒ LangChain å’Œ Langraph çš„ä¼˜ç¼ºç‚¹", "answer": "..."},

    # è¾¹ç•Œæƒ…å†µ
    {"question": "", "answer": "è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"},
    {"question": "???.|||", "answer": "è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"},

    # å¤šè½®å¯¹è¯
    {"question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ", "answer": "..."},
    {"question": "èƒ½ç»™ä¸ªä¾‹å­å—ï¼Ÿ", "answer": "..."},
]
```

**2. è¶³å¤Ÿçš„æ•°æ®é‡**

- åˆæœŸï¼š10-20 ä¸ªé«˜è´¨é‡æ ·ä¾‹ï¼ˆæ‰‹å·¥ç²¾é€‰ï¼‰
- ä¸­æœŸï¼š50-100 ä¸ªæ ·ä¾‹ï¼ˆåŒ…æ‹¬è¦†ç›–å„ç§åœºæ™¯ï¼‰
- æˆç†ŸæœŸï¼š1000+ ä¸ªæ ·ä¾‹ï¼ˆä»ç”Ÿäº§ç¯å¢ƒé‡‡é›†ï¼‰

**3. åŒ…å«å…ƒæ•°æ®**

```python
client.create_example(
    dataset_id=dataset.id,
    inputs={"question": "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"},
    outputs={"answer": "LangChain æ˜¯..."},
    metadata={
        "difficulty": "easy",
        "category": "framework",
        "source": "user_feedback",
        "language": "chinese"
    }
)
```

#### 1.2.2 è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€ç›¸å…³æ€§ã€ä¸€è‡´æ€§ã€å»¶è¿Ÿã€æˆæœ¬

**è¯„ä¼°å™¨è¿”å›æ ¼å¼è¯¦è§£**ï¼š

è¯„ä¼°å™¨å¿…é¡»è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å­—æ®µ | å¿…éœ€ | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|
| `key` | æ˜¯ | `str` | è¯„ä¼°æŒ‡æ ‡çš„åç§°ï¼ˆå¦‚ `"accuracy"`, `"relevance"`) |
| `score` | æ˜¯ | `float/int/bool` | è¯„ä¼°åˆ†æ•°ï¼ˆé€šå¸¸ 0-1ï¼Œä¹Ÿå¯ä»¥æ˜¯ä»»æ„æ•°å€¼æˆ–å¸ƒå°”å€¼ï¼‰ |
| `value` | å¦ | `Any` | å®é™…å€¼ï¼ˆé€šå¸¸ä¸ score ç›¸åŒï¼Œç”¨äºè®°å½•åŸå§‹å€¼ï¼‰ |
| `comment` | å¦ | `str` | è¯„è®ºæˆ–è§£é‡Šï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰ |
| `correction` | å¦ | `dict` | ä¿®æ­£å»ºè®®ï¼ˆç”¨äºæ ‡æ³¨æ­£ç¡®ç­”æ¡ˆï¼‰ |

**å®Œæ•´è¿”å›æ ¼å¼ç¤ºä¾‹**ï¼š

```python
# æœ€ç®€æ ¼å¼ï¼ˆåªåŒ…å«å¿…éœ€å­—æ®µï¼‰
return {
    "key": "accuracy",
    "score": 1.0
}

# å®Œæ•´æ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰
return {
    "key": "accuracy",           # æŒ‡æ ‡åç§°
    "score": 0.85,                # åˆ†æ•°ï¼ˆ0-1 æˆ–ä»»æ„æ•°å€¼ï¼‰
    "value": 0.85,                # å®é™…å€¼ï¼ˆå¯é€‰ï¼Œé€šå¸¸ä¸ score ç›¸åŒï¼‰
    "comment": "Good answer!",    # è¯„è®ºï¼ˆå¯é€‰ï¼Œç”¨äºè§£é‡Šï¼‰
    "correction": {               # ä¿®æ­£å»ºè®®ï¼ˆå¯é€‰ï¼‰
        "expected": "Paris",
        "actual": "paris"
    }
}

# å¸ƒå°”åˆ†æ•°
return {
    "key": "is_correct",
    "score": True  # å¸ƒå°”å€¼ä¹Ÿå¯ä»¥
}

# å¤šä¸ªè¯„ä¼°ç»“æœï¼ˆè¿”å›åˆ—è¡¨ï¼‰
return [
    {"key": "accuracy", "score": 0.9},
    {"key": "latency", "score": 1.5, "comment": "1.5 seconds"}
]
```

**è¯„ä¼°å™¨ç±»å‹**ï¼š

1. **å¯å‘å¼è¯„ä¼°å™¨**ï¼ˆHeuristic Evaluatorï¼‰ï¼šç¡®å®šæ€§è§„åˆ™

```python
def is_valid_code(run, example):
    """æ£€æŸ¥è¾“å‡ºæ˜¯å¦æ˜¯æœ‰æ•ˆçš„ Python ä»£ç """
    code = run.outputs.get("code", "")

    try:
        compile(code, "<string>", "exec")
        return {
            "key": "valid_code",
            "score": 1.0,
            "comment": "Valid Python code"
        }
    except SyntaxError as e:
        return {
            "key": "valid_code",
            "score": 0.0,
            "comment": f"Syntax error: {str(e)}",
            "correction": {"error": str(e)}
        }

def exact_match(run, example):
    """ç²¾ç¡®åŒ¹é…è¯„ä¼°å™¨"""
    actual = run.outputs.get("answer")
    expected = example.outputs.get("answer")
    is_match = actual == expected

    return {
        "key": "exact_match",
        "score": 1.0 if is_match else 0.0,
        "value": is_match,
        "comment": "Match!" if is_match else f"Expected: {expected}, Got: {actual}",
        "correction": {"expected": expected} if not is_match else None
    }

def length_check(run, example):
    """æ£€æŸ¥è¾“å‡ºé•¿åº¦"""
    answer = run.outputs.get("answer", "")
    length = len(answer)

    return {
        "key": "answer_length",
        "score": length,  # åˆ†æ•°å¯ä»¥æ˜¯ä»»æ„æ•°å€¼
        "value": length,
        "comment": f"Answer has {length} characters"
    }
```

**2. LLM ä½œä¸ºè¯„ä¼°å™¨**ï¼ˆLLM-as-Judgeï¼‰ï¼šä½¿ç”¨ LLM æ‰“åˆ†

```python
from langchain_openai import ChatOpenAI
from langsmith.evaluation import LangChainStringEvaluator

# ä½¿ç”¨å®˜æ–¹æä¾›çš„ LLM è¯„ä¼°å™¨
from langsmith.evaluation import (
    EvaluateStrings,
    evaluate_strings
)

# è‡ªå®šä¹‰ LLM è¯„ä¼°å™¨
def llm_evaluator(run, example):
    """ä½¿ç”¨ LLM è¯„ä¼°ç›¸å…³æ€§"""
    model = ChatOpenAI(model="gpt-4o-mini")

    evaluation_prompt = f"""
    é—®é¢˜ï¼š{example.inputs.get("question")}
    å‚è€ƒç­”æ¡ˆï¼š{example.outputs.get("answer")}
    å®é™…ç­”æ¡ˆï¼š{run.outputs.get("answer")}

    è¯·è¯„ä¼°å®é™…ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆçš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰ã€‚
    """

    response = model.invoke(evaluation_prompt)

    # è§£æå“åº”
    score = float(response.content.split("\n")[0])

    return {
        "score": score,
        "key": "relevance",
        "comment": response.content
    }
```

**3. äººå·¥è¯„ä¼°**ï¼šé€šè¿‡ UI æ‰‹åŠ¨æ ‡æ³¨

```python
# LangSmith UI ä¸­å¯ä»¥ï¼š
# 1. åœ¨ Annotation Queue ä¸­æ ‡æ³¨
# 2. ä¸ºæ¯æ¡ç»“æœæ‰“åˆ†æˆ–å†™è¯„è®º
# 3. å¯¼å‡ºè¯„ä¼°ç»“æœä¾›ç»Ÿè®¡
```

**å¸¸è§è¯„ä¼°æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | å®ç°æ–¹å¼ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| **Accuracy** | ç²¾ç¡®åŒ¹é…æˆ– LLM | åˆ†ç±»ä»»åŠ¡ |
| **BLEU/ROUGE** | æ–‡æœ¬ç›¸ä¼¼åº¦ | æ–‡æœ¬ç”Ÿæˆ |
| **Precision/Recall** | é›†åˆæ“ä½œ | æ£€ç´¢ä»»åŠ¡ |
| **Relevance** | LLM è¯„ä¼° | QA ä»»åŠ¡ |
| **Consistency** | å¤šæ¬¡è¿è¡Œå¯¹æ¯” | éç¡®å®šæ€§ä»»åŠ¡ |
| **Latency** | æ—¶é—´æˆ³è®¡ç®— | æ€§èƒ½åˆ†æ |
| **Cost** | Token ä½¿ç”¨é‡ | æˆæœ¬æ§åˆ¶ |

#### 1.2.3 evaluate() æ‰¹é‡æµ‹è¯•

> **æ³¨æ„**ï¼šLangSmith 1.0+ ä¸­ `run_on_dataset()` å·²é‡å‘½åä¸º `evaluate()`ï¼Œæ¨èä½¿ç”¨æ–° API

**evaluate() å‡½æ•°çš„ data å‚æ•°æ”¯æŒçš„ç±»å‹**ï¼š

`evaluate()` å‡½æ•°çš„ `data` å‚æ•°éå¸¸çµæ´»ï¼Œæ”¯æŒå¤šç§æ•°æ®æºï¼š

| ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `str` (æ•°æ®é›†åç§°) | LangSmith å¹³å°ä¸Šçš„æ•°æ®é›†åç§° | `"qa_benchmark"` |
| `str` (UUID) | LangSmith æ•°æ®é›†çš„ UUID | `"a3d2f1b8-..."` |
| `list[dict]` | ç›´æ¥ä¼ å…¥ç¤ºä¾‹åˆ—è¡¨ | `[{"inputs": {...}, "outputs": {...}}, ...]` |
| `Iterator[dict]` | ç¤ºä¾‹è¿­ä»£å™¨ï¼ˆç”¨äºå¤§æ•°æ®é›†ï¼‰ | `iter([{...}, {...}])` |
| `Iterator[Example]` | LangSmith Example å¯¹è±¡è¿­ä»£å™¨ | `client.list_examples(dataset_name="...")` |

**ä½¿ç”¨ä¸åŒç±»å‹çš„ç¤ºä¾‹**ï¼š

```python
from langsmith.evaluation import evaluate
from langsmith import Client

client = Client()

# 1. ä½¿ç”¨æ•°æ®é›†åç§°ï¼ˆæœ€å¸¸ç”¨ï¼‰
results = evaluate(
    predict,
    data="qa_benchmark",  # æ•°æ®é›†åç§°
    evaluators=[accuracy]
)

# 2. ä½¿ç”¨æ•°æ®é›† UUID
results = evaluate(
    predict,
    data="a3d2f1b8-1234-5678-90ab-cdef12345678",  # UUID
    evaluators=[accuracy]
)

# 3. ç›´æ¥ä¼ å…¥ç¤ºä¾‹åˆ—è¡¨ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
examples = [
    {
        "inputs": {"question": "What is LangChain?"},
        "outputs": {"answer": "LangChain is a framework..."}
    },
    {
        "inputs": {"question": "What is Python?"},
        "outputs": {"answer": "Python is a programming language..."}
    }
]
results = evaluate(
    predict,
    data=examples,  # åˆ—è¡¨
    evaluators=[accuracy]
)

# 4. ä½¿ç”¨è¿­ä»£å™¨ï¼ˆå¤§æ•°æ®é›†ï¼ŒèŠ‚çœå†…å­˜ï¼‰
def example_generator():
    """ç”Ÿæˆå™¨å‡½æ•°ï¼Œé€ä¸ªäº§ç”Ÿç¤ºä¾‹"""
    for i in range(1000):
        yield {
            "inputs": {"question": f"Question {i}"},
            "outputs": {"answer": f"Answer {i}"}
        }

results = evaluate(
    predict,
    data=example_generator(),  # è¿­ä»£å™¨
    evaluators=[accuracy]
)

# 5. ä½¿ç”¨ LangSmith Example å¯¹è±¡è¿­ä»£å™¨
examples_iter = client.list_examples(
    dataset_name="qa_benchmark",
    limit=100  # åªæµ‹è¯•å‰ 100 ä¸ª
)
results = evaluate(
    predict,
    data=examples_iter,
    evaluators=[accuracy]
)
```

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langsmith.evaluation import evaluate
from langsmith import Client

client = Client()

# å®šä¹‰é¢„æµ‹å‡½æ•°
def predict(input_dict):
    """åº”ç”¨çš„é¢„æµ‹å‡½æ•°"""
    question = input_dict.get("question")

    # è°ƒç”¨ Agent
    agent = create_agent(model=ChatOpenAI(model="gpt-4o-mini"), tools=[...])
    result = agent.invoke({"messages": [("user", question)]})

    return {"answer": result["messages"][-1].content}

# å®šä¹‰è¯„ä¼°å™¨
def exact_match(run, example):
    """ç²¾ç¡®åŒ¹é…è¯„ä¼°"""
    return {
        "score": 1.0 if run.outputs["answer"] == example.outputs["answer"] else 0.0,
        "key": "exact_match"
    }

def relevance(run, example):
    """ç›¸å…³æ€§è¯„ä¼°"""
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.invoke(
        f"é—®é¢˜: {example.inputs['question']}\nç­”æ¡ˆ: {run.outputs['answer']}\n"
        f"è¯„ä¼°ç›¸å…³æ€§(0-1):"
    )
    score = float(response.content)
    return {"score": score, "key": "relevance"}

# è¿è¡Œè¯„ä¼°
experiment_results = evaluate(
    predict,                                    # é¢„æµ‹å‡½æ•°
    data="qa_benchmark",                       # æ•°æ®é›†åç§°
    evaluators=[exact_match, relevance],       # è¡Œçº§è¯„ä¼°å™¨
    experiment_prefix="Model v1",              # å®éªŒåç§°å‰ç¼€
    description="æµ‹è¯•æ–°æ¨¡å‹ç‰ˆæœ¬",
    num_repetitions=2,                         # è¿è¡Œ 2 æ¬¡ï¼ˆå¤„ç† LLM éç¡®å®šæ€§ï¼‰
    max_concurrency=10                         # å¹¶å‘æ•°
)

# æŸ¥çœ‹ç»“æœ
print(f"å®éªŒå: {experiment_results.experiment_name}")
print(f"æ€»æ ·ä¾‹æ•°: {len(experiment_results.results)}")

# ç»Ÿè®¡æŒ‡æ ‡
scores = [r.evaluation_results[0].score for r in experiment_results.results]
print(f"å¹³å‡å‡†ç¡®ç‡: {sum(scores) / len(scores):.2%}")
```

**é«˜çº§ç”¨æ³•ï¼šSummary Evaluator**

```python
from langsmith.evaluation import evaluate

def accuracy_summary(runs, examples):
    """å®éªŒçº§è¯„ä¼°å™¨ï¼ˆåœ¨æ‰€æœ‰è¿è¡Œä¸Šè®¡ç®—ï¼‰"""
    correct = 0
    for run, example in zip(runs, examples):
        if run.outputs["answer"] == example.outputs["answer"]:
            correct += 1

    accuracy = correct / len(runs)
    return {"score": accuracy, "key": "accuracy"}

def precision_recall_summary(runs, examples):
    """è®¡ç®— Precision å’Œ Recall"""
    predictions = [run.outputs["answer"] for run in runs]
    references = [example.outputs["answer"] for example in examples]

    # å‡è®¾è¾“å‡ºæ˜¯åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
    tp = sum(1 for p, r in zip(predictions, references) if p == r and p == "positive")
    fp = sum(1 for p, r in zip(predictions, references) if p != r and p == "positive")
    fn = sum(1 for p, r in zip(predictions, references) if p != r and r == "positive")

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0

    return [
        {"score": precision, "key": "precision"},
        {"score": recall, "key": "recall"}
    ]

# ä½¿ç”¨ Summary Evaluator
evaluate(
    predict,
    data="classification_dataset",
    evaluators=[exact_match],
    summary_evaluators=[accuracy_summary, precision_recall_summary],
    experiment_prefix="Classification v1"
)
```

**å¼‚æ­¥è¯„ä¼°ï¼ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼‰**ï¼š

```python
import asyncio
from langsmith.evaluation import aevaluate

async def main():
    # å¼‚æ­¥é¢„æµ‹å‡½æ•°
    async def async_predict(input_dict):
        # å¼‚æ­¥ Agent è°ƒç”¨
        result = await async_agent.ainvoke({"messages": [("user", input_dict["question"])]})
        return {"answer": result["messages"][-1].content}

    # è¿è¡Œå¼‚æ­¥è¯„ä¼°
    results = await aevaluate(
        async_predict,
        data="large_dataset",
        evaluators=[exact_match, relevance],
        experiment_prefix="Async Model v1",
        max_concurrency=50  # å¹¶å‘ 50 ä¸ªè¯·æ±‚
    )

    return results

# æ‰§è¡Œ
results = asyncio.run(main())
```

#### 1.2.4 äººå·¥æ ‡æ³¨ä¸åé¦ˆæ”¶é›†

**é€šè¿‡ LangSmith UI æ ‡æ³¨**ï¼š

1. **åˆ›å»º Annotation Queue**
   - åœ¨é¡¹ç›®ä¸­ç‚¹å‡» "Create Annotation Queue"
   - é€‰æ‹©æ•°æ®é›†å’Œè¦æ ‡æ³¨çš„å­—æ®µ

2. **æ ‡æ³¨æµç¨‹**
   - é€æ¡æŸ¥çœ‹ä¾‹å­
   - ç»™æ¯æ¡ç»“æœè¯„åˆ†æˆ–å†™è¯„è®º
   - å¯¼å‡ºæ ‡æ³¨ç»“æœ

**ç¼–ç¨‹æ–¹å¼æ”¶é›†åé¦ˆ**ï¼š

```python
from langsmith import Client

client = Client()

# åº”ç”¨ä¸­æ”¶é›†ç”¨æˆ·åé¦ˆ
run_id = "run-123"

# ç”¨æˆ·ç‚¹èµ/ç‚¹è¸©
client.create_feedback(
    run_id=run_id,
    key="user_rating",
    score=1.0,  # 1.0 = å¥½, 0.0 = ä¸å¥½
    comment="å›ç­”å¾ˆå‡†ç¡®ï¼"
)

# ä¸“å®¶æ ‡æ³¨
client.create_feedback(
    run_id=run_id,
    key="expert_annotation",
    score=0.8,
    comment="å¤§éƒ¨åˆ†æ­£ç¡®ï¼Œä½†ç¬¬äºŒç‚¹ä¸å¤Ÿå‡†ç¡®"
)

# å¯¼å‡ºæ‰€æœ‰åé¦ˆç”¨äºåˆ†æ
feedbacks = client.list_feedbacks(
    run_ids=[run_id]
)

for feedback in feedbacks:
    print(f"{feedback.key}: {feedback.score}")
    print(f"  {feedback.comment}")
```

---

### 1.3 æŒç»­ä¼˜åŒ–

#### 1.3.1 Prompt Hub ç‰ˆæœ¬æ§åˆ¶

**åœºæ™¯**ï¼šç³»ç»Ÿæç¤ºè¯éœ€è¦ç‰ˆæœ¬ç®¡ç†å’Œå¯¹æ¯”ã€‚

**ä½¿ç”¨ LangSmith Prompt Hub**ï¼š

```python
from langsmith import Client
from langchain_core.prompts import ChatPromptTemplate

client = Client()

# æ–¹æ³• 1ï¼šå°†æç¤ºè¯æ¨é€åˆ° Hub
system_prompt_v1 = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚

èƒ½åŠ›ï¼š
1. å›ç­”ç¼–ç¨‹é—®é¢˜
2. ç”Ÿæˆä»£ç 
3. è°ƒè¯•é”™è¯¯

çº¦æŸï¼š
- åªè®¨è®ºç¼–ç¨‹ç›¸å…³è¯é¢˜
- ç”Ÿæˆçš„ä»£ç è¦æœ‰æ³¨é‡Š
"""

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt_v1),
    ("user", "{input}")
])

# æ¨é€åˆ° Hubï¼ˆéœ€è¦ API æƒé™ï¼‰
client.push_prompt(
    prompt_identifier="programming_assistant_system_prompt",
    object=prompt_template,
    description="ç¼–ç¨‹åŠ©æ‰‹çš„ç³»ç»Ÿæç¤ºè¯"
)

# æ–¹æ³• 2ï¼šä» Hub æ‹‰å–æç¤ºè¯
system_prompt = client.pull_prompt("programming_assistant_system_prompt")

# æ–¹æ³• 3ï¼šç‰ˆæœ¬ç®¡ç†
# æ¯æ¬¡æ¨é€è‡ªåŠ¨åˆ›å»ºæ–°ç‰ˆæœ¬ï¼Œå¯ä»¥ï¼š
# 1. æŸ¥çœ‹å†å²ç‰ˆæœ¬
# 2. å¯¹æ¯”ç‰ˆæœ¬å·®å¼‚
# 3. æ‹‰å–ç‰¹å®šç‰ˆæœ¬

updated_prompt_v2 = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚

å¢å¼ºèƒ½åŠ›ï¼š
1. å›ç­”ç¼–ç¨‹é—®é¢˜
2. ç”Ÿæˆä»£ç 
3. è°ƒè¯•é”™è¯¯

çº¦æŸï¼š
- åªè®¨è®ºç¼–ç¨‹ç›¸å…³è¯é¢˜
- ç”Ÿæˆçš„ä»£ç è¦æœ‰æ³¨é‡Š
"""

# æ¨é€æ–°ç‰ˆæœ¬ï¼ˆè‡ªåŠ¨åˆ›å»ºç‰ˆæœ¬ 2ï¼‰
updated_template = ChatPromptTemplate.from_messages([
    ("system", updated_prompt_v2),
    ("user", "{input}")
])

client.push_prompt(
    prompt_identifier="programming_assistant_system_prompt",
    object=updated_template,
    description="æ·»åŠ  'å¢å¼º' å‰ç¼€ä»¥å¼ºè°ƒèƒ½åŠ›"
)
```

**ç‰ˆæœ¬å¯¹æ¯”**ï¼š

```python
# åœ¨ LangSmith UI ä¸­ï¼š
# 1. æ‰“å¼€ Prompt Hub
# 2. é€‰æ‹©æç¤ºè¯
# 3. ç‚¹å‡» "Compare Versions"
# 4. æŸ¥çœ‹å˜åŒ–å’Œå¯¹æ¯”ç»“æœ
```

#### 1.3.2 A/B æµ‹è¯•

**å®Œæ•´çš„ A/B æµ‹è¯•æµç¨‹**ï¼š

```python
from langsmith.evaluation import evaluate
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent

# ç‰ˆæœ¬ Aï¼šåŸå§‹æ¨¡å‹å’Œæç¤ºè¯
def predict_v_a(input_dict):
    agent = create_agent(
        model=ChatOpenAI(model="gpt-4o-mini"),
        tools=[...],
        system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
    )
    result = agent.invoke({"messages": [("user", input_dict["question"])]})
    return {"answer": result["messages"][-1].content}

# ç‰ˆæœ¬ Bï¼šæ”¹è¿›çš„æç¤ºè¯
def predict_v_b(input_dict):
    agent = create_agent(
        model=ChatOpenAI(model="gpt-4o-mini"),
        tools=[...],
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚

åœ¨å›ç­”å‰ï¼š
1. ç†è§£é—®é¢˜çš„å…³é”®ä¿¡æ¯
2. åˆ†è§£å¤æ‚é—®é¢˜ä¸ºå­é—®é¢˜
3. ä½¿ç”¨å·¥å…·è·å–å¿…è¦ä¿¡æ¯

æœ€åæä¾›æ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚"""
    )
    result = agent.invoke({"messages": [("user", input_dict["question"])]})
    return {"answer": result["messages"][-1].content}

# å®šä¹‰è¯„ä¼°å™¨
def exact_match(run, example):
    return {
        "score": 1.0 if run.outputs["answer"] == example.outputs["answer"] else 0.0,
        "key": "exact_match"
    }

# è¿è¡Œç‰ˆæœ¬ A
experiment_a = evaluate(
    predict_v_a,
    data="qa_benchmark",
    evaluators=[exact_match],
    experiment_prefix="Version A",
    description="åŸºå‡†ç‰ˆæœ¬"
)

# è¿è¡Œç‰ˆæœ¬ B
experiment_b = evaluate(
    predict_v_b,
    data="qa_benchmark",
    evaluators=[exact_match],
    experiment_prefix="Version B",
    description="æ”¹è¿›çš„æç¤ºè¯"
)

# å¯¹æ¯”ç»“æœ
print("=== ç‰ˆæœ¬å¯¹æ¯” ===")
print(f"ç‰ˆæœ¬ A å‡†ç¡®ç‡: {calculate_accuracy(experiment_a):.2%}")
print(f"ç‰ˆæœ¬ B å‡†ç¡®ç‡: {calculate_accuracy(experiment_b):.2%}")

def calculate_accuracy(experiment):
    scores = [r.evaluation_results[0].score for r in experiment.results]
    return sum(scores) / len(scores)
```

**ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ**ï¼š

```python
from scipy import stats

# è·å–ä¸¤ä¸ªç‰ˆæœ¬çš„è¯„åˆ†
scores_a = [r.evaluation_results[0].score for r in experiment_a.results]
scores_b = [r.evaluation_results[0].score for r in experiment_b.results]

# T æ£€éªŒ
t_stat, p_value = stats.ttest_ind(scores_a, scores_b)

print(f"T ç»Ÿè®¡é‡: {t_stat:.4f}")
print(f"P å€¼: {p_value:.4f}")

if p_value < 0.05:
    if sum(scores_b) > sum(scores_a):
        print("âœ… ç‰ˆæœ¬ B æ˜¾è‘—æ›´å¥½ï¼ˆp < 0.05ï¼‰")
    else:
        print("âœ… ç‰ˆæœ¬ A æ˜¾è‘—æ›´å¥½ï¼ˆp < 0.05ï¼‰")
else:
    print("âš ï¸  å·®å¼‚ä¸æ˜¾è‘—ï¼ˆp >= 0.05ï¼‰")
```

#### 1.3.3 ç›‘æ§æŒ‡æ ‡ä¸é—®é¢˜å‘ç°

**å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š

```python
from langsmith import Client
from datetime import datetime, timedelta

client = Client()

# ç›‘æ§æŒ‡æ ‡æ”¶é›†
def monitor_agent_health():
    """Agent å¥åº·ç›‘æµ‹"""

    # è·å–æœ€è¿‘ 24 å°æ—¶çš„è¿è¡Œ
    yesterday = datetime.now() - timedelta(days=1)
    runs = client.list_runs(
        project_name="my_agent",
        start_time=yesterday
    )

    # è®¡ç®—å…³é”®æŒ‡æ ‡
    total_runs = len(runs)
    successful_runs = sum(1 for run in runs if run.status == "success")
    error_runs = sum(1 for run in runs if run.status == "error")

    avg_duration = sum(
        (run.end_time - run.start_time).total_seconds()
        for run in runs
    ) / total_runs

    total_cost = sum(run.cost for run in runs if run.cost)

    # è®¡ç®—é”™è¯¯ç‡
    error_rate = error_runs / total_runs if total_runs > 0 else 0

    # è®¡ç®—æˆæœ¬æ•ˆç‡
    cost_per_success = total_cost / successful_runs if successful_runs > 0 else 0

    # è¿”å›æŒ‡æ ‡
    return {
        "total_runs": total_runs,
        "success_rate": successful_runs / total_runs,
        "error_rate": error_rate,
        "avg_duration": avg_duration,
        "total_cost": total_cost,
        "cost_per_success": cost_per_success
    }

metrics = monitor_agent_health()

print("=== Agent å¥åº·ç›‘æµ‹ ===")
print(f"æ€»è¯·æ±‚: {metrics['total_runs']}")
print(f"æˆåŠŸç‡: {metrics['success_rate']:.2%}")
print(f"é”™è¯¯ç‡: {metrics['error_rate']:.2%}")
print(f"å¹³å‡è€—æ—¶: {metrics['avg_duration']:.2f}s")
print(f"æ€»æˆæœ¬: ${metrics['total_cost']:.2f}")
print(f"æˆæœ¬/æˆåŠŸ: ${metrics['cost_per_success']:.4f}")
```

**é—®é¢˜è‡ªåŠ¨å‘Šè­¦**ï¼š

```python
def check_health_alerts():
    """æ£€æŸ¥æ˜¯å¦æœ‰å‘Šè­¦æ¡ä»¶"""
    metrics = monitor_agent_health()

    alerts = []

    # é”™è¯¯ç‡è¿‡é«˜
    if metrics["error_rate"] > 0.1:  # > 10%
        alerts.append(f"âš ï¸  é”™è¯¯ç‡è¿‡é«˜: {metrics['error_rate']:.2%}")

    # å“åº”æ—¶é—´è¿‡é•¿
    if metrics["avg_duration"] > 30:  # > 30ç§’
        alerts.append(f"âš ï¸  å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {metrics['avg_duration']:.2f}s")

    # æˆæœ¬çªå¢
    if metrics["cost_per_success"] > 0.01:  # > $0.01 per success
        alerts.append(f"âš ï¸  æˆæœ¬è¿‡é«˜: ${metrics['cost_per_success']:.4f}")

    if alerts:
        print("=== å‘Šè­¦ ===")
        for alert in alerts:
            print(alert)
    else:
        print("âœ… ç³»ç»Ÿå¥åº·")
```

**åé¦ˆé©±åŠ¨çš„é—®é¢˜å‘ç°**ï¼š

```python
# æŸ¥æ‰¾ç”¨æˆ·æ ‡è®°ä¸º"ä¸å¥½"çš„è¿è¡Œ
bad_runs = client.list_runs(
    project_name="my_agent",
    filter='feedback_key:"user_rating" AND feedback_score:0'
)

print(f"æ‰¾åˆ° {len(bad_runs)} æ¡å·®è¯„è¿è¡Œ")

# åˆ†æå¤±è´¥åŸå› 
failure_patterns = {}

for run in bad_runs:
    # è·å–å¤±è´¥çš„å·¥å…·è°ƒç”¨
    if run.child_runs:
        for child in run.child_runs:
            if child.status == "error":
                tool_name = child.name
                if tool_name not in failure_patterns:
                    failure_patterns[tool_name] = 0
                failure_patterns[tool_name] += 1

# æ’åºå¹¶æ˜¾ç¤º
for tool, count in sorted(failure_patterns.items(), key=lambda x: x[1], reverse=True):
    print(f"{tool}: {count} æ¬¡å¤±è´¥")
```

#### 1.3.4 ä¼˜åŒ–è¿­ä»£æµç¨‹

**å®Œæ•´çš„ä¼˜åŒ–å¾ªç¯**ï¼š

```
1. åŸºå‡†æµ‹è¯• â†’ è·å¾—å½“å‰æ€§èƒ½åŸºçº¿
   â†“
2. å‡è®¾æå‡º â†’ ä¾‹å¦‚"æ”¹è¿›æç¤ºè¯å¯ä»¥æé«˜å‡†ç¡®ç‡ 5%"
   â†“
3. åˆ¶å®šå¯¹ç­– â†’ ç¼–å†™æ–°çš„æç¤ºè¯æˆ–æ”¹è¿›æ¨¡å‹é€‰æ‹©
   â†“
4. è¿è¡Œ A/B æµ‹è¯• â†’ å¯¹æ¯”æ–°æ—§ç‰ˆæœ¬
   â†“
5. åˆ†æç»“æœ â†’ æ˜¯å¦æ»¡è¶³å‡è®¾ï¼Ÿ
   â”œâ”€ æ˜¯ â†’ æ›´æ–°ç‰ˆæœ¬åˆ°ç”Ÿäº§
   â””â”€ å¦ â†’ è¿”å›æ­¥éª¤ 2ï¼Œå°è¯•æ–°å‡è®¾
   â†“
6. ç›‘æ§ä¸Šçº¿ â†’ æŒç»­ç›‘æ§ç”Ÿäº§æŒ‡æ ‡
   â†“
7. æ”¶é›†åé¦ˆ â†’ ç”¨æˆ·æ ‡æ³¨ã€é—®é¢˜æŠ¥å‘Š
   â†“
8. è¿”å›æ­¥éª¤ 1 â†’ å¼€å§‹æ–°ä¸€è½®ä¼˜åŒ–
```

**å®ç°ä¼˜åŒ–å¾ªç¯çš„ä»£ç æ¡†æ¶**ï¼š

```python
from langsmith import Client
from langsmith.evaluation import evaluate
from datetime import datetime

class ContinuousOptimization:
    def __init__(self, project_name: str):
        self.client = Client()
        self.project_name = project_name
        self.iteration = 0

    def baseline(self):
        """è·å¾—åŸºå‡†æµ‹è¯•ç»“æœ"""
        results = evaluate(
            predict,
            data="qa_benchmark",
            evaluators=[exact_match, relevance],
            experiment_prefix=f"Baseline_{datetime.now().isoformat()}",
            description="åŸºå‡†ç‰ˆæœ¬"
        )

        accuracy = self.calculate_accuracy(results)
        print(f"ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {accuracy:.2%}")

        return results

    def propose_hypothesis(self, hypothesis: str):
        """æå‡ºå‡è®¾"""
        self.hypothesis = hypothesis
        print(f"ğŸ’¡ å‡è®¾: {hypothesis}")

    def implement_changes(self, changes: dict):
        """å®ç°æ”¹è¿›"""
        self.changes = changes
        print(f"âš™ï¸  æ”¹è¿›: {changes}")

    def run_experiment(self):
        """è¿è¡Œå®éªŒ"""
        self.iteration += 1

        # è¿™é‡Œè°ƒç”¨æ”¹è¿›åçš„é¢„æµ‹å‡½æ•°
        results = evaluate(
            predict_improved,  # æ”¹è¿›åçš„ç‰ˆæœ¬
            data="qa_benchmark",
            evaluators=[exact_match, relevance],
            experiment_prefix=f"Iteration_{self.iteration}_{datetime.now().isoformat()}",
            description=f"æ”¹è¿›: {self.hypothesis}"
        )

        new_accuracy = self.calculate_accuracy(results)
        return new_accuracy

    def compare_with_baseline(self, baseline_acc, new_acc):
        """å¯¹æ¯”æ”¹è¿›æ•ˆæœ"""
        improvement = (new_acc - baseline_acc) / baseline_acc * 100

        if improvement > 0:
            print(f"âœ… æ”¹è¿›æˆåŠŸ! æå‡ {improvement:.2%}")
            return True
        else:
            print(f"âŒ æœªè¾¾åˆ°é¢„æœŸï¼Œä¸‹é™ {abs(improvement):.2%}")
            return False

    def deploy_to_production(self):
        """éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"""
        print("ğŸš€ éƒ¨ç½²æ–°ç‰ˆæœ¬åˆ°ç”Ÿäº§")
        # æ›´æ–°ç”Ÿäº§ Agent çš„é…ç½®

    def calculate_accuracy(self, results):
        scores = [r.evaluation_results[0].score for r in results.results]
        return sum(scores) / len(scores)

# ä½¿ç”¨ç¤ºä¾‹
optimization = ContinuousOptimization("my_agent")

# ç¬¬ 1 è½®ï¼šåŸºå‡†
baseline_results = optimization.baseline()  # å‡†ç¡®ç‡: 75%

# ç¬¬ 2 è½®ï¼šæ”¹è¿›æç¤ºè¯
optimization.propose_hypothesis("è¯¦ç»†çš„ç³»ç»Ÿæç¤ºè¯èƒ½æé«˜å‡†ç¡®ç‡")
optimization.implement_changes({"system_prompt": "æ–°æç¤ºè¯..."})
acc_v2 = optimization.run_experiment()  # å‡†ç¡®ç‡: 78%

if optimization.compare_with_baseline(0.75, acc_v2):
    optimization.deploy_to_production()

# ç¬¬ 3 è½®ï¼šåˆ‡æ¢æ¨¡å‹
optimization.propose_hypothesis("ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹èƒ½è¿›ä¸€æ­¥æé«˜å‡†ç¡®ç‡")
optimization.implement_changes({"model": "gpt-4o"})
acc_v3 = optimization.run_experiment()  # å‡†ç¡®ç‡: 82%

if optimization.compare_with_baseline(0.75, acc_v3):
    optimization.deploy_to_production()
```

---

### æœ¬ç« å°ç»“

1. **è¿½è¸ªä½“ç³»**ï¼šè‡ªåŠ¨å’Œæ‰‹åŠ¨è¿½è¸ªï¼Œå½¢æˆå®Œæ•´çš„æ‰§è¡Œæ ‘ï¼Œç”¨äºè°ƒè¯•å’Œåˆ†æ
2. **Evaluation æ¡†æ¶**ï¼šDataset + Evaluator çš„ç»„åˆï¼Œæ”¯æŒå¯å‘å¼ã€LLMã€äººå·¥è¯„ä¼°
3. **æ‰¹é‡æµ‹è¯•**ï¼š`evaluate()` å’Œ `aevaluate()` å‡½æ•°ï¼Œæ”¯æŒè¡Œçº§å’Œå®éªŒçº§è¯„ä¼°
4. **æŒç»­ä¼˜åŒ–**ï¼šA/B æµ‹è¯•ã€ç‰ˆæœ¬æ§åˆ¶ã€ç›‘æ§å‘Šè­¦ã€è¿­ä»£æµç¨‹

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **æ€è€ƒ**ï¼šä¸ºä»€ä¹ˆéœ€è¦ `num_repetitions > 1` åœ¨ `evaluate()` ä¸­ï¼Ÿ

   <details>
   <summary>ç­”æ¡ˆ</summary>

   LLM çš„è¾“å‡ºæœ‰éšæœºæ€§ï¼ˆç”± temperature å‚æ•°æ§åˆ¶ï¼‰ã€‚è¿è¡Œå¤šæ¬¡å¯ä»¥ï¼š
   1. è¯„ä¼°æ¨¡å‹çš„ç¨³å®šæ€§
   2. è·å¾—æ›´å‡†ç¡®çš„å¹³å‡æŒ‡æ ‡
   3. æ£€æµ‹éšæœºå¯¼è‡´çš„å¶ç„¶å¥½/åç»“æœ
   </details>

2. **ç»ƒä¹ **ï¼šå®ç°ä¸€ä¸ª `f1_score_summary()` è¯„ä¼°å™¨ï¼Œè®¡ç®— F1 åˆ†æ•°ã€‚

3. **æ€è€ƒ**ï¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ªåé¦ˆå¾ªç¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤±è´¥æ¨¡å¼å¹¶æå‡ºæ”¹è¿›å»ºè®®ï¼Ÿ

---

### æ€»ç»“ä¸å±•æœ›

é€šè¿‡ LangSmith çš„è¿½è¸ªå’Œè¯„ä¼°ä½“ç³»ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†ï¼š

- **å¯è§‚æµ‹æ€§**ï¼šå®Œæ•´çš„æ‰§è¡Œé“¾è·¯å¯è§
- **å¯æµ‹è¯•æ€§**ï¼šç§‘å­¦çš„è¯„ä¼°æ¡†æ¶
- **å¯ä¼˜åŒ–æ€§**ï¼šæ•°æ®é©±åŠ¨çš„è¿­ä»£

è¿™äº›èƒ½åŠ›ä¸ºåç»­çš„é«˜çº§åº”ç”¨ï¼ˆå¤š Agentã€MCP é›†æˆï¼‰å’Œç”Ÿäº§å®è·µæä¾›äº†åšå®çš„åŸºç¡€ã€‚

---

**å‚è€ƒèµ„æº**ï¼š

- [LangSmith å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/langsmith)
- [LangSmith Python SDK](https://langsmith-sdk.readthedocs.io)
- [LangSmith æœ€ä½³å®è·µ](https://blog.langchain.com)


---

## ç¬¬2ç« ï¼š æ¶æ„è®¾è®¡æ¨¡å¼

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ç”Ÿäº§çº§ LangChain åº”ç”¨çš„æ¶æ„è®¾è®¡ã€‚

### 1.1 RAG æ¶æ„è®¾è®¡

**ç”Ÿäº§çº§ RAG ç³»ç»Ÿæ¶æ„**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·æ¥å£å±‚                             â”‚
â”‚                    (Web/API/Chat Interface)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åº”ç”¨æœåŠ¡å±‚                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ æŸ¥è¯¢å¤„ç†å™¨   â”‚  â”‚  Agent å¼•æ“   â”‚  â”‚ ç»“æœåå¤„ç†   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ ¸å¿ƒå±‚                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ å‘é‡æ£€ç´¢å™¨  â”‚  â”‚ é‡æ’åºå™¨    â”‚  â”‚  LLM ç½‘å…³    â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®å±‚                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ å‘é‡æ•°æ®åº“  â”‚  â”‚ æ–‡æ¡£å­˜å‚¨    â”‚  â”‚  ç¼“å­˜å±‚     â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°ä»£ç **ï¼š

```python
from typing import List, Optional, Dict, Any
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Pinecone
from langchain_community.retrievers import ContextualCompressionRetriever
from langchain_community.retrievers.document_compressors import CohereRerank
from langchain_core.documents import Document
import redis
import hashlib
import json

class ProductionRAGSystem:
    """ç”Ÿäº§çº§ RAG ç³»ç»Ÿ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        self.vector_store = self._init_vector_store()
        self.cache = self._init_cache()
        self.llm = self._init_llm()
        self.retriever = self._init_retriever()

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        return Pinecone.from_existing_index(
            index_name=self.config["pinecone_index"],
            embedding=self.embeddings,
            namespace=self.config.get("namespace", "default")
        )

    def _init_cache(self):
        """åˆå§‹åŒ–ç¼“å­˜å±‚"""
        return redis.Redis(
            host=self.config["redis_host"],
            port=self.config["redis_port"],
            db=0,
            decode_responses=True
        )

    def _init_llm(self):
        """åˆå§‹åŒ– LLM ç½‘å…³"""
        return ChatOpenAI(
            model=self.config["llm_model"],
            temperature=0.7,
            max_retries=3,
            request_timeout=30
        )

    def _init_retriever(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆå¸¦é‡æ’åºï¼‰"""
        base_retriever = self.vector_store.as_retriever(
            search_kwargs={"k": self.config.get("retrieve_k", 10)}
        )

        # æ·»åŠ å‹ç¼©/é‡æ’åºï¼ˆä½¿ç”¨ CohereRerankï¼‰
        compressor = CohereRerank(
            model="rerank-multilingual-v3.0",
            top_n=self.config.get("top_k", 5)
        )

        return ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )

    def _get_cache_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"rag:{hashlib.md5(query.encode()).hexdigest()}"

    def retrieve(self, query: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # æŸ¥è¯¢æ”¹å†™
        rewritten_query = self._rewrite_query(query)

        # æ£€ç´¢
        docs = self.retriever.get_relevant_documents(rewritten_query)

        # åå¤„ç†
        docs = self._post_process_docs(docs)

        return docs

    def _rewrite_query(self, query: str) -> str:
        """æŸ¥è¯¢æ”¹å†™ï¼ˆæé«˜æ£€ç´¢è´¨é‡ï¼‰"""
        prompt = f"""è¯·æ”¹å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆå‘é‡æ£€ç´¢ï¼š

åŸå§‹æŸ¥è¯¢ï¼š{query}

æ”¹å†™åçš„æŸ¥è¯¢ï¼ˆä¿æŒè¯­ä¹‰ï¼Œä¼˜åŒ–å…³é”®è¯ï¼‰ï¼š"""

        response = self.llm.invoke(prompt)
        return response.content

    def _post_process_docs(self, docs: List[Document]) -> List[Document]:
        """æ–‡æ¡£åå¤„ç†"""
        # å»é‡
        seen = set()
        unique_docs = []

        for doc in docs:
            doc_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            if doc_hash not in seen:
                seen.add(doc_hash)
                unique_docs.append(doc)

        # æŒ‰ç›¸å…³æ€§é‡æ–°æ’åº
        unique_docs.sort(key=lambda x: x.metadata.get("score", 0), reverse=True)

        return unique_docs[:self.config.get("top_k", 5)]

    def generate_answer(self, query: str, docs: List[Document]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([doc.page_content for doc in docs])

        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""

        response = self.llm.invoke(prompt)
        return response.content

    def query(self, query: str, use_cache: bool = True) -> Dict[str, Any]:
        """å®Œæ•´æŸ¥è¯¢æµç¨‹"""
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cache_key = self._get_cache_key(query)
            cached = self.cache.get(cache_key)

            if cached:
                return json.loads(cached)

        # æ£€ç´¢
        docs = self.retrieve(query)

        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(query, docs)

        result = {
            "query": query,
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }

        # å†™å…¥ç¼“å­˜
        if use_cache:
            self.cache.setex(
                cache_key,
                self.config.get("cache_ttl", 3600),
                json.dumps(result)
            )

        return result
```

**RAG Agent å®ç°ï¼ˆæ¨èï¼‰**ï¼š

ä½¿ç”¨ `@tool(response_format="content_and_artifact")` æ˜¯æ„å»º RAG Agent çš„å®˜æ–¹æ¨èæ–¹å¼ï¼Œå…è®¸å·¥å…·åŒæ—¶è¿”å›å†…å®¹å’ŒåŸå§‹æ•°æ®ï¼š

```python
from langchain_core.tools import tool
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from typing import List

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(
    documents=[...],  # ä½ çš„æ–‡æ¡£
    embedding=OpenAIEmbeddings()
)

# å®šä¹‰æ£€ç´¢å·¥å…·ï¼ˆä½¿ç”¨ content_and_artifact æ ¼å¼ï¼‰
@tool(response_format="content_and_artifact")
def retrieve_docs(query: str) -> tuple[str, List]:
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢

    Returns:
        (content, artifact): å†…å®¹æ‘˜è¦å’ŒåŸå§‹æ–‡æ¡£åˆ—è¡¨
    """
    # æ£€ç´¢æ–‡æ¡£
    docs = vectorstore.similarity_search(query, k=3)

    # æ„é€ è¿”å›å†…å®¹
    content = f"æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£"
    artifact = [doc.page_content for doc in docs]  # åŸå§‹æ–‡æ¡£

    return content, artifact

# åˆ›å»º RAG Agent
rag_agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[retrieve_docs],
    prompt="""ä½ æ˜¯ä¸€ä¸ª RAG åŠ©æ‰‹ã€‚

ä½¿ç”¨ retrieve_docs å·¥å…·è·å–ç›¸å…³æ–‡æ¡£ï¼Œç„¶ååŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘Šè¯‰ç”¨æˆ·ã€‚
"""
)

# ä½¿ç”¨
result = rag_agent.invoke({
    "messages": [("user", "LangChain 1.0 æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ")]
})

print(result["messages"][-1].content)
# Agent ä¼šè‡ªåŠ¨ï¼š
# 1. è°ƒç”¨ retrieve_docs å·¥å…·
# 2. è®¿é—® artifact ä¸­çš„åŸå§‹æ–‡æ¡£
# 3. åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆç­”æ¡ˆ
```

**ä½¿ç”¨ RunnablePassthrough.assign() çš„ LCEL æ–¹å¼**ï¼š

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å®šä¹‰ RAG æç¤ºè¯
rag_prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:
""")

# æ„å»º RAG é“¾ï¼ˆä½¿ç”¨ assign æ·»åŠ æ£€ç´¢ç»“æœï¼‰
rag_chain = (
    RunnablePassthrough.assign(
        context=lambda x: vectorstore.similarity_search(x["question"], k=3)
    )
    | RunnablePassthrough.assign(
        context=lambda x: "\n\n".join([doc.page_content for doc in x["context"]])
    )
    | rag_prompt
    | ChatOpenAI(model="gpt-4o")
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke({
    "question": "LangChain 1.0 æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ"
})
```

**ä½¿ç”¨ @dynamic_prompt çš„ Middleware æ–¹å¼ï¼ˆæ¨èï¼‰**ï¼š

```python
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

@dynamic_prompt
def prompt_with_context(request: ModelRequest) -> str:
    """åŠ¨æ€æ³¨å…¥æ£€ç´¢ä¸Šä¸‹æ–‡"""
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_query = request.state["messages"][-1].text

    # æ‰§è¡Œæ£€ç´¢
    retrieved_docs = vectorstore.similarity_search(last_query, k=3)

    # æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹
    docs_content = "\n\n".join([
        f"æ–‡æ¡£ {i+1}:\n{doc.page_content}"
        for i, doc in enumerate(retrieved_docs)
    ])

    # è¿”å›ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä¼šè‡ªåŠ¨æ³¨å…¥åˆ°è¯·æ±‚ä¸­ï¼‰
    system_message = (
        "ä½ æ˜¯ä¸€ä¸ª RAG åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜:\n\n"
        f"{docs_content}\n\n"
        "å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚"
    )

    return system_message

# åˆ›å»º Agentï¼ˆä¼ å…¥ middlewareï¼‰
rag_agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[],  # ä¸éœ€è¦å·¥å…·ï¼Œæ£€ç´¢åœ¨ middleware ä¸­å®Œæˆ
    middleware=[prompt_with_context]  # âœ… å…³é”®ï¼šä¼ å…¥ dynamic_prompt
)

# ä½¿ç”¨ï¼ˆè‡ªåŠ¨åœ¨æ¯æ¬¡è¯·æ±‚å‰æ£€ç´¢ï¼‰
result = rag_agent.invoke({
    "messages": [("user", "LangChain 1.0 æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ")]
})
```

**@dynamic_prompt çš„ä¼˜åŠ¿**ï¼š
1. **è‡ªåŠ¨æ‰§è¡Œ**ï¼šæ¯æ¬¡æ¨¡å‹è°ƒç”¨å‰è‡ªåŠ¨æ£€ç´¢ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨å·¥å…·
2. **å•æ¬¡æ¨ç†**ï¼šåªéœ€ 1 æ¬¡ LLM è°ƒç”¨ï¼ˆvs Agent å·¥å…·æ¨¡å¼éœ€è¦ 2 æ¬¡ï¼‰
3. **é€æ˜æ³¨å…¥**ï¼šä¸Šä¸‹æ–‡è‡ªåŠ¨æ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ¨¡å‹æ— æ„ŸçŸ¥
4. **é€‚åˆç®€å•åœºæ™¯**ï¼šå›ºå®šçš„æ£€ç´¢-å›ç­”æµç¨‹ï¼Œæ€§èƒ½æœ€ä¼˜

**ä¸‰ç§ RAG å®ç°æ–¹å¼å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | Agent + Tool | @dynamic_prompt | LCEL Chain |
|------|-------------|-----------------|------------|
| **LLM è°ƒç”¨æ¬¡æ•°** | 2 æ¬¡ï¼ˆå†³ç­–+å›ç­”ï¼‰ | 1 æ¬¡ | 1 æ¬¡ |
| **çµæ´»æ€§** | é«˜ï¼ˆè‡ªä¸»å†³ç­–ï¼‰ | ä¸­ï¼ˆå›ºå®šæ£€ç´¢ï¼‰ | ä½ï¼ˆå›ºå®šæµç¨‹ï¼‰ |
| **å¤æ‚åº¦** | ä½ï¼ˆè‡ªåŠ¨æ¨ç†ï¼‰ | ä½ï¼ˆMiddlewareï¼‰ | ä¸­ï¼ˆéœ€è®¾è®¡é“¾ï¼‰ |
| **æˆæœ¬** | è¾ƒé«˜ | ä¸­ç­‰ | è¾ƒä½ |
| **å¯æ§æ€§** | ä½ | ä¸­ | é«˜ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚æŸ¥è¯¢ã€å¤šæ­¥æ¨ç† | ç®€å• RAGã€æ€§èƒ½ä¼˜åŒ– | å®Œå…¨è‡ªå®šä¹‰æµç¨‹ |

**æ¨èå®è·µ**ï¼š

1. **ç®€å• RAGï¼ˆå•æ¬¡æ£€ç´¢ï¼‰**ï¼šä½¿ç”¨ `@dynamic_prompt`ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
2. **å¤æ‚æŸ¥è¯¢ï¼ˆå¤šæ¬¡æ£€ç´¢ï¼‰**ï¼šä½¿ç”¨ Agent + Toolï¼ˆè‡ªåŠ¨å†³ç­–ï¼‰
3. **å®Œå…¨è‡ªå®šä¹‰æµç¨‹**ï¼šä½¿ç”¨ LCEL Chainï¼ˆæœ€å¤§æ§åˆ¶ï¼‰
4. **æ··åˆæ–¹æ¡ˆ**ï¼šAgent + dynamic_promptï¼ˆçµæ´»æ€§ä¸æ•ˆç‡å…¼é¡¾ï¼‰

---

**RAG ä¼˜åŒ–ç­–ç•¥**ï¼š

```python
class OptimizedRAGPipeline:
    """ä¼˜åŒ–çš„ RAG æµæ°´çº¿"""

    def __init__(self):
        self.strategies = {
            "hybrid_search": self.hybrid_search,
            "multi_query": self.multi_query_retrieval,
            "iterative": self.iterative_retrieval,
            "ensemble": self.ensemble_retrieval
        }

    def hybrid_search(self, query: str) -> List[Document]:
        """æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰"""
        # å‘é‡æœç´¢
        vector_results = self.vector_store.similarity_search(query, k=10)

        # BM25 å…³é”®è¯æœç´¢
        keyword_results = self.bm25_retriever.get_relevant_documents(query)

        # åˆå¹¶ç»“æœ
        all_docs = vector_results + keyword_results

        # ä½¿ç”¨ RRF (Reciprocal Rank Fusion) é‡æ’åº
        return self._reciprocal_rank_fusion(all_docs)

    def multi_query_retrieval(self, query: str) -> List[Document]:
        """å¤šæŸ¥è¯¢æ£€ç´¢"""
        # ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
        queries = self._generate_query_variants(query)

        all_docs = []
        for q in queries:
            docs = self.retriever.get_relevant_documents(q)
            all_docs.extend(docs)

        # å»é‡å¹¶é‡æ’åº
        return self._deduplicate_and_rank(all_docs)

    def iterative_retrieval(self, query: str, max_iterations: int = 3) -> List[Document]:
        """è¿­ä»£æ£€ç´¢"""
        docs = []
        current_query = query

        for i in range(max_iterations):
            # æ£€ç´¢
            new_docs = self.retriever.get_relevant_documents(current_query)
            docs.extend(new_docs)

            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­
            if self._is_sufficient(docs, query):
                break

            # åŸºäºå·²æœ‰ç»“æœç”Ÿæˆæ–°æŸ¥è¯¢
            current_query = self._generate_followup_query(query, docs)

        return docs

    def ensemble_retrieval(self, query: str) -> List[Document]:
        """é›†æˆæ£€ç´¢"""
        retrievers = [
            self.dense_retriever,    # å¯†é›†å‘é‡
            self.sparse_retriever,   # ç¨€ç–å‘é‡
            self.cross_encoder      # äº¤å‰ç¼–ç å™¨
        ]

        all_results = []
        weights = [0.5, 0.3, 0.2]  # æƒé‡

        for retriever, weight in zip(retrievers, weights):
            docs = retriever.get_relevant_documents(query)
            for doc in docs:
                doc.metadata["weight"] = weight
            all_results.extend(docs)

        return self._weighted_merge(all_results)
```

### 1.2 Multi-Agent æ¶æ„é€‰æ‹©

**ç”Ÿäº§ç¯å¢ƒå¤š Agent æ¶æ„å†³ç­–**ï¼š

```python
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional

class AgentArchitecture(Enum):
    """Agent æ¶æ„ç±»å‹"""
    MONOLITHIC = "monolithic"           # å•ä½“ Agent
    SUPERVISOR_WORKER = "supervisor"     # ç›‘ç£è€…-å·¥ä½œè€…
    PIPELINE = "pipeline"                # æµæ°´çº¿
    HIERARCHICAL = "hierarchical"       # å±‚çº§
    MESH = "mesh"                       # ç½‘çŠ¶

@dataclass
class ArchitectureDecision:
    """æ¶æ„å†³ç­–"""
    architecture: AgentArchitecture
    reason: str
    pros: List[str]
    cons: List[str]
    implementation_complexity: int  # 1-10

class ArchitectureSelector:
    """æ¶æ„é€‰æ‹©å™¨"""

    def select(
        self,
        task_complexity: int,      # 1-10
        team_size: int,            # Agent æ•°é‡
        coordination_level: str,    # low/medium/high
        latency_requirement: str,   # low/medium/high
        scalability_need: str       # low/medium/high
    ) -> ArchitectureDecision:
        """é€‰æ‹©åˆé€‚çš„æ¶æ„"""

        # ç®€å•ä»»åŠ¡
        if task_complexity <= 3:
            if team_size <= 1:
                return ArchitectureDecision(
                    architecture=AgentArchitecture.MONOLITHIC,
                    reason="ç®€å•ä»»åŠ¡ä½¿ç”¨å•ä½“ Agent å³å¯",
                    pros=["ç®€å•", "ä½å»¶è¿Ÿ", "æ˜“è°ƒè¯•"],
                    cons=["æ‰©å±•æ€§å·®", "åŠŸèƒ½å—é™"],
                    implementation_complexity=2
                )

        # ä¸­ç­‰å¤æ‚åº¦
        if task_complexity <= 6:
            if coordination_level == "high":
                return ArchitectureDecision(
                    architecture=AgentArchitecture.SUPERVISOR_WORKER,
                    reason="éœ€è¦é«˜åº¦åè°ƒï¼Œä½¿ç”¨ç›‘ç£è€…æ¨¡å¼",
                    pros=["æ§åˆ¶æ¸…æ™°", "æ˜“äºç®¡ç†", "è´£ä»»æ˜ç¡®"],
                    cons=["ä¸­å¤®ç“¶é¢ˆ", "ç›‘ç£è€…å¤æ‚"],
                    implementation_complexity=5
                )
            else:
                return ArchitectureDecision(
                    architecture=AgentArchitecture.PIPELINE,
                    reason="é¡ºåºå¤„ç†ä»»åŠ¡ï¼Œä½¿ç”¨æµæ°´çº¿",
                    pros=["æµç¨‹æ¸…æ™°", "æ˜“äºæ‰©å±•", "å¯å¹¶è¡Œ"],
                    cons=["çµæ´»æ€§å·®", "é”™è¯¯ä¼ æ’­"],
                    implementation_complexity=4
                )

        # é«˜å¤æ‚åº¦
        if team_size > 10:
            return ArchitectureDecision(
                architecture=AgentArchitecture.HIERARCHICAL,
                reason="å¤§è§„æ¨¡å›¢é˜Ÿéœ€è¦å±‚çº§ç®¡ç†",
                pros=["å¯æ‰©å±•", "èŒè´£æ¸…æ™°", "æ˜“äºç®¡ç†å¤§å›¢é˜Ÿ"],
                cons=["å¤æ‚", "è°ƒè¯•å›°éš¾", "å±‚çº§å»¶è¿Ÿ"],
                implementation_complexity=8
            )

        # é»˜è®¤
        return ArchitectureDecision(
            architecture=AgentArchitecture.SUPERVISOR_WORKER,
            reason="é€šç”¨æ¶æ„ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯",
            pros=["å¹³è¡¡", "æˆç†Ÿ", "çµæ´»"],
            cons=["å¯èƒ½è¿‡åº¦è®¾è®¡"],
            implementation_complexity=5
        )

# ç”Ÿäº§çº§å¤š Agent å®ç°
class ProductionMultiAgentSystem:
    """ç”Ÿäº§çº§å¤š Agent ç³»ç»Ÿ"""

    def __init__(self, architecture: AgentArchitecture):
        self.architecture = architecture
        self.agents = {}
        self.metrics = {}
        self.circuit_breakers = {}

    def register_agent(self, name: str, agent: Any):
        """æ³¨å†Œ Agentï¼ˆå¸¦å¥åº·æ£€æŸ¥ï¼‰"""
        self.agents[name] = agent
        self.circuit_breakers[name] = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60
        )

    def execute_with_fallback(self, agent_name: str, task: dict) -> dict:
        """å¸¦é™çº§çš„æ‰§è¡Œ"""
        breaker = self.circuit_breakers[agent_name]

        if breaker.is_open():
            # ç†”æ–­å™¨æ‰“å¼€ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
            return self._fallback_strategy(agent_name, task)

        try:
            # æ­£å¸¸æ‰§è¡Œ
            result = self.agents[agent_name].invoke(task)
            breaker.record_success()
            return result

        except Exception as e:
            breaker.record_failure()

            if breaker.is_open():
                # åˆšåˆšç†”æ–­
                self._alert_circuit_open(agent_name)

            # ä½¿ç”¨é™çº§ç­–ç•¥
            return self._fallback_strategy(agent_name, task)

    def _fallback_strategy(self, agent_name: str, task: dict) -> dict:
        """é™çº§ç­–ç•¥"""
        # ç­–ç•¥ 1ï¼šä½¿ç”¨å¤‡ç”¨ Agent
        backup_agent = self.agents.get(f"{agent_name}_backup")
        if backup_agent:
            return backup_agent.invoke(task)

        # ç­–ç•¥ 2ï¼šè¿”å›ç¼“å­˜ç»“æœ
        cached = self.get_cached_result(task)
        if cached:
            return cached

        # ç­–ç•¥ 3ï¼šè¿”å›é»˜è®¤å“åº”
        return {
            "status": "degraded",
            "message": f"{agent_name} æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
        }
```

### 1.3 Workflow æ¶æ„æ¨¡å¼

**å·¥ä½œæµç¼–æ’æ¶æ„**ï¼š

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

class WorkflowOrchestrator:
    """å·¥ä½œæµç¼–æ’å™¨"""

    def __init__(self):
        self.workflows = {}
        self.templates = self._load_templates()

    def _load_templates(self):
        """åŠ è½½å·¥ä½œæµæ¨¡æ¿"""
        return {
            "approval_flow": self._create_approval_workflow(),
            "data_pipeline": self._create_data_pipeline(),
            "customer_service": self._create_cs_workflow()
        }

    def _create_approval_workflow(self):
        """åˆ›å»ºå®¡æ‰¹å·¥ä½œæµ"""

        class ApprovalState(TypedDict):
            request: dict
            approvals: Annotated[List[dict], operator.add]
            status: str
            level: int

        workflow = StateGraph(ApprovalState)

        # èŠ‚ç‚¹
        workflow.add_node("validate", self.validate_request)
        workflow.add_node("level1_approval", self.level1_approval)
        workflow.add_node("level2_approval", self.level2_approval)
        workflow.add_node("execute", self.execute_request)
        workflow.add_node("notify", self.notify_result)

        # è¾¹
        workflow.add_edge("validate", "level1_approval")

        # æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "level1_approval",
            lambda x: "level2" if x["request"]["amount"] > 10000 else "execute",
            {
                "level2": "level2_approval",
                "execute": "execute"
            }
        )

        workflow.add_edge("level2_approval", "execute")
        workflow.add_edge("execute", "notify")
        workflow.add_edge("notify", END)

        workflow.set_entry_point("validate")

        return workflow.compile()

    def create_dynamic_workflow(self, config: dict):
        """åŠ¨æ€åˆ›å»ºå·¥ä½œæµ"""
        workflow = StateGraph(dict)

        # åŠ¨æ€æ·»åŠ èŠ‚ç‚¹
        for node_config in config["nodes"]:
            node_func = self._create_node_function(node_config)
            workflow.add_node(node_config["id"], node_func)

        # åŠ¨æ€æ·»åŠ è¾¹
        for edge_config in config["edges"]:
            if edge_config["type"] == "direct":
                workflow.add_edge(edge_config["from"], edge_config["to"])
            elif edge_config["type"] == "conditional":
                workflow.add_conditional_edges(
                    edge_config["from"],
                    self._create_condition_function(edge_config["condition"]),
                    edge_config["branches"]
                )

        workflow.set_entry_point(config["entry"])

        return workflow.compile()
```

---

## ç¬¬3ç« ï¼š æ€§èƒ½ä¸æˆæœ¬ä¼˜åŒ–

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½è°ƒä¼˜å’Œæˆæœ¬æ§åˆ¶ã€‚

### 2.1 å»¶è¿Ÿä¼˜åŒ–

**ç»¼åˆå»¶è¿Ÿä¼˜åŒ–ç­–ç•¥**ï¼š

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time
from functools import lru_cache
from typing import List, Dict, Any

class LatencyOptimizer:
    """å»¶è¿Ÿä¼˜åŒ–å™¨"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=10)
        self.model_latencies = {}  # è®°å½•æ¨¡å‹å»¶è¿Ÿ

    def select_optimal_model(self, task_type: str, max_latency: float) -> str:
        """æ ¹æ®å»¶è¿Ÿè¦æ±‚é€‰æ‹©æ¨¡å‹"""
        model_configs = {
            "gpt-4o": {"latency": 2.5, "quality": 0.95, "cost": 0.03},
            "gpt-4o-mini": {"latency": 0.8, "quality": 0.85, "cost": 0.001},
            "gpt-3.5-turbo": {"latency": 0.5, "quality": 0.80, "cost": 0.0005},
            "claude-3-haiku": {"latency": 0.3, "quality": 0.82, "cost": 0.0003}
        }

        # ç­›é€‰æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„æ¨¡å‹
        eligible_models = [
            model for model, config in model_configs.items()
            if config["latency"] <= max_latency
        ]

        if not eligible_models:
            raise ValueError(f"æ²¡æœ‰æ¨¡å‹æ»¡è¶³ {max_latency}s å»¶è¿Ÿè¦æ±‚")

        # é€‰æ‹©è´¨é‡æœ€é«˜çš„
        return max(eligible_models, key=lambda x: model_configs[x]["quality"])

    async def parallel_execution(self, tasks: List[Dict[str, Any]]) -> List[Any]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡"""

        async def execute_task(task):
            """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
            start = time.time()

            # å¼‚æ­¥æ‰§è¡Œ
            result = await self._async_invoke(task)

            # è®°å½•å»¶è¿Ÿ
            latency = time.time() - start
            self._record_latency(task["model"], latency)

            return result

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*[execute_task(task) for task in tasks])

        return results

    @lru_cache(maxsize=1000)
    def cached_inference(self, prompt_hash: str) -> str:
        """ç¼“å­˜çš„æ¨ç†"""
        # ç¼“å­˜ä¼šè‡ªåŠ¨å¤„ç†
        return self._invoke_llm(prompt_hash)

    def optimize_prompt_batching(self, prompts: List[str]) -> List[str]:
        """ä¼˜åŒ–æç¤ºè¯æ‰¹å¤„ç†"""
        # æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–
        optimal_batch_size = self._calculate_optimal_batch_size(len(prompts))

        results = []
        for i in range(0, len(prompts), optimal_batch_size):
            batch = prompts[i:i + optimal_batch_size]

            # æ‰¹é‡å¤„ç†
            batch_results = self._batch_inference(batch)
            results.extend(batch_results)

        return results

    def _calculate_optimal_batch_size(self, total_count: int) -> int:
        """è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        # åŸºäºç»éªŒå…¬å¼
        if total_count < 10:
            return total_count
        elif total_count < 100:
            return 10
        else:
            return min(50, total_count // 10)

    def pipeline_optimization(self):
        """æµæ°´çº¿ä¼˜åŒ–"""

        class Pipeline:
            def __init__(self):
                self.stages = []

            def add_stage(self, func, parallel=False):
                self.stages.append((func, parallel))

            async def execute(self, input_data):
                current = input_data

                for func, parallel in self.stages:
                    if parallel and isinstance(current, list):
                        # å¹¶è¡Œå¤„ç†åˆ—è¡¨
                        current = await asyncio.gather(*[func(item) for item in current])
                    else:
                        # ä¸²è¡Œå¤„ç†
                        current = await func(current)

                return current

        return Pipeline()
```

### 2.2 ååé‡ä¼˜åŒ–

**ååé‡ä¼˜åŒ–å®ç°**ï¼š

```python
import asyncio
from asyncio import Queue, Semaphore
from typing import Optional
import aiohttp

class ThroughputOptimizer:
    """ååé‡ä¼˜åŒ–å™¨"""

    def __init__(self, max_concurrent: int = 100):
        self.semaphore = Semaphore(max_concurrent)
        self.request_queue = Queue()
        self.connection_pool = None

    async def init_connection_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        connector = aiohttp.TCPConnector(
            limit=100,  # æ€»è¿æ¥æ•°
            limit_per_host=30,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
            ttl_dns_cache=300  # DNS ç¼“å­˜æ—¶é—´
        )

        timeout = aiohttp.ClientTimeout(
            total=300,
            connect=10,
            sock_read=30
        )

        self.connection_pool = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )

    async def batch_processor(self, batch_size: int = 10):
        """æ‰¹å¤„ç†å¤„ç†å™¨"""
        batch = []

        while True:
            try:
                # æ”¶é›†æ‰¹æ¬¡
                while len(batch) < batch_size:
                    item = await asyncio.wait_for(
                        self.request_queue.get(),
                        timeout=1.0  # 1ç§’è¶…æ—¶
                    )
                    batch.append(item)

            except asyncio.TimeoutError:
                # è¶…æ—¶ä½†æœ‰æ•°æ®ï¼Œå¤„ç†ç°æœ‰æ‰¹æ¬¡
                if batch:
                    await self._process_batch(batch)
                    batch = []

            # æ‰¹æ¬¡æ»¡ï¼Œå¤„ç†
            if len(batch) >= batch_size:
                await self._process_batch(batch)
                batch = []

    async def _process_batch(self, batch: List[dict]):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        async with self.semaphore:
            # æ‰¹é‡ API è°ƒç”¨
            tasks = [self._make_request(item) for item in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for item, result in zip(batch, results):
                if isinstance(result, Exception):
                    await self._handle_error(item, result)
                else:
                    await self._handle_success(item, result)

    async def adaptive_concurrency(self):
        """è‡ªé€‚åº”å¹¶å‘æ§åˆ¶"""

        class AdaptiveLimiter:
            def __init__(self):
                self.current_limit = 10
                self.min_limit = 5
                self.max_limit = 100
                self.success_rate = 1.0
                self.adjustment_interval = 10  # ç§’

            async def adjust(self):
                """è°ƒæ•´å¹¶å‘é™åˆ¶"""
                while True:
                    await asyncio.sleep(self.adjustment_interval)

                    if self.success_rate > 0.95:
                        # æˆåŠŸç‡é«˜ï¼Œå¢åŠ å¹¶å‘
                        self.current_limit = min(
                            self.current_limit * 1.2,
                            self.max_limit
                        )
                    elif self.success_rate < 0.8:
                        # æˆåŠŸç‡ä½ï¼Œå‡å°‘å¹¶å‘
                        self.current_limit = max(
                            self.current_limit * 0.8,
                            self.min_limit
                        )

        return AdaptiveLimiter()
```

### 2.3 Token ç®¡ç†ä¸æˆæœ¬æ§åˆ¶

**Token æˆæœ¬ä¼˜åŒ–ç³»ç»Ÿ**ï¼š

```python
from dataclasses import dataclass
from typing import Dict, List
import tiktoken

@dataclass
class TokenBudget:
    """Token é¢„ç®—"""
    total_budget: int
    used: int = 0

    @property
    def remaining(self) -> int:
        return self.total_budget - self.used

    def can_afford(self, tokens: int) -> bool:
        return self.remaining >= tokens

class TokenOptimizer:
    """Token ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        self.pricing = {
            "gpt-4o": {"input": 0.0025, "output": 0.01},  # per 1K tokens (2024å¹´æœ€æ–°ä»·æ ¼)
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015}
        }

    def count_tokens(self, text: str) -> int:
        """è®¡ç®— Token æ•°é‡"""
        return len(self.encoding.encode(text))

    def optimize_prompt(self, prompt: str, max_tokens: int) -> str:
        """ä¼˜åŒ–æç¤ºè¯é•¿åº¦"""
        tokens = self.count_tokens(prompt)

        if tokens <= max_tokens:
            return prompt

        # ç­–ç•¥ 1ï¼šæˆªæ–­
        if tokens < max_tokens * 1.5:
            return self._truncate_prompt(prompt, max_tokens)

        # ç­–ç•¥ 2ï¼šæ‘˜è¦
        return self._summarize_prompt(prompt, max_tokens)

    def _truncate_prompt(self, prompt: str, max_tokens: int) -> str:
        """æˆªæ–­æç¤ºè¯"""
        tokens = self.encoding.encode(prompt)

        # ä¿ç•™å¼€å¤´å’Œç»“å°¾
        if max_tokens > 200:
            start_tokens = tokens[:max_tokens//2]
            end_tokens = tokens[-(max_tokens//2):]

            truncated = start_tokens + end_tokens
            return self.encoding.decode(truncated)

        # åªä¿ç•™å¼€å¤´
        return self.encoding.decode(tokens[:max_tokens])

    def _summarize_prompt(self, prompt: str, max_tokens: int) -> str:
        """æ‘˜è¦æç¤ºè¯"""
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œæ‘˜è¦
        summary_prompt = f"å°†ä»¥ä¸‹å†…å®¹æ‘˜è¦åˆ° {max_tokens} tokens ä»¥å†…ï¼š\n\n{prompt}"

        # è°ƒç”¨ gpt-3.5-turbo è¿›è¡Œæ‘˜è¦
        # ... å®ç°ç•¥

    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        if model not in self.pricing:
            raise ValueError(f"Unknown model: {model}")

        pricing = self.pricing[model]

        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost

    def cost_aware_routing(self, task: dict, budget: TokenBudget) -> str:
        """æˆæœ¬æ„ŸçŸ¥è·¯ç”±"""
        # ä¼°ç®—ä¸åŒæ¨¡å‹çš„æˆæœ¬
        estimates = {}

        for model in self.pricing.keys():
            estimated_tokens = self._estimate_tokens(task, model)
            estimated_cost = self.calculate_cost(
                model,
                estimated_tokens["input"],
                estimated_tokens["output"]
            )

            if budget.can_afford(estimated_cost * 1000):  # è½¬æ¢ä¸º token å•ä½
                estimates[model] = {
                    "cost": estimated_cost,
                    "quality": self._get_model_quality(model)
                }

        if not estimates:
            raise ValueError("é¢„ç®—ä¸è¶³")

        # åœ¨é¢„ç®—å†…é€‰æ‹©è´¨é‡æœ€å¥½çš„
        best_model = max(estimates.items(), key=lambda x: x[1]["quality"])

        return best_model[0]
```

### 2.4 ç¼“å­˜å¤ç”¨ç­–ç•¥

**å¤šå±‚ç¼“å­˜æ¶æ„**ï¼š

```python
import hashlib
import pickle
from typing import Any, Optional
from datetime import datetime, timedelta

class MultiLevelCache:
    """å¤šå±‚ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜
        self.l2_cache = Redis()  # Redis ç¼“å­˜
        self.l3_cache = S3Cache()  # S3 é•¿æœŸç¼“å­˜

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ï¼ˆé€å±‚æŸ¥æ‰¾ï¼‰"""
        # L1 ç¼“å­˜
        if key in self.l1_cache:
            self._promote_to_l1(key, self.l1_cache[key])
            return self.l1_cache[key]

        # L2 ç¼“å­˜
        value = self.l2_cache.get(key)
        if value:
            self._promote_to_l1(key, value)
            return value

        # L3 ç¼“å­˜
        value = self.l3_cache.get(key)
        if value:
            self._promote_to_l2(key, value)
            self._promote_to_l1(key, value)
            return value

        return None

    def set(
        self,
        key: str,
        value: Any,
        ttl_l1: int = 300,  # 5 åˆ†é’Ÿ
        ttl_l2: int = 3600,  # 1 å°æ—¶
        ttl_l3: int = 86400  # 1 å¤©
    ):
        """è®¾ç½®ç¼“å­˜ï¼ˆå†™å…¥æ‰€æœ‰å±‚ï¼‰"""
        # å†™å…¥ L1
        self.l1_cache[key] = value
        self._schedule_l1_eviction(key, ttl_l1)

        # å†™å…¥ L2
        self.l2_cache.setex(key, ttl_l2, pickle.dumps(value))

        # å†™å…¥ L3
        self.l3_cache.put(key, value, ttl_l3)

    def _promote_to_l1(self, key: str, value: Any):
        """æå‡åˆ° L1 ç¼“å­˜"""
        self.l1_cache[key] = value

    def _promote_to_l2(self, key: str, value: Any):
        """æå‡åˆ° L2 ç¼“å­˜"""
        self.l2_cache.setex(key, 3600, pickle.dumps(value))

class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜"""

    def __init__(self, similarity_threshold: float = 0.95):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = FAISS()
        self.cache = {}
        self.similarity_threshold = similarity_threshold

    def get(self, query: str) -> Optional[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è·å–ç¼“å­˜"""
        # è·å–æŸ¥è¯¢å‘é‡
        query_embedding = self.embeddings.embed_query(query)

        # æœç´¢ç›¸ä¼¼æŸ¥è¯¢
        similar_docs = self.vector_store.similarity_search_with_score(
            query,
            k=1
        )

        if similar_docs:
            doc, score = similar_docs[0]

            if score >= self.similarity_threshold:
                # æ‰¾åˆ°ç›¸ä¼¼æŸ¥è¯¢ï¼Œè¿”å›ç¼“å­˜ç»“æœ
                cache_key = doc.metadata["cache_key"]
                return self.cache.get(cache_key)

        return None

    def set(self, query: str, result: str):
        """è®¾ç½®è¯­ä¹‰ç¼“å­˜"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(query.encode()).hexdigest()

        # å­˜å‚¨ç»“æœ
        self.cache[cache_key] = result

        # å­˜å‚¨æŸ¥è¯¢å‘é‡
        self.vector_store.add_texts(
            [query],
            metadatas=[{"cache_key": cache_key, "timestamp": datetime.now()}]
        )
```

### 2.5 è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©

**æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹**ï¼š

```python
class AdaptiveModelSelector:
    """è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©å™¨"""

    def select_model(self, query: str, context: dict) -> str:
        """æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
        complexity = self._estimate_complexity(query, context)

        if complexity < 0.3:
            return "gpt-3.5-turbo"  # ç®€å•æŸ¥è¯¢ï¼ŒèŠ‚çœæˆæœ¬
        elif complexity < 0.7:
            return "gpt-4o-mini"    # ä¸­ç­‰æŸ¥è¯¢
        else:
            return "gpt-4o"         # å¤æ‚æŸ¥è¯¢ï¼Œç¡®ä¿è´¨é‡

    def _estimate_complexity(self, query: str, context: dict) -> float:
        """ä¼°ç®—æŸ¥è¯¢å¤æ‚åº¦ï¼ˆ0-1ï¼‰"""
        score = 0

        # æŸ¥è¯¢é•¿åº¦
        if len(query) > 100:
            score += 0.2

        # æ˜¯å¦éœ€è¦å¤šæ­¥æ¨ç†
        reasoning_keywords = ["ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "åˆ†æ", "å¯¹æ¯”"]
        if any(kw in query for kw in reasoning_keywords):
            score += 0.3

        # æ˜¯å¦éœ€è¦å·¥å…·
        if context.get("tools_required"):
            score += 0.2

        # ä¸Šä¸‹æ–‡é•¿åº¦
        if len(context.get("history", [])) > 10:
            score += 0.3

        return min(score, 1.0)

# ä½¿ç”¨ç¤ºä¾‹
selector = AdaptiveModelSelector()

queries = [
    "ä½ å¥½",  # ç®€å• â†’ gpt-3.5-turbo
    "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½ï¼Ÿ",  # ä¸­ç­‰ â†’ gpt-4o-mini
    "åˆ†æè¿™æ®µä»£ç çš„æ—¶é—´å¤æ‚åº¦å¹¶æå‡ºä¼˜åŒ–æ–¹æ¡ˆ",  # å¤æ‚ â†’ gpt-4o
]

for query in queries:
    model = selector.select_model(query, {})
    print(f"æŸ¥è¯¢: {query}\né€‰æ‹©æ¨¡å‹: {model}\n")

# æˆæœ¬èŠ‚çœï¼š
# å‡è®¾100æ¬¡æŸ¥è¯¢ï¼š
# - 60%ç®€å•ï¼ˆgpt-3.5-turbo: $0.001/æ¬¡ï¼‰ = $0.06
# - 30%ä¸­ç­‰ï¼ˆgpt-4o-mini: $0.005/æ¬¡ï¼‰ = $0.15
# - 10%å¤æ‚ï¼ˆgpt-4o: $0.01/æ¬¡ï¼‰ = $0.10
# æ€»æˆæœ¬ = $0.31
# vs å…¨ç”¨gpt-4o = $1.00
# èŠ‚çœ69%
```

---

### 2.6 æ‰¹å¤„ç†ä¼˜åŒ–

**æ‰¹é‡å¤„ç†å‡å°‘ç½‘ç»œå¼€é”€**ï¼š

```python
import asyncio
from typing import List, Dict

async def batch_process(
    queries: List[str],
    batch_size: int = 10
) -> List[Dict]:
    """æ‰¹é‡å¤„ç†æŸ¥è¯¢ï¼ˆå‡å°‘overheadï¼‰"""

    results = []

    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]

        # å¹¶å‘å¤„ç†batchå†…çš„æŸ¥è¯¢
        tasks = [
            asyncio.create_task(agent.ainvoke({"messages": [("user", q)]}))
            for q in batch
        ]

        batch_results = await asyncio.gather(*tasks)
        results.extend(batch_results)

    return results

# ä½¿ç”¨
queries = [f"æŸ¥è¯¢{i}" for i in range(100)]
results = asyncio.run(batch_process(queries, batch_size=10))

# ä¼˜åŠ¿ï¼š
# 1. å‡å°‘ç½‘ç»œoverheadï¼ˆè¿æ¥å¤ç”¨ï¼‰
# 2. å¯ä»¥åˆ©ç”¨æ‰¹é‡å®šä»·ï¼ˆæŸäº›APIæä¾›å•†ï¼‰
# 3. æé«˜æ•´ä½“ååé‡
```

---

### 2.7 æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–æ¸…å•

**å®æ—¶æˆæœ¬è¿½è¸ªç³»ç»Ÿ**ï¼š

```python
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Dict

@dataclass
class CostMetrics:
    """æˆæœ¬æŒ‡æ ‡"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    cost_usd: float

class CostMonitor:
    """æˆæœ¬ç›‘æ§å™¨"""

    # æ¨¡å‹ä»·æ ¼ï¼ˆ$/1M tokensï¼Œ2025å¹´11æœˆï¼‰
    PRICING = {
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
    }

    def __init__(self):
        self.metrics: List[CostMetrics] = []

    def record(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int
    ):
        """è®°å½•ä¸€æ¬¡è°ƒç”¨çš„æˆæœ¬"""
        pricing = self.PRICING[model]
        cost = (
            input_tokens * pricing["input"] / 1_000_000 +
            output_tokens * pricing["output"] / 1_000_000
        )

        self.metrics.append(CostMetrics(
            timestamp=datetime.now(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost_usd=cost
        ))

    def get_total_cost(self, hours: int = 24) -> Dict:
        """è·å–æ€»æˆæœ¬ç»Ÿè®¡"""
        cutoff = datetime.now() - timedelta(hours=hours)
        recent = [m for m in self.metrics if m.timestamp > cutoff]

        return {
            "total_cost_usd": sum(m.cost_usd for m in recent),
            "total_requests": len(recent),
            "avg_cost_per_request": sum(m.cost_usd for m in recent) / len(recent) if recent else 0,
            "by_model": {
                model: {
                    "requests": len([m for m in recent if m.model == model]),
                    "cost": sum(m.cost_usd for m in recent if m.model == model)
                }
                for model in self.PRICING.keys()
            }
        }

# ä½¿ç”¨
monitor = CostMonitor()

# è®°å½•è°ƒç”¨
monitor.record("gpt-4o", input_tokens=2000, output_tokens=500)
monitor.record("gpt-4o-mini", input_tokens=1500, output_tokens=300)

# æŸ¥çœ‹æˆæœ¬
stats = monitor.get_total_cost(hours=24)
print(f"24å°æ—¶æ€»æˆæœ¬: ${stats['total_cost_usd']:.4f}")
print(f"å¹³å‡æ¯æ¬¡è¯·æ±‚: ${stats['avg_cost_per_request']:.6f}")
print(f"æŒ‰æ¨¡å‹åˆ†ç»„: {stats['by_model']}")
```

**æˆæœ¬ä¼˜åŒ–æ¸…å•**ï¼š

- [ ] **Promptä¼˜åŒ–**
  - [ ] ç²¾ç®€ç³»ç»Ÿæç¤ºè¯ï¼ˆé¿å…å†—ä½™æè¿°ï¼‰
  - [ ] ç§»é™¤æ— å…³ä¿¡æ¯
  - [ ] ä½¿ç”¨ç®€æ´è¡¨è¾¾

- [ ] **ä¸Šä¸‹æ–‡ç®¡ç†**
  - [ ] é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡ï¼ˆtrim_messagesï¼‰
  - [ ] å‹ç¼©æ—§æ¶ˆæ¯ä¸ºæ‘˜è¦
  - [ ] æ™ºèƒ½è£å‰ªä¸Šä¸‹æ–‡çª—å£

- [ ] **ç¼“å­˜ç­–ç•¥**
  - [ ] ç²¾ç¡®ç¼“å­˜ï¼ˆç›¸åŒæŸ¥è¯¢ï¼‰
  - [ ] è¯­ä¹‰ç¼“å­˜ï¼ˆç›¸ä¼¼æŸ¥è¯¢ï¼‰
  - [ ] è®¾ç½®åˆç†TTL

- [ ] **æ¨¡å‹é€‰æ‹©**
  - [ ] æ ¹æ®å¤æ‚åº¦è‡ªé€‚åº”é€‰æ‹©
  - [ ] ç®€å•ä»»åŠ¡ç”¨gpt-3.5-turbo
  - [ ] å·¥å…·è°ƒç”¨ä¼˜å…ˆç”¨miniæ¨¡å‹

- [ ] **æ‰¹å¤„ç†**
  - [ ] åˆå¹¶ç›¸ä¼¼è¯·æ±‚
  - [ ] æ‰¹é‡å¤„ç†é™ä½overhead
  - [ ] å¹¶å‘æ‰§è¡Œæé«˜æ•ˆç‡

**æˆæœ¬ç›®æ ‡**ï¼š

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | èŠ‚çœ |
|------|-------|-------|------|
| ç®€å•é—®ç­” | $0.01/æ¬¡ | $0.001/æ¬¡ | 90% |
| å¤æ‚æ¨ç† | $0.05/æ¬¡ | $0.02/æ¬¡ | 60% |
| å¤šè½®å¯¹è¯ | $0.10/æ¬¡ | $0.04/æ¬¡ | 60% |

**ç›‘æ§æŒ‡æ ‡**ï¼š
- å¹³å‡æ¯æ¬¡è¯·æ±‚æˆæœ¬
- æ¯æ—¥/æ¯æœˆæ€»æˆæœ¬
- å„æ¨¡å‹æˆæœ¬å æ¯”
- Tokenä½¿ç”¨æ•ˆç‡ï¼ˆè¾“å‡º/è¾“å…¥æ¯”ï¼‰

---
## ç¬¬4ç« ï¼š å®‰å…¨åˆè§„ä¸é˜²æŠ¤

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ç”Ÿäº§ç¯å¢ƒçš„å®‰å…¨é˜²æŠ¤å’Œåˆè§„è¦æ±‚ã€‚

### 3.1 Guardrails é˜²æŠ¤ä½“ç³»

**åŒå±‚é˜²æŠ¤æ¶æ„**ï¼š

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class Guardrail(ABC):
    """é˜²æŠ¤æ åŸºç±»"""

    @abstractmethod
    def check(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥å†…å®¹"""
        pass

class DeterministicGuardrail(Guardrail):
    """ç¡®å®šæ€§é˜²æŠ¤ï¼ˆåŸºäºè§„åˆ™ï¼‰"""

    def __init__(self):
        self.rules = self._load_rules()

    def _load_rules(self):
        """åŠ è½½è§„åˆ™"""
        return {
            "sql_injection": r"(\bSELECT\b.*\bFROM\b|\bDROP\b|\bDELETE\b.*\bFROM\b)",
            "xss": r"(<script|javascript:|onerror=|onclick=)",
            "pii_email": r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            "pii_phone": r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b",
            "pii_ssn": r"\b\d{3}-\d{2}-\d{4}\b",
            "profanity": self._load_profanity_list()
        }

    def check(self, content: str) -> Dict[str, Any]:
        """è§„åˆ™æ£€æŸ¥"""
        violations = []

        for rule_name, pattern in self.rules.items():
            if re.search(pattern, content, re.IGNORECASE):
                violations.append({
                    "rule": rule_name,
                    "severity": self._get_severity(rule_name),
                    "action": self._get_action(rule_name)
                })

        return {
            "passed": len(violations) == 0,
            "violations": violations
        }

    def _get_severity(self, rule: str) -> str:
        """è·å–ä¸¥é‡ç¨‹åº¦"""
        severity_map = {
            "sql_injection": "critical",
            "xss": "critical",
            "pii_ssn": "high",
            "pii_email": "medium",
            "profanity": "low"
        }
        return severity_map.get(rule, "medium")

    def _get_action(self, rule: str) -> str:
        """è·å–å¤„ç†åŠ¨ä½œ"""
        action_map = {
            "sql_injection": "block",
            "xss": "block",
            "pii_ssn": "redact",
            "pii_email": "mask",
            "profanity": "warn"
        }
        return action_map.get(rule, "warn")

class ModelBasedGuardrail(Guardrail):
    """åŸºäºæ¨¡å‹çš„é˜²æŠ¤"""

    def __init__(self, model: ChatOpenAI):
        self.model = model
        self.categories = [
            "harmful_content",
            "bias",
            "misinformation",
            "inappropriate",
            "off_topic"
        ]

    def check(self, content: str) -> Dict[str, Any]:
        """æ¨¡å‹æ£€æŸ¥"""
        prompt = f"""åˆ†æä»¥ä¸‹å†…å®¹æ˜¯å¦å­˜åœ¨é—®é¢˜ã€‚

å†…å®¹ï¼š{content}

æ£€æŸ¥ç±»åˆ«ï¼š
{', '.join(self.categories)}

è¿”å› JSON æ ¼å¼ï¼š
{{
    "passed": true/false,
    "violations": [
        {{"category": "...", "confidence": 0.0-1.0, "reason": "..."}}
    ]
}}"""

        response = self.model.invoke(prompt)

        # è§£æå“åº”
        try:
            result = json.loads(response.content)
            return result
        except:
            # è§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†
            return {
                "passed": False,
                "violations": [{
                    "category": "parse_error",
                    "confidence": 1.0,
                    "reason": "æ— æ³•è§£ææ£€æŸ¥ç»“æœ"
                }]
            }

class HybridGuardrailSystem:
    """æ··åˆé˜²æŠ¤ç³»ç»Ÿ"""

    def __init__(self):
        self.deterministic = DeterministicGuardrail()
        self.model_based = ModelBasedGuardrail(ChatOpenAI(model="gpt-4o"))
        self.cache = {}

    def check(self, content: str, use_cache: bool = True) -> Dict[str, Any]:
        """ç»¼åˆæ£€æŸ¥"""
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cache_key = hashlib.md5(content.encode()).hexdigest()
            if cache_key in self.cache:
                return self.cache[cache_key]

        # ç¬¬ä¸€å±‚ï¼šç¡®å®šæ€§æ£€æŸ¥ï¼ˆå¿«é€Ÿï¼‰
        deterministic_result = self.deterministic.check(content)

        # å¦‚æœç¡®å®šæ€§æ£€æŸ¥å‘ç°ä¸¥é‡é—®é¢˜ï¼Œç›´æ¥è¿”å›
        if not deterministic_result["passed"]:
            critical_violations = [
                v for v in deterministic_result["violations"]
                if v["severity"] == "critical"
            ]

            if critical_violations:
                result = {
                    "passed": False,
                    "stage": "deterministic",
                    "violations": critical_violations,
                    "action": "block"
                }

                if use_cache:
                    self.cache[cache_key] = result

                return result

        # ç¬¬äºŒå±‚ï¼šæ¨¡å‹æ£€æŸ¥ï¼ˆæ›´å…¨é¢ä½†è¾ƒæ…¢ï¼‰
        model_result = self.model_based.check(content)

        # åˆå¹¶ç»“æœ
        all_violations = (
            deterministic_result.get("violations", []) +
            model_result.get("violations", [])
        )

        result = {
            "passed": len(all_violations) == 0,
            "stage": "hybrid",
            "violations": all_violations,
            "action": self._determine_action(all_violations)
        }

        if use_cache:
            self.cache[cache_key] = result

        return result

    def _determine_action(self, violations: List[dict]) -> str:
        """ç¡®å®šå¤„ç†åŠ¨ä½œ"""
        if not violations:
            return "pass"

        # æ ¹æ®æœ€ä¸¥é‡çš„è¿è§„ç¡®å®šåŠ¨ä½œ
        severities = [v.get("severity", "medium") for v in violations]

        if "critical" in severities:
            return "block"
        elif "high" in severities:
            return "review"
        else:
            return "warn"
```

### 3.2 PII æ£€æµ‹ç­–ç•¥

**PII æ£€æµ‹ä¸å¤„ç†**ï¼š

```python
import re
from enum import Enum
from typing import List, Tuple

class PIIType(Enum):
    """PII ç±»å‹"""
    EMAIL = "email"
    PHONE = "phone"
    SSN = "ssn"
    CREDIT_CARD = "credit_card"
    IP_ADDRESS = "ip"
    NAME = "name"
    ADDRESS = "address"

class PIIStrategy(Enum):
    """PII å¤„ç†ç­–ç•¥"""
    REDACT = "redact"      # å®Œå…¨åˆ é™¤
    MASK = "mask"          # éƒ¨åˆ†é®è”½
    HASH = "hash"          # å“ˆå¸Œæ›¿æ¢
    ENCRYPT = "encrypt"    # åŠ å¯†
    TOKENIZE = "tokenize"  # ä»¤ç‰ŒåŒ–

class PIIDetector:
    """PII æ£€æµ‹å™¨"""

    def __init__(self):
        self.patterns = {
            PIIType.EMAIL: r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            PIIType.PHONE: r'(\+\d{1,3}[-.\s]?)?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',
            PIIType.SSN: r'\b\d{3}-\d{2}-\d{4}\b',
            PIIType.CREDIT_CARD: r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',
            PIIType.IP_ADDRESS: r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
        }

        # åç§°æ£€æµ‹éœ€è¦ NER æ¨¡å‹
        self.ner_model = self._load_ner_model()

    def detect(self, text: str) -> List[Tuple[PIIType, str, int, int]]:
        """æ£€æµ‹ PII"""
        detections = []

        # æ­£åˆ™æ£€æµ‹
        for pii_type, pattern in self.patterns.items():
            for match in re.finditer(pattern, text):
                detections.append((
                    pii_type,
                    match.group(),
                    match.start(),
                    match.end()
                ))

        # NER æ£€æµ‹ï¼ˆåç§°ã€åœ°å€ï¼‰
        entities = self.ner_model.extract_entities(text)
        for entity in entities:
            if entity["type"] == "PERSON":
                detections.append((
                    PIIType.NAME,
                    entity["text"],
                    entity["start"],
                    entity["end"]
                ))
            elif entity["type"] == "LOCATION":
                detections.append((
                    PIIType.ADDRESS,
                    entity["text"],
                    entity["start"],
                    entity["end"]
                ))

        return detections

    def process(self, text: str, strategy: PIIStrategy) -> str:
        """å¤„ç† PII"""
        detections = self.detect(text)

        # æŒ‰ä½ç½®å€’åºå¤„ç†ï¼ˆé¿å…ç´¢å¼•åç§»ï¼‰
        detections.sort(key=lambda x: x[2], reverse=True)

        processed_text = text

        for pii_type, value, start, end in detections:
            replacement = self._get_replacement(pii_type, value, strategy)
            processed_text = processed_text[:start] + replacement + processed_text[end:]

        return processed_text

    def _get_replacement(self, pii_type: PIIType, value: str, strategy: PIIStrategy) -> str:
        """è·å–æ›¿æ¢æ–‡æœ¬"""
        if strategy == PIIStrategy.REDACT:
            return f"[{pii_type.value.upper()}_REDACTED]"

        elif strategy == PIIStrategy.MASK:
            if pii_type == PIIType.EMAIL:
                # ä¿ç•™åŸŸå
                parts = value.split('@')
                if len(parts) == 2:
                    masked_local = parts[0][0] + '*' * (len(parts[0]) - 2) + parts[0][-1]
                    return f"{masked_local}@{parts[1]}"

            elif pii_type == PIIType.PHONE:
                # ä¿ç•™å‰3ä½å’Œå2ä½
                if len(value) >= 5:
                    return value[:3] + '*' * (len(value) - 5) + value[-2:]

            # é»˜è®¤é®è”½
            return '*' * len(value)

        elif strategy == PIIStrategy.HASH:
            return f"[HASH:{hashlib.sha256(value.encode()).hexdigest()[:8]}]"

        elif strategy == PIIStrategy.ENCRYPT:
            # å®é™…åŠ å¯†å®ç°
            encrypted = self._encrypt(value)
            return f"[ENC:{encrypted}]"

        elif strategy == PIIStrategy.TOKENIZE:
            # ç”Ÿæˆå”¯ä¸€ä»¤ç‰Œ
            token = self._generate_token(pii_type, value)
            return f"[TOKEN:{token}]"

        return value
```

### 3.3 åˆ†å±‚é˜²æŠ¤ç»„åˆ

**å¤šå±‚é˜²æŠ¤æ¶æ„**ï¼š

```python
class LayeredDefenseSystem:
    """åˆ†å±‚é˜²æŠ¤ç³»ç»Ÿ"""

    def __init__(self):
        self.layers = [
            InputValidationLayer(),
            RateLimitLayer(),
            AuthenticationLayer(),
            PIIProtectionLayer(),
            ContentModerationLayer(),
            OutputSanitizationLayer(),
            AuditLoggingLayer()
        ]

    async def process_request(self, request: dict) -> dict:
        """å¤„ç†è¯·æ±‚ï¼ˆé€šè¿‡æ‰€æœ‰é˜²æŠ¤å±‚ï¼‰"""
        context = {
            "request": request,
            "user": request.get("user"),
            "timestamp": datetime.now(),
            "trace_id": str(uuid.uuid4())
        }

        # é€å±‚å¤„ç†
        for layer in self.layers:
            try:
                result = await layer.process(context)

                if not result["passed"]:
                    # æŸå±‚æœªé€šè¿‡ï¼Œè®°å½•å¹¶è¿”å›
                    self._log_rejection(layer, context, result)

                    return {
                        "status": "rejected",
                        "layer": layer.__class__.__name__,
                        "reason": result.get("reason"),
                        "trace_id": context["trace_id"]
                    }

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context.update(result.get("context_updates", {}))

            except Exception as e:
                # å±‚å¤„ç†å¼‚å¸¸
                self._log_error(layer, context, e)

                # æ ¹æ®å±‚çš„é‡è¦æ€§å†³å®šæ˜¯å¦ç»§ç»­
                if layer.is_critical():
                    return {
                        "status": "error",
                        "layer": layer.__class__.__name__,
                        "error": str(e),
                        "trace_id": context["trace_id"]
                    }

        # æ‰€æœ‰å±‚éƒ½é€šè¿‡
        return {
            "status": "approved",
            "context": context,
            "trace_id": context["trace_id"]
        }

class DefenseLayer(ABC):
    """é˜²æŠ¤å±‚åŸºç±»"""

    @abstractmethod
    async def process(self, context: dict) -> dict:
        """å¤„ç†è¯·æ±‚"""
        pass

    def is_critical(self) -> bool:
        """æ˜¯å¦å…³é”®å±‚ï¼ˆå¤±è´¥æ—¶å¿…é¡»åœæ­¢ï¼‰"""
        return False

class InputValidationLayer(DefenseLayer):
    """è¾“å…¥éªŒè¯å±‚"""

    async def process(self, context: dict) -> dict:
        request = context["request"]

        # éªŒè¯å¿…å¡«å­—æ®µ
        required_fields = ["user_id", "content", "timestamp"]
        for field in required_fields:
            if field not in request:
                return {
                    "passed": False,
                    "reason": f"Missing required field: {field}"
                }

        # éªŒè¯è¾“å…¥é•¿åº¦
        if len(request.get("content", "")) > 10000:
            return {
                "passed": False,
                "reason": "Content too long (max 10000 chars)"
            }

        # éªŒè¯è¾“å…¥æ ¼å¼
        if not self._validate_format(request):
            return {
                "passed": False,
                "reason": "Invalid input format"
            }

        return {"passed": True}

    def is_critical(self) -> bool:
        return True

class RateLimitLayer(DefenseLayer):
    """é€Ÿç‡é™åˆ¶å±‚"""

    def __init__(self):
        self.limits = {}  # user_id -> requests

    async def process(self, context: dict) -> dict:
        user_id = context["user"]["id"]

        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        current_count = self.limits.get(user_id, 0)

        if current_count >= 100:  # æ¯åˆ†é’Ÿ 100 ä¸ªè¯·æ±‚
            return {
                "passed": False,
                "reason": "Rate limit exceeded"
            }

        # æ›´æ–°è®¡æ•°
        self.limits[user_id] = current_count + 1

        # å¼‚æ­¥é‡ç½®è®¡æ•°å™¨
        asyncio.create_task(self._reset_counter(user_id))

        return {"passed": True}

    async def _reset_counter(self, user_id: str):
        """60ç§’åé‡ç½®è®¡æ•°å™¨"""
        await asyncio.sleep(60)
        self.limits[user_id] = 0
```

### 3.4 è‡ªå®šä¹‰ Guardrails å¼€å‘

**è‡ªå®šä¹‰é˜²æŠ¤æ æ¡†æ¶**ï¼š

```python
from typing import Callable, List, Optional
import inspect

class GuardrailFramework:
    """é˜²æŠ¤æ æ¡†æ¶"""

    def __init__(self):
        self.pre_processors = []
        self.validators = []
        self.post_processors = []

    def pre_process(self, func: Callable) -> Callable:
        """æ³¨å†Œé¢„å¤„ç†å™¨"""
        self.pre_processors.append(func)
        return func

    def validate(self, func: Callable) -> Callable:
        """æ³¨å†ŒéªŒè¯å™¨"""
        self.validators.append(func)
        return func

    def post_process(self, func: Callable) -> Callable:
        """æ³¨å†Œåå¤„ç†å™¨"""
        self.post_processors.append(func)
        return func

    async def execute(self, input_data: Any) -> Any:
        """æ‰§è¡Œé˜²æŠ¤æµç¨‹"""
        # é¢„å¤„ç†
        processed_data = input_data
        for processor in self.pre_processors:
            if inspect.iscoroutinefunction(processor):
                processed_data = await processor(processed_data)
            else:
                processed_data = processor(processed_data)

        # éªŒè¯
        for validator in self.validators:
            if inspect.iscoroutinefunction(validator):
                is_valid = await validator(processed_data)
            else:
                is_valid = validator(processed_data)

            if not is_valid:
                raise ValidationError(f"Validation failed: {validator.__name__}")

        # åå¤„ç†
        for processor in self.post_processors:
            if inspect.iscoroutinefunction(processor):
                processed_data = await processor(processed_data)
            else:
                processed_data = processor(processed_data)

        return processed_data

# ä½¿ç”¨ç¤ºä¾‹
guardrail = GuardrailFramework()

@guardrail.pre_process
def sanitize_input(data: dict) -> dict:
    """æ¸…ç†è¾“å…¥"""
    data["content"] = data["content"].strip()
    return data

@guardrail.validate
def check_content_length(data: dict) -> bool:
    """æ£€æŸ¥å†…å®¹é•¿åº¦"""
    return len(data["content"]) <= 1000

@guardrail.validate
async def check_toxicity(data: dict) -> bool:
    """æ£€æŸ¥æ¯’æ€§ï¼ˆä½¿ç”¨å¤–éƒ¨ APIï¼‰"""
    # è°ƒç”¨æ¯’æ€§æ£€æµ‹ API
    result = await toxicity_api.check(data["content"])
    return result["toxicity_score"] < 0.7

@guardrail.post_process
def add_metadata(data: dict) -> dict:
    """æ·»åŠ å…ƒæ•°æ®"""
    data["processed_at"] = datetime.now()
    data["guardrail_version"] = "1.0"
    return data

# æ‰§è¡Œ
result = await guardrail.execute({"content": "ç”¨æˆ·è¾“å…¥"})
```

---

### 3.5 è¾“å…¥è¾“å‡ºå®‰å…¨

**å®‰å…¨å¤„ç†æ¡†æ¶**ï¼š

```python
import html
import re
from typing import Optional

class SecurityFramework:
    """å®‰å…¨æ¡†æ¶"""

    def __init__(self):
        self.input_sanitizer = InputSanitizer()
        self.output_validator = OutputValidator()
        self.encryption = EncryptionService()

    def secure_input(self, raw_input: str) -> str:
        """å®‰å…¨åŒ–è¾“å…¥"""
        # 1. HTML å®ä½“ç¼–ç 
        sanitized = html.escape(raw_input)

        # 2. SQL æ³¨å…¥é˜²æŠ¤
        sanitized = self._prevent_sql_injection(sanitized)

        # 3. å‘½ä»¤æ³¨å…¥é˜²æŠ¤
        sanitized = self._prevent_command_injection(sanitized)

        # 4. è·¯å¾„éå†é˜²æŠ¤
        sanitized = self._prevent_path_traversal(sanitized)

        return sanitized

    def _prevent_sql_injection(self, text: str) -> str:
        """é˜²æ­¢ SQL æ³¨å…¥"""
        # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹
        dangerous_patterns = [
            r"('\s*OR\s*')",
            r"(;\s*DROP\s+TABLE)",
            r"(UNION\s+SELECT)",
            r"(INSERT\s+INTO)",
            r"(UPDATE\s+.*\s+SET)"
        ]

        for pattern in dangerous_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                # è®°å½•å¹¶æ¸…ç†
                self._log_security_event("sql_injection_attempt", text)
                text = re.sub(pattern, "", text, flags=re.IGNORECASE)

        return text

    def secure_output(self, output: str, context: dict) -> str:
        """å®‰å…¨åŒ–è¾“å‡º"""
        # 1. æ•æ„Ÿä¿¡æ¯è„±æ•
        output = self._redact_sensitive_info(output)

        # 2. XSS é˜²æŠ¤
        output = self._prevent_xss(output)

        # 3. ä¿¡æ¯æ³„éœ²é˜²æŠ¤
        output = self._prevent_info_disclosure(output)

        return output

    def _redact_sensitive_info(self, text: str) -> str:
        """è„±æ•æ•æ„Ÿä¿¡æ¯"""
        # API å¯†é’¥
        text = re.sub(r'(api[_-]?key[\s:=]+)[\w-]+', r'\1[REDACTED]', text, flags=re.IGNORECASE)

        # å¯†ç 
        text = re.sub(r'(password[\s:=]+)\S+', r'\1[REDACTED]', text, flags=re.IGNORECASE)

        # Token
        text = re.sub(r'(token[\s:=]+)[\w-]+', r'\1[REDACTED]', text, flags=re.IGNORECASE)

        return text
```

### 3.6 æ•°æ®åˆè§„

**GDPR åˆè§„å®ç°**ï¼š

```python
class GDPRCompliance:
    """GDPR åˆè§„ç®¡ç†"""

    def __init__(self):
        self.consent_manager = ConsentManager()
        self.data_processor = DataProcessor()
        self.audit_logger = AuditLogger()

    def process_personal_data(self, data: dict, user_id: str) -> dict:
        """å¤„ç†ä¸ªäººæ•°æ®"""
        # 1. æ£€æŸ¥åŒæ„
        if not self.consent_manager.has_consent(user_id, "data_processing"):
            raise PermissionError("No consent for data processing")

        # 2. æ•°æ®æœ€å°åŒ–
        minimal_data = self._minimize_data(data)

        # 3. å‡ååŒ–
        pseudonymized = self._pseudonymize(minimal_data, user_id)

        # 4. è®°å½•å¤„ç†æ´»åŠ¨
        self.audit_logger.log_processing_activity(
            user_id=user_id,
            purpose="service_provision",
            legal_basis="consent",
            data_categories=self._get_data_categories(minimal_data)
        )

        return pseudonymized

    def _minimize_data(self, data: dict) -> dict:
        """æ•°æ®æœ€å°åŒ–"""
        # åªä¿ç•™å¿…è¦å­—æ®µ
        required_fields = ["name", "email", "query"]

        minimal = {}
        for field in required_fields:
            if field in data:
                minimal[field] = data[field]

        return minimal

    def _pseudonymize(self, data: dict, user_id: str) -> dict:
        """å‡ååŒ–å¤„ç†"""
        # ç”Ÿæˆå‡å
        pseudo_id = hashlib.sha256(f"{user_id}{SECRET_SALT}".encode()).hexdigest()[:16]

        # æ›¿æ¢æ ‡è¯†ç¬¦
        data["user_id"] = pseudo_id

        # åŠ å¯†æ•æ„Ÿå­—æ®µ
        if "email" in data:
            data["email_encrypted"] = self.encryption.encrypt(data["email"])
            del data["email"]

        return data

    def handle_data_request(self, request_type: str, user_id: str) -> dict:
        """å¤„ç†æ•°æ®è¯·æ±‚ï¼ˆGDPR æƒåˆ©ï¼‰"""
        if request_type == "access":
            # æ•°æ®è®¿é—®æƒ
            return self._export_user_data(user_id)

        elif request_type == "portability":
            # æ•°æ®å¯æºå¸¦æƒ
            return self._export_portable_data(user_id)

        elif request_type == "erasure":
            # è¢«é—å¿˜æƒ
            return self._erase_user_data(user_id)

        elif request_type == "rectification":
            # æ•°æ®æ›´æ­£æƒ
            return self._rectify_user_data(user_id)

        else:
            raise ValueError(f"Unknown request type: {request_type}")
```

### 3.7 å®¡è®¡æ—¥å¿—

**å®Œæ•´å®¡è®¡ç³»ç»Ÿ**ï¼š

```python
class AuditSystem:
    """å®¡è®¡ç³»ç»Ÿ"""

    def __init__(self):
        self.storage = AuditStorage()
        self.analyzer = AuditAnalyzer()

    def log_event(
        self,
        event_type: str,
        user_id: str,
        action: str,
        resource: str,
        result: str,
        metadata: Optional[dict] = None
    ):
        """è®°å½•å®¡è®¡äº‹ä»¶"""
        event = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": event_type,
            "user_id": user_id,
            "action": action,
            "resource": resource,
            "result": result,
            "metadata": metadata or {},
            "ip_address": self._get_client_ip(),
            "user_agent": self._get_user_agent(),
            "session_id": self._get_session_id()
        }

        # å­˜å‚¨
        self.storage.store(event)

        # å®æ—¶åˆ†æ
        self.analyzer.analyze(event)

        return event["id"]

    def query_logs(
        self,
        filters: dict,
        start_time: datetime,
        end_time: datetime,
        limit: int = 100
    ) -> List[dict]:
        """æŸ¥è¯¢å®¡è®¡æ—¥å¿—"""
        return self.storage.query(filters, start_time, end_time, limit)

    def generate_compliance_report(self, period: str) -> dict:
        """ç”Ÿæˆåˆè§„æŠ¥å‘Š"""
        report = {
            "period": period,
            "generated_at": datetime.utcnow().isoformat(),
            "statistics": self._calculate_statistics(period),
            "anomalies": self._detect_anomalies(period),
            "compliance_status": self._check_compliance(period)
        }

        return report

class AuditStorage:
    """å®¡è®¡å­˜å‚¨"""

    def __init__(self):
        # ä½¿ç”¨ä¸å¯å˜å­˜å‚¨
        self.immutable_store = ImmutableLogStore()

    def store(self, event: dict):
        """å­˜å‚¨å®¡è®¡äº‹ä»¶"""
        # æ·»åŠ å®Œæ•´æ€§æ ¡éªŒ
        event["hash"] = self._calculate_hash(event)

        # ç­¾å
        event["signature"] = self._sign_event(event)

        # å­˜å‚¨åˆ°ä¸å¯å˜å­˜å‚¨
        self.immutable_store.append(event)

        # å¼‚æ­¥å¤åˆ¶åˆ°é•¿æœŸå­˜å‚¨
        asyncio.create_task(self._replicate_to_cold_storage(event))

    def _calculate_hash(self, event: dict) -> str:
        """è®¡ç®—äº‹ä»¶å“ˆå¸Œ"""
        # ç¡®ä¿é¡ºåºä¸€è‡´
        canonical = json.dumps(event, sort_keys=True)
        return hashlib.sha256(canonical.encode()).hexdigest()

    def _sign_event(self, event: dict) -> str:
        """æ•°å­—ç­¾å"""
        # ä½¿ç”¨ç§é’¥ç­¾å
        from cryptography.hazmat.primitives import hashes
        from cryptography.hazmat.primitives.asymmetric import padding

        message = json.dumps(event, sort_keys=True).encode()
        signature = PRIVATE_KEY.sign(
            message,
            padding.PSS(
                mgf=padding.MGF1(hashes.SHA256()),
                salt_length=padding.PSS.MAX_LENGTH
            ),
            hashes.SHA256()
        )

        return base64.b64encode(signature).decode()
```

---

## ç¬¬5ç« ï¼š éƒ¨ç½²ä¸è¿ç»´

> **å…³æ³¨ç‚¹**ï¼šæŒæ¡ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²ç­–ç•¥å’Œè¿ç»´å®è·µã€‚

### 4.1 éƒ¨ç½²ç­–ç•¥

**å®¹å™¨åŒ–éƒ¨ç½²**ï¼š

**ç”Ÿäº§çº§Dockerfile**ï¼š

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# å®‰å…¨ï¼šé root ç”¨æˆ·
RUN useradd -m -u 1000 langchain && \
    mkdir -p /app && \
    chown -R langchain:langchain /app

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# ä¾èµ–ï¼ˆåˆ†å±‚ç¼“å­˜ä¼˜åŒ–ï¼‰
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# åº”ç”¨ä»£ç 
COPY --chown=langchain:langchain . .

# åˆ‡æ¢ç”¨æˆ·
USER langchain

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --retries=3 \
  CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=2)"

# å¯åŠ¨ï¼ˆä½¿ç”¨ç”Ÿäº§çº§é…ç½®ï¼‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

**å®Œæ•´docker-compose.yml**ï¼š

```yaml
# docker-compose.yml
version: '3.8'

services:
  langchain-app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡åŠ è½½
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
    depends_on:
      - redis
      - prometheus
    networks:
      - langchain-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - langchain-network
    command: redis-server --appendonly yes

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus_alerts.yml:/etc/prometheus/prometheus_alerts.yml
      - prometheus-data:/prometheus
    restart: unless-stopped
    networks:
      - langchain-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped
    networks:
      - langchain-network
    depends_on:
      - prometheus

volumes:
  redis-data:
  prometheus-data:
  grafana-data:

networks:
  langchain-network:
    driver: bridge
```

**requirements.txtç¤ºä¾‹**ï¼š

```txt
# requirements.txt
langchain>=1.0.7
langchain-openai>=1.0.3
langchain-core>=1.0.7
langchain-community>=1.0.7
langgraph>=1.0.3
langsmith>=0.4.43

# Webæ¡†æ¶
fastapi==0.115.6
uvicorn[standard]==0.34.0

# ç›‘æ§
prometheus-client==0.21.0

# ç¼“å­˜
redis==5.2.1

# å·¥å…·
python-dotenv==1.0.1
pydantic==2.10.4
pydantic-settings==2.7.0
```

**.envç¤ºä¾‹**ï¼š

```bash
# .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
OPENAI_API_KEY=your_openai_key
GRAFANA_PASSWORD=your_grafana_password
```

**.dockerignore**ï¼š

```
# .dockerignore
__pycache__/
*.py[cod]
*$py.class
*.so
.env
.venv/
venv/
*.log
.git/
.gitignore
.pytest_cache/
.coverage
htmlcov/
dist/
build/
*.egg-info/
.DS_Store
```

**éƒ¨ç½²å‘½ä»¤**ï¼š

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f langchain-app

# åœæ­¢æœåŠ¡
docker-compose down

# å®Œå…¨æ¸…ç†ï¼ˆåŒ…æ‹¬æ•°æ®å·ï¼‰
docker-compose down -v
```

**Kubernetes éƒ¨ç½²é…ç½®**ï¼š

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langchain-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langchain
  template:
    metadata:
      labels:
        app: langchain
    spec:
      containers:
      - name: app
        image: langchain-app:latest
        ports:
        - containerPort: 8080
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: langchain-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: langchain-service
spec:
  selector:
    app: langchain
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langchain-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langchain-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Serverless éƒ¨ç½²ï¼ˆAWS Lambdaï¼‰**ï¼š

```python
# handler.py
import json
from mangum import Mangum
from fastapi import FastAPI
from langchain.agents import create_agent

app = FastAPI()

# åˆå§‹åŒ–ï¼ˆå†·å¯åŠ¨ä¼˜åŒ–ï¼‰
agent = None

def get_agent():
    global agent
    if agent is None:
        agent = create_agent(
            model=ChatOpenAI(model="gpt-4o-mini"),
            tools=[],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
        )
    return agent

@app.post("/chat")
async def chat(request: dict):
    agent = get_agent()
    response = agent.invoke(request)
    return response

# Lambda handler
handler = Mangum(app)
```

```yaml
# serverless.yml
service: langchain-service

provider:
  name: aws
  runtime: python3.11
  region: us-east-1
  timeout: 30
  memorySize: 1024
  environment:
    OPENAI_API_KEY: ${ssm:/langchain/openai_api_key}

functions:
  chat:
    handler: handler.handler
    events:
      - http:
          path: /chat
          method: post
          cors: true
    reservedConcurrency: 10
    provisionedConcurrency: 2  # å‡å°‘å†·å¯åŠ¨

plugins:
  - serverless-python-requirements

custom:
  pythonRequirements:
    dockerizePip: true
    slim: true
    strip: false
```

### 4.2 ç›‘æ§å‘Šè­¦

#### 4.2.1 Prometheusç›‘æ§é›†æˆ

**1. å®‰è£…ä¾èµ–**

```bash
pip install prometheus-client
```

**2. å®Œæ•´ç¤ºä¾‹**

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from typing import Dict, Any
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# å®šä¹‰PrometheusæŒ‡æ ‡
llm_requests = Counter(
    'langchain_llm_requests_total',
    'Total LLM requests',
    ['model', 'status']
)

llm_latency = Histogram(
    'langchain_llm_latency_seconds',
    'LLM request latency',
    ['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

llm_tokens = Counter(
    'langchain_llm_tokens_total',
    'Total tokens used',
    ['model', 'type']  # type: prompt/completion
)

llm_errors = Counter(
    'langchain_llm_errors_total',
    'Total LLM errors',
    ['model', 'error_type']
)

# é›†æˆåˆ°LangChainå›è°ƒ
class PrometheusCallback(BaseCallbackHandler):
    """Prometheusç›‘æ§å›è°ƒ"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLMå¼€å§‹æ—¶è®°å½•"""
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸæ—¶è®°å½•æŒ‡æ ‡"""
        duration = time.time() - self.start_time

        # è·å–æ¨¡å‹ä¿¡æ¯
        model = response.llm_output.get("model_name", "unknown") if response.llm_output else "unknown"

        # è®°å½•æˆåŠŸæŒ‡æ ‡
        llm_requests.labels(model=model, status="success").inc()
        llm_latency.labels(model=model).observe(duration)

        # è®°å½•tokenä½¿ç”¨
        if response.llm_output:
            token_usage = response.llm_output.get("token_usage", {})
            llm_tokens.labels(model=model, type="prompt").inc(
                token_usage.get("prompt_tokens", 0)
            )
            llm_tokens.labels(model=model, type="completion").inc(
                token_usage.get("completion_tokens", 0)
            )

    def on_llm_error(self, error, **kwargs):
        """LLMé”™è¯¯æ—¶è®°å½•"""
        error_type = type(error).__name__
        llm_requests.labels(model="unknown", status="error").inc()
        llm_errors.labels(model="unknown", error_type=error_type).inc()

# å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨ï¼ˆæš´éœ²æŒ‡æ ‡ï¼‰
start_http_server(8001)  # åœ¨ç‹¬ç«‹ç«¯å£æš´éœ²æŒ‡æ ‡(é¿å…ä¸åº”ç”¨ç«¯å£8000å†²çª)

# ä½¿ç”¨ç¤ºä¾‹
chain = ChatOpenAI(model="gpt-4o-mini") | StrOutputParser()
result = chain.invoke(
    "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹LangChain",
    config={"callbacks": [PrometheusCallback()]}
)

print(f"è®¿é—® http://localhost:8001/metrics æŸ¥çœ‹æŒ‡æ ‡")
```

**3. Grafanaå¯è§†åŒ–é…ç½®**

```json
{
  "dashboard": {
    "title": "LangChainç›‘æ§é¢æ¿",
    "panels": [
      {
        "title": "LLMè¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰",
        "targets": [{
          "expr": "rate(langchain_llm_requests_total[5m])",
          "legendFormat": "{{model}} - {{status}}"
        }],
        "type": "graph"
      },
      {
        "title": "LLMå»¶è¿Ÿåˆ†å¸ƒï¼ˆP50/P95/P99ï¼‰",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(langchain_llm_latency_seconds_bucket[5m]))",
            "legendFormat": "P50 - {{model}}"
          },
          {
            "expr": "histogram_quantile(0.95, rate(langchain_llm_latency_seconds_bucket[5m]))",
            "legendFormat": "P95 - {{model}}"
          },
          {
            "expr": "histogram_quantile(0.99, rate(langchain_llm_latency_seconds_bucket[5m]))",
            "legendFormat": "P99 - {{model}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Tokenä½¿ç”¨é‡ï¼ˆæ¯åˆ†é’Ÿï¼‰",
        "targets": [{
          "expr": "rate(langchain_llm_tokens_total[1m])",
          "legendFormat": "{{model}} - {{type}}"
        }],
        "type": "graph"
      },
      {
        "title": "é”™è¯¯ç‡",
        "targets": [{
          "expr": "rate(langchain_llm_errors_total[5m])",
          "legendFormat": "{{model}} - {{error_type}}"
        }],
        "type": "graph"
      }
    ]
  }
}
```

**4. Prometheuså‘Šè­¦è§„åˆ™**

```yaml
# prometheus_alerts.yml
groups:
  - name: langchain_alerts
    rules:
      - alert: HighLLMLatency
        expr: histogram_quantile(0.95, rate(langchain_llm_latency_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLMå»¶è¿Ÿè¿‡é«˜"
          description: "P95å»¶è¿Ÿè¶…è¿‡5ç§’: {{ $value }}s"

      - alert: CriticalLLMLatency
        expr: histogram_quantile(0.95, rate(langchain_llm_latency_seconds_bucket[5m])) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "LLMå»¶è¿Ÿä¸¥é‡è¿‡é«˜"
          description: "P95å»¶è¿Ÿè¶…è¿‡10ç§’: {{ $value }}s"

      - alert: HighErrorRate
        expr: rate(langchain_llm_requests_total{status="error"}[5m]) / rate(langchain_llm_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "LLMé”™è¯¯ç‡è¿‡é«˜"
          description: "é”™è¯¯ç‡è¶…è¿‡10%: {{ $value | humanizePercentage }}"

      - alert: TokenBudgetExceeded
        expr: increase(langchain_llm_tokens_total[1h]) > 1000000
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Tokené¢„ç®—è¶…é™"
          description: "è¿‡å»1å°æ—¶ä½¿ç”¨äº†{{ $value }}ä¸ªtokens"

      - alert: NoRecentRequests
        expr: rate(langchain_llm_requests_total[10m]) == 0
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "æ— LLMè¯·æ±‚"
          description: "è¿‡å»15åˆ†é’Ÿæ— LLMè¯·æ±‚ï¼Œå¯èƒ½æœåŠ¡å¼‚å¸¸"
```

**5. Prometheusé…ç½®æ–‡ä»¶**

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - "prometheus_alerts.yml"

scrape_configs:
  - job_name: 'langchain_app'
    static_configs:
      - targets: ['localhost:8001']  # LangChainåº”ç”¨çš„metricsç«¯å£
```

#### 4.2.2 ç›‘æ§ç³»ç»Ÿå®ç°

**ç›‘æ§ä¸­é—´ä»¶**ï¼š

```python
from prometheus_client import Counter, Histogram, Gauge
import logging

# Prometheus æŒ‡æ ‡
request_count = Counter('langchain_requests_total', 'Total requests', ['endpoint', 'status'])
request_duration = Histogram('langchain_request_duration_seconds', 'Request duration', ['endpoint'])
active_requests = Gauge('langchain_active_requests', 'Active requests')
model_tokens = Counter('langchain_model_tokens_total', 'Total tokens used', ['model'])
error_count = Counter('langchain_errors_total', 'Total errors', ['error_type'])

class MonitoringMiddleware:
    """ç›‘æ§ä¸­é—´ä»¶"""

    async def __call__(self, request, call_next):
        endpoint = request.url.path

        # è®°å½•æ´»è·ƒè¯·æ±‚
        active_requests.inc()

        # è®°å½•è¯·æ±‚æ—¶é—´
        with request_duration.labels(endpoint=endpoint).time():
            try:
                response = await call_next(request)

                # è®°å½•è¯·æ±‚æ•°
                request_count.labels(
                    endpoint=endpoint,
                    status=response.status_code
                ).inc()

                return response

            except Exception as e:
                # è®°å½•é”™è¯¯
                error_count.labels(error_type=type(e).__name__).inc()
                raise

            finally:
                active_requests.dec()

# å‘Šè­¦è§„åˆ™ï¼ˆPrometheus AlertManagerï¼‰
ALERT_RULES = """
groups:
- name: langchain_alerts
  interval: 30s
  rules:

  - alert: HighErrorRate
    expr: rate(langchain_errors_total[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: High error rate detected
      description: "Error rate is {{ $value }} errors per second"

  - alert: HighLatency
    expr: histogram_quantile(0.95, langchain_request_duration_seconds) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: High latency detected
      description: "95th percentile latency is {{ $value }} seconds"

  - alert: TokenBudgetExceeded
    expr: increase(langchain_model_tokens_total[1h]) > 1000000
    labels:
      severity: warning
    annotations:
      summary: Token budget exceeded
      description: "Used {{ $value }} tokens in the last hour"
"""
```

### 4.3 æ•…éšœå¤„ç†

**æ•…éšœæ¢å¤ç³»ç»Ÿ**ï¼š

```python
class FaultToleranceSystem:
    """æ•…éšœå®¹é”™ç³»ç»Ÿ"""

    def __init__(self):
        self.health_checker = HealthChecker()
        self.circuit_breaker = CircuitBreaker()
        self.fallback_handler = FallbackHandler()

    async def handle_request(self, request: dict) -> dict:
        """å¤„ç†è¯·æ±‚ï¼ˆå¸¦æ•…éšœå®¹é”™ï¼‰"""

        # å¥åº·æ£€æŸ¥
        if not await self.health_checker.is_healthy():
            return await self.fallback_handler.handle(request)

        # ç†”æ–­å™¨
        if self.circuit_breaker.is_open():
            return await self.fallback_handler.handle(request)

        try:
            # æ­£å¸¸å¤„ç†
            response = await self.process_request(request)
            self.circuit_breaker.record_success()
            return response

        except Exception as e:
            self.circuit_breaker.record_failure()

            # æ•…éšœåˆ†ç±»å¤„ç†
            if isinstance(e, RateLimitError):
                # é€Ÿç‡é™åˆ¶ï¼šç­‰å¾…åé‡è¯•
                await asyncio.sleep(e.retry_after)
                return await self.handle_request(request)

            elif isinstance(e, ModelError):
                # æ¨¡å‹é”™è¯¯ï¼šåˆ‡æ¢å¤‡ç”¨æ¨¡å‹
                return await self.fallback_to_backup_model(request)

            elif isinstance(e, TimeoutError):
                # è¶…æ—¶ï¼šè¿”å›ç¼“å­˜æˆ–é»˜è®¤å“åº”
                cached = await self.get_cached_response(request)
                if cached:
                    return cached

                return self.get_default_response(request)

            else:
                # æœªçŸ¥é”™è¯¯ï¼šè®°å½•å¹¶è¿”å›é”™è¯¯å“åº”
                await self.log_error(e)
                return {
                    "status": "error",
                    "message": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
                }

class DisasterRecovery:
    """ç¾éš¾æ¢å¤"""

    def __init__(self):
        self.backup_regions = ["us-west-2", "eu-west-1", "ap-northeast-1"]
        self.primary_region = "us-east-1"

    async def check_region_health(self, region: str) -> bool:
        """æ£€æŸ¥åŒºåŸŸå¥åº·çŠ¶æ€"""
        try:
            response = await self.ping_region(region)
            return response.status_code == 200
        except:
            return False

    async def failover(self):
        """æ•…éšœè½¬ç§»"""
        # æ£€æŸ¥ä¸»åŒºåŸŸ
        if await self.check_region_health(self.primary_region):
            return self.primary_region

        # ä¸»åŒºåŸŸæ•…éšœï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨åŒºåŸŸ
        for region in self.backup_regions:
            if await self.check_region_health(region):
                await self.switch_traffic(region)
                await self.notify_ops_team(f"Failover to {region}")
                return region

        # æ‰€æœ‰åŒºåŸŸéƒ½æ•…éšœ
        raise CriticalError("All regions are down")
```

---

---

### 4.4 ç”Ÿäº§éƒ¨ç½²æ£€æŸ¥æ¸…å•

#### 4.4.1 å®‰å…¨æ£€æŸ¥æ¸…å•

**APIå¯†é’¥ä¸å‡­è¯ç®¡ç†**ï¼š

- [ ] **ç¯å¢ƒå˜é‡ç®¡ç†**ï¼šæ‰€æœ‰APIå¯†é’¥é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥ï¼Œç¦æ­¢ç¡¬ç¼–ç 
- [ ] **å¯†é’¥ç®¡ç†æœåŠ¡**ï¼šä½¿ç”¨AWS Secrets Managerã€HashiCorp Vaultç­‰å¯†é’¥ç®¡ç†æœåŠ¡
- [ ] **å¯†é’¥è½®æ¢**ï¼šå®šæœŸè½®æ¢APIå¯†é’¥å’Œè®¿é—®ä»¤ç‰Œ
- [ ] **æœ€å°æƒé™åŸåˆ™**ï¼šä¸ºä¸åŒç¯å¢ƒé…ç½®ä¸åŒæƒé™çº§åˆ«çš„å¯†é’¥

**ç½‘ç»œå®‰å…¨**ï¼š

- [ ] **å¯ç”¨HTTPS/TLS**ï¼šæ‰€æœ‰å¤–éƒ¨é€šä¿¡å¼ºåˆ¶ä½¿ç”¨HTTPS
- [ ] **TLSç‰ˆæœ¬**ï¼šä½¿ç”¨TLS 1.2æˆ–æ›´é«˜ç‰ˆæœ¬
- [ ] **è¯ä¹¦éªŒè¯**ï¼šéªŒè¯SSLè¯ä¹¦æœ‰æ•ˆæ€§
- [ ] **APIç½‘å…³**ï¼šé€šè¿‡APIç½‘å…³ç»Ÿä¸€ç®¡ç†å’Œä¿æŠ¤APIç«¯ç‚¹

**é€Ÿç‡é™åˆ¶ä¸é˜²æŠ¤**ï¼š

- [ ] **å…¨å±€é€Ÿç‡é™åˆ¶**ï¼šé™åˆ¶æ¯ä¸ªç”¨æˆ·/IPçš„è¯·æ±‚é¢‘ç‡
- [ ] **ç«¯ç‚¹çº§é™åˆ¶**ï¼šä¸ºä¸åŒç«¯ç‚¹è®¾ç½®ä¸åŒé€Ÿç‡é™åˆ¶
- [ ] **DDoSé˜²æŠ¤**ï¼šé…ç½®CloudFlareæˆ–AWS Shieldç­‰é˜²æŠ¤æœåŠ¡
- [ ] **è¯·æ±‚å¤§å°é™åˆ¶**ï¼šé™åˆ¶è¯·æ±‚payloadå¤§å°

**è¾“å…¥éªŒè¯ä¸æ¸…æ´—**ï¼š

- [ ] **è¾“å…¥é•¿åº¦éªŒè¯**ï¼šé™åˆ¶æç¤ºè¯å’Œè¾“å…¥æ•°æ®é•¿åº¦
- [ ] **SQLæ³¨å…¥é˜²æŠ¤**ï¼šä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼Œç¦æ­¢æ‹¼æ¥SQL
- [ ] **XSSé˜²æŠ¤**ï¼šå¯¹æ‰€æœ‰ç”¨æˆ·è¾“å…¥è¿›è¡ŒHTMLå®ä½“ç¼–ç 
- [ ] **å‘½ä»¤æ³¨å…¥é˜²æŠ¤**ï¼šç¦æ­¢ç›´æ¥æ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‘½ä»¤
- [ ] **è·¯å¾„éå†é˜²æŠ¤**ï¼šéªŒè¯å’Œæ¸…æ´—æ–‡ä»¶è·¯å¾„

**æ•æ„Ÿä¿¡æ¯è„±æ•**ï¼š

- [ ] **PIIæ£€æµ‹**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œè„±æ•ä¸ªäººèº«ä»½ä¿¡æ¯
- [ ] **è¾“å‡ºè¿‡æ»¤**ï¼šè¿‡æ»¤APIå¯†é’¥ã€å¯†ç ã€Tokenç­‰æ•æ„Ÿä¿¡æ¯
- [ ] **æ—¥å¿—è„±æ•**ï¼šç¡®ä¿æ—¥å¿—ä¸­ä¸åŒ…å«æ•æ„Ÿä¿¡æ¯
- [ ] **é”™è¯¯ä¿¡æ¯å¤„ç†**ï¼šé¿å…æ³„éœ²ç³»ç»Ÿå†…éƒ¨ä¿¡æ¯

#### 4.4.2 æ€§èƒ½æ£€æŸ¥æ¸…å•

**è¿æ¥æ± é…ç½®**ï¼š

- [ ] **HTTPè¿æ¥æ± **ï¼šé…ç½®åˆç†çš„è¿æ¥æ± å¤§å°ï¼ˆå»ºè®®ï¼šæ¯ä¸ªä¸»æœº10-30ä¸ªè¿æ¥ï¼‰
- [ ] **æ•°æ®åº“è¿æ¥æ± **ï¼šé…ç½®æ•°æ®åº“è¿æ¥æ± ï¼ˆå»ºè®®ï¼šæœ€å°5ä¸ªï¼Œæœ€å¤§20ä¸ªï¼‰
- [ ] **Redisè¿æ¥æ± **ï¼šé…ç½®Redisè¿æ¥æ± å‚æ•°
- [ ] **è¿æ¥è¶…æ—¶**ï¼šè®¾ç½®åˆç†çš„è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®®ï¼š10ç§’ï¼‰
- [ ] **è¯»å–è¶…æ—¶**ï¼šè®¾ç½®è¯»å–è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®®ï¼š30ç§’ï¼‰

**ç¼“å­˜ç­–ç•¥**ï¼š

- [ ] **LLMå“åº”ç¼“å­˜**ï¼šå¯ç”¨è¯­ä¹‰ç¼“å­˜æˆ–ç²¾ç¡®åŒ¹é…ç¼“å­˜
- [ ] **ç¼“å­˜å±‚çº§**ï¼šé…ç½®L1ï¼ˆå†…å­˜ï¼‰ã€L2ï¼ˆRedisï¼‰ã€L3ï¼ˆS3ï¼‰å¤šå±‚ç¼“å­˜
- [ ] **TTLé…ç½®**ï¼šä¸ºä¸åŒæ•°æ®ç±»å‹è®¾ç½®åˆç†çš„è¿‡æœŸæ—¶é—´
- [ ] **ç¼“å­˜é¢„çƒ­**ï¼šåœ¨å¯åŠ¨æ—¶é¢„åŠ è½½çƒ­ç‚¹æ•°æ®
- [ ] **ç¼“å­˜å¤±æ•ˆ**ï¼šå®ç°ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°ç­–ç•¥

**è¶…æ—¶æ§åˆ¶**ï¼š

- [ ] **LLMè°ƒç”¨è¶…æ—¶**ï¼šè®¾ç½®LLM APIè°ƒç”¨è¶…æ—¶ï¼ˆå»ºè®®ï¼š30-60ç§’ï¼‰
- [ ] **å·¥å…·æ‰§è¡Œè¶…æ—¶**ï¼šä¸ºæ¯ä¸ªå·¥å…·è®¾ç½®ç‹¬ç«‹è¶…æ—¶æ—¶é—´
- [ ] **æ€»ä½“è¶…æ—¶**ï¼šè®¾ç½®è¯·æ±‚æ€»è¶…æ—¶æ—¶é—´
- [ ] **æµå¼å“åº”è¶…æ—¶**ï¼šé…ç½®æµå¼å“åº”çš„è¶…æ—¶ç­–ç•¥

**æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š

- [ ] **æ‰¹é‡æ¨ç†**ï¼šæ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
- [ ] **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®æ¨¡å‹å’Œç¡¬ä»¶ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
- [ ] **å¼‚æ­¥å¤„ç†**ï¼šä½¿ç”¨å¼‚æ­¥I/Oæå‡å¹¶å‘èƒ½åŠ›
- [ ] **å¹¶è¡Œæ‰§è¡Œ**ï¼šæ”¯æŒå¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·æˆ–LLM

#### 4.4.3 å¯é æ€§æ£€æŸ¥æ¸…å•

**é‡è¯•ç­–ç•¥**ï¼š

- [ ] **æŒ‡æ•°é€€é¿**ï¼šå®ç°æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
- [ ] **é‡è¯•æ¬¡æ•°**ï¼šè®¾ç½®åˆç†çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå»ºè®®ï¼š3æ¬¡ï¼‰
- [ ] **é‡è¯•æ¡ä»¶**ï¼šä»…å¯¹å¯æ¢å¤çš„é”™è¯¯ï¼ˆ429ã€500ã€503ï¼‰è¿›è¡Œé‡è¯•
- [ ] **å¹‚ç­‰æ€§**ï¼šç¡®ä¿é‡è¯•æ“ä½œæ˜¯å¹‚ç­‰çš„
- [ ] **è¶…æ—¶é‡è¯•**ï¼šå¯¹è¶…æ—¶è¯·æ±‚è¿›è¡Œé‡è¯•

**ç†”æ–­å™¨**ï¼š

- [ ] **ç†”æ–­é˜ˆå€¼**ï¼šè®¾ç½®å¤±è´¥ç‡é˜ˆå€¼ï¼ˆå»ºè®®ï¼š50%ï¼‰
- [ ] **æ¢å¤æ—¶é—´**ï¼šé…ç½®ç†”æ–­å™¨è‡ªåŠ¨æ¢å¤æ—¶é—´ï¼ˆå»ºè®®ï¼š60ç§’ï¼‰
- [ ] **åŠå¼€çŠ¶æ€**ï¼šå®ç°åŠå¼€çŠ¶æ€æ¢æµ‹æœåŠ¡æ¢å¤
- [ ] **ç†”æ–­å‘Šè­¦**ï¼šç†”æ–­å™¨æ‰“å¼€æ—¶å‘é€å‘Šè­¦

**é™çº§æ–¹æ¡ˆ**ï¼š

- [ ] **å¤‡ç”¨æ¨¡å‹**ï¼šé…ç½®å¤‡ç”¨LLMæ¨¡å‹
- [ ] **ç¼“å­˜é™çº§**ï¼šæœåŠ¡ä¸å¯ç”¨æ—¶è¿”å›ç¼“å­˜ç»“æœ
- [ ] **é»˜è®¤å“åº”**ï¼šæä¾›å‹å¥½çš„é»˜è®¤å“åº”
- [ ] **åŠŸèƒ½é™çº§**ï¼šå…³é”®è·¯å¾„å¤±è´¥æ—¶é™çº§åˆ°ç®€åŒ–åŠŸèƒ½

**å¥åº·æ£€æŸ¥**ï¼š

- [ ] **å°±ç»ªæ£€æŸ¥**ï¼šå®ç°`/ready`ç«¯ç‚¹æ£€æŸ¥ä¾èµ–æœåŠ¡
- [ ] **å­˜æ´»æ£€æŸ¥**ï¼šå®ç°`/health`ç«¯ç‚¹æ£€æŸ¥åº”ç”¨çŠ¶æ€
- [ ] **æ·±åº¦æ£€æŸ¥**ï¼šå¯é€‰çš„æ·±åº¦å¥åº·æ£€æŸ¥ï¼ˆéªŒè¯LLMè¿æ¥ç­‰ï¼‰
- [ ] **æ£€æŸ¥é—´éš”**ï¼šé…ç½®åˆç†çš„å¥åº·æ£€æŸ¥é—´éš”ï¼ˆå»ºè®®ï¼š30ç§’ï¼‰

#### 4.4.4 ç›‘æ§æ£€æŸ¥æ¸…å•

**LangSmithè¿½è¸ª**ï¼š

- [ ] **å¯ç”¨è¿½è¸ª**ï¼šè®¾ç½®`LANGCHAIN_TRACING_V2=true`
- [ ] **é¡¹ç›®é…ç½®**ï¼šä¸ºä¸åŒç¯å¢ƒé…ç½®ä¸åŒé¡¹ç›®
- [ ] **é‡‡æ ·ç‡**ï¼šç”Ÿäº§ç¯å¢ƒé…ç½®åˆç†çš„é‡‡æ ·ç‡ï¼ˆå»ºè®®ï¼š10%ï¼‰
- [ ] **æ•æ„Ÿä¿¡æ¯è¿‡æ»¤**ï¼šè¿‡æ»¤è¿½è¸ªæ•°æ®ä¸­çš„æ•æ„Ÿä¿¡æ¯

**PrometheusæŒ‡æ ‡**ï¼š

- [ ] **è¯·æ±‚æŒ‡æ ‡**ï¼šè®°å½•è¯·æ±‚æ€»æ•°ã€æˆåŠŸç‡ã€å¤±è´¥ç‡
- [ ] **å»¶è¿ŸæŒ‡æ ‡**ï¼šè®°å½•P50ã€P95ã€P99å»¶è¿Ÿ
- [ ] **TokenæŒ‡æ ‡**ï¼šè®°å½•è¾“å…¥/è¾“å‡ºtokenä½¿ç”¨é‡
- [ ] **é”™è¯¯æŒ‡æ ‡**ï¼šè®°å½•ä¸åŒç±»å‹çš„é”™è¯¯æ•°é‡
- [ ] **è‡ªå®šä¹‰æŒ‡æ ‡**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡

**æ—¥å¿—èšåˆ**ï¼š

- [ ] **ç»“æ„åŒ–æ—¥å¿—**ï¼šä½¿ç”¨JSONæ ¼å¼è®°å½•ç»“æ„åŒ–æ—¥å¿—
- [ ] **æ—¥å¿—çº§åˆ«**ï¼šç”Ÿäº§ç¯å¢ƒä½¿ç”¨INFOçº§åˆ«ï¼Œå¼€å‘ç¯å¢ƒä½¿ç”¨DEBUG
- [ ] **æ—¥å¿—èšåˆ**ï¼šé…ç½®ELKã€Lokiæˆ–CloudWatchæ—¥å¿—èšåˆ
- [ ] **æ—¥å¿—ä¿ç•™**ï¼šè®¾ç½®æ—¥å¿—ä¿ç•™ç­–ç•¥ï¼ˆå»ºè®®ï¼š30-90å¤©ï¼‰
- [ ] **è¯·æ±‚ID**ï¼šä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆå”¯ä¸€IDç”¨äºè¿½è¸ª

**å‘Šè­¦è§„åˆ™**ï¼š

- [ ] **å»¶è¿Ÿå‘Šè­¦**ï¼šP95å»¶è¿Ÿè¶…è¿‡é˜ˆå€¼æ—¶å‘Šè­¦
- [ ] **é”™è¯¯ç‡å‘Šè­¦**ï¼šé”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼æ—¶å‘Šè­¦
- [ ] **æˆæœ¬å‘Šè­¦**ï¼šTokenä½¿ç”¨é‡è¶…è¿‡é¢„ç®—æ—¶å‘Šè­¦
- [ ] **å¯ç”¨æ€§å‘Šè­¦**ï¼šæœåŠ¡ä¸å¯ç”¨æ—¶ç«‹å³å‘Šè­¦
- [ ] **å‘Šè­¦é€šé“**ï¼šé…ç½®PagerDutyã€Slackã€é‚®ä»¶ç­‰å‘Šè­¦é€šé“

#### 4.4.5 æˆæœ¬æ£€æŸ¥æ¸…å•

**Tokenä½¿ç”¨ç›‘æ§**ï¼š

- [ ] **å®æ—¶ç›‘æ§**ï¼šå®æ—¶ç›‘æ§tokenä½¿ç”¨é‡
- [ ] **ç”¨æˆ·çº§ç»Ÿè®¡**ï¼šæŒ‰ç”¨æˆ·ç»Ÿè®¡tokenä½¿ç”¨é‡
- [ ] **æ¨¡å‹çº§ç»Ÿè®¡**ï¼šæŒ‰æ¨¡å‹ç»Ÿè®¡æˆæœ¬
- [ ] **æˆæœ¬åˆ†æ**ï¼šå®šæœŸåˆ†ææˆæœ¬æ„æˆ

**æˆæœ¬é¢„ç®—æ§åˆ¶**ï¼š

- [ ] **é¢„ç®—å‘Šè­¦**ï¼šè®¾ç½®æ¯æ—¥/æ¯æœˆé¢„ç®—å‘Šè­¦
- [ ] **ç”¨æˆ·é…é¢**ï¼šä¸ºæ¯ä¸ªç”¨æˆ·è®¾ç½®tokené…é¢
- [ ] **é€Ÿç‡é™åˆ¶**ï¼šé€šè¿‡é€Ÿç‡é™åˆ¶æ§åˆ¶æˆæœ¬
- [ ] **è‡ªåŠ¨ç†”æ–­**ï¼šè¶…å‡ºé¢„ç®—æ—¶è‡ªåŠ¨åœæ­¢æœåŠ¡

**ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–**ï¼š

- [ ] **ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§**ï¼šç›‘æ§ç¼“å­˜å‘½ä¸­ç‡ï¼ˆç›®æ ‡ï¼š>80%ï¼‰
- [ ] **ç¼“å­˜é¢„çƒ­**ï¼šé¢„åŠ è½½å¸¸ç”¨æŸ¥è¯¢
- [ ] **è¯­ä¹‰ç¼“å­˜**ï¼šä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜æå‡å‘½ä¸­ç‡
- [ ] **ç¼“å­˜åˆ†æ**ï¼šå®šæœŸåˆ†æç¼“å­˜æ•ˆæœ

**æ¨¡å‹é€‰æ‹©ç­–ç•¥**ï¼š

- [ ] **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚æ¨¡å‹
- [ ] **æˆæœ¬ä¼˜åŒ–**ï¼šä¼˜å…ˆä½¿ç”¨ä¾¿å®œæ¨¡å‹ï¼ˆå¦‚gpt-4o-miniï¼‰
- [ ] **è´¨é‡å¹³è¡¡**ï¼šåœ¨æˆæœ¬å’Œè´¨é‡é—´æ‰¾åˆ°å¹³è¡¡ç‚¹
- [ ] **A/Bæµ‹è¯•**ï¼šå¯¹æ¯”ä¸åŒæ¨¡å‹çš„æˆæœ¬æ•ˆç›Š

#### 4.4.6 æ•°æ®åˆè§„æ£€æŸ¥æ¸…å•

**GDPRåˆè§„**ï¼š

- [ ] **ç”¨æˆ·åŒæ„**ï¼šæ”¶é›†ç”¨æˆ·æ•°æ®å¤„ç†åŒæ„
- [ ] **æ•°æ®æœ€å°åŒ–**ï¼šä»…æ”¶é›†å¿…è¦çš„ä¸ªäººæ•°æ®
- [ ] **è®¿é—®æƒ**ï¼šå®ç°ç”¨æˆ·æ•°æ®è®¿é—®æ¥å£
- [ ] **åˆ é™¤æƒ**ï¼šå®ç°ç”¨æˆ·æ•°æ®åˆ é™¤åŠŸèƒ½ï¼ˆè¢«é—å¿˜æƒï¼‰
- [ ] **å¯æºå¸¦æƒ**ï¼šæ”¯æŒå¯¼å‡ºç”¨æˆ·æ•°æ®
- [ ] **æ•°æ®å‡ååŒ–**ï¼šå¯¹ä¸ªäººæ•°æ®è¿›è¡Œå‡ååŒ–å¤„ç†

**æ•°æ®åŠ å¯†**ï¼š

- [ ] **ä¼ è¾“åŠ å¯†**ï¼šä½¿ç”¨TLSåŠ å¯†æ‰€æœ‰ç½‘ç»œä¼ è¾“
- [ ] **å­˜å‚¨åŠ å¯†**ï¼šåŠ å¯†å­˜å‚¨æ•æ„Ÿæ•°æ®
- [ ] **å¯†é’¥ç®¡ç†**ï¼šä½¿ç”¨KMSç®¡ç†åŠ å¯†å¯†é’¥
- [ ] **åŠ å¯†ç®—æ³•**ï¼šä½¿ç”¨AES-256ç­‰å¼ºåŠ å¯†ç®—æ³•

**å®¡è®¡æ—¥å¿—**ï¼š

- [ ] **å®Œæ•´è®°å½•**ï¼šè®°å½•æ‰€æœ‰æ•°æ®è®¿é—®å’Œä¿®æ”¹æ“ä½œ
- [ ] **ä¸å¯ç¯¡æ”¹**ï¼šä½¿ç”¨åªè¿½åŠ å­˜å‚¨æˆ–åŒºå—é“¾æŠ€æœ¯
- [ ] **æ—¥å¿—ç­¾å**ï¼šå¯¹å®¡è®¡æ—¥å¿—è¿›è¡Œæ•°å­—ç­¾å
- [ ] **å®šæœŸå®¡è®¡**ï¼šå®šæœŸå®¡æŸ¥å®¡è®¡æ—¥å¿—
- [ ] **åˆè§„æŠ¥å‘Š**ï¼šç”Ÿæˆåˆè§„å®¡è®¡æŠ¥å‘Š

#### 4.4.7 éƒ¨ç½²å‰æœ€ç»ˆæ£€æŸ¥

**ä»£ç æ£€æŸ¥**ï¼š

- [ ] **å•å…ƒæµ‹è¯•**ï¼šå•å…ƒæµ‹è¯•è¦†ç›–ç‡>80%
- [ ] **é›†æˆæµ‹è¯•**ï¼šæ‰€æœ‰å…³é”®è·¯å¾„æœ‰é›†æˆæµ‹è¯•
- [ ] **æ€§èƒ½æµ‹è¯•**ï¼šé€šè¿‡è´Ÿè½½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•
- [ ] **å®‰å…¨æ‰«æ**ï¼šé€šè¿‡å®‰å…¨æ¼æ´æ‰«æ

**é…ç½®æ£€æŸ¥**ï¼š

- [ ] **ç¯å¢ƒå˜é‡**ï¼šæ‰€æœ‰ç¯å¢ƒå˜é‡æ­£ç¡®é…ç½®
- [ ] **èµ„æºé™åˆ¶**ï¼šé…ç½®CPUã€å†…å­˜é™åˆ¶
- [ ] **å‰¯æœ¬æ•°**ï¼šé…ç½®åˆç†çš„å‰¯æœ¬æ•°é‡ï¼ˆå»ºè®®ï¼š>=2ï¼‰
- [ ] **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šé…ç½®HPAè‡ªåŠ¨æ‰©ç¼©å®¹ç­–ç•¥

**æ–‡æ¡£æ£€æŸ¥**ï¼š

- [ ] **éƒ¨ç½²æ–‡æ¡£**ï¼šå®Œæ•´çš„éƒ¨ç½²æ“ä½œæ–‡æ¡£
- [ ] **è¿ç»´æ‰‹å†Œ**ï¼šæ•…éšœæ’æŸ¥å’Œåº”æ€¥å“åº”æ‰‹å†Œ
- [ ] **APIæ–‡æ¡£**ï¼šå®Œæ•´çš„APIæ¥å£æ–‡æ¡£
- [ ] **å˜æ›´è®°å½•**ï¼šè®°å½•æ‰€æœ‰é‡è¦å˜æ›´

**æ¼”ç»ƒæ£€æŸ¥**ï¼š

- [ ] **ç°åº¦å‘å¸ƒ**ï¼šå…ˆåœ¨å°èŒƒå›´ç”¨æˆ·æµ‹è¯•
- [ ] **å›æ»šè®¡åˆ’**ï¼šå‡†å¤‡å¿«é€Ÿå›æ»šæ–¹æ¡ˆ
- [ ] **æ•…éšœæ¼”ç»ƒ**ï¼šè¿›è¡Œæ•…éšœæ³¨å…¥å’Œæ¢å¤æ¼”ç»ƒ
- [ ] **åº”æ€¥å“åº”**ï¼šæ˜ç¡®åº”æ€¥å“åº”æµç¨‹å’Œäººå‘˜

---

### æœ¬ç« å°ç»“

1. **æ¶æ„è®¾è®¡**ï¼šRAGã€Multi-Agentã€Workflow ä¸‰ç§æ ¸å¿ƒæ¶æ„æ¨¡å¼
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šå»¶è¿Ÿä¼˜åŒ–ã€ååé‡æå‡ã€Token æˆæœ¬æ§åˆ¶ã€å¤šå±‚ç¼“å­˜
3. **å®‰å…¨é˜²æŠ¤**ï¼šGuardrails ä½“ç³»ã€PII ä¿æŠ¤ã€åˆ†å±‚é˜²æŠ¤
4. **åˆè§„è¦æ±‚**ï¼šè¾“å…¥è¾“å‡ºå®‰å…¨ã€GDPR åˆè§„ã€å®¡è®¡æ—¥å¿—
5. **éƒ¨ç½²è¿ç»´**ï¼šå®¹å™¨åŒ–ã€Serverlessã€ç›‘æ§å‘Šè­¦ã€æ•…éšœæ¢å¤
6. **ç”Ÿäº§æ¸…å•**ï¼šå…¨é¢çš„å®‰å…¨ã€æ€§èƒ½ã€å¯é æ€§ã€ç›‘æ§ã€æˆæœ¬ã€åˆè§„æ£€æŸ¥æ¸…å•

---

# ç¬¬17ç« ï¼šæµ‹è¯•ä¸è´¨é‡ä¿è¯

> **ç›®æ ‡**ï¼šæŒæ¡LangChainåº”ç”¨çš„æµ‹è¯•æ–¹æ³•ï¼Œæ„å»ºå¯é çš„ç”Ÿäº§ç³»ç»Ÿ

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒAgentçš„è¡Œä¸ºå¿…é¡»æ˜¯**å¯é¢„æµ‹**å’Œ**å¯éªŒè¯**çš„ã€‚æœ¬ç« å°†æ·±å…¥è®²è§£å¦‚ä½•ç³»ç»Ÿåœ°æµ‹è¯•LangChainåº”ç”¨ï¼Œä»å•å…ƒæµ‹è¯•åˆ°é›†æˆæµ‹è¯•ï¼Œå†åˆ°æ€§èƒ½æµ‹è¯•å’Œè¯„ä¼°ä½“ç³»ã€‚

---

### 17.1 ä¸ºä»€ä¹ˆéœ€è¦æµ‹è¯•

#### 17.1.1 LLMåº”ç”¨çš„æµ‹è¯•æŒ‘æˆ˜

**LLMåº”ç”¨æµ‹è¯•çš„ä¸‰å¤§æŒ‘æˆ˜**ï¼š

1. **éç¡®å®šæ€§**ï¼šLLMè¾“å‡ºä¸ç¨³å®šï¼ŒåŒæ ·çš„è¾“å…¥å¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡º
2. **æˆæœ¬é—®é¢˜**ï¼šæ¯æ¬¡æµ‹è¯•è°ƒç”¨çœŸå®LLMéƒ½ä¼šäº§ç”Ÿè´¹ç”¨
3. **ä¾èµ–å¤–éƒ¨æœåŠ¡**ï¼šéœ€è¦ç½‘ç»œã€API Keyã€ç¬¬ä¸‰æ–¹æœåŠ¡ç­‰

---

### 17.2 å•å…ƒæµ‹è¯•

#### 17.2.1 Mock LLM - GenericFakeChatModel

**æ ¸å¿ƒç‰¹ç‚¹**ï¼šæ¨¡æ‹ŸLLMå“åº”ï¼Œå®ç°ç¡®å®šæ€§æµ‹è¯•ã€‚

```python
import pytest
from langchain_core.language_models.fake_chat_models import GenericFakeChatModel
from langchain_core.messages import AIMessage, HumanMessage

def test_fake_llm_basic():
    """æµ‹è¯•åŸºç¡€çš„Fake LLM"""
    # åˆ›å»ºFake LLM(è¿”å›å›ºå®šå“åº”)
    fake_llm = GenericFakeChatModel(messages=iter([
        AIMessage(content="è¿™æ˜¯ç¬¬ä¸€æ¡å“åº”"),
        AIMessage(content="è¿™æ˜¯ç¬¬äºŒæ¡å“åº”"),
    ]))

    # æµ‹è¯•è°ƒç”¨
    response1 = fake_llm.invoke([HumanMessage(content="ä½ å¥½")])
    assert response1.content == "è¿™æ˜¯ç¬¬ä¸€æ¡å“åº”"

    response2 = fake_llm.invoke([HumanMessage(content="å†æ¬¡é—®å€™")])
    assert response2.content == "è¿™æ˜¯ç¬¬äºŒæ¡å“åº”"
```

#### 17.2.2 æµ‹è¯•å¸¦å·¥å…·è°ƒç”¨çš„Agent

```python
from langchain.agents import create_agent
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver

@tool
def calculator(expression: str) -> float:
    """å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼è®¡ç®—å™¨

    æ”¯æŒåŸºæœ¬è¿ç®—: +, -, *, /, ()
    ä¸æ”¯æŒå˜é‡å’Œå‡½æ•°è°ƒç”¨,é¿å…ä»£ç æ³¨å…¥é£é™©
    """
    import ast
    import operator

    # å…è®¸çš„å®‰å…¨æ“ä½œç¬¦
    allowed_operators = {
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.Pow: operator.pow,
        ast.USub: operator.neg,
    }

    def eval_node(node):
        # Python 3.8+ ä½¿ç”¨ ast.Constant æ›¿ä»£ ast.Num
        if isinstance(node, ast.Constant):  # æ•°å­— (Python 3.8+)
            return node.value
        elif isinstance(node, ast.Num):  # æ•°å­— (å‘åå…¼å®¹ Python 3.7)
            return node.n
        elif isinstance(node, ast.BinOp):  # äºŒå…ƒè¿ç®—
            if type(node.op) not in allowed_operators:
                raise ValueError(f"ä¸æ”¯æŒçš„è¿ç®—ç¬¦: {type(node.op).__name__}")
            return allowed_operators[type(node.op)](
                eval_node(node.left),
                eval_node(node.right)
            )
        elif isinstance(node, ast.UnaryOp):  # ä¸€å…ƒè¿ç®—
            if type(node.op) not in allowed_operators:
                raise ValueError(f"ä¸æ”¯æŒçš„è¿ç®—ç¬¦: {type(node.op).__name__}")
            return allowed_operators[type(node.op)](eval_node(node.operand))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¡¨è¾¾å¼ç±»å‹: {type(node).__name__}")

    try:
        tree = ast.parse(expression, mode='eval')
        return float(eval_node(tree.body))
    except (ValueError, SyntaxError, TypeError) as e:
        raise ValueError(f"è¡¨è¾¾å¼è§£æå¤±è´¥: {str(e)}")

def test_agent_with_tool_call():
    """æµ‹è¯•Agentçš„å·¥å…·è°ƒç”¨è¡Œä¸º"""
    # åˆ›å»ºFake LLM(æ¨¡æ‹Ÿè°ƒç”¨calculatorå·¥å…·)
    fake_llm = GenericFakeChatModel(messages=iter([
        AIMessage(
            content="",
            tool_calls=[{
                "id": "call_123",
                "name": "calculator",
                "args": {"expression": "2 + 3"}
            }]
        ),
        AIMessage(content="è®¡ç®—ç»“æœæ˜¯5")
    ]))

    # åˆ›å»ºAgent
    agent = create_agent(
        model=fake_llm,
        tools=[calculator],
        checkpointer=MemorySaver()
    )

    # æ‰§è¡Œå¹¶éªŒè¯
    result = agent.invoke(
        {"messages": [("user", "è®¡ç®—2+3")]},
        {"configurable": {"thread_id": "test-1"}}
    )

    # æ–­è¨€
    messages = result["messages"]
    assert messages[0].content == "è®¡ç®—2+3"
    assert hasattr(messages[1], "tool_calls")
    assert messages[1].tool_calls[0]["name"] == "calculator"
```

---

### 17.3 é›†æˆæµ‹è¯•

#### 17.3.1 ç«¯åˆ°ç«¯Agentæµ‹è¯•

```python
import pytest
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver

@tool
def search(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœï¼š{query}"

@pytest.mark.integration
def test_agent_e2e_flow():
    """æµ‹è¯•Agentå®Œæ•´æ‰§è¡Œæµç¨‹(é›†æˆæµ‹è¯•)"""
    # åˆ›å»ºçœŸå®Agent(ä½†ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹)
    agent = create_agent(
        model=ChatOpenAI(model="gpt-4o-mini", temperature=0),
        tools=[search],
        checkpointer=MemorySaver()
    )

    # æµ‹è¯•å¤šè½®å¯¹è¯
    thread_id = "integration-test-1"

    # ç¬¬ä¸€è½®ï¼šéœ€è¦æœç´¢
    result1 = agent.invoke(
        {"messages": [("user", "æœç´¢LangChainçš„æœ€æ–°ç‰ˆæœ¬")]},
        {"configurable": {"thread_id": thread_id}}
    )

    assert len(result1["messages"]) >= 3  # user + ai(tool_call) + tool + ai(final)

    # éªŒè¯å¯¹è¯å†å²è¢«ä¿ç•™
    assert len(result1["messages"]) >= 3
```

---

### 17.4 æ€§èƒ½æµ‹è¯•

#### 17.4.1 å»¶è¿Ÿæµ‹è¯•

```python
import time
from statistics import mean

def test_agent_latency():
    """æµ‹è¯•Agentå“åº”å»¶è¿Ÿ"""
    agent = create_agent(
        model=ChatOpenAI(model="gpt-4o-mini", temperature=0),
        tools=[search],
        checkpointer=MemorySaver()
    )

    latencies = []

    # è¿è¡Œ10æ¬¡æµ‹è¯•
    for i in range(10):
        start = time.time()
        agent.invoke(
            {"messages": [("user", f"æµ‹è¯•æŸ¥è¯¢{i}")]},
            {"configurable": {"thread_id": f"latency-{i}"}}
        )
        end = time.time()
        latencies.append(end - start)

    # ç»Ÿè®¡å»¶è¿Ÿ
    avg_latency = mean(latencies)
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]

    print(f"\nå»¶è¿Ÿç»Ÿè®¡:")
    print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
    print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}s")

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert avg_latency < 5.0, f"å¹³å‡å»¶è¿Ÿè¿‡é«˜: {avg_latency:.2f}s"
```

---

### 17.5 è¯„ä¼°æµ‹è¯• - LangSmith

#### 17.5.1 æ‰¹é‡è¯„ä¼°

```python
from langsmith import evaluate
from langchain.agents import create_agent

def agent_runner(inputs: dict, agent) -> dict:
    """è¿è¡ŒAgent"""
    result = agent.invoke(
        {"messages": [("user", inputs["question"])]},
        {"configurable": {"thread_id": "eval"}}
    )
    return {"answer": result["messages"][-1].content}

# è‡ªå®šä¹‰è¯„ä¼°å™¨
def answer_correctness(run, example) -> dict:
    """è¯„ä¼°å›ç­”æ­£ç¡®æ€§"""
    expected = example.outputs["answer"].lower()
    actual = run.outputs["answer"].lower()
    score = 1.0 if expected in actual or actual in expected else 0.0
    return {"score": score}

# è¿è¡Œè¯„ä¼°
results = evaluate(
    agent_runner,
    data="agent_qa_dataset",
    evaluators=[answer_correctness],
    experiment_prefix="agent_v1"
)
```

---

### 17.6 æµ‹è¯•æœ€ä½³å®è·µ

#### 17.6.1 æµ‹è¯•é‡‘å­—å¡”åŸåˆ™

```python
# âœ… å¥½çš„æµ‹è¯•ç»“æ„
tests/
â”œâ”€â”€ unit/                    # 75% - å¿«é€Ÿã€ç¨³å®š
â”‚   â”œâ”€â”€ test_tools.py
â”‚   â”œâ”€â”€ test_prompts.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ integration/             # 20% - ä¸­ç­‰é€Ÿåº¦
â”‚   â”œâ”€â”€ test_agent_flow.py
â”‚   â””â”€â”€ test_rag_system.py
â””â”€â”€ e2e/                     # 5% - æ…¢ä½†å…³é”®
    â””â”€â”€ test_production_scenarios.py
```

#### 17.6.2 æˆæœ¬æ§åˆ¶ç­–ç•¥

| ç­–ç•¥ | æ–¹æ³• | èŠ‚çœæˆæœ¬ |
|------|------|---------|
| **ä½¿ç”¨Fake LLM** | GenericFakeChatModel | 100% |
| **ä½¿ç”¨ä¾¿å®œæ¨¡å‹** | gpt-4o-miniä»£æ›¿gpt-4o | 95% |
| **å¹¶è¡Œæ‰§è¡Œ** | pytest-xdist | èŠ‚çœæ—¶é—´ |

---

### 17.7 æ€»ç»“

#### 17.7.1 æµ‹è¯•ç­–ç•¥å¿«é€Ÿé€‰æ‹©

| åœºæ™¯ | æ¨èç­–ç•¥ | å…³é”®å·¥å…· |
|------|---------|---------|
| **å¿«é€ŸéªŒè¯åŠŸèƒ½** | å•å…ƒæµ‹è¯• + Fake LLM | GenericFakeChatModel |
| **éªŒè¯Agentè¡Œä¸º** | é›†æˆæµ‹è¯• | pytest + MemorySaver |
| **è¯„ä¼°è¾“å‡ºè´¨é‡** | LLM-as-Judge | LangSmith |
| **æ€§èƒ½åŸºå‡†æµ‹è¯•** | å»¶è¿Ÿ/ååé‡æµ‹è¯• | pytest-benchmark |

#### 17.7.2 æµ‹è¯•æ¸…å•

**å¼€å‘é˜¶æ®µ**ï¼š
- [ ] ä¸ºæ¯ä¸ªæ–°å·¥å…·ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] ä¸ºæ¯ä¸ªAgentèŠ‚ç‚¹ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] ä½¿ç”¨Fake LLMç¡®ä¿æµ‹è¯•ç¨³å®š
- [ ] è¾¾åˆ°80%+ä»£ç è¦†ç›–ç‡

**é›†æˆé˜¶æ®µ**ï¼š
- [ ] ç¼–å†™ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å½•åˆ¶æµ‹è¯•ç”¨ä¾‹é¿å…é‡å¤è´¹ç”¨

**å‘å¸ƒé˜¶æ®µ**ï¼š
- [ ] åœ¨çœŸå®LLMä¸Šè¿è¡Œè¯„ä¼°
- [ ] LangSmithå›å½’æµ‹è¯•
- [ ] æ‰€æœ‰CIæ£€æŸ¥é€šè¿‡

---

**æœ¬ç« å…³é”®ä»£ç **ï¼š

```python
# ç”Ÿäº§çº§æµ‹è¯•å¥—ä»¶æ¨¡æ¿
import pytest
from langchain_core.language_models.fake_chat_models import GenericFakeChatModel
from langchain_core.messages import AIMessage
from langchain.agents import create_agent
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver

# ===== Fixtures =====
@pytest.fixture
def fake_llm():
    return GenericFakeChatModel(messages=iter([
        AIMessage(content="æµ‹è¯•å“åº”")
    ]))

@pytest.fixture
def agent(fake_llm):
    @tool
    def search(query: str) -> str:
        return f"æœç´¢ç»“æœï¼š{query}"

    return create_agent(
        model=fake_llm,
        tools=[search],
        checkpointer=MemorySaver()
    )

# ===== å•å…ƒæµ‹è¯• =====
def test_tool_execution():
    """æµ‹è¯•å·¥å…·æ‰§è¡Œ"""
    @tool
    def add(a: int, b: int) -> int:
        return a + b

    result = add.invoke({"a": 2, "b": 3})
    assert result == 5

# ===== é›†æˆæµ‹è¯• =====
@pytest.mark.integration
def test_agent_flow(agent):
    """æµ‹è¯•Agentå®Œæ•´æµç¨‹"""
    result = agent.invoke(
        {"messages": [("user", "æµ‹è¯•")]},
        {"configurable": {"thread_id": "test-1"}}
    )
    assert len(result["messages"]) > 0
```

é…åˆ**ç¬¬16ç« çš„ç”Ÿäº§å®è·µ**å’Œ**ç¬¬äº”ç¯‡çš„Middlewareå·¥ç¨‹åŒ–**ï¼Œä½ å·²ç»å…·å¤‡æ„å»ºä¼ä¸šçº§LangChainåº”ç”¨çš„å®Œæ•´èƒ½åŠ›ã€‚

---

###æ€è€ƒä¸ç»ƒä¹ 

1. **æ€è€ƒ**ï¼šåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥é€‰æ‹© Serverless è€Œä¸æ˜¯å®¹å™¨åŒ–éƒ¨ç½²ï¼Ÿ

   <details>
   <summary>ç­”æ¡ˆ</summary>

   Serverless é€‚åˆï¼š
   - è¯·æ±‚é‡æ³¢åŠ¨å¤§çš„åœºæ™¯
   - å¶å‘æ€§ä»»åŠ¡
   - å¿«é€ŸåŸå‹å¼€å‘
   - æˆæœ¬æ•æ„Ÿä¸”ä½¿ç”¨é‡ä¸å¤§

   å®¹å™¨åŒ–é€‚åˆï¼š
   - ç¨³å®šé«˜å¹¶å‘
   - éœ€è¦æŒä¹…è¿æ¥
   - å¤æ‚çš„æœ‰çŠ¶æ€æœåŠ¡
   - éœ€è¦ç²¾ç»†èµ„æºæ§åˆ¶
   </details>

2. **ç»ƒä¹ **ï¼šè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨é™çº§çš„å¤šå±‚ç¼“å­˜ç³»ç»Ÿã€‚

3. **æ€è€ƒ**ï¼šå¦‚ä½•åœ¨ä¿è¯å®‰å…¨çš„å‰æä¸‹ä¼˜åŒ– PII æ£€æµ‹çš„æ€§èƒ½ï¼Ÿ

---

## ç»“è¯­

æ­å–œä½ å®Œæˆäº† LangChain 1.0 çš„å®Œæ•´å­¦ä¹ ï¼ä»åŸºç¡€æ¦‚å¿µåˆ°ç”Ÿäº§å®è·µï¼Œä½ å·²ç»æŒæ¡äº†æ„å»ºç”Ÿäº§çº§ AI åº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š

1. **å®è·µé¡¹ç›®**ï¼šé€‰æ‹©ä¸€ä¸ªçœŸå®åœºæ™¯ï¼Œä»é›¶æ„å»ºå®Œæ•´åº”ç”¨
2. **æ·±å…¥ç ”ç©¶**ï¼šå…³æ³¨ LangChain ç¤¾åŒºæœ€æ–°å‘å±•
3. **è´¡çŒ®å¼€æº**ï¼šå‚ä¸ LangChain ç”Ÿæ€å»ºè®¾
4. **ç”Ÿäº§è½åœ°**ï¼šå°†æ‰€å­¦åº”ç”¨åˆ°å®é™…äº§å“ä¸­

**æŒç»­å­¦ä¹ èµ„æº**ï¼š

- [LangChain å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com)
- [LangChain GitHub](https://github.com/langchain-ai/langchain)
- [LangChain Discord ç¤¾åŒº](https://discord.gg/langchain)
- [LangChain Blog](https://blog.langchain.com)

ç¥ä½ åœ¨ AI åº”ç”¨å¼€å‘çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼

---

> "The best way to predict the future is to invent it." - Alan Kay

è®©æˆ‘ä»¬ä¸€èµ·ç”¨ LangChain åˆ›é€  AI çš„æœªæ¥ï¼
---


---

## ç¬¬6ç« ï¼šæµ‹è¯•ä¸è´¨é‡ä¿éšœ

> **å…³æ³¨ç‚¹**: æ„å»ºAgentçš„å®Œæ•´æµ‹è¯•ä½“ç³»

### 6.1 æµ‹è¯•é‡‘å­—å¡”

Agentæµ‹è¯•ä¸åŒäºä¼ ç»Ÿè½¯ä»¶æµ‹è¯•ï¼Œéœ€è¦å¤„ç†LLMçš„ä¸ç¡®å®šæ€§ï¼š

```
         /\
        /  \     E2Eè¯„ä¼°ï¼ˆå°‘é‡ã€æ…¢ã€è´µï¼‰
       /____\    - LangSmith Evaluation
      /      \   - çœŸå®åœºæ™¯å›å½’
     /        \
    /          \ é›†æˆæµ‹è¯•ï¼ˆä¸­ç­‰ï¼‰
   /____________\  - Agentå·¥ä½œæµæµ‹è¯•
  /              \ - å·¥å…·è°ƒç”¨é“¾è·¯æµ‹è¯•
 /                \
/__________________\ å•å…ƒæµ‹è¯•ï¼ˆå¤§é‡ã€å¿«ã€ä¾¿å®œï¼‰
                     - Toolå‡½æ•°æµ‹è¯•
                     - Promptæ¨¡æ¿æµ‹è¯•
                     - è¾“å‡ºè§£æå™¨æµ‹è¯•
```

**åŸåˆ™**ï¼š70%å•å…ƒæµ‹è¯• + 20%é›†æˆæµ‹è¯• + 10%E2Eè¯„ä¼°

---

### 6.2 å•å…ƒæµ‹è¯•å®è·µ

#### 6.2.1 Toolå‡½æ•°æµ‹è¯•

```python
# test_tools.py
import pytest
from langchain_core.tools import tool

@tool
def extract_order_id(text: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–è®¢å•å·"""
    import re
    match = re.search(r'è®¢å•å·[ï¼š:]\s*(\w+)', text)
    return match.group(1) if match else "æœªæ‰¾åˆ°è®¢å•å·"

class TestTools:
    """Toolå‡½æ•°å•å…ƒæµ‹è¯•"""

    @pytest.mark.parametrize("text,expected", [
        ("æ‚¨çš„è®¢å•å·ï¼šABC123å·²ç¡®è®¤", "ABC123"),
        ("è®¢å•å·:XYZ789", "XYZ789"),
        ("è¿™æ˜¯ä¸€æ®µæ²¡æœ‰è®¢å•å·çš„æ–‡æœ¬", "æœªæ‰¾åˆ°è®¢å•å·"),
    ])
    def test_extract_order_id(self, text, expected):
        """å‚æ•°åŒ–æµ‹è¯•å¤šç§æ ¼å¼"""
        result = extract_order_id.invoke({"text": text})
        assert result == expected

    def test_tool_metadata(self):
        """æµ‹è¯•Toolå…ƒæ•°æ®"""
        assert extract_order_id.name == "extract_order_id"
        assert "è®¢å•å·" in extract_order_id.description
```

**è¿è¡Œ**ï¼š
```bash
pytest test_tools.py -v
# âœ… test_extract_order_id[...] PASSED
# âœ… test_tool_metadata PASSED
```

---

#### 6.2.2 Agentå·¥ä½œæµé›†æˆæµ‹è¯•

```python
# test_agent_workflow.py
import pytest
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from unittest.mock import Mock, patch

class TestAgentWorkflow:
    """Agenté›†æˆæµ‹è¯•"""

    @pytest.fixture
    def agent(self):
        """åˆ›å»ºæµ‹è¯•ç”¨Agent"""
        return create_agent(
            ChatOpenAI(model="gpt-4o-mini", temperature=0),
            tools=[search_tool, calculator_tool],
            system_prompt="ä½ æ˜¯åŠ©æ‰‹"
        )

    def test_tool_selection(self, agent):
        """æµ‹è¯•å·¥å…·é€‰æ‹©æ­£ç¡®æ€§"""
        result = agent.invoke({
            "messages": [("user", "æœç´¢Python")]
        })

        messages = result["messages"]
        tool_calls = [m for m in messages if hasattr(m, 'name')]

        # éªŒè¯è°ƒç”¨äº†search_tool
        assert any(m.name == "search_tool" for m in tool_calls)

    @patch('langchain_openai.ChatOpenAI')
    def test_with_mocked_llm(self, mock_llm_class):
        """Mock LLMé¿å…çœŸå®APIè°ƒç”¨"""
        mock_llm = Mock()
        mock_llm_class.return_value = mock_llm

        from langchain_core.messages import AIMessage
        mock_llm.invoke.return_value = AIMessage(content="æ¨¡æ‹Ÿå›å¤")

        agent = create_agent(mock_llm, tools=[], system_prompt="æµ‹è¯•")
        result = agent.invoke({"messages": [("user", "æµ‹è¯•")]})

        assert "æ¨¡æ‹Ÿå›å¤" in result["messages"][-1].content
        assert mock_llm.invoke.called
```

---

### 6.3 LangSmithè‡ªåŠ¨åŒ–è¯„ä¼°

#### 6.3.1 åˆ›å»ºè¯„ä¼°Dataset

```python
from langsmith import Client

client = Client()

# åˆ›å»ºDataset
dataset = client.create_dataset(
    dataset_name="customer_support_qa",
    description="å®¢æœé—®ç­”è¯„ä¼°æ•°æ®é›†"
)

# æ·»åŠ æµ‹è¯•ç”¨ä¾‹
test_cases = [
    {
        "inputs": {"question": "å¦‚ä½•é€€æ¬¾ï¼Ÿ"},
        "outputs": {
            "expected_keywords": ["è®¢å•è¯¦æƒ…", "ç”³è¯·é€€æ¬¾"],
            "relevance_score": 5
        }
    },
    # ... æ›´å¤šç”¨ä¾‹
]

for case in test_cases:
    client.create_example(
        dataset_id=dataset.id,
        inputs=case["inputs"],
        outputs=case["outputs"]
    )
```

---

#### 6.3.2 å®šä¹‰Evaluators

```python
from langsmith.evaluation import evaluator

@evaluator
def keyword_coverage_evaluator(run, example):
    """å…³é”®è¯è¦†ç›–ç‡è¯„ä¼°"""
    answer = run.outputs.get("answer", "")
    expected_keywords = example.outputs.get("expected_keywords", [])

    found = [kw for kw in expected_keywords if kw in answer]
    score = len(found) / len(expected_keywords) if expected_keywords else 0

    return {
        "key": "keyword_coverage",
        "score": score,
        "comment": f"è¦†ç›– {len(found)}/{len(expected_keywords)} ä¸ªå…³é”®è¯"
    }

# LLM-as-Judgeè¯„ä¼°å™¨
from langsmith.evaluation import LangChainStringEvaluator

llm_evaluator = LangChainStringEvaluator(
    "cot_qa",
    prepare_data=lambda run, example: {
        "prediction": run.outputs.get("answer", ""),
        "reference": example.outputs.get("expected_answer", ""),
        "input": example.inputs.get("question", "")
    }
)
```

---

#### 6.3.3 è¿è¡Œè¯„ä¼°ä¸A/Bæµ‹è¯•

```python
from langsmith.evaluation import evaluate

# ç‰ˆæœ¬A
def predict_v_a(inputs):
    agent = create_agent(
        ChatOpenAI(model="gpt-4o"),
        tools=[],
        system_prompt="ä½ æ˜¯å®¢æœåŠ©æ‰‹"
    )
    result = agent.invoke({"messages": [("user", inputs["question"])]})
    return {"answer": result["messages"][-1].content}

# ç‰ˆæœ¬Bï¼ˆæ”¹è¿›çš„Promptï¼‰
def predict_v_b(inputs):
    agent = create_agent(
        ChatOpenAI(model="gpt-4o"),
        tools=[],
        system_prompt="""ä½ æ˜¯ä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚

å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®ç†è§£ç”¨æˆ·é—®é¢˜
2. æä¾›æ¸…æ™°çš„æ­¥éª¤è¯´æ˜
3. ä½¿ç”¨å‹å¥½çš„è¯­æ°”"""
    )
    result = agent.invoke({"messages": [("user", inputs["question"])]})
    return {"answer": result["messages"][-1].content}

# A/Bæµ‹è¯•
results_a = evaluate(
    predict_v_a,
    data="customer_support_qa",
    evaluators=[keyword_coverage_evaluator, llm_evaluator],
    experiment_prefix="Version_A"
)

results_b = evaluate(
    predict_v_b,
    data="customer_support_qa",
    evaluators=[keyword_coverage_evaluator, llm_evaluator],
    experiment_prefix="Version_B"
)

# å¯¹æ¯”
print(f"ç‰ˆæœ¬Aå…³é”®è¯è¦†ç›–ç‡: {results_a['keyword_coverage_avg']:.2%}")
print(f"ç‰ˆæœ¬Bå…³é”®è¯è¦†ç›–ç‡: {results_b['keyword_coverage_avg']:.2%}")

improvement = ((results_b['keyword_coverage_avg'] - results_a['keyword_coverage_avg']) /
               results_a['keyword_coverage_avg'] * 100)
print(f"æ”¹è¿›å¹…åº¦: {improvement:+.1f}%")

# ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
from scipy import stats
scores_a = [r.evaluation_results["keyword_coverage"] for r in results_a.results]
scores_b = [r.evaluation_results["keyword_coverage"] for r in results_b.results]

t_stat, p_value = stats.ttest_ind(scores_a, scores_b)
print(f"æ˜¾è‘—æ€§: {'âœ… æ˜¾è‘—' if p_value < 0.05 else 'âš ï¸ ä¸æ˜¾è‘—'} (p={p_value:.4f})")
```

---

### 6.4 CI/CDé›†æˆ

#### 6.4.1 GitHub Actionsé…ç½®

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run Unit Tests
        run: |
          pytest tests/unit -v --cov=src --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run Integration Tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          pytest tests/integration -v

  langsmith-evaluation:
    runs-on: ubuntu-latest
    needs: integration-tests
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Run LangSmith Evaluation
        env:
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/run_evaluation.py

      - name: Quality Gate
        run: |
          python scripts/quality_gate.py
```

---

#### 6.4.2 è´¨é‡é—¨ç¦

```python
# scripts/quality_gate.py
from langsmith.evaluation import evaluate
import sys

def run_quality_gate():
    """è´¨é‡é—¨ç¦æ£€æŸ¥"""
    print("ğŸš¦ è¿è¡Œè´¨é‡é—¨ç¦æ£€æŸ¥...\n")

    results = evaluate(
        predict,
        data="customer_support_qa",
        evaluators=[keyword_coverage_evaluator]
    )

    # è´¨é‡æ ‡å‡†
    quality_standards = {
        "keyword_coverage_avg": 0.80,  # >= 80%
        "pass_rate": 0.90  # >= 90%
    }

    all_passed = True
    for metric, threshold in quality_standards.items():
        actual = results.get(metric, 0)
        passed = actual >= threshold
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{metric}: {actual:.2%} (é˜ˆå€¼: {threshold:.2%}) - {status}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\nâœ… è´¨é‡é—¨ç¦é€šè¿‡ï¼Œå…è®¸éƒ¨ç½²")
        return 0
    else:
        print("\nâŒ è´¨é‡é—¨ç¦å¤±è´¥ï¼Œç¦æ­¢éƒ¨ç½²")
        return 1

if __name__ == "__main__":
    sys.exit(run_quality_gate())
```

---

### 6.5 æµ‹è¯•æœ€ä½³å®è·µ

**æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡**ï¼š

| æµ‹è¯•ç±»å‹ | æ¯”ä¾‹ | é€Ÿåº¦ | æˆæœ¬ | ä½•æ—¶è¿è¡Œ |
|---------|------|------|------|---------|
| å•å…ƒæµ‹è¯• | 70% | å¿«ï¼ˆç§’çº§ï¼‰ | ä½ | æ¯æ¬¡commit |
| é›†æˆæµ‹è¯• | 20% | ä¸­ï¼ˆåˆ†é’Ÿçº§ï¼‰ | ä¸­ | æ¯æ¬¡PR |
| LangSmithè¯„ä¼° | 10% | æ…¢ï¼ˆå°æ—¶çº§ï¼‰ | é«˜ | å‘å¸ƒå‰ |

**å…³é”®åŸåˆ™**ï¼š
1. âœ… å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰Toolå‡½æ•°
2. âœ… é›†æˆæµ‹è¯•è¦†ç›–å…³é”®å·¥ä½œæµ
3. âœ… Mockå¤–éƒ¨ä¾èµ–ï¼ˆLLMã€APIï¼‰é™ä½æˆæœ¬
4. âœ… LangSmithè¯„ä¼°ç”¨äºè´¨é‡å¯¹æ¯”
5. âœ… è´¨é‡é—¨ç¦ç¡®ä¿ç”Ÿäº§æ ‡å‡†
6. âœ… CI/CDè‡ªåŠ¨åŒ–è¿è¡Œ

---

## ç¬¬7ç« ï¼šé”™è¯¯å¤„ç†ä¸é™çº§ç­–ç•¥

> **å…³æ³¨ç‚¹**: ç”Ÿäº§ç¯å¢ƒçš„å®¹é”™ä¸é«˜å¯ç”¨

### 7.1 é”™è¯¯å¤„ç†å±‚æ¬¡

```
ç”¨æˆ·è¯·æ±‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. è¾“å…¥éªŒè¯                  â”‚ â† é˜²æ­¢æ— æ•ˆè¾“å…¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. é‡è¯•æœºåˆ¶                  â”‚ â† å¤„ç†ä¸´æ—¶æ•…éšœ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. é™çº§ç­–ç•¥                  â”‚ â† ä¿è¯åŸºæœ¬æœåŠ¡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å…œåº•å“åº”                  â”‚ â† ä¼˜é›…å¤±è´¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    è¿”å›ç»“æœ
```

---

### 7.2 è¾“å…¥éªŒè¯

```python
from pydantic import BaseModel, Field, validator
from typing import Optional

class AgentRequest(BaseModel):
    """Agentè¯·æ±‚éªŒè¯"""
    query: str = Field(..., min_length=1, max_length=1000)
    user_id: str = Field(..., pattern=r'^[a-zA-Z0-9_-]+$')
    context: Optional[dict] = None

    @validator('query')
    def validate_query(cls, v):
        """éªŒè¯æŸ¥è¯¢å†…å®¹"""
        # æ£€æµ‹æ¶æ„è¾“å…¥
        malicious_patterns = ['<script>', 'DROP TABLE', 'rm -rf']
        if any(pattern in v for pattern in malicious_patterns):
            raise ValueError("æ£€æµ‹åˆ°æ¶æ„è¾“å…¥")

        # æ£€æµ‹æ•æ„Ÿä¿¡æ¯
        import re
        if re.search(r'\d{15,19}', v):  # ä¿¡ç”¨å¡å·æ¨¡å¼
            raise ValueError("è¯·å‹¿è¾“å…¥æ•æ„Ÿä¿¡æ¯")

        return v

# ä½¿ç”¨
try:
    request = AgentRequest(
        query="å¸®æˆ‘æŸ¥è¯¢è®¢å•",
        user_id="user_123"
    )
    result = agent.invoke({"messages": [("user", request.query)]})
except ValueError as e:
    return {"error": str(e), "code": "INVALID_INPUT"}
```

---

### 7.3 æ™ºèƒ½é‡è¯•æœºåˆ¶

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from langchain_openai import ChatOpenAI
import logging

logger = logging.getLogger(__name__)

class RetryableAgent:
    """å¸¦é‡è¯•çš„Agent"""

    def __init__(self):
        self.agent = create_agent(
            ChatOpenAI(model="gpt-4o"),
            tools=[],
            system_prompt="ä½ æ˜¯åŠ©æ‰‹"
        )

    @retry(
        stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯•3æ¬¡
        wait=wait_exponential(multiplier=1, min=1, max=10),  # æŒ‡æ•°é€€é¿
        retry=retry_if_exception_type((TimeoutError, ConnectionError)),
        before_sleep=lambda retry_state: logger.warning(
            f"é‡è¯• {retry_state.attempt_number}/3: {retry_state.outcome.exception()}"
        )
    )
    def invoke_with_retry(self, inputs: dict) -> dict:
        """å¸¦é‡è¯•çš„è°ƒç”¨"""
        try:
            return self.agent.invoke(inputs)
        except Exception as e:
            logger.error(f"Agentè°ƒç”¨å¤±è´¥: {e}")
            raise

# ä½¿ç”¨
agent = RetryableAgent()
try:
    result = agent.invoke_with_retry({"messages": [("user", "æµ‹è¯•")]})
except Exception:
    # é‡è¯•3æ¬¡åä»å¤±è´¥
    result = {"error": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"}
```

---

### 7.4 å¤šçº§é™çº§ç­–ç•¥

```python
from enum import Enum
from typing import Dict, Any

class FallbackLevel(Enum):
    """é™çº§çº§åˆ«"""
    PRIMARY = 1      # GPT-4oï¼ˆæœ€ä¼˜ï¼‰
    SECONDARY = 2    # GPT-4o-miniï¼ˆæ¬¡ä¼˜ï¼‰
    TERTIARY = 3     # GPT-3.5-turboï¼ˆä¿åº•ï¼‰
    CACHE = 4        # ç¼“å­˜ç»“æœ
    STATIC = 5       # é™æ€å›å¤

class FallbackAgent:
    """å¤šçº§é™çº§Agent"""

    def __init__(self):
        self.cache = {}  # ç®€å•ç¼“å­˜
        self.fallback_responses = {
            "default": "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "timeout": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "rate_limit": "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        }

    def invoke(self, inputs: dict) -> Dict[str, Any]:
        """å¤šçº§é™çº§è°ƒç”¨"""
        query = inputs["messages"][0][1]

        # çº§åˆ«1ï¼šGPT-4o
        try:
            return self._invoke_primary(inputs)
        except Exception as e:
            logger.warning(f"Primaryå¤±è´¥: {e}")

        # çº§åˆ«2ï¼šGPT-4o-mini
        try:
            return self._invoke_secondary(inputs)
        except Exception as e:
            logger.warning(f"Secondaryå¤±è´¥: {e}")

        # çº§åˆ«3ï¼šGPT-3.5-turbo
        try:
            return self._invoke_tertiary(inputs)
        except Exception as e:
            logger.warning(f"Tertiaryå¤±è´¥: {e}")

        # çº§åˆ«4ï¼šç¼“å­˜
        cached = self.cache.get(query)
        if cached:
            logger.info("ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return {"messages": [cached], "fallback_level": FallbackLevel.CACHE}

        # çº§åˆ«5ï¼šé™æ€å›å¤
        logger.error("æ‰€æœ‰é™çº§ç­–ç•¥å¤±è´¥ï¼Œä½¿ç”¨é™æ€å›å¤")
        return {
            "messages": [("assistant", self.fallback_responses["default"])],
            "fallback_level": FallbackLevel.STATIC
        }

    def _invoke_primary(self, inputs: dict) -> dict:
        """ä¸»è¦æ¨¡å‹"""
        agent = create_agent(ChatOpenAI(model="gpt-4o", timeout=5), tools=[], system_prompt="ä½ æ˜¯åŠ©æ‰‹")
        result = agent.invoke(inputs)
        return {**result, "fallback_level": FallbackLevel.PRIMARY}

    def _invoke_secondary(self, inputs: dict) -> dict:
        """æ¬¡ä¼˜æ¨¡å‹"""
        agent = create_agent(ChatOpenAI(model="gpt-4o-mini", timeout=3), tools=[], system_prompt="ä½ æ˜¯åŠ©æ‰‹")
        result = agent.invoke(inputs)
        return {**result, "fallback_level": FallbackLevel.SECONDARY}

    def _invoke_tertiary(self, inputs: dict) -> dict:
        """ä¿åº•æ¨¡å‹"""
        agent = create_agent(ChatOpenAI(model="gpt-3.5-turbo", timeout=2), tools=[], system_prompt="ä½ æ˜¯åŠ©æ‰‹")
        result = agent.invoke(inputs)
        return {**result, "fallback_level": FallbackLevel.TERTIARY}
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
agent = FallbackAgent()
result = agent.invoke({"messages": [("user", "ä½ å¥½")]})

print(f"å“åº”: {result['messages'][-1].content}")
print(f"é™çº§çº§åˆ«: {result['fallback_level'].name}")

# ç›‘æ§é™çº§ç‡
from collections import Counter
fallback_counter = Counter()

for _ in range(100):
    result = agent.invoke({"messages": [("user", "æµ‹è¯•")]})
    fallback_counter[result['fallback_level'].name] += 1

print("\né™çº§ç»Ÿè®¡:")
for level, count in fallback_counter.items():
    print(f"  {level}: {count}æ¬¡ ({count/100:.1%})")

# è¾“å‡º:
# PRIMARY: 85æ¬¡ (85.0%)
# SECONDARY: 10æ¬¡ (10.0%)
# TERTIARY: 3æ¬¡ (3.0%)
# CACHE: 2æ¬¡ (2.0%)
```

---

### 7.5 ç†”æ–­å™¨æ¨¡å¼

```python
from datetime import datetime, timedelta
from enum import Enum

class CircuitState(Enum):
    """ç†”æ–­å™¨çŠ¶æ€"""
    CLOSED = "closed"      # æ­£å¸¸
    OPEN = "open"          # ç†”æ–­ï¼ˆæ‹’ç»è¯·æ±‚ï¼‰
    HALF_OPEN = "half_open"  # åŠå¼€ï¼ˆå°è¯•æ¢å¤ï¼‰

class CircuitBreaker:
    """ç†”æ–­å™¨"""

    def __init__(
        self,
        failure_threshold: int = 5,  # å¤±è´¥é˜ˆå€¼
        timeout: int = 60,  # ç†”æ–­è¶…æ—¶(ç§’)
        half_open_max_calls: int = 3  # åŠå¼€çŠ¶æ€æœ€å¤§å°è¯•æ¬¡æ•°
    ):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.half_open_max_calls = half_open_max_calls

        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.last_failure_time = None
        self.half_open_calls = 0

    def call(self, func, *args, **kwargs):
        """æ‰§è¡Œè°ƒç”¨"""
        if self.state == CircuitState.OPEN:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬ä¸ºåŠå¼€çŠ¶æ€
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.state = CircuitState.HALF_OPEN
                self.half_open_calls = 0
                logger.info("ç†”æ–­å™¨è½¬ä¸ºåŠå¼€çŠ¶æ€")
            else:
                raise Exception("ç†”æ–­å™¨å¼€å¯ï¼Œæ‹’ç»è¯·æ±‚")

        try:
            result = func(*args, **kwargs)

            # æˆåŠŸ
            if self.state == CircuitState.HALF_OPEN:
                self.half_open_calls += 1
                if self.half_open_calls >= self.half_open_max_calls:
                    self.state = CircuitState.CLOSED
                    self.failure_count = 0
                    logger.info("ç†”æ–­å™¨æ¢å¤æ­£å¸¸")

            return result

        except Exception as e:
            # å¤±è´¥
            self.failure_count += 1
            self.last_failure_time = datetime.now()

            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                logger.error(f"ç†”æ–­å™¨å¼€å¯ï¼ˆå¤±è´¥{self.failure_count}æ¬¡ï¼‰")

            raise

# ä½¿ç”¨
circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=60)

def invoke_agent(inputs):
    """è°ƒç”¨Agentï¼ˆå¯èƒ½å¤±è´¥ï¼‰"""
    return circuit_breaker.call(
        lambda: agent.invoke(inputs)
    )

# æµ‹è¯•
for i in range(10):
    try:
        result = invoke_agent({"messages": [("user", "æµ‹è¯•")]})
        print(f"è¯·æ±‚{i}: âœ… æˆåŠŸ")
    except Exception as e:
        print(f"è¯·æ±‚{i}: âŒ {e}")
```

---

### 7.6 ç›‘æ§ä¸å‘Šè­¦

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List
import time

@dataclass
class ErrorMetrics:
    """é”™è¯¯æŒ‡æ ‡"""
    timestamp: datetime
    error_type: str
    error_message: str
    fallback_level: Optional[str] = None

class ErrorMonitor:
    """é”™è¯¯ç›‘æ§"""

    def __init__(self):
        self.errors: List[ErrorMetrics] = []

    def record_error(
        self,
        error_type: str,
        error_message: str,
        fallback_level: Optional[str] = None
    ):
        """è®°å½•é”™è¯¯"""
        self.errors.append(ErrorMetrics(
            timestamp=datetime.now(),
            error_type=error_type,
            error_message=error_message,
            fallback_level=fallback_level
        ))

    def get_error_rate(self, window_minutes: int = 5) -> float:
        """è·å–é”™è¯¯ç‡"""
        cutoff_time = datetime.now() - timedelta(minutes=window_minutes)
        recent_errors = [
            e for e in self.errors
            if e.timestamp > cutoff_time
        ]
        # å‡è®¾æ€»è¯·æ±‚æ•°
        total_requests = 100
        return len(recent_errors) / total_requests if total_requests else 0

    def check_alerts(self) -> List[str]:
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = []

        # é”™è¯¯ç‡å‘Šè­¦
        error_rate = self.get_error_rate(window_minutes=5)
        if error_rate > 0.1:  # 10%
            alerts.append(f"âš ï¸  é”™è¯¯ç‡è¿‡é«˜: {error_rate:.1%}")

        # é™çº§å‘Šè­¦
        recent_fallbacks = [
            e for e in self.errors[-100:]
            if e.fallback_level in ["TERTIARY", "CACHE", "STATIC"]
        ]
        if len(recent_fallbacks) > 20:
            alerts.append(f"âš ï¸  é¢‘ç¹é™çº§: {len(recent_fallbacks)}æ¬¡")

        return alerts
```

---

### 7.7 é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

**ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•**ï¼š

- [ ] **è¾“å…¥éªŒè¯**
  - [ ] é•¿åº¦é™åˆ¶
  - [ ] æ ¼å¼éªŒè¯
  - [ ] æ¶æ„è¾“å…¥æ£€æµ‹
  - [ ] æ•æ„Ÿä¿¡æ¯è¿‡æ»¤

- [ ] **é‡è¯•æœºåˆ¶**
  - [ ] æŒ‡æ•°é€€é¿
  - [ ] æœ€å¤§é‡è¯•æ¬¡æ•°
  - [ ] å¯é‡è¯•å¼‚å¸¸ç±»å‹

- [ ] **é™çº§ç­–ç•¥**
  - [ ] å¤šçº§æ¨¡å‹é™çº§
  - [ ] ç¼“å­˜é™çº§
  - [ ] é™æ€å“åº”å…œåº•

- [ ] **ç†”æ–­ä¿æŠ¤**
  - [ ] å¤±è´¥é˜ˆå€¼é…ç½®
  - [ ] ç†”æ–­è¶…æ—¶é…ç½®
  - [ ] åŠå¼€çŠ¶æ€å°è¯•

- [ ] **ç›‘æ§å‘Šè­¦**
  - [ ] é”™è¯¯ç‡ç›‘æ§
  - [ ] é™çº§ç‡ç›‘æ§
  - [ ] å‘Šè­¦è§„åˆ™é…ç½®

**å…³é”®æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å‘Šè­¦é˜ˆå€¼ |
|------|--------|---------|
| é”™è¯¯ç‡ | < 1% | > 5% |
| é™çº§ç‡ | < 5% | > 20% |
| é‡è¯•æˆåŠŸç‡ | > 80% | < 50% |
| ç†”æ–­è§¦å‘æ¬¡æ•° | 0 | > 3æ¬¡/å°æ—¶ |

---

