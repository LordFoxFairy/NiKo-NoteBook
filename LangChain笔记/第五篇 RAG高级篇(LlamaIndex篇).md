# ç¬¬äº”ç¯‡è¡¥å……ï¼šRAGé«˜çº§ç¯‡ (LlamaIndex)

> **ç‰ˆæœ¬è¦æ±‚**:
> - llama-index-core: 0.14.8
> - llama-index-llms-openai: 0.2.0+
> - llama-index-retrievers-bm25: æœ€æ–°ç‰ˆ
> - llama-index-postprocessor-cohere-rerank: æœ€æ–°ç‰ˆ
> - Python: 3.10+
> - æ›´æ–°æ—¥æœŸ: 2025-11-29

## æ¦‚è¿°

LlamaIndex æ˜¯ä¸“ä¸º LLM åº”ç”¨è®¾è®¡çš„æ•°æ®æ¡†æ¶ï¼Œç‰¹åˆ«æ“…é•¿æ„å»ºé«˜çº§ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿã€‚æœ¬ç¯‡å°†æ·±å…¥æ¢è®¨ LlamaIndex çš„é«˜çº§æ£€ç´¢æŠ€æœ¯ã€æŸ¥è¯¢ä¼˜åŒ–ã€é‡æ’åºå’ŒçŸ¥è¯†å›¾è°±é›†æˆç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

**æœ¬ç¯‡æ ¸å¿ƒå†…å®¹**ï¼š
- æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡æ£€ç´¢ï¼‰
- æŸ¥è¯¢ä¼˜åŒ–ä¸è½¬æ¢
- é«˜çº§ Query Engineï¼ˆRouterã€SubQuestionã€MultiStepï¼‰
- é‡æ’åºæŠ€æœ¯ï¼ˆCohere Rerankã€ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰
- Chat Engine å¯¹è¯ç³»ç»Ÿ
- çŸ¥è¯†å›¾è°± RAG
- Agent ä¸ RAG çš„ç»“åˆ

---

## ç¬¬ 1 ç« ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Retrievalï¼‰

æ··åˆæ£€ç´¢ç»“åˆäº†åŸºäºå…³é”®è¯çš„ BM25 æ£€ç´¢å’ŒåŸºäºè¯­ä¹‰çš„å‘é‡æ£€ç´¢ï¼Œèƒ½å¤ŸåŒæ—¶æ•è·ç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰ç›¸å…³æ€§ã€‚

### 1.1 BM25 æ£€ç´¢å™¨

BM25ï¼ˆBest Matching 25ï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„ TF-IDF ç®—æ³•ï¼Œé€šè¿‡è¯é¢‘é¥±å’Œåº¦å’Œæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–æä¾›æ›´å¥½çš„æ’åºæ•ˆæœã€‚

#### åŸºç¡€ç”¨æ³•

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.retrievers.bm25 import BM25Retriever
import Stemmer

# 1. åŠ è½½å¹¶è§£ææ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()
splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
nodes = splitter.get_nodes_from_documents(documents)

# 2. åˆ›å»º BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=5,
    stemmer=Stemmer.Stemmer("english"),  # è¯å¹²æå–
    language="english"
)

# 3. æ‰§è¡Œæ£€ç´¢
query = "What is artificial intelligence?"
retrieved_nodes = bm25_retriever.retrieve(query)

for node in retrieved_nodes:
    print(f"Score: {node.score:.4f}")
    print(f"Text: {node.text[:200]}...\n")
```

#### æŒä¹…åŒ–ä¸åŠ è½½

```python
# ä¿å­˜åˆ°ç£ç›˜
bm25_retriever.persist("./storage/bm25_retriever")

# ä»ç£ç›˜åŠ è½½
from llama_index.retrievers.bm25 import BM25Retriever
loaded_retriever = BM25Retriever.from_persist_dir("./storage/bm25_retriever")
```

#### ä½¿ç”¨ Docstore

```python
from llama_index.core.storage.docstore import SimpleDocumentStore

# åˆ›å»º docstore å¹¶æ·»åŠ èŠ‚ç‚¹
docstore = SimpleDocumentStore()
docstore.add_documents(nodes)

# ä» docstore åˆ›å»º BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_defaults(
    docstore=docstore,
    similarity_top_k=5
)
```

### 1.2 å‘é‡æ£€ç´¢å™¨

ä½¿ç”¨å‘é‡åµŒå…¥è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼š

```python
from llama_index.core import VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# é…ç½® LLM å’Œ Embedding
Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# åˆ›å»ºå‘é‡ç´¢å¼•
vector_index = VectorStoreIndex(nodes=nodes)

# åˆ›å»ºå‘é‡æ£€ç´¢å™¨
vector_retriever = vector_index.as_retriever(similarity_top_k=5)

# æ‰§è¡Œæ£€ç´¢
retrieved_nodes = vector_retriever.retrieve(query)
```

### 1.3 æ··åˆæ£€ç´¢ï¼šQueryFusionRetriever

ç»“åˆ BM25 å’Œå‘é‡æ£€ç´¢çš„ä¼˜åŠ¿ï¼š

```python
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# 1. è®¾ç½® Chroma å‘é‡å­˜å‚¨
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("hybrid_retrieval")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# 2. åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
from llama_index.core import StorageContext
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# 3. åˆ›å»ºå‘é‡ç´¢å¼•
vector_index = VectorStoreIndex(
    nodes=nodes,
    storage_context=storage_context
)

# 4. åˆ›å»ºæ··åˆæ£€ç´¢å™¨
hybrid_retriever = QueryFusionRetriever(
    retrievers=[
        vector_index.as_retriever(similarity_top_k=5),  # å‘é‡æ£€ç´¢
        BM25Retriever.from_defaults(
            docstore=vector_index.docstore,
            similarity_top_k=5
        )  # BM25 æ£€ç´¢
    ],
    num_queries=1,  # ä¸ç”Ÿæˆé¢å¤–æŸ¥è¯¢å˜ä½“
    use_async=True,  # å¼‚æ­¥å¹¶è¡Œæ£€ç´¢
    similarity_top_k=10  # èåˆåè¿”å›çš„èŠ‚ç‚¹æ•°
)

# 5. æ‰§è¡Œæ··åˆæ£€ç´¢
retrieved_nodes = hybrid_retriever.retrieve(query)

print(f"Retrieved {len(retrieved_nodes)} nodes from hybrid search")
```

### 1.4 å…ƒæ•°æ®è¿‡æ»¤

åœ¨æ£€ç´¢æ—¶åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼š

```python
from llama_index.core.vector_stores.types import (
    MetadataFilters,
    MetadataFilter,
    FilterOperator
)

# å®šä¹‰è¿‡æ»¤æ¡ä»¶
filters = MetadataFilters(
    filters=[
        MetadataFilter(
            key="category",
            value="technology",
            operator=FilterOperator.EQ
        ),
        MetadataFilter(
            key="publish_date",
            value="2024-01-01",
            operator=FilterOperator.GTE
        )
    ]
)

# åº”ç”¨è¿‡æ»¤å™¨
filtered_retriever = BM25Retriever.from_defaults(
    docstore=docstore,
    similarity_top_k=5,
    filters=filters
)

# æˆ–åœ¨å‘é‡æ£€ç´¢ä¸­ä½¿ç”¨
vector_retriever = vector_index.as_retriever(
    similarity_top_k=5,
    filters=filters
)
```

### 1.5 ä½¿ç”¨ Query Engine

å°†æ··åˆæ£€ç´¢å™¨é›†æˆåˆ°æŸ¥è¯¢å¼•æ“ï¼š

```python
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import get_response_synthesizer

# åˆ›å»ºå“åº”åˆæˆå™¨
response_synthesizer = get_response_synthesizer(
    response_mode="compact",  # ç´§å‡‘æ¨¡å¼
    use_async=True
)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = RetrieverQueryEngine(
    retriever=hybrid_retriever,
    response_synthesizer=response_synthesizer
)

# æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query(
    "Explain the key concepts of machine learning"
)
print(response)

# æŸ¥çœ‹æºèŠ‚ç‚¹
for node in response.source_nodes:
    print(f"\nSource: {node.metadata.get('file_name', 'Unknown')}")
    print(f"Score: {node.score:.4f}")
    print(f"Text: {node.text[:150]}...")
```

---

## ç¬¬ 2 ç« ï¼šæŸ¥è¯¢ä¼˜åŒ–ï¼ˆQuery Optimizationï¼‰

æŸ¥è¯¢ä¼˜åŒ–é€šè¿‡è½¬æ¢ã€æ‰©å±•æˆ–é‡å†™ç”¨æˆ·æŸ¥è¯¢æ¥æé«˜æ£€ç´¢è´¨é‡ã€‚

### 2.1 æŸ¥è¯¢è½¬æ¢

#### HyDEï¼ˆHypothetical Document Embeddingsï¼‰

HyDE ç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆæ–‡æ¡£ï¼Œç„¶åç”¨è¯¥æ–‡æ¡£çš„åµŒå…¥è¿›è¡Œæ£€ç´¢ï¼š

```python
from llama_index.core.indices.query.query_transform import HyDEQueryTransform
from llama_index.core.query_engine import TransformQueryEngine

# 1. åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“
base_query_engine = vector_index.as_query_engine(similarity_top_k=5)

# 2. åˆ›å»º HyDE è½¬æ¢å™¨
hyde_transform = HyDEQueryTransform(include_original=True)

# 3. åŒ…è£…æŸ¥è¯¢å¼•æ“
hyde_query_engine = TransformQueryEngine(
    base_query_engine,
    query_transform=hyde_transform
)

# 4. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¼šå…ˆç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ï¼‰
response = hyde_query_engine.query(
    "What are the benefits of using LlamaIndex?"
)
print(response)
```

**å·¥ä½œåŸç†**ï¼š
1. ç”¨æˆ·æŸ¥è¯¢ â†’ LLM ç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆ
2. ä½¿ç”¨å‡è®¾æ€§ç­”æ¡ˆçš„åµŒå…¥è¿›è¡Œæ£€ç´¢
3. æ£€ç´¢åˆ°çš„çœŸå®æ–‡æ¡£ç”¨äºç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

#### å¤šæŸ¥è¯¢ç”Ÿæˆ

ç”ŸæˆæŸ¥è¯¢çš„å¤šä¸ªå˜ä½“ä»¥æé«˜å¬å›ç‡ï¼š

```python
from llama_index.core.retrievers import QueryFusionRetriever

# QueryFusionRetriever å¯ä»¥è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢å˜ä½“
fusion_retriever = QueryFusionRetriever(
    retrievers=[vector_retriever],
    num_queries=4,  # ç”Ÿæˆ 4 ä¸ªæŸ¥è¯¢å˜ä½“
    use_async=True,
    mode="reciprocal_rerank"  # ä½¿ç”¨å€’æ•°æ’åèåˆ
)

retrieved_nodes = fusion_retriever.retrieve(
    "How does neural network training work?"
)
```

### 2.2 æŸ¥è¯¢åˆ†è§£ï¼ˆQuery Decompositionï¼‰

å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œåˆ†è§£æˆå¤šä¸ªå­æŸ¥è¯¢ï¼š

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# 1. åˆ›å»ºä¸“é—¨çš„æŸ¥è¯¢å¼•æ“
ml_query_engine = ml_index.as_query_engine(similarity_top_k=3)
dl_query_engine = dl_index.as_query_engine(similarity_top_k=3)
nlp_query_engine = nlp_index.as_query_engine(similarity_top_k=3)

# 2. å®šä¹‰æŸ¥è¯¢å·¥å…·
query_engine_tools = [
    QueryEngineTool(
        query_engine=ml_query_engine,
        metadata=ToolMetadata(
            name="machine_learning",
            description="Provides information about traditional machine learning algorithms and techniques"
        )
    ),
    QueryEngineTool(
        query_engine=dl_query_engine,
        metadata=ToolMetadata(
            name="deep_learning",
            description="Provides information about deep learning, neural networks, and modern AI architectures"
        )
    ),
    QueryEngineTool(
        query_engine=nlp_query_engine,
        metadata=ToolMetadata(
            name="natural_language_processing",
            description="Provides information about NLP techniques, language models, and text processing"
        )
    )
]

# 3. åˆ›å»º SubQuestionQueryEngine
from llama_index.core.query_engine import SubQuestionQueryEngine

sub_question_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    use_async=True,
    verbose=True  # æ˜¾ç¤ºå­æŸ¥è¯¢ç”Ÿæˆè¿‡ç¨‹
)

# 4. æ‰§è¡Œå¤æ‚æŸ¥è¯¢
response = sub_question_engine.query(
    "Compare traditional machine learning with deep learning approaches "
    "and explain how they are used in natural language processing"
)

print(response)

# æŸ¥çœ‹ç”Ÿæˆçš„å­æŸ¥è¯¢
for sub_q in response.metadata.get("sub_questions", []):
    print(f"\nSub-question: {sub_q.sub_question}")
    print(f"Tool used: {sub_q.tool_name}")
```

### 2.3 æŸ¥è¯¢é‡å†™

ä½¿ç”¨ LLM æ”¹è¿›æŸ¥è¯¢è¡¨è¾¾ï¼š

```python
from llama_index.core.indices.query.query_transform import DecomposeQueryTransform

# åˆ›å»ºæŸ¥è¯¢åˆ†è§£è½¬æ¢å™¨
decompose_transform = DecomposeQueryTransform(
    llm=Settings.llm,
    verbose=True
)

# åº”ç”¨åˆ°æŸ¥è¯¢å¼•æ“
decompose_query_engine = TransformQueryEngine(
    base_query_engine,
    query_transform=decompose_transform
)

response = decompose_query_engine.query(
    "Tell me about AI and its applications"
)
```

---

## ç¬¬ 3 ç« ï¼šé«˜çº§ Query Engine

### 3.1 RouterQueryEngine

æ ¹æ®æŸ¥è¯¢å†…å®¹å°†è¯·æ±‚è·¯ç”±åˆ°æœ€åˆé€‚çš„æŸ¥è¯¢å¼•æ“ã€‚

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector
from llama_index.core.tools import QueryEngineTool

# 1. åˆ›å»ºå¤šä¸ªä¸“é—¨çš„æŸ¥è¯¢å¼•æ“
summary_query_engine = summary_index.as_query_engine(
    response_mode="tree_summarize"
)

vector_query_engine = vector_index.as_query_engine(
    similarity_top_k=5
)

# 2. å®šä¹‰æŸ¥è¯¢å·¥å…·
query_engine_tools = [
    QueryEngineTool(
        query_engine=summary_query_engine,
        metadata=ToolMetadata(
            name="summary_tool",
            description=(
                "Use this tool for questions that require summarization "
                "or high-level overview of documents"
            )
        )
    ),
    QueryEngineTool(
        query_engine=vector_query_engine,
        metadata=ToolMetadata(
            name="vector_tool",
            description=(
                "Use this tool for specific factual questions "
                "that require precise information retrieval"
            )
        )
    )
]

# 3. åˆ›å»ºè·¯ç”±å™¨æŸ¥è¯¢å¼•æ“
router_query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=query_engine_tools,
    verbose=True
)

# 4. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼‰
# è¿™ä¸ªæŸ¥è¯¢ä¼šè·¯ç”±åˆ° summary_tool
response1 = router_query_engine.query(
    "Give me an overview of all the documents"
)

# è¿™ä¸ªæŸ¥è¯¢ä¼šè·¯ç”±åˆ° vector_tool
response2 = router_query_engine.query(
    "What is the exact definition of transformer architecture?"
)

print(f"Query 1 used: {response1.metadata.get('selector_result')}")
print(f"Query 2 used: {response2.metadata.get('selector_result')}")
```

### 3.2 SubQuestionQueryEngine

å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜å¹¶åˆ†åˆ«å›ç­”ï¼ˆè§ 2.2 èŠ‚ï¼‰ã€‚

**é«˜çº§é…ç½®**ï¼š

```python
from llama_index.core.question_gen import LLMQuestionGenerator
from llama_index.core.question_gen.prompts import (
    DEFAULT_SUB_QUESTION_PROMPT_TMPL
)

# è‡ªå®šä¹‰å­é—®é¢˜ç”Ÿæˆå™¨
custom_question_gen = LLMQuestionGenerator.from_defaults(
    llm=Settings.llm,
    prompt_template_str="""
Given a user question, generate {num_questions} related sub-questions
that help answer the original question comprehensively.

Original Question: {question}

Sub-questions:
"""
)

sub_question_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    question_gen=custom_question_gen,
    use_async=True
)
```

### 3.3 MultiStepQueryEngine

æ‰§è¡Œå¤šæ­¥éª¤æ¨ç†æŸ¥è¯¢ï¼š

```python
from llama_index.core.query_engine import MultiStepQueryEngine
from llama_index.core.indices.query.query_transform import (
    StepDecomposeQueryTransform
)

# 1. åˆ›å»ºæ­¥éª¤åˆ†è§£è½¬æ¢å™¨
step_decompose_transform = StepDecomposeQueryTransform(
    llm=Settings.llm,
    verbose=True
)

# 2. åˆ›å»ºå¤šæ­¥éª¤æŸ¥è¯¢å¼•æ“
multi_step_engine = MultiStepQueryEngine(
    query_engine=base_query_engine,
    query_transform=step_decompose_transform,
    num_steps=3,  # æœ€å¤š 3 ä¸ªæ¨ç†æ­¥éª¤
    index_summary="This index contains technical documentation about AI"
)

# 3. æ‰§è¡Œéœ€è¦å¤šæ­¥æ¨ç†çš„æŸ¥è¯¢
response = multi_step_engine.query(
    "First explain what transformers are, then describe how they are used "
    "in modern language models, and finally compare them with RNNs"
)

print(response)
```

### 3.4 RetrieverQueryEngine

ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨çš„æŸ¥è¯¢å¼•æ“ï¼ˆè§ 1.5 èŠ‚ï¼‰ã€‚

**é«˜çº§å“åº”åˆæˆæ¨¡å¼**ï¼š

```python
from llama_index.core.response_synthesizers import ResponseMode

# compact: åˆå¹¶æ–‡æœ¬å—ç›´åˆ°è¾¾åˆ° token é™åˆ¶
query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    response_mode=ResponseMode.COMPACT
)

# refine: é€ä¸ªå¤„ç†æ–‡æœ¬å—ï¼Œä¸æ–­ç²¾ç‚¼ç­”æ¡ˆ
query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    response_mode=ResponseMode.REFINE
)

# tree_summarize: ä½¿ç”¨æ ‘å½¢ç»“æ„æ±‡æ€»
query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    response_mode=ResponseMode.TREE_SUMMARIZE
)

# simple_summarize: æˆªæ–­æ‰€æœ‰æ–‡æœ¬å—å¹¶ä¸€æ¬¡æ€§å‘é€
query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    response_mode=ResponseMode.SIMPLE_SUMMARIZE
)
```

---

## ç¬¬ 4 ç« ï¼šé‡æ’åºæŠ€æœ¯ï¼ˆRerankingï¼‰

é‡æ’åºåœ¨æ£€ç´¢åå¯¹å€™é€‰æ–‡æ¡£é‡æ–°è¯„åˆ†ï¼Œæé«˜æœ€ç»ˆç»“æœçš„ç›¸å…³æ€§ã€‚

### 4.1 ç›¸ä¼¼åº¦åå¤„ç†å™¨

åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœï¼š

```python
from llama_index.core.postprocessor import SimilarityPostprocessor

# åˆ›å»ºç›¸ä¼¼åº¦åå¤„ç†å™¨
similarity_processor = SimilarityPostprocessor(
    similarity_cutoff=0.75  # åªä¿ç•™ç›¸ä¼¼åº¦ >= 0.75 çš„èŠ‚ç‚¹
)

# åº”ç”¨åˆ°æŸ¥è¯¢å¼•æ“
query_engine = vector_index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[similarity_processor]
)

response = query_engine.query("What is machine learning?")

# æŸ¥çœ‹è¿‡æ»¤åçš„èŠ‚ç‚¹
print(f"Returned {len(response.source_nodes)} nodes (filtered from 10)")
for node in response.source_nodes:
    print(f"Score: {node.score:.4f}")
```

### 4.2 Cohere Rerank

ä½¿ç”¨ Cohere çš„ä¸“ä¸šé‡æ’åºæ¨¡å‹ï¼š

```python
from llama_index.postprocessor.cohere_rerank import CohereRerank

# 1. åˆ›å»º Cohere Rerank åå¤„ç†å™¨
cohere_rerank = CohereRerank(
    api_key="your-cohere-api-key",
    top_n=3,  # é‡æ’åè¿”å›å‰ 3 ä¸ªç»“æœ
    model="rerank-english-v3.0"
)

# 2. åº”ç”¨åˆ°æŸ¥è¯¢å¼•æ“
query_engine = vector_index.as_query_engine(
    similarity_top_k=10,  # å…ˆæ£€ç´¢ 10 ä¸ªå€™é€‰
    node_postprocessors=[cohere_rerank]  # é‡æ’åè¿”å› top 3
)

# 3. æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query(
    "What are the latest developments in large language models?"
)

print(response)

# æŸ¥çœ‹é‡æ’åºåçš„åˆ†æ•°
for idx, node in enumerate(response.source_nodes, 1):
    print(f"\n{idx}. Rerank Score: {node.score:.4f}")
    print(f"Text: {node.text[:200]}...")
```

**Cohere Rerank ä¼˜åŠ¿**ï¼š
- ä¸“é—¨è®­ç»ƒçš„è·¨è¯­è¨€é‡æ’åºæ¨¡å‹
- æ”¯æŒ 100+ ç§è¯­è¨€
- æ¯”ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—æ›´å‡†ç¡®
- è€ƒè™‘æŸ¥è¯¢å’Œæ–‡æ¡£çš„æ·±å±‚è¯­ä¹‰å…³ç³»

### 4.3 SentenceTransformer Rerank

ä½¿ç”¨ Sentence Transformers è¿›è¡Œé‡æ’åºï¼š

```python
from llama_index.core.postprocessor import SentenceTransformerRerank

# åˆ›å»º SentenceTransformer Rerank åå¤„ç†å™¨
sentence_rerank = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-6-v2",  # è·¨ç¼–ç å™¨æ¨¡å‹
    top_n=5
)

query_engine = vector_index.as_query_engine(
    similarity_top_k=15,
    node_postprocessors=[sentence_rerank]
)

response = query_engine.query("Explain transformer architecture")
```

**è·¨ç¼–ç å™¨ vs åŒç¼–ç å™¨**ï¼š
- **åŒç¼–ç å™¨**ï¼ˆç”¨äºåˆå§‹æ£€ç´¢ï¼‰ï¼šåˆ†åˆ«ç¼–ç æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œé€‚åˆå¤§è§„æ¨¡æ£€ç´¢
- **è·¨ç¼–ç å™¨**ï¼ˆç”¨äºé‡æ’åºï¼‰ï¼šè”åˆç¼–ç æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œæ›´å‡†ç¡®ä½†è®¡ç®—æˆæœ¬é«˜

### 4.4 ç»„åˆå¤šä¸ªåå¤„ç†å™¨

```python
from llama_index.core.postprocessor import (
    SimilarityPostprocessor,
    KeywordNodePostprocessor
)

# 1. å…³é”®è¯è¿‡æ»¤
keyword_processor = KeywordNodePostprocessor(
    required_keywords=["AI", "machine learning"],
    exclude_keywords=["deprecated"]
)

# 2. ç›¸ä¼¼åº¦è¿‡æ»¤
similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.7)

# 3. Cohere é‡æ’åº
cohere_rerank = CohereRerank(api_key="your-key", top_n=3)

# 4. é“¾å¼åº”ç”¨ï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰
query_engine = vector_index.as_query_engine(
    similarity_top_k=20,
    node_postprocessors=[
        keyword_processor,      # å…ˆè¿‡æ»¤å…³é”®è¯
        similarity_processor,   # å†è¿‡æ»¤ç›¸ä¼¼åº¦
        cohere_rerank          # æœ€åé‡æ’åº
    ]
)
```

### 4.5 æ—¶é—´åŠ æƒåå¤„ç†å™¨

æ ¹æ®æ–‡æ¡£çš„æ—¶é—´æˆ³è°ƒæ•´ç›¸å…³æ€§åˆ†æ•°ï¼š

```python
from llama_index.core.postprocessor import TimeWeightedPostprocessor

# åˆ›å»ºæ—¶é—´åŠ æƒåå¤„ç†å™¨
time_processor = TimeWeightedPostprocessor(
    time_decay=0.5,  # æ—¶é—´è¡°å‡ç³»æ•°
    time_access_refresh=False,  # æ˜¯å¦åˆ·æ–°è®¿é—®æ—¶é—´
    top_k=5
)

query_engine = vector_index.as_query_engine(
    node_postprocessors=[time_processor]
)

# é€‚ç”¨äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„æŸ¥è¯¢
response = query_engine.query("What are the latest AI trends?")
```

---

## ç¬¬ 5 ç« ï¼šChat Engineï¼ˆå¯¹è¯ç³»ç»Ÿï¼‰

Chat Engine æä¾›æœ‰çŠ¶æ€çš„å¯¹è¯æ¥å£ï¼Œç»´æŠ¤å†å²ä¸Šä¸‹æ–‡ã€‚

### 5.1 SimpleChatEngine

åŸºç¡€å¯¹è¯å¼•æ“ï¼Œä¸è¿›è¡Œæ£€ç´¢å¢å¼ºï¼š

```python
from llama_index.core.chat_engine import SimpleChatEngine
from llama_index.core.memory import ChatMemoryBuffer

# 1. åˆ›å»ºèŠå¤©è®°å¿†
chat_memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

# 2. åˆ›å»ºç®€å•èŠå¤©å¼•æ“
chat_engine = SimpleChatEngine.from_defaults(
    llm=Settings.llm,
    memory=chat_memory
)

# 3. è¿›è¡Œå¯¹è¯
response1 = chat_engine.chat("Hello! I want to learn about AI.")
print(response1)

response2 = chat_engine.chat("What are neural networks?")
print(response2)

response3 = chat_engine.chat("Can you summarize what we discussed?")
print(response3)

# 4. æµå¼å“åº”
streaming_response = chat_engine.stream_chat("Tell me more about deep learning")
for token in streaming_response.response_gen:
    print(token, end="", flush=True)
```

### 5.2 CondensePlusContextChatEngine

å‹ç¼©å¯¹è¯å†å²å¹¶æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼š

```python
from llama_index.core.chat_engine import CondensePlusContextChatEngine
from llama_index.core.storage.chat_store import SimpleChatStore

# 1. åˆ›å»ºèŠå¤©å­˜å‚¨
chat_store = SimpleChatStore()

# 2. åˆ›å»ºèŠå¤©è®°å¿†
chat_memory = ChatMemoryBuffer.from_defaults(
    token_limit=3000,
    chat_store=chat_store,
    chat_store_key="user_123"
)

# 3. åˆ›å»º CondensePlusContext å¼•æ“
chat_engine = vector_index.as_chat_engine(
    chat_mode="condense_plus_context",
    memory=chat_memory,
    similarity_top_k=5,
    verbose=True
)

# 4. è¿›è¡Œ RAG å¯¹è¯
response1 = chat_engine.chat(
    "What is the main topic of the documents?"
)
print(response1)

response2 = chat_engine.chat(
    "Can you give me more details about that?"  # å¼•ç”¨å‰é¢çš„å¯¹è¯
)
print(response2)

# 5. æŸ¥çœ‹æ£€ç´¢åˆ°çš„æº
for node in response2.source_nodes:
    print(f"\nSource: {node.metadata.get('file_name')}")
    print(f"Text: {node.text[:150]}...")
```

**å·¥ä½œæµç¨‹**ï¼š
1. å°†å¯¹è¯å†å²å‹ç¼©æˆç‹¬ç«‹çš„æŸ¥è¯¢
2. ä½¿ç”¨å‹ç¼©åçš„æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£
3. å°†æ£€ç´¢ç»“æœå’Œå¯¹è¯å†å²ä¸€èµ·å‘é€ç»™ LLM

### 5.3 ReActChatEngine

ä½¿ç”¨ ReActï¼ˆæ¨ç† + è¡ŒåŠ¨ï¼‰æ¨¡å¼çš„æ™ºèƒ½ä½“å¯¹è¯å¼•æ“ï¼š

```python
from llama_index.core.chat_engine import ReActChatEngine
from llama_index.core.tools import QueryEngineTool

# 1. åˆ›å»ºå·¥å…·
query_tool = QueryEngineTool(
    query_engine=vector_index.as_query_engine(),
    metadata=ToolMetadata(
        name="knowledge_base",
        description="Search the knowledge base for information"
    )
)

# 2. åˆ›å»º ReAct èŠå¤©å¼•æ“
react_chat_engine = vector_index.as_chat_engine(
    chat_mode="react",
    verbose=True,
    tools=[query_tool]
)

# 3. æ‰§è¡Œéœ€è¦æ¨ç†çš„å¯¹è¯
response = react_chat_engine.chat(
    "First check what topics are covered in the knowledge base, "
    "then explain the most important one in detail"
)

print(response)
```

### 5.4 æŒä¹…åŒ–èŠå¤©å†å²

```python
from llama_index.core.storage.chat_store import SimpleChatStore

# 1. åˆ›å»ºå¹¶ä¿å­˜èŠå¤©å†å²
chat_store = SimpleChatStore()

chat_memory = ChatMemoryBuffer.from_defaults(
    token_limit=3000,
    chat_store=chat_store,
    chat_store_key="session_001"
)

chat_engine = vector_index.as_chat_engine(
    chat_mode="condense_plus_context",
    memory=chat_memory
)

# è¿›è¡Œå¯¹è¯...
chat_engine.chat("Hello!")

# 2. ä¿å­˜åˆ°ç£ç›˜
chat_store.persist(persist_path="./storage/chat_history.json")

# 3. ä»ç£ç›˜åŠ è½½
loaded_chat_store = SimpleChatStore.from_persist_path(
    persist_path="./storage/chat_history.json"
)

loaded_memory = ChatMemoryBuffer.from_defaults(
    token_limit=3000,
    chat_store=loaded_chat_store,
    chat_store_key="session_001"
)

# ç»§ç»­ä¹‹å‰çš„å¯¹è¯
chat_engine = vector_index.as_chat_engine(
    chat_mode="condense_plus_context",
    memory=loaded_memory
)

response = chat_engine.chat("What did we discuss earlier?")
```

### 5.5 å¤šç”¨æˆ·èŠå¤©ç®¡ç†

```python
from llama_index.core.storage.chat_store import SimpleChatStore

chat_store = SimpleChatStore()

def get_chat_engine_for_user(user_id: str):
    """ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„èŠå¤©å¼•æ“"""
    memory = ChatMemoryBuffer.from_defaults(
        token_limit=3000,
        chat_store=chat_store,
        chat_store_key=user_id  # ä½¿ç”¨ user_id ä½œä¸ºé”®
    )

    return vector_index.as_chat_engine(
        chat_mode="condense_plus_context",
        memory=memory
    )

# ç”¨æˆ· A çš„å¯¹è¯
chat_engine_a = get_chat_engine_for_user("user_a")
response_a = chat_engine_a.chat("Tell me about AI")

# ç”¨æˆ· B çš„å¯¹è¯ï¼ˆç‹¬ç«‹çš„ä¸Šä¸‹æ–‡ï¼‰
chat_engine_b = get_chat_engine_for_user("user_b")
response_b = chat_engine_b.chat("What is machine learning?")

# ä¿å­˜æ‰€æœ‰ç”¨æˆ·çš„èŠå¤©å†å²
chat_store.persist("./storage/all_chats.json")
```

---

## ç¬¬ 6 ç« ï¼šçŸ¥è¯†å›¾è°± RAG

çŸ¥è¯†å›¾è°±å°†æ–‡æ¡£ä¸­çš„å®ä½“å’Œå…³ç³»æ˜¾å¼å»ºæ¨¡ï¼Œæ”¯æŒå¤æ‚çš„å›¾æŸ¥è¯¢å’Œæ¨ç†ã€‚

### 6.1 PropertyGraphIndex åŸºç¡€

#### 6.1.1 åŸºç¡€æ„å»º

```python
from llama_index.core.indices.property_graph import PropertyGraphIndex
from llama_index.core.graph_stores import SimplePropertyGraphStore
from llama_index.core import Settings

# 1. åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# 2. åˆ›å»ºå›¾å­˜å‚¨
graph_store = SimplePropertyGraphStore()

# 3. åˆ›å»º PropertyGraphIndex (åŸºç¡€ç‰ˆæœ¬)
pg_index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    show_progress=True
)

# 4. ä¿å­˜å›¾
pg_index.storage_context.persist(persist_dir="./storage/property_graph")

# 5. åŠ è½½å›¾
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(
    persist_dir="./storage/property_graph"
)
pg_index = load_index_from_storage(storage_context)
```

---

#### 6.1.2 å…³é”®å‚æ•°: embed_kg_nodes

**`embed_kg_nodes`å‚æ•°**æ˜¯å¯ç”¨å‘é‡æ£€ç´¢çš„å…³é”®é…ç½®,**å¼ºçƒˆæ¨èè®¾ç½®ä¸ºTrue**ã€‚

**å®Œæ•´é…ç½®ç¤ºä¾‹**:

```python
from llama_index.core.indices.property_graph import PropertyGraphIndex
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# é…ç½®å…¨å±€LLMå’ŒEmbedding
Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# åˆ›å»ºPropertyGraphIndex (æ¨èé…ç½®)
pg_index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    embed_kg_nodes=True,  # ğŸ”‘ å…³é”®å‚æ•°: ä¸ºå›¾èŠ‚ç‚¹ç”Ÿæˆembedding
    show_progress=True
)
```

**embed_kg_nodeså‚æ•°è¯¦è§£**:

| å‚æ•°å€¼ | è¡Œä¸º | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|--------|------|---------|------|------|
| `True` (æ¨è) | ä¸ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹å’Œå…³ç³»ç”Ÿæˆembedding | éœ€è¦å‘é‡æ£€ç´¢ã€è¯­ä¹‰æœç´¢ | âœ… å¯ç”¨VectorContextRetriever<br>âœ… æ”¯æŒç›¸ä¼¼åº¦æœç´¢<br>âœ… æå‡æ£€ç´¢ç²¾åº¦ | âŒ å¢åŠ æ„å»ºæ—¶é—´<br>âŒ å¢åŠ å­˜å‚¨æˆæœ¬ |
| `False` (é»˜è®¤) | ä»…ä¸ºåŸå§‹æ–‡æœ¬å—ç”Ÿæˆembedding | ä»…ä½¿ç”¨LLMç”ŸæˆCypheræŸ¥è¯¢ | âœ… æ„å»ºé€Ÿåº¦å¿«<br>âœ… å­˜å‚¨æˆæœ¬ä½ | âŒ æ— æ³•ä½¿ç”¨VectorContextRetriever<br>âŒ è¯­ä¹‰æœç´¢å—é™ |

**å½±å“çš„åŠŸèƒ½**:

```python
# embed_kg_nodes=True æ—¶,å¯ä½¿ç”¨VectorContextRetriever
from llama_index.core.indices.property_graph import VectorContextRetriever

vector_retriever = VectorContextRetriever(
    pg_index.property_graph_store,
    embed_model=Settings.embed_model,
    similarity_top_k=5  # å‘é‡æ£€ç´¢éœ€è¦embedding
)

# embed_kg_nodes=False æ—¶,VectorContextRetrieverå°†æ— æ³•å·¥ä½œ
# åªèƒ½ä½¿ç”¨LLMSynonymRetrieverç­‰éå‘é‡æ–¹æ³•
```

**æœ€ä½³å®è·µ**:

```python
# âœ… æ¨è: ç”Ÿäº§ç¯å¢ƒé…ç½®
pg_index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    embed_kg_nodes=True,        # å¯ç”¨å‘é‡æ£€ç´¢
    show_progress=True,
    vector_store=vector_store,  # å¯é€‰: è‡ªå®šä¹‰å‘é‡å­˜å‚¨
)

# âš ï¸ ä»…ç”¨äºæµ‹è¯•/è°ƒè¯•
pg_index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    embed_kg_nodes=False,  # å¿«é€Ÿæ„å»º,ä½†åŠŸèƒ½å—é™
    show_progress=True
)
```

---

#### 6.1.3 å›¾å­˜å‚¨é€‰é¡¹ (Graph Stores)

LlamaIndexæ”¯æŒå¤šç§å›¾å­˜å‚¨åç«¯,æ ¹æ®åœºæ™¯é€‰æ‹©:

**æ‰€æœ‰æ”¯æŒçš„Graph Stores**:

| Store | ç±»å‹ | Native Embedding | æŒä¹…åŒ– | é€‚ç”¨åœºæ™¯ | æ¨èåº¦ |
|-------|------|-----------------|--------|---------|--------|
| **SimplePropertyGraphStore** | å†…å­˜ | âœ… | ç£ç›˜æ–‡ä»¶ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡ | â­â­â­â­ |
| **Neo4jPropertyGraphStore** | æœåŠ¡å™¨ | âŒ | æ•°æ®åº“ | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡ | â­â­â­â­â­ |
| **NebulaPropertyGraphStore** | åˆ†å¸ƒå¼ | âŒ | æ•°æ®åº“ | è¶…å¤§è§„æ¨¡ã€åˆ†å¸ƒå¼ | â­â­â­ |
| **TiDBPropertyGraphStore** | HTAP | âŒ | æ•°æ®åº“ | æ··åˆè´Ÿè½½(OLTP+OLAP) | â­â­â­ |
| **FalkorDBPropertyGraphStore** | Redis | âŒ | Redis | ä½å»¶è¿Ÿã€å®æ—¶æŸ¥è¯¢ | â­â­ |

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# 1. SimplePropertyGraphStore (é»˜è®¤,æ¨èå…¥é—¨)
from llama_index.core.graph_stores import SimplePropertyGraphStore

graph_store = SimplePropertyGraphStore()

# 2. Neo4jPropertyGraphStore (æ¨èç”Ÿäº§)
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

graph_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="your-password",
    url="bolt://localhost:7687",
    database="neo4j"
)

# 3. NebulaPropertyGraphStore (è¶…å¤§è§„æ¨¡)
from llama_index.graph_stores.nebula import NebulaPropertyGraphStore

graph_store = NebulaPropertyGraphStore(
    space="my_graph",
    host="127.0.0.1",
    port=9669
)

# 4. TiDBPropertyGraphStore (æ··åˆè´Ÿè½½)
from llama_index.graph_stores.tidb import TiDBPropertyGraphStore

graph_store = TiDBPropertyGraphStore(
    host="localhost",
    port=4000,
    user="root",
    password="password",
    database="graph_db"
)

# 5. FalkorDBPropertyGraphStore (ä½å»¶è¿Ÿ)
from llama_index.graph_stores.falkordb import FalkorDBPropertyGraphStore

graph_store = FalkorDBPropertyGraphStore(
    host="localhost",
    port=6379,
    graph_name="my_graph"
)
```

**é€‰å‹å»ºè®®**:

```
å¼€å‘/å­¦ä¹  â†’ SimplePropertyGraphStore
    â†“
ç”Ÿäº§ç¯å¢ƒ(ä¸­å°è§„æ¨¡) â†’ Neo4jPropertyGraphStore
    â†“
ç”Ÿäº§ç¯å¢ƒ(è¶…å¤§è§„æ¨¡) â†’ NebulaPropertyGraphStore
    â†“
æ··åˆè´Ÿè½½(OLTP+OLAP) â†’ TiDBPropertyGraphStore
    â†“
å®æ—¶ä½å»¶è¿Ÿ â†’ FalkorDBPropertyGraphStore
```

**åŠŸèƒ½å¯¹æ¯”**:

| åŠŸèƒ½ | Simple | Neo4j | Nebula | TiDB | FalkorDB |
|------|--------|-------|--------|------|----------|
| CypheræŸ¥è¯¢ | âŒ | âœ… | âœ… | âš ï¸ éƒ¨åˆ† | âœ… |
| å‘é‡å­˜å‚¨ | âœ… | âŒ (éœ€å¤–ç½®) | âŒ (éœ€å¤–ç½®) | âŒ (éœ€å¤–ç½®) | âŒ (éœ€å¤–ç½®) |
| åˆ†å¸ƒå¼ | âŒ | âš ï¸ ä¼ä¸šç‰ˆ | âœ… | âœ… | âŒ |
| æ€§èƒ½ | ä½ | é«˜ | æé«˜ | é«˜ | é«˜ |
| æ˜“ç”¨æ€§ | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ | â­â­â­ |

**å®‰è£…ä¾èµ–**:

```bash
# Neo4j
pip install llama-index-graph-stores-neo4j

# Nebula
pip install llama-index-graph-stores-nebula

# TiDB
pip install llama-index-graph-stores-tidb

# FalkorDB
pip install llama-index-graph-stores-falkordb
```

---

#### 6.1.4 ç´¢å¼•å¢åˆ æ”¹æŸ¥æ“ä½œ (CRUD)

PropertyGraphIndexæ”¯æŒåŠ¨æ€æ›´æ–°å’ŒæŸ¥è¯¢æ“ä½œ,é€‚åˆç”Ÿäº§ç¯å¢ƒä¸­æŒç»­æ›´æ–°çŸ¥è¯†å›¾è°±çš„åœºæ™¯ã€‚

**åŸºç¡€CRUDæ“ä½œ**:

```python
from llama_index.core.indices.property_graph import PropertyGraphIndex
from llama_index.core import Document

# å‡è®¾å·²åˆ›å»ºç´¢å¼•
pg_index = PropertyGraphIndex.from_documents(documents, ...)

# ========== 1. Insert (æ’å…¥) ==========

# æ’å…¥æ–°æ–‡æ¡£
new_document = Document(text="æ–°å†…å®¹: Claude is an AI assistant created by Anthropic.")
pg_index.insert(new_document)

# æ’å…¥å¤šä¸ªèŠ‚ç‚¹
from llama_index.core.schema import TextNode

new_nodes = [
    TextNode(text="Node 1 content", metadata={"source": "manual"}),
    TextNode(text="Node 2 content", metadata={"source": "manual"})
]
pg_index.insert_nodes(new_nodes)

# ========== 2. Get (æŸ¥è¯¢) - é€šè¿‡Graph Store ==========

# è·å–ç‰¹å®šå®ä½“
entities = pg_index.property_graph_store.get(
    ids=["entity_id_1", "entity_id_2"]
)

# æŒ‰å±æ€§æŸ¥è¯¢
entities = pg_index.property_graph_store.get(
    properties={"label": "PERSON", "name": "Elon Musk"}
)

# è·å–å…³ç³»å›¾è°± (depthæŒ‡å®šè·³æ•°)
rel_map = pg_index.property_graph_store.get_rel_map(
    [entity_node],
    depth=2  # 2-hopå…³ç³»
)

# è·å–åŸå§‹æ–‡æœ¬å—
llama_nodes = pg_index.property_graph_store.get_llama_nodes(['chunk_id_1'])

# ========== 3. Update (æ›´æ–°) - é€šè¿‡Upsert ==========

from llama_index.core.graph_stores import EntityNode, Relation

# æ›´æ–°æˆ–æ’å…¥å®ä½“ (å¦‚æœIDå­˜åœ¨åˆ™æ›´æ–°,å¦åˆ™æ’å…¥)
updated_entity = EntityNode(
    name="Elon Musk",
    label="PERSON",
    properties={"title": "CEO", "company": "SpaceX"}  # æ›´æ–°å±æ€§
)
pg_index.property_graph_store.upsert_nodes([updated_entity])

# æ›´æ–°æˆ–æ’å…¥å…³ç³»
updated_relation = Relation(
    label="WORKS_AT",
    source_id="elon_musk_id",
    target_id="spacex_id",
    properties={"since": "2002"}  # æ›´æ–°å…³ç³»å±æ€§
)
pg_index.property_graph_store.upsert_relations([updated_relation])

# ========== 4. Delete (åˆ é™¤) ==========

# æŒ‰IDåˆ é™¤
pg_index.property_graph_store.delete(
    ids=["entity_id_to_delete"]
)

# æŒ‰å±æ€§åˆ é™¤
pg_index.property_graph_store.delete(
    properties={"source": "deprecated"}
)
```

**å¼‚æ­¥æ“ä½œ**:

```python
# æ‰€æœ‰æ“ä½œéƒ½æœ‰å¼‚æ­¥ç‰ˆæœ¬
entities = await pg_index.property_graph_store.aget(ids=[...])
await pg_index.property_graph_store.adelete(ids=[...])
# å…¶ä»–å¼‚æ­¥æ–¹æ³•: aupsert_nodes, aupsert_relations, aget_rel_mapç­‰
```

**æ‰¹é‡æ›´æ–°ç¤ºä¾‹**:

```python
from llama_index.core.graph_stores import EntityNode, Relation
from llama_index.core.schema import TextNode

# æ‰¹é‡æ’å…¥å®ä½“
entities = [
    EntityNode(name="Person1", label="PERSON", properties={"age": 30}),
    EntityNode(name="Person2", label="PERSON", properties={"age": 25}),
    EntityNode(name="Company1", label="ORGANIZATION", properties={})
]
pg_index.property_graph_store.upsert_nodes(entities)

# æ‰¹é‡æ’å…¥å…³ç³»
relations = [
    Relation(label="WORKS_AT", source_id=entities[0].id, target_id=entities[2].id),
    Relation(label="WORKS_AT", source_id=entities[1].id, target_id=entities[2].id),
]
pg_index.property_graph_store.upsert_relations(relations)

# å…³è”åˆ°åŸå§‹æ–‡æœ¬å—
source_chunk = TextNode(id_="source_1", text="Person1 and Person2 work at Company1.")
pg_index.property_graph_store.upsert_llama_nodes([source_chunk])

# åˆ›å»ºæ–‡æœ¬å—åˆ°å®ä½“çš„å…³ç³»
source_relations = [
    Relation(label="MENTIONS", source_id=entities[0].id, target_id="source_1"),
    Relation(label="MENTIONS", source_id=entities[1].id, target_id="source_1"),
]
pg_index.property_graph_store.upsert_relations(source_relations)
```

**ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ**:

```python
from llama_index.core import Document

# æŒç»­æ›´æ–°åœºæ™¯
def update_knowledge_graph(pg_index, new_documents):
    """æŒç»­æ›´æ–°çŸ¥è¯†å›¾è°±"""

    # 1. æ’å…¥æ–°æ–‡æ¡£ (è‡ªåŠ¨æå–å®ä½“å’Œå…³ç³»)
    for doc in new_documents:
        pg_index.insert(doc)

    # 2. å®šæœŸæŒä¹…åŒ–
    pg_index.storage_context.persist(persist_dir="./storage/property_graph")

    # 3. éªŒè¯æ’å…¥ç»“æœ
    total_nodes = len(pg_index.property_graph_store.get(properties={}))
    print(f"Total entities in graph: {total_nodes}")

# æ•°æ®æ¸…ç†åœºæ™¯
def cleanup_old_data(pg_index, cutoff_date):
    """åˆ é™¤è¿‡æœŸæ•°æ®"""

    # æŸ¥è¯¢éœ€è¦åˆ é™¤çš„èŠ‚ç‚¹
    old_nodes = pg_index.property_graph_store.get(
        properties={"created_at": {"$lt": cutoff_date}}
    )

    # æ‰¹é‡åˆ é™¤
    old_ids = [node.id for node in old_nodes]
    pg_index.property_graph_store.delete(ids=old_ids)
```

**ä¸from_existingç»“åˆä½¿ç”¨**:

```python
from llama_index.core.indices.property_graph import PropertyGraphIndex
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

# 1. åŠ è½½å·²å­˜åœ¨çš„å›¾
graph_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="password",
    url="bolt://localhost:7687"
)

pg_index = PropertyGraphIndex.from_existing(
    property_graph_store=graph_store,
    embed_kg_nodes=True
)

# 2. å¢é‡æ›´æ–°
new_docs = [Document(text="Latest news...")]
for doc in new_docs:
    pg_index.insert(doc)

# 3. æŸ¥è¯¢æ›´æ–°åçš„å›¾
query_engine = pg_index.as_query_engine()
response = query_engine.query("What are the latest updates?")
```

**CRUDæ“ä½œæ€»ç»“**:

| æ“ä½œ | Indexå±‚æ–¹æ³• | Graph Storeå±‚æ–¹æ³• | å¼‚æ­¥ç‰ˆæœ¬ |
|------|-----------|-----------------|---------|
| **Create/Insert** | `insert(doc)`<br>`insert_nodes(nodes)` | `upsert_nodes(entities)`<br>`upsert_relations(relations)` | âœ… æœ‰ |
| **Read/Get** | - | `get(ids=...)`<br>`get(properties=...)`<br>`get_rel_map(...)` | âœ… æœ‰ |
| **Update** | `insert(doc)` (é‡æ–°æå–) | `upsert_nodes(...)`<br>`upsert_relations(...)` | âœ… æœ‰ |
| **Delete** | - | `delete(ids=...)`<br>`delete(properties=...)` | âœ… æœ‰ |

**æ³¨æ„äº‹é¡¹**:
- âœ… `insert()` ä¼šè‡ªåŠ¨è°ƒç”¨é…ç½®çš„kg_extractorsæå–æ–°å®ä½“å’Œå…³ç³»
- âœ… `upsert` æ˜¯å¹‚ç­‰æ“ä½œ,ç›¸åŒIDä¼šæ›´æ–°è€Œéé‡å¤æ’å…¥
- âœ… åˆ é™¤å®ä½“æ—¶,ç›¸å…³å…³ç³»ä¹Ÿä¼šè¢«åˆ é™¤ (å–å†³äºGraph Storeå®ç°)
- âš ï¸ Neo4jç­‰å¤–éƒ¨æ•°æ®åº“éœ€è¦æ‰‹åŠ¨ç®¡ç†äº‹åŠ¡å’Œè¿æ¥

---

### 6.2 KG Extractorsï¼ˆçŸ¥è¯†æŠ½å–å™¨ï¼‰

#### 6.2.1 ImplicitPathExtractor (é›¶æˆæœ¬æå–å™¨)

**ImplicitPathExtractor**æ˜¯å®˜æ–¹é»˜è®¤æå–å™¨ä¹‹ä¸€,ä»èŠ‚ç‚¹çš„ç°æœ‰`relationships`å±æ€§æ¨æ–­å…³ç³»,**æ— éœ€è°ƒç”¨LLM**ã€‚

**é€‚ç”¨åœºæ™¯**:
- âœ… æ–‡æ¡£å·²æœ‰å…ƒæ•°æ®å…³ç³»
- âœ… éœ€è¦é›¶æˆæœ¬å¿«é€Ÿæ„å»ºå›¾è°±
- âœ… ä¸å…¶ä»–æå–å™¨ç»„åˆä½¿ç”¨

**å·¥ä½œåŸç†**:

```python
# å‡è®¾æ–‡æ¡£èŠ‚ç‚¹å·²æœ‰relationshipså±æ€§
node.relationships = {
    RelatedNodeInfo(
        node_id="doc_2",
        metadata={"relationship": "FOLLOWS"}
    )
}

# ImplicitPathExtractorä¼šè‡ªåŠ¨æå–:
# (doc_1) -[FOLLOWS]-> (doc_2)
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from llama_index.core.indices.property_graph import ImplicitPathExtractor

# åˆ›å»ºé›¶æˆæœ¬æå–å™¨
implicit_extractor = ImplicitPathExtractor()

# ä½¿ç”¨ (æ— éœ€LLMé…ç½®)
pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[implicit_extractor],  # é›¶æˆæœ¬æå–
    show_progress=True
)
```

**ä¸å…¶ä»–æå–å™¨ç»„åˆ**:

```python
from llama_index.core.indices.property_graph import (
    ImplicitPathExtractor,
    SimpleLLMPathExtractor
)

# ç»„åˆä½¿ç”¨: éšå¼å…³ç³» + LLMæå–
pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[
        ImplicitPathExtractor(),      # æå–å·²æœ‰å…³ç³» (å…è´¹)
        SimpleLLMPathExtractor(       # æå–è¯­ä¹‰å…³ç³» (ä»˜è´¹)
            llm=Settings.llm,
            max_paths_per_chunk=10
        )
    ],
    show_progress=True
)
```

**æˆæœ¬å¯¹æ¯”**:

| æå–å™¨ | LLMè°ƒç”¨ | æˆæœ¬ | æå–è´¨é‡ | æ¨èåœºæ™¯ |
|--------|---------|------|---------|---------|
| ImplicitPathExtractor | âŒ å¦ | $0 | ä¸­ç­‰ (ä¾èµ–å…ƒæ•°æ®) | å·²æœ‰å…ƒæ•°æ®ã€é¢„ç®—æœ‰é™ |
| SimpleLLMPathExtractor | âœ… æ˜¯ | $$ | é«˜ | é€šç”¨åœºæ™¯ |
| SchemaLLMPathExtractor | âœ… æ˜¯ | $$ | æé«˜ | éœ€è¦ç»“æ„åŒ–çŸ¥è¯† |

---

#### 6.2.2 SimpleLLMPathExtractor

ä½¿ç”¨ LLM æå–ç®€å•çš„ä¸‰å…ƒç»„ï¼ˆä¸»ä½“-å…³ç³»-å®¢ä½“ï¼‰ï¼š

```python
from llama_index.core.indices.property_graph import (
    SimpleLLMPathExtractor
)

# åˆ›å»ºç®€å•è·¯å¾„æå–å™¨
simple_extractor = SimpleLLMPathExtractor(
    llm=Settings.llm,
    max_paths_per_chunk=10,  # æ¯ä¸ªæ–‡æœ¬å—æœ€å¤šæå– 10 ä¸ªè·¯å¾„
    num_workers=4  # å¹¶è¡Œå¤„ç†
)

# ä½¿ç”¨æå–å™¨åˆ›å»ºç´¢å¼•
pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[simple_extractor],
    show_progress=True
)
```

---

#### 6.2.3 SchemaLLMPathExtractor

ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å¼çº¦æŸå®ä½“å’Œå…³ç³»ç±»å‹ï¼š

```python
from llama_index.core.indices.property_graph import (
    SchemaLLMPathExtractor
)
from typing import Literal

# å®šä¹‰å®ä½“å’Œå…³ç³»ç±»å‹
entities = Literal["PERSON", "ORGANIZATION", "LOCATION", "TECHNOLOGY"]
relations = Literal["WORKS_AT", "LOCATED_IN", "DEVELOPED", "USES"]

# å®šä¹‰å…è®¸çš„å…³ç³»æ¨¡å¼
validation_schema = {
    "PERSON": ["WORKS_AT", "LOCATED_IN"],
    "ORGANIZATION": ["LOCATED_IN", "DEVELOPED"],
    "TECHNOLOGY": ["DEVELOPED", "USES"]
}

# åˆ›å»ºæ¨¡å¼æå–å™¨
schema_extractor = SchemaLLMPathExtractor(
    llm=Settings.llm,
    possible_entities=entities,
    possible_relations=relations,
    kg_validation_schema=validation_schema,
    strict=True  # ä¸¥æ ¼æ¨¡å¼ï¼šæ‹’ç»ä¸ç¬¦åˆæ¨¡å¼çš„ä¸‰å…ƒç»„
)

pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[schema_extractor],
    show_progress=True
)
```

---

#### 6.2.4 DynamicLLMPathExtractor (å‚æ•°è¯¦è§£)

**DynamicLLMPathExtractor**åŠ¨æ€æå–å®ä½“å’Œå…³ç³»ï¼Œæ”¯æŒ**å¯é€‰çš„ç±»å‹çº¦æŸ**,æ¯”SchemaLLMPathExtractoræ›´çµæ´»ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**:
- âœ… å…è®¸å®ä½“/å…³ç³»ç±»å‹ä½œä¸º**æç¤º**è€Œéç¡¬çº¦æŸ
- âœ… LLMå¯ä»¥æå–è¶…å‡ºallowedèŒƒå›´çš„ç±»å‹
- âœ… é€‚åˆçŸ¥è¯†å‘ç°åœºæ™¯ (ä¸ç¡®å®šæ‰€æœ‰å®ä½“ç±»å‹)

**å®Œæ•´å‚æ•°è¯´æ˜**:

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|--------|------|------|
| **llm** | BaseLLM | å¿…å¡« | LLMå®ä¾‹ | `OpenAI(model="gpt-4")` |
| **max_triplets_per_chunk** | int | 10 | æ¯ä¸ªæ–‡æœ¬å—æœ€å¤šæå–çš„ä¸‰å…ƒç»„æ•° | `15` |
| **num_workers** | int | 4 | å¹¶è¡Œå¤„ç†workeræ•° | `8` |
| **allowed_entity_types** | List[str] | `None` | **å¯é€‰æç¤º**: å»ºè®®çš„å®ä½“ç±»å‹ | `["PERSON", "ORG"]` |
| **allowed_relation_types** | List[str] | `None` | **å¯é€‰æç¤º**: å»ºè®®çš„å…³ç³»ç±»å‹ | `["WORKS_AT", "USES"]` |

**ä¸SchemaLLMPathExtractorçš„åŒºåˆ«**:

| ç‰¹æ€§ | DynamicLLMPathExtractor | SchemaLLMPathExtractor |
|------|----------------------|----------------------|
| ç±»å‹çº¦æŸ | **è½¯çº¦æŸ** (æç¤º) | **ç¡¬çº¦æŸ** (strict=Trueæ‹’ç») |
| allowedå‚æ•° | ä½œä¸ºLLMæç¤º | ä¸¥æ ¼éªŒè¯ |
| è¶…å‡ºèŒƒå›´ç±»å‹ | âœ… å…è®¸æå– | âŒ æ‹’ç» (strict=True) |
| é€‚ç”¨åœºæ™¯ | çŸ¥è¯†å‘ç°ã€æ¢ç´¢æ€§åˆ†æ | ç»“æ„åŒ–çŸ¥è¯†åº“ã€ä¸¥æ ¼schema |
| çµæ´»æ€§ | â­â­â­â­â­ | â­â­â­ |

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from llama_index.core.indices.property_graph import DynamicLLMPathExtractor

# ç¤ºä¾‹1: æä¾›ç±»å‹æç¤º (æ¨è)
dynamic_extractor = DynamicLLMPathExtractor(
    llm=Settings.llm,
    max_triplets_per_chunk=15,
    num_workers=4,
    allowed_entity_types=["PERSON", "ORGANIZATION", "TECHNOLOGY", "LOCATION"],
    allowed_relation_types=["WORKS_AT", "FOUNDED", "USES", "LOCATED_IN"]
)

# LLMä¼šä¼˜å…ˆæå–è¿™äº›ç±»å‹,ä½†ä¹Ÿå¯èƒ½å‘ç°æ–°ç±»å‹
# ä¾‹å¦‚: å¯èƒ½æå–åˆ° "PRODUCT" (æœªåœ¨allowedä¸­) å¦‚æœæ–‡æ¡£æåˆ°äº§å“

pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[dynamic_extractor],
    show_progress=True
)

# ç¤ºä¾‹2: æ— ç±»å‹çº¦æŸ (å®Œå…¨è‡ªç”±)
dynamic_extractor_free = DynamicLLMPathExtractor(
    llm=Settings.llm,
    max_triplets_per_chunk=20,
    # ä¸æä¾›allowedå‚æ•°,LLMè‡ªç”±æå–
)

# ç¤ºä¾‹3: è°ƒä¼˜max_triplets_per_chunk
# æ–‡æ¡£å¯†é›† â†’ å¢åŠ max_triplets
dynamic_extractor_dense = DynamicLLMPathExtractor(
    llm=Settings.llm,
    max_triplets_per_chunk=30,  # æå–æ›´å¤šå…³ç³»
    allowed_entity_types=["PERSON", "ORG"]
)

# æ–‡æ¡£ç¨€ç– â†’ å‡å°‘max_triplets (èŠ‚çœæˆæœ¬)
dynamic_extractor_sparse = DynamicLLMPathExtractor(
    llm=Settings.llm,
    max_triplets_per_chunk=5,
    allowed_entity_types=["PERSON"]
)
```

**å‚æ•°è°ƒä¼˜å»ºè®®**:

```python
# ğŸ¯ é€šç”¨åœºæ™¯ (å¹³è¡¡è´¨é‡ä¸æˆæœ¬)
DynamicLLMPathExtractor(
    llm=OpenAI(model="gpt-4", temperature=0),
    max_triplets_per_chunk=10,
    num_workers=4,
    allowed_entity_types=["PERSON", "ORGANIZATION", "TECHNOLOGY"],
    allowed_relation_types=["WORKS_AT", "USES", "FOUNDED"]
)

# ğŸš€ é«˜è´¨é‡åœºæ™¯ (è¿½æ±‚å®Œæ•´æ€§)
DynamicLLMPathExtractor(
    llm=OpenAI(model="gpt-4", temperature=0),
    max_triplets_per_chunk=20,  # æå–æ›´å¤š
    num_workers=8,              # åŠ é€Ÿå¤„ç†
    allowed_entity_types=["PERSON", "ORG", "TECH", "LOCATION", "EVENT"],
    allowed_relation_types=["WORKS_AT", "FOUNDED", "USES", "LOCATED_IN", "PARTICIPATES"]
)

# ğŸ’° æˆæœ¬ä¼˜åŒ–åœºæ™¯
DynamicLLMPathExtractor(
    llm=OpenAI(model="gpt-3.5-turbo", temperature=0),  # ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
    max_triplets_per_chunk=5,   # å‡å°‘æå–
    num_workers=2,
    allowed_entity_types=["PERSON", "ORG"]  # èšç„¦æ ¸å¿ƒç±»å‹
)
```

**å®é™…æ•ˆæœå¯¹æ¯”**:

```python
# è¾“å…¥æ–‡æœ¬:
# "Elon Musk founded SpaceX in 2002. The company developed Falcon 9 rocket."

# DynamicLLMPathExtractor (allowed_entity_types=["PERSON", "ORG"]):
# âœ… (Elon Musk, PERSON) -[FOUNDED]-> (SpaceX, ORG)
# âœ… (SpaceX, ORG) -[DEVELOPED]-> (Falcon 9, PRODUCT)  â† å‘ç°æ–°ç±»å‹!
# âœ… (SpaceX, ORG) -[FOUNDED_IN]-> (2002, DATE)         â† å‘ç°æ–°ç±»å‹!

# SchemaLLMPathExtractor (strict=True, possible_entities=["PERSON", "ORG"]):
# âœ… (Elon Musk, PERSON) -[FOUNDED]-> (SpaceX, ORG)
# âŒ æ‹’ç» (Falcon 9, PRODUCT) - ä¸åœ¨schemaä¸­
# âŒ æ‹’ç» (2002, DATE) - ä¸åœ¨schemaä¸­
```

**ä¸‰ç§æå–å™¨é€‰æ‹©æŒ‡å—**:

```
åœºæ™¯: å·²çŸ¥æ‰€æœ‰å®ä½“ç±»å‹,éœ€è¦ä¸¥æ ¼ç»“æ„åŒ–
     â†’ SchemaLLMPathExtractor (strict=True)

åœºæ™¯: å¤§è‡´çŸ¥é“å®ä½“ç±»å‹,ä½†å¸Œæœ›å‘ç°æ–°ç±»å‹
     â†’ DynamicLLMPathExtractor (æ¨è)

åœºæ™¯: å®Œå…¨ä¸ç¡®å®šæœ‰å“ªäº›å®ä½“ç±»å‹ (æ¢ç´¢æ€§åˆ†æ)
     â†’ SimpleLLMPathExtractor æˆ– DynamicLLMPathExtractor (æ— allowed)

åœºæ™¯: æ–‡æ¡£å·²æœ‰å…ƒæ•°æ®å…³ç³»
     â†’ ImplicitPathExtractor
```

---

#### 6.2.5 è‡ªå®šä¹‰çŸ¥è¯†æŠ½å–å™¨

LlamaIndexå…è®¸åˆ›å»ºè‡ªå®šä¹‰çŸ¥è¯†æŠ½å–å™¨,é€‚åˆç‰¹å®šé¢†åŸŸæˆ–ç‰¹æ®Šéœ€æ±‚ã€‚

**æ ¸å¿ƒè¦ç‚¹**:
- âœ… ç»§æ‰¿`TransformComponent`åŸºç±»
- âœ… ä½¿ç”¨`KG_NODES_KEY`å’Œ`KG_RELATIONS_KEY`å­˜å‚¨å®ä½“å’Œå…³ç³»
- âœ… ä¿ç•™ç°æœ‰çš„å®ä½“å’Œå…³ç³» (ä¸å…¶ä»–æå–å™¨ç»„åˆä½¿ç”¨)
- âœ… ä¸Ingestion Pipelineå…¼å®¹

**å®Œæ•´å®ç°ç¤ºä¾‹**:

```python
from llama_index.core.graph_stores.types import (
    EntityNode,
    Relation,
    KG_NODES_KEY,
    KG_RELATIONS_KEY
)
from llama_index.core.schema import TransformComponent, BaseNode

class MyGraphExtractor(TransformComponent):
    """è‡ªå®šä¹‰çŸ¥è¯†æŠ½å–å™¨ç¤ºä¾‹"""

    def __call__(self, llama_nodes: list[BaseNode], **kwargs) -> list[BaseNode]:
        """
        å¤„ç†èŠ‚ç‚¹å¹¶æå–å®ä½“å’Œå…³ç³»

        Args:
            llama_nodes: æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            å¸¦æœ‰å®ä½“å’Œå…³ç³»å…ƒæ•°æ®çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        for llama_node in llama_nodes:
            # 1. è·å–ç°æœ‰å®ä½“å’Œå…³ç³» (ä¿ç•™å…¶ä»–æå–å™¨çš„ç»“æœ)
            existing_nodes = llama_node.metadata.pop(KG_NODES_KEY, [])
            existing_relations = llama_node.metadata.pop(KG_RELATIONS_KEY, [])

            # 2. è‡ªå®šä¹‰æå–é€»è¾‘
            # ç¤ºä¾‹: ä»æ–‡æœ¬ä¸­æå–ç‰¹å®šæ¨¡å¼çš„å®ä½“
            text = llama_node.get_content()

            # ç®€å•çš„è§„åˆ™æå– (å®é™…åº”ç”¨ä¸­å¯èƒ½ç”¨NERæ¨¡å‹æˆ–LLM)
            if "llama" in text.lower():
                llama_entity = EntityNode(
                    name="llama",
                    label="ANIMAL",
                    properties={"source": llama_node.node_id}
                )
                existing_nodes.append(llama_entity)

            if "index" in text.lower():
                index_entity = EntityNode(
                    name="index",
                    label="THING",
                    properties={"source": llama_node.node_id}
                )
                existing_nodes.append(index_entity)

                # åˆ›å»ºå…³ç³»
                if any(n.name == "llama" for n in existing_nodes):
                    relation = Relation(
                        label="HAS",
                        source_id="llama",  # æˆ–ä½¿ç”¨EntityNode.id
                        target_id="index",
                        properties={"confidence": 0.95}
                    )
                    existing_relations.append(relation)

            # 3. å°†å®ä½“å’Œå…³ç³»å­˜å›å…ƒæ•°æ®
            llama_node.metadata[KG_NODES_KEY] = existing_nodes
            llama_node.metadata[KG_RELATIONS_KEY] = existing_relations

        return llama_nodes

    # å¯é€‰: å¼‚æ­¥ç‰ˆæœ¬
    # async def acall(self, llama_nodes: list[BaseNode], **kwargs) -> list[BaseNode]:
    #     # å¼‚æ­¥å®ç°
    #     pass
```

**ä½¿ç”¨è‡ªå®šä¹‰æå–å™¨**:

```python
from llama_index.core.indices.property_graph import PropertyGraphIndex

# åˆ›å»ºè‡ªå®šä¹‰æå–å™¨å®ä¾‹
my_extractor = MyGraphExtractor()

# ä¸å…¶ä»–æå–å™¨ç»„åˆä½¿ç”¨
pg_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[
        my_extractor,              # è‡ªå®šä¹‰è§„åˆ™æå–
        SimpleLLMPathExtractor()   # LLMé€šç”¨æå–
    ],
    show_progress=True
)
```

**é«˜çº§ç¤ºä¾‹: åŸºäºNERæ¨¡å‹çš„æå–å™¨**:

```python
from llama_index.core.graph_stores.types import EntityNode, Relation, KG_NODES_KEY, KG_RELATIONS_KEY
from llama_index.core.schema import TransformComponent, BaseNode

class NERGraphExtractor(TransformComponent):
    """åŸºäºNERæ¨¡å‹çš„çŸ¥è¯†æŠ½å–å™¨"""

    def __init__(self, ner_model=None):
        """
        Args:
            ner_model: NERæ¨¡å‹ (ä¾‹å¦‚spaCy, transformers)
        """
        self.ner_model = ner_model

    def __call__(self, llama_nodes: list[BaseNode], **kwargs) -> list[BaseNode]:
        for llama_node in llama_nodes:
            existing_nodes = llama_node.metadata.pop(KG_NODES_KEY, [])
            existing_relations = llama_node.metadata.pop(KG_RELATIONS_KEY, [])

            text = llama_node.get_content()

            # ä½¿ç”¨NERæ¨¡å‹æå–å®ä½“
            if self.ner_model:
                entities = self.ner_model(text)  # å‡è®¾è¿”å› [(text, label, start, end), ...]

                for ent_text, ent_label, start, end in entities:
                    entity = EntityNode(
                        name=ent_text,
                        label=ent_label,
                        properties={
                            "start": start,
                            "end": end,
                            "source": llama_node.node_id
                        }
                    )
                    existing_nodes.append(entity)

            llama_node.metadata[KG_NODES_KEY] = existing_nodes
            llama_node.metadata[KG_RELATIONS_KEY] = existing_relations

        return llama_nodes

# ä½¿ç”¨ç¤ºä¾‹
# import spacy
# nlp = spacy.load("en_core_web_sm")
# ner_extractor = NERGraphExtractor(ner_model=nlp)
#
# pg_index = PropertyGraphIndex.from_documents(
#     documents,
#     kg_extractors=[ner_extractor],
#     show_progress=True
# )
```

**ä¸Ingestion Pipelineé›†æˆ**:

```python
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import SentenceSplitter

# åˆ›å»ºåŒ…å«è‡ªå®šä¹‰æå–å™¨çš„Pipeline
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=1024, chunk_overlap=20),
        MyGraphExtractor(),  # âœ… è‡ªå®šä¹‰æå–å™¨ä¸Pipelineå…¼å®¹
        SimpleLLMPathExtractor(llm=Settings.llm)
    ]
)

# è¿è¡ŒPipeline
nodes = pipeline.run(documents=documents)

# ä½¿ç”¨å¤„ç†åçš„èŠ‚ç‚¹åˆ›å»ºç´¢å¼•
pg_index = PropertyGraphIndex(
    nodes=nodes,
    property_graph_store=graph_store
)
```

**æœ€ä½³å®è·µ**:

1. **ä¿ç•™ç°æœ‰æ•°æ®**: ä½¿ç”¨`pop()`è·å–ç°æœ‰å®ä½“/å…³ç³»,å¤„ç†åé‡æ–°å­˜å…¥
2. **å¹‚ç­‰æ€§**: ç›¸åŒè¾“å…¥åº”äº§ç”Ÿç›¸åŒè¾“å‡º
3. **é”™è¯¯å¤„ç†**: æ•è·å¼‚å¸¸é¿å…Pipelineå¤±è´¥
4. **æ—¥å¿—è®°å½•**: è®°å½•æå–çš„å®ä½“æ•°é‡,ä¾¿äºè°ƒè¯•

```python
import logging

class RobustGraphExtractor(TransformComponent):
    def __call__(self, llama_nodes: list[BaseNode], **kwargs) -> list[BaseNode]:
        logger = logging.getLogger(__name__)

        for llama_node in llama_nodes:
            try:
                existing_nodes = llama_node.metadata.pop(KG_NODES_KEY, [])
                existing_relations = llama_node.metadata.pop(KG_RELATIONS_KEY, [])

                # è‡ªå®šä¹‰æå–é€»è¾‘
                # ...

                logger.info(f"Extracted {len(existing_nodes)} entities from node {llama_node.node_id}")

                llama_node.metadata[KG_NODES_KEY] = existing_nodes
                llama_node.metadata[KG_RELATIONS_KEY] = existing_relations

            except Exception as e:
                logger.error(f"Failed to extract from node {llama_node.node_id}: {e}")
                # è¿”å›åŸèŠ‚ç‚¹,ä¸ä¸­æ–­Pipeline
                continue

        return llama_nodes
```

---

### 6.3 å›¾æ£€ç´¢å™¨

#### LLMSynonymRetriever

ç”ŸæˆæŸ¥è¯¢çš„åŒä¹‰è¯å’Œç›¸å…³å…³é”®è¯è¿›è¡Œæ£€ç´¢ï¼š

```python
from llama_index.core.indices.property_graph import (
    LLMSynonymRetriever
)

synonym_retriever = LLMSynonymRetriever(
    index=pg_index,
    llm=Settings.llm,
    similarity_top_k=5,
    include_text=True  # åŒ…å«èŠ‚ç‚¹æ–‡æœ¬
)

# æ‰§è¡Œæ£€ç´¢
retrieved_nodes = synonym_retriever.retrieve(
    "machine learning frameworks"
)

for node in retrieved_nodes:
    print(f"Entity: {node.metadata.get('entity_name')}")
    print(f"Type: {node.metadata.get('entity_type')}")
    print(f"Relations: {node.metadata.get('relationships', [])}\n")
```

#### VectorContextRetriever

ä½¿ç”¨è¯­ä¹‰å‘é‡æ£€ç´¢å›¾èŠ‚ç‚¹ï¼š

```python
from llama_index.core.indices.property_graph import (
    VectorContextRetriever
)

vector_context_retriever = VectorContextRetriever(
    index=pg_index,
    similarity_top_k=10,
    include_text=True,
    embed_model=Settings.embed_model
)

retrieved_nodes = vector_context_retriever.retrieve(
    "What technologies are used in AI development?"
)
```

#### TextToCypherRetriever

ä½¿ç”¨ LLM ç”Ÿæˆ Cypher æŸ¥è¯¢ï¼ˆéœ€è¦ Neo4jï¼‰ï¼š

```python
from llama_index.core.indices.property_graph import (
    TextToCypherRetriever
)

# éœ€è¦ Neo4j å›¾å­˜å‚¨
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

neo4j_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="password",
    url="bolt://localhost:7687",
    database="neo4j"
)

text2cypher_retriever = TextToCypherRetriever(
    graph_store=neo4j_store,
    llm=Settings.llm
)

# LLM ä¼šç”Ÿæˆ Cypher æŸ¥è¯¢å¹¶æ‰§è¡Œ
retrieved_nodes = text2cypher_retriever.retrieve(
    "Find all people who work at AI companies"
)
```

---

#### 6.3.5 è‡ªå®šä¹‰å›¾æ£€ç´¢å™¨

LlamaIndexæ”¯æŒåˆ›å»ºè‡ªå®šä¹‰å›¾æ£€ç´¢å™¨,å®ç°ç‰¹å®šçš„æ£€ç´¢é€»è¾‘ã€‚

**æ–¹æ³•1: ä½¿ç”¨CustomPGRetriever (æ¨è)**

```python
from llama_index.core.indices.property_graph import CustomPGRetriever

class MyCustomRetriever(CustomPGRetriever):
    """è‡ªå®šä¹‰å›¾æ£€ç´¢å™¨"""

    def init(self, my_option_1: bool = False, **kwargs):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰æ£€ç´¢å™¨

        Args:
            my_option_1: è‡ªå®šä¹‰é€‰é¡¹
            **kwargs: çˆ¶ç±»å‚æ•° (ä¼šè‡ªåŠ¨è®¾ç½®self.graph_store)
        """
        self.my_option_1 = my_option_1
        # self.graph_store è‡ªåŠ¨å¯ç”¨

    def custom_retrieve(self, query_str: str):
        """
        è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘

        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str, TextNode, NodeWithScore, æˆ–ä»¥ä¸Šç±»å‹çš„åˆ—è¡¨
        """
        # è®¿é—®å›¾å­˜å‚¨
        entities = self.graph_store.get(
            properties={"label": "PERSON"}
        )

        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        if self.my_option_1:
            # ç‰¹æ®Šå¤„ç†é€»è¾‘
            pass

        # è¿”å›ç»“æœ
        return f"Found {len(entities)} entities matching '{query_str}'"

    # å¯é€‰: å¼‚æ­¥ç‰ˆæœ¬
    # async def acustom_retrieve(self, query_str: str):
    #     # å¼‚æ­¥æ£€ç´¢é€»è¾‘
    #     pass
```

**ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨**:

```python
# åˆ›å»ºè‡ªå®šä¹‰æ£€ç´¢å™¨
my_retriever = MyCustomRetriever(
    graph_store=pg_index.property_graph_store,
    my_option_1=True
)

# æ£€ç´¢
results = my_retriever.retrieve("Find all people")
print(results)
```

**å®Œæ•´ç¤ºä¾‹: åŸºäºå…³é”®è¯çš„å›¾æ£€ç´¢å™¨**:

```python
from llama_index.core.indices.property_graph import CustomPGRetriever
from llama_index.core.schema import NodeWithScore, TextNode

class KeywordGraphRetriever(CustomPGRetriever):
    """åŸºäºå…³é”®è¯çš„å›¾æ£€ç´¢å™¨"""

    def init(self, keywords: list[str] = None, top_k: int = 10, **kwargs):
        """
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            top_k: è¿”å›top Kä¸ªèŠ‚ç‚¹
        """
        self.keywords = keywords or []
        self.top_k = top_k

    def custom_retrieve(self, query_str: str):
        """åŸºäºå…³é”®è¯åŒ¹é…æ£€ç´¢å®ä½“"""
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ (ç®€åŒ–ç‰ˆ,å®é™…åº”è¯¥ç”¨NLP)
        query_keywords = set(query_str.lower().split())
        query_keywords.update(self.keywords)

        # ä»å›¾ä¸­è·å–æ‰€æœ‰å®ä½“
        all_entities = self.graph_store.get(properties={})

        # è®¡ç®—åŒ¹é…åˆ†æ•°
        scored_entities = []
        for entity in all_entities:
            entity_name = getattr(entity, 'name', '').lower()
            entity_label = getattr(entity, 'label', '').lower()

            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            score = 0.0
            for keyword in query_keywords:
                if keyword in entity_name:
                    score += 1.0
                if keyword in entity_label:
                    score += 0.5

            if score > 0:
                # è·å–å®ä½“å‘¨è¾¹çš„ä¸Šä¸‹æ–‡
                rel_map = self.graph_store.get_rel_map([entity], depth=1)

                # åˆ›å»ºTextNodeåŒ…å«å®ä½“ä¿¡æ¯
                text = f"Entity: {entity.name} (Type: {entity.label})\n"
                text += f"Properties: {entity.properties}\n"
                text += f"Relations: {rel_map}\n"

                node = TextNode(
                    text=text,
                    metadata={
                        "entity_id": entity.id,
                        "entity_name": entity.name,
                        "entity_label": entity.label
                    }
                )

                scored_entities.append(NodeWithScore(node=node, score=score))

        # æ’åºå¹¶è¿”å›Top K
        scored_entities.sort(key=lambda x: x.score, reverse=True)
        return scored_entities[:self.top_k]

# ä½¿ç”¨ç¤ºä¾‹
keyword_retriever = KeywordGraphRetriever(
    graph_store=pg_index.property_graph_store,
    keywords=["technology", "company", "person"],
    top_k=5
)

results = keyword_retriever.retrieve("Find AI companies")
for node_with_score in results:
    print(f"Score: {node_with_score.score}")
    print(f"Content: {node_with_score.node.get_content()}\n")
```

**é«˜çº§ç¤ºä¾‹: CypheræŸ¥è¯¢æ£€ç´¢å™¨**:

```python
from llama_index.core.indices.property_graph import CustomPGRetriever
from llama_index.core.schema import NodeWithScore, TextNode

class CypherQueryRetriever(CustomPGRetriever):
    """æ‰§è¡Œé¢„å®šä¹‰CypheræŸ¥è¯¢çš„æ£€ç´¢å™¨"""

    def init(self, cypher_template: str, **kwargs):
        """
        Args:
            cypher_template: CypheræŸ¥è¯¢æ¨¡æ¿ (æ”¯æŒå‚æ•°åŒ–)
        """
        self.cypher_template = cypher_template

    def custom_retrieve(self, query_str: str):
        """æ‰§è¡ŒCypheræŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        # æ£€æŸ¥Graph Storeæ˜¯å¦æ”¯æŒCypher
        if not hasattr(self.graph_store, 'structured_query'):
            return "Graph store does not support Cypher queries"

        try:
            # æ‰§è¡ŒCypheræŸ¥è¯¢ (å‡è®¾query_stråŒ…å«å‚æ•°)
            query = self.cypher_template.format(keyword=query_str)
            results = self.graph_store.structured_query(query)

            # æ ¼å¼åŒ–ç»“æœ
            nodes_with_scores = []
            for i, result in enumerate(results):
                text = str(result)
                node = TextNode(
                    text=text,
                    metadata={"result_index": i, "query": query}
                )
                nodes_with_scores.append(NodeWithScore(node=node, score=1.0))

            return nodes_with_scores

        except Exception as e:
            return f"Error executing Cypher query: {e}"

# ä½¿ç”¨ç¤ºä¾‹ (Neo4j)
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

neo4j_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="password",
    url="bolt://localhost:7687"
)

cypher_query = """
MATCH (p:PERSON)-[r:WORKS_AT]->(o:ORGANIZATION)
WHERE o.name CONTAINS '{keyword}'
RETURN p.name, o.name, r
LIMIT 10
"""

cypher_retriever = CypherQueryRetriever(
    graph_store=neo4j_store,
    cypher_template=cypher_query
)

results = cypher_retriever.retrieve("OpenAI")
```

**æ–¹æ³•2: ç»§æ‰¿BasePGRetriever (é«˜çº§)**

```python
from llama_index.core.indices.property_graph.retrievers.base import BasePGRetriever
from llama_index.core.schema import QueryBundle, NodeWithScore

class AdvancedPGRetriever(BasePGRetriever):
    """é«˜çº§è‡ªå®šä¹‰æ£€ç´¢å™¨ (ç»§æ‰¿BasePGRetriever)"""

    def __init__(self, graph_store, custom_param: str, **kwargs):
        super().__init__(**kwargs)
        self.graph_store = graph_store
        self.custom_param = custom_param

    def _retrieve(self, query_bundle: QueryBundle) -> list[NodeWithScore]:
        """
        å®ç°æ£€ç´¢é€»è¾‘

        Args:
            query_bundle: åŒ…å«æŸ¥è¯¢å­—ç¬¦ä¸²å’ŒåµŒå…¥çš„Bundle

        Returns:
            NodeWithScoreåˆ—è¡¨
        """
        query_str = query_bundle.query_str

        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        # ...

        return []

    async def _aretrieve(self, query_bundle: QueryBundle) -> list[NodeWithScore]:
        """å¼‚æ­¥æ£€ç´¢"""
        # å¼‚æ­¥å®ç°
        pass
```

**ç»„åˆå¤šä¸ªè‡ªå®šä¹‰æ£€ç´¢å™¨**:

```python
from llama_index.core.retrievers import QueryFusionRetriever

# åˆ›å»ºå¤šä¸ªè‡ªå®šä¹‰æ£€ç´¢å™¨
keyword_retriever = KeywordGraphRetriever(
    graph_store=pg_index.property_graph_store,
    keywords=["AI", "machine learning"]
)

custom_retriever = MyCustomRetriever(
    graph_store=pg_index.property_graph_store,
    my_option_1=True
)

# ä½¿ç”¨Fusionç»„åˆ
fusion_retriever = QueryFusionRetriever(
    retrievers=[keyword_retriever, custom_retriever],
    similarity_top_k=10,
    num_queries=1  # ä¸ç”Ÿæˆé¢å¤–æŸ¥è¯¢
)

# è”åˆæ£€ç´¢
results = fusion_retriever.retrieve("Find AI technologies")
```

**æœ€ä½³å®è·µ**:

1. **è¿”å›ç±»å‹çµæ´»**: æ”¯æŒstring, TextNode, NodeWithScoreæˆ–å®ƒä»¬çš„åˆ—è¡¨
2. **å¼‚æ­¥æ”¯æŒ**: å®ç°`acustom_retrieve`æå‡æ€§èƒ½
3. **é”™è¯¯å¤„ç†**: æ•è·å¼‚å¸¸é¿å…æ£€ç´¢å¤±è´¥
4. **è¯„åˆ†æœºåˆ¶**: ä¸ºNodeWithScoreæä¾›æœ‰æ„ä¹‰çš„åˆ†æ•° (0.0-1.0)
5. **å…ƒæ•°æ®ä¸°å¯Œ**: åœ¨è¿”å›çš„Nodeä¸­åŒ…å«ä¸°å¯Œçš„å…ƒæ•°æ®,ä¾¿äºåå¤„ç†

```python
class BestPracticeRetriever(CustomPGRetriever):
    def custom_retrieve(self, query_str: str):
        try:
            # æ£€ç´¢é€»è¾‘
            results = []

            # ç¡®ä¿è¿”å›NodeWithScoreå¹¶åŒ…å«åˆ†æ•°
            for result in results:
                node = TextNode(
                    text=result['text'],
                    metadata={
                        "entity_id": result['id'],
                        "retriever": "BestPracticeRetriever",
                        "query": query_str,
                        "timestamp": datetime.now().isoformat()
                    }
                )
                # æä¾›æœ‰æ„ä¹‰çš„è¯„åˆ†
                score = self._calculate_score(result, query_str)
                results.append(NodeWithScore(node=node, score=score))

            return results

        except Exception as e:
            # æ—¥å¿—è®°å½•é”™è¯¯
            logger.error(f"Retrieval failed for query '{query_str}': {e}")
            # è¿”å›ç©ºç»“æœè€ŒéæŠ›å‡ºå¼‚å¸¸
            return []

    def _calculate_score(self, result, query_str):
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        # è‡ªå®šä¹‰è¯„åˆ†é€»è¾‘
        return 1.0
```

---

### 6.4 çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“

```python
# 1. ä½¿ç”¨é»˜è®¤æ£€ç´¢å™¨
query_engine = pg_index.as_query_engine(
    include_text=True,
    similarity_top_k=5
)

response = query_engine.query(
    "What are the main technologies mentioned and how are they related?"
)
print(response)

# 2. ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨
from llama_index.core.query_engine import RetrieverQueryEngine

custom_retriever = LLMSynonymRetriever(
    index=pg_index,
    llm=Settings.llm,
    similarity_top_k=10
)

query_engine = RetrieverQueryEngine.from_args(
    retriever=custom_retriever,
    response_mode="tree_summarize"
)

response = query_engine.query("Explain the relationships between entities")
```

### 6.5 ç»„åˆå¤šä¸ªå›¾æ£€ç´¢å™¨

```python
from llama_index.core.retrievers import QueryFusionRetriever

# åˆ›å»ºå¤šä¸ªæ£€ç´¢å™¨
synonym_retriever = LLMSynonymRetriever(
    index=pg_index,
    similarity_top_k=5
)

vector_retriever = VectorContextRetriever(
    index=pg_index,
    similarity_top_k=5
)

# èåˆæ£€ç´¢ç»“æœ
fusion_retriever = QueryFusionRetriever(
    retrievers=[synonym_retriever, vector_retriever],
    num_queries=1,
    use_async=True
)

query_engine = RetrieverQueryEngine.from_args(retriever=fusion_retriever)
response = query_engine.query("Complex knowledge graph query")
```

---

### 6.6 å¤šè·³æ£€ç´¢ä¸è·¯å¾„æ¨ç†

**å¤šè·³æ£€ç´¢ï¼ˆMulti-hop Retrievalï¼‰** æ˜¯çŸ¥è¯†å›¾è°±çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€,é€šè¿‡éå†å¤šå±‚å…³ç³»æ¥å‘ç°éšè—çš„çŸ¥è¯†å…³è”,å®ç°å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚

#### 6.6.1 å¤šè·³æ£€ç´¢æ¦‚å¿µ

**ä»€ä¹ˆæ˜¯å¤šè·³æ£€ç´¢?**

```
å•è·³ (1-hop):  A â†’ B
ä¸¤è·³ (2-hop):  A â†’ B â†’ C
ä¸‰è·³ (3-hop):  A â†’ B â†’ C â†’ D
Nè·³ (N-hop):   A â†’ ... â†’ Z  (é€šè¿‡å¤šå±‚å…³ç³»è¿æ¥)
```

**åº”ç”¨åœºæ™¯**:
- **å‘ç°éšè—å…³è”**: ä¸¤ä¸ªçœ‹ä¼¼æ— å…³çš„å®ä½“é€šè¿‡ä¸­é—´èŠ‚ç‚¹è¿æ¥
- **çŸ¥è¯†æ¨ç†é“¾**: æ„å»ºå› æœé“¾ã€ç»§æ‰¿é“¾ã€å½±å“é“¾
- **ç¤¾äº¤ç½‘ç»œåˆ†æ**: æœ‹å‹çš„æœ‹å‹ã€å¤šåº¦äººè„‰
- **å­¦æœ¯æ–‡çŒ®æº¯æº**: å¼•ç”¨é“¾ã€ç ”ç©¶è„‰ç»œè¿½è¸ª

#### 6.6.2 Neo4j Cypherå¤šè·³æŸ¥è¯¢

LlamaIndexçš„Neo4jé›†æˆæ”¯æŒé€šè¿‡ `structured_query()` æ‰§è¡ŒåŸç”ŸCypheræŸ¥è¯¢:

```python
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

# åˆå§‹åŒ–Neo4jå›¾å­˜å‚¨
graph_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="password",
    url="bolt://localhost:7687",
    database="neo4j"
)

# 1. å›ºå®šè·³æ•°æŸ¥è¯¢ (1-3è·³)
def multi_hop_query(entity_name: str, max_hops: int = 3, limit: int = 10):
    """æŸ¥è¯¢æŒ‡å®šè·³æ•°èŒƒå›´å†…çš„æ‰€æœ‰è·¯å¾„"""
    cypher_query = f"""
    MATCH path = (start {{name: '{entity_name}'}})-[*1..{max_hops}]->(target)
    WHERE start <> target
    RETURN
        start.name as èµ·ç‚¹,
        [node in nodes(path) | node.name] as è·¯å¾„èŠ‚ç‚¹,
        [rel in relationships(path) | type(rel)] as è·¯å¾„å…³ç³»,
        length(path) as è·³æ•°,
        target.name as ç›®æ ‡
    ORDER BY è·³æ•°, target.name
    LIMIT {limit}
    """

    results = graph_store.structured_query(cypher_query)
    return results

# ä½¿ç”¨ç¤ºä¾‹
results = multi_hop_query("äººå·¥æ™ºèƒ½", max_hops=3, limit=15)

for r in results:
    path_length = r['è·³æ•°']
    start = r['èµ·ç‚¹']
    target = r['ç›®æ ‡']
    path_nodes = r['è·¯å¾„èŠ‚ç‚¹']
    path_rels = r['è·¯å¾„å…³ç³»']

    # æ„å»ºè·¯å¾„å­—ç¬¦ä¸²
    path_str = path_nodes[0]
    for i in range(len(path_rels)):
        path_str += f" --[{path_rels[i]}]--> {path_nodes[i+1]}"

    print(f"[{path_length}è·³] {path_str}")
```

**è¾“å‡ºç¤ºä¾‹**:
```
[1è·³] äººå·¥æ™ºèƒ½ --[åŒ…å«]--> æœºå™¨å­¦ä¹ 
[2è·³] äººå·¥æ™ºèƒ½ --[åŒ…å«]--> æœºå™¨å­¦ä¹  --[åŒ…å«]--> æ·±åº¦å­¦ä¹ 
[3è·³] äººå·¥æ™ºèƒ½ --[åŒ…å«]--> æœºå™¨å­¦ä¹  --[åŒ…å«]--> æ·±åº¦å­¦ä¹  --[ä½¿ç”¨]--> ç¥ç»ç½‘ç»œ
```

#### 6.6.3 æœ€çŸ­è·¯å¾„æŸ¥è¯¢

Cypherçš„ `shortestPath()` å‡½æ•°æ‰¾åˆ°ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„æœ€çŸ­è·¯å¾„:

```python
def shortest_path_query(start_entity: str, end_entity: str, max_hops: int = 5):
    """æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„æœ€çŸ­è·¯å¾„"""
    cypher_query = f"""
    MATCH path = shortestPath(
        (start {{name: '{start_entity}'}})-[*1..{max_hops}]-(end {{name: '{end_entity}'}})
    )
    RETURN
        start.name as èµ·ç‚¹,
        end.name as ç»ˆç‚¹,
        [node in nodes(path) | node.name] as å®Œæ•´è·¯å¾„,
        [rel in relationships(path) | type(rel)] as è·¯å¾„å…³ç³»,
        length(path) as è·¯å¾„é•¿åº¦
    """

    results = graph_store.structured_query(cypher_query)

    if results:
        result = results[0]
        print(f"æœ€çŸ­è·¯å¾„ (é•¿åº¦: {result['è·¯å¾„é•¿åº¦']}è·³):")

        full_path = result['å®Œæ•´è·¯å¾„']
        rels = result['è·¯å¾„å…³ç³»']

        path_str = full_path[0]
        for i in range(len(rels)):
            path_str += f" --[{rels[i]}]--> {full_path[i+1]}"
        print(path_str)
    else:
        print(f"æœªæ‰¾åˆ°ä» '{start_entity}' åˆ° '{end_entity}' çš„è·¯å¾„")

    return results

# ä½¿ç”¨ç¤ºä¾‹
shortest_path_query("æ·±åº¦å­¦ä¹ ", "å›¾éå†", max_hops=5)
```

**è¾“å‡º**:
```
æœ€çŸ­è·¯å¾„ (é•¿åº¦: 4è·³):
æ·±åº¦å­¦ä¹  --[ä½¿ç”¨]--> ç¥ç»ç½‘ç»œ --[åº”ç”¨äº]--> å›¾åƒè¯†åˆ« --[å±äº]--> è®¡ç®—æœºè§†è§‰ --[éœ€è¦]--> å›¾éå†
```

#### 6.6.4 å•è·³ã€åŒè·³ã€ä¸‰è·³æ£€ç´¢å¯¹æ¯”

```python
class MultiHopRetriever:
    """å¤šè·³æ£€ç´¢å™¨"""

    def __init__(self, graph_store: Neo4jPropertyGraphStore):
        self.graph_store = graph_store

    def single_hop(self, entity: str, top_k: int = 5):
        """1è·³æ£€ç´¢: æŸ¥æ‰¾ç›´æ¥ç›¸å…³çš„èŠ‚ç‚¹"""
        cypher = f"""
        MATCH (start {{name: '{entity}'}})-[r]->(target)
        RETURN
            start.name as èµ·ç‚¹,
            type(r) as å…³ç³»,
            target.name as ç›®æ ‡,
            target.category as ç±»åˆ«
        LIMIT {top_k}
        """
        return self.graph_store.structured_query(cypher)

    def two_hop(self, entity: str, top_k: int = 10):
        """2è·³æ£€ç´¢: æŸ¥æ‰¾è·ç¦»2æ­¥çš„èŠ‚ç‚¹"""
        cypher = f"""
        MATCH path = (start {{name: '{entity}'}})-[r1]->(mid)-[r2]->(target)
        WHERE start <> target
        RETURN
            start.name as èµ·ç‚¹,
            type(r1) as å…³ç³»1,
            mid.name as ä¸­é—´èŠ‚ç‚¹,
            type(r2) as å…³ç³»2,
            target.name as ç›®æ ‡
        LIMIT {top_k}
        """
        return self.graph_store.structured_query(cypher)

    def three_hop(self, entity: str, top_k: int = 10):
        """3è·³æ£€ç´¢: æŸ¥æ‰¾è·ç¦»3æ­¥çš„èŠ‚ç‚¹"""
        cypher = f"""
        MATCH path = (start {{name: '{entity}'}})-[r1]->(n1)-[r2]->(n2)-[r3]->(target)
        WHERE start <> target
        RETURN
            start.name as èµ·ç‚¹,
            type(r1) as å…³ç³»1,
            n1.name as èŠ‚ç‚¹1,
            type(r2) as å…³ç³»2,
            n2.name as èŠ‚ç‚¹2,
            type(r3) as å…³ç³»3,
            target.name as ç›®æ ‡
        LIMIT {top_k}
        """
        return self.graph_store.structured_query(cypher)

# ä½¿ç”¨ç¤ºä¾‹
retriever = MultiHopRetriever(graph_store)

print("=== 1è·³æ£€ç´¢ ===")
results = retriever.single_hop("äººå·¥æ™ºèƒ½")
for r in results:
    print(f"{r['èµ·ç‚¹']} --[{r['å…³ç³»']}]--> {r['ç›®æ ‡']}")

print("\n=== 2è·³æ£€ç´¢ ===")
results = retriever.two_hop("äººå·¥æ™ºèƒ½")
for r in results:
    print(f"{r['èµ·ç‚¹']} --[{r['å…³ç³»1']}]--> {r['ä¸­é—´èŠ‚ç‚¹']} --[{r['å…³ç³»2']}]--> {r['ç›®æ ‡']}")

print("\n=== 3è·³æ£€ç´¢ ===")
results = retriever.three_hop("äººå·¥æ™ºèƒ½")
for r in results:
    print(f"{r['èµ·ç‚¹']} --[{r['å…³ç³»1']}]--> {r['èŠ‚ç‚¹1']} --[{r['å…³ç³»2']}]--> {r['èŠ‚ç‚¹2']} --[{r['å…³ç³»3']}]--> {r['ç›®æ ‡']}")
```

#### 6.6.5 å¤šè·³æ¨ç†: åŸºäºè·¯å¾„ç”Ÿæˆç­”æ¡ˆ

å°†å¤šè·³æ£€ç´¢ä¸LLMç»“åˆ,å®ç°åŸºäºçŸ¥è¯†è·¯å¾„çš„æ¨ç†:

```python
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI

def multi_hop_reasoning(
    graph_store: Neo4jPropertyGraphStore,
    question: str,
    start_entity: str,
    max_hops: int = 3,
    top_k: int = 20
):
    """å¤šè·³æ¨ç†: åŸºäºé—®é¢˜å’Œèµ·å§‹å®ä½“,æ£€ç´¢è·¯å¾„å¹¶ç”Ÿæˆç­”æ¡ˆ"""

    # 1. æ£€ç´¢å¤šè·³è·¯å¾„
    cypher_query = f"""
    MATCH path = (start {{name: '{start_entity}'}})-[*1..{max_hops}]->(target)
    WHERE start <> target
    RETURN
        [node in nodes(path) | node.name] as è·¯å¾„,
        [rel in relationships(path) | type(rel)] as å…³ç³»,
        length(path) as è·³æ•°
    ORDER BY è·³æ•°
    LIMIT {top_k}
    """

    paths = graph_store.structured_query(cypher_query)

    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    print(f"æ£€ç´¢åˆ° {len(paths)} æ¡çŸ¥è¯†è·¯å¾„:\n")

    for idx, path_data in enumerate(paths[:10], 1):
        path_nodes = path_data['è·¯å¾„']
        relations = path_data['å…³ç³»']
        hops = path_data['è·³æ•°']

        # æ„å»ºè·¯å¾„å­—ç¬¦ä¸²
        path_str = path_nodes[0]
        for i in range(len(relations)):
            path_str += f" --[{relations[i]}]--> {path_nodes[i+1]}"

        print(f"[{idx}] ({hops}è·³) {path_str}")
        context_parts.append(path_str)

    context = "\n".join(context_parts)

    # 3. ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
    prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±è·¯å¾„ä¿¡æ¯,å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†è·¯å¾„:
{context}

é—®é¢˜: {question}

è¯·åŸºäºä¸Šè¿°è·¯å¾„ä¿¡æ¯ç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆ,å¹¶è¯´æ˜æ¨ç†è¿‡ç¨‹ã€‚
"""

    llm = Settings.llm or OpenAI(model="gpt-4")
    response = llm.complete(prompt)

    print(f"\n{'='*60}")
    print("æ¨ç†ç­”æ¡ˆ:")
    print(f"{'='*60}")
    print(response.text)

    return response.text

# ä½¿ç”¨ç¤ºä¾‹
multi_hop_reasoning(
    graph_store=graph_store,
    question="æ·±åº¦å­¦ä¹ ä¸å¤šè·³æ£€ç´¢ä¹‹é—´æœ‰ä»€ä¹ˆè”ç³»?",
    start_entity="æ·±åº¦å­¦ä¹ ",
    max_hops=4
)
```

**è¾“å‡ºç¤ºä¾‹**:
```
æ£€ç´¢åˆ° 15 æ¡çŸ¥è¯†è·¯å¾„:

[1] (1è·³) æ·±åº¦å­¦ä¹  --[ä½¿ç”¨]--> ç¥ç»ç½‘ç»œ
[2] (2è·³) æ·±åº¦å­¦ä¹  --[ä½¿ç”¨]--> ç¥ç»ç½‘ç»œ --[æ¨¡æ‹Ÿ]--> äººè„‘ç»“æ„
[3] (3è·³) æ·±åº¦å­¦ä¹  --[ä½¿ç”¨]--> ç¥ç»ç½‘ç»œ --[åº”ç”¨äº]--> å›¾åƒè¯†åˆ« --[å±äº]--> è®¡ç®—æœºè§†è§‰
[4] (4è·³) æ·±åº¦å­¦ä¹  --[åŒ…å«äº]--> æœºå™¨å­¦ä¹  --[åº”ç”¨äº]--> çŸ¥è¯†å›¾è°± --[æ”¯æŒ]--> å¤šè·³æ£€ç´¢
...

============================================================
æ¨ç†ç­”æ¡ˆ:
============================================================
åŸºäºçŸ¥è¯†å›¾è°±è·¯å¾„åˆ†æ,æ·±åº¦å­¦ä¹ ä¸å¤šè·³æ£€ç´¢çš„è”ç³»ä½“ç°åœ¨:

1. **é—´æ¥å…³è”** (4è·³è·¯å¾„):
   æ·±åº¦å­¦ä¹  â†’ æœºå™¨å­¦ä¹  â†’ çŸ¥è¯†å›¾è°± â†’ å¤šè·³æ£€ç´¢

2. **æŠ€æœ¯æ”¯æ’‘**:
   - æ·±åº¦å­¦ä¹ ä½œä¸ºæœºå™¨å­¦ä¹ çš„å­é¢†åŸŸ,æä¾›äº†å¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›
   - çŸ¥è¯†å›¾è°±åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–
   - å¤šè·³æ£€ç´¢ä¾èµ–çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†å­˜å‚¨

3. **åº”ç”¨ååŒ**:
   æ·±åº¦å­¦ä¹ å¯ç”¨äºå¢å¼ºçŸ¥è¯†å›¾è°±çš„æ„å»ºè´¨é‡,è¿›è€Œæå‡å¤šè·³æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚
```

#### 6.6.6 ä¸PropertyGraphIndexé›†æˆ

å°†å¤šè·³æ£€ç´¢ä¸PropertyGraphIndexç»“åˆ:

```python
from llama_index.core import PropertyGraphIndex, Document
from llama_index.embeddings.openai import OpenAIEmbedding

# 1. åˆ›å»ºPropertyGraphIndex
documents = [
    Document(text="""
    äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸã€‚
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ã€‚ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯ã€‚
    çŸ¥è¯†å›¾è°±å­˜å‚¨ç»“æ„åŒ–çŸ¥è¯†ã€‚å¤šè·³æ£€ç´¢åŸºäºçŸ¥è¯†å›¾è°±å®ç°å¤æ‚æ¨ç†ã€‚
    """)
]

Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

pg_index = PropertyGraphIndex.from_documents(
    documents,
    property_graph_store=graph_store,
    show_progress=True
)

# 2. å®šä¹‰å¤šè·³æ£€ç´¢å·¥å…·
class GraphMultiHopRetriever:
    """å›¾è°±å¤šè·³æ£€ç´¢å™¨ (é›†æˆPropertyGraphIndex)"""

    def __init__(self, index: PropertyGraphIndex):
        self.index = index
        self.graph_store = index.property_graph_store

    def retrieve_paths(self, start_entity: str, max_hops: int = 3):
        """æ£€ç´¢å¤šè·³è·¯å¾„"""
        cypher = f"""
        MATCH path = (start {{name: '{start_entity}'}})-[*1..{max_hops}]->(target)
        RETURN path
        LIMIT 20
        """
        return self.graph_store.structured_query(cypher)

    def retrieve_with_context(self, query: str, start_entity: str, max_hops: int = 2):
        """ç»“åˆå‘é‡æ£€ç´¢å’Œå¤šè·³è·¯å¾„"""
        # 1. å‘é‡æ£€ç´¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        query_engine = self.index.as_query_engine(similarity_top_k=5)
        vector_response = query_engine.query(query)

        # 2. å¤šè·³æ£€ç´¢è·å–è·¯å¾„
        paths = self.retrieve_paths(start_entity, max_hops)

        # 3. åˆå¹¶ç»“æœ
        return {
            "vector_context": vector_response.response,
            "graph_paths": paths
        }

# ä½¿ç”¨ç¤ºä¾‹
retriever = GraphMultiHopRetriever(pg_index)
result = retriever.retrieve_with_context(
    query="å¤šè·³æ£€ç´¢çš„å·¥ä½œåŸç†",
    start_entity="çŸ¥è¯†å›¾è°±",
    max_hops=2
)

print("å‘é‡æ£€ç´¢ç»“æœ:", result["vector_context"])
print("å›¾è°±è·¯å¾„:", result["graph_paths"])
```

#### 6.6.7 å¤šè·³æ£€ç´¢çš„å®é™…åº”ç”¨

**æ¡ˆä¾‹1: ç¤¾äº¤ç½‘ç»œåˆ†æ - æœ‹å‹æ¨è**

```python
# æŸ¥æ‰¾"æœ‹å‹çš„æœ‹å‹"ä½†ä¸æ˜¯ç›´æ¥æœ‹å‹
cypher_friend_of_friend = """
MATCH (user {name: 'å¼ ä¸‰'})-[:FRIEND]->(friend)-[:FRIEND]->(fof)
WHERE NOT (user)-[:FRIEND]->(fof) AND user <> fof
RETURN DISTINCT fof.name as æ¨èå¥½å‹, count(friend) as å…±åŒå¥½å‹æ•°
ORDER BY å…±åŒå¥½å‹æ•° DESC
LIMIT 10
"""

recommendations = graph_store.structured_query(cypher_friend_of_friend)
```

**æ¡ˆä¾‹2: å­¦æœ¯æº¯æº - å¼•ç”¨é“¾è¿½è¸ª**

```python
# è¿½è¸ªè®ºæ–‡çš„å¼•ç”¨é“¾ (æ‰¾åˆ°é—´æ¥å½±å“)
cypher_citation_chain = """
MATCH path = (paper {title: 'Attention Is All You Need'})<-[:CITES*1..3]-(citing_paper)
RETURN
    [node in nodes(path) | node.title] as å¼•ç”¨é“¾,
    length(path) as å¼•ç”¨æ·±åº¦
ORDER BY å¼•ç”¨æ·±åº¦
LIMIT 20
"""

citation_chains = graph_store.structured_query(cypher_citation_chain)
```

**æ¡ˆä¾‹3: ä¼ä¸šå…³ç³»æŒ–æ˜ - ä¾›åº”é“¾è¿½æº¯**

```python
# è¿½è¸ªä¾›åº”é“¾çš„å¤šå±‚å…³ç³»
cypher_supply_chain = """
MATCH path = (company {name: 'è‹¹æœå…¬å¸'})-[:SUPPLIES_TO*1..4]->(end_customer)
RETURN
    [node in nodes(path) | node.name] as ä¾›åº”é“¾,
    length(path) as å±‚çº§
ORDER BY å±‚çº§
"""

supply_chains = graph_store.structured_query(cypher_supply_chain)
```

#### 6.6.8 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**1. é™åˆ¶è·³æ•°èŒƒå›´**

```python
# âŒ ä¸æ¨è: æ— é™åˆ¶è·³æ•°å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜
cypher_bad = "MATCH path = (start)-[*]-(end) RETURN path"

# âœ… æ¨è: æ˜ç¡®è·³æ•°ä¸Šé™
cypher_good = "MATCH path = (start)-[*1..3]-(end) RETURN path LIMIT 100"
```

**2. ä½¿ç”¨ç´¢å¼•**

```python
# åœ¨Neo4jä¸­åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢
create_index_cypher = """
CREATE INDEX entity_name_index IF NOT EXISTS
FOR (n:Entity) ON (n.name)
"""
graph_store.structured_query(create_index_cypher)
```

**3. è·¯å¾„è¿‡æ»¤**

```python
# åªè¿”å›ç¬¦åˆæ¡ä»¶çš„è·¯å¾„
cypher_filtered = """
MATCH path = (start {name: 'äººå·¥æ™ºèƒ½'})-[*1..3]->(target)
WHERE ALL(rel in relationships(path) WHERE type(rel) IN ['åŒ…å«', 'åº”ç”¨äº'])
AND target.category = 'æŠ€æœ¯'
RETURN path
LIMIT 20
"""
```

#### 6.6.9 å°ç»“

**å¤šè·³æ£€ç´¢çš„æ ¸å¿ƒä»·å€¼**:
- âœ… å‘ç°éšè—çš„çŸ¥è¯†å…³è”
- âœ… æ”¯æŒå¤æ‚çš„æ¨ç†ä»»åŠ¡
- âœ… æ­ç¤ºå®ä½“é—´çš„é—´æ¥å…³ç³»
- âœ… æ„å»ºå®Œæ•´çš„çŸ¥è¯†é“¾æ¡

**å…³é”®API**:
- `graph_store.structured_query()` - æ‰§è¡ŒCypheræŸ¥è¯¢
- `MATCH path = (start)-[*1..N]->(end)` - å¯å˜è·³æ•°
- `shortestPath()` - æœ€çŸ­è·¯å¾„
- `length(path)` - è·¯å¾„é•¿åº¦
- `nodes(path)`, `relationships(path)` - æå–è·¯å¾„å…ƒç´ 

**æœ€ä½³å®è·µ**:
1. æ§åˆ¶è·³æ•°ä¸Šé™ (é€šå¸¸1-4è·³)
2. ä½¿ç”¨LIMITé™åˆ¶ç»“æœæ•°é‡
3. åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
4. ç»“åˆå‘é‡æ£€ç´¢å’Œè·¯å¾„æ£€ç´¢
5. å°†è·¯å¾„ä¿¡æ¯æä¾›ç»™LLMè¿›è¡Œæ¨ç†

---

### 6.7 å®ä½“æ¶ˆæ­§ä¸å†²çªè§£å†³

åœ¨çœŸå®ä¸–ç•Œçš„çŸ¥è¯†å›¾è°±ä¸­,**å®ä½“æ¶ˆæ­§ï¼ˆEntity Resolutionï¼‰** æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜:å¦‚ä½•åˆ¤æ–­ä¸¤ä¸ªçœ‹ä¼¼ä¸åŒçš„å®ä½“è®°å½•æ˜¯å¦æŒ‡å‘åŒä¸€ä¸ªçœŸå®å®ä½“?

#### 6.7.1 å®ä½“æ¶ˆæ­§é—®é¢˜

**é—®é¢˜åœºæ™¯**:

```
å®ä½“A: åå­—="å¼ ä¼Ÿ", èŒä¸š="å·¥ç¨‹å¸ˆ", ç¤¾åª’="@zhangwei123"
å®ä½“B: åå­—="å¼ ä¼Ÿ", èŒä¸š="æ•™å¸ˆ",   ç¤¾åª’="@zhangwei456"
å®ä½“C: åå­—="Zhang Wei", èŒä¸š="å·¥ç¨‹å¸ˆ", ç¤¾åª’="@zhangwei123"

é—®é¢˜:
- A vs B: æ˜¯åŒä¸€ä¸ªäººå—? â†’ ä¸æ˜¯ (èŒä¸šã€ç¤¾åª’è´¦å·éƒ½ä¸åŒ)
- A vs C: æ˜¯åŒä¸€ä¸ªäººå—? â†’ æ˜¯ (ç¤¾åª’è´¦å·ç›¸åŒ,åªæ˜¯ä¸­è‹±æ–‡å)
```

**æŒ‘æˆ˜**:
- **åŒåå¼‚äºº**: å¤šä¸ªä¸åŒçš„äººæœ‰ç›¸åŒåå­—
- **å¼‚ååŒäºº**: åŒä¸€ä¸ªäººæœ‰å¤šä¸ªåå­—(åˆ«åã€æ›¾ç”¨åã€ä¸­è‹±æ–‡å)
- **æ•°æ®è´¨é‡**: æ‹¼å†™é”™è¯¯ã€æ ¼å¼ä¸ä¸€è‡´
- **å±æ€§å†²çª**: åŒä¸€ä¸ªäººçš„ä¸åŒè®°å½•å¯èƒ½æœ‰çŸ›ç›¾çš„å±æ€§

#### 6.7.2 LLMè¾…åŠ©å®ä½“æ¶ˆæ­§

ä½¿ç”¨LLMçš„ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›è¿›è¡Œæ™ºèƒ½åˆ¤æ–­:

```python
from pydantic import BaseModel, Field
from llama_index.core.llms import ChatMessage
from llama_index.llms.openai import OpenAI

class EntityMatch(BaseModel):
    """å®ä½“åŒ¹é…ç»“æœ"""
    is_same_entity: bool = Field(description="æ˜¯å¦æ˜¯åŒä¸€ä¸ªå®ä½“")
    confidence: float = Field(
        description="ç½®ä¿¡åº¦ 0-1,è¡¨ç¤ºåˆ¤æ–­çš„å¯ä¿¡ç¨‹åº¦",
        ge=0.0,
        le=1.0
    )
    reason: str = Field(description="åˆ¤æ–­ç†ç”±,è¯´æ˜ä¾æ®å“ªäº›ä¿¡æ¯åšå‡ºåˆ¤æ–­")
    merge_strategy: str = Field(
        description="åˆå¹¶ç­–ç•¥: keep_both(ä¿ç•™ä¸¤ä¸ª), merge_to_first(åˆå¹¶åˆ°ç¬¬ä¸€ä¸ª), merge_to_second(åˆå¹¶åˆ°ç¬¬äºŒä¸ª)"
    )

def resolve_entities(entity_a: dict, entity_b: dict, llm: OpenAI) -> EntityMatch:
    """ä½¿ç”¨LLMåˆ¤æ–­ä¸¤ä¸ªå®ä½“æ˜¯å¦ç›¸åŒ"""

    prompt = f"""åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªå®ä½“æ˜¯å¦æ˜¯åŒä¸€ä¸ªäºº/ç‰©:

å®ä½“A: {entity_a}
å®ä½“B: {entity_b}

åˆ¤æ–­ä¾æ®:
1. **ç¤¾åª’è´¦å·ç›¸åŒ** â†’ å¤§æ¦‚ç‡åŒä¸€äºº (å¼ºåŒ¹é…ä¿¡å·)
2. **é‚®ç®±/ç”µè¯ç›¸åŒ** â†’ å¤§æ¦‚ç‡åŒä¸€äºº
3. **åå­—ç›¸ä¼¼** + å…¶ä»–å±æ€§ç›¸ä¼¼ â†’ å¯èƒ½åŒä¸€äºº
4. **åå­—ç›¸åŒ** ä½†å…¶ä»–å±æ€§å®Œå…¨ä¸åŒ â†’ å¯èƒ½æ˜¯åŒåå¼‚äºº

è¯·åŸºäºä¸Šè¿°è§„åˆ™,åˆ¤æ–­æ˜¯å¦æ˜¯åŒä¸€ä¸ªå®ä½“,å¹¶ç»™å‡ºç½®ä¿¡åº¦å’Œç†ç”±ã€‚
"""

    # ä½¿ç”¨LlamaIndexçš„structured_predict
    result = llm.structured_predict(
        EntityMatch,
        prompt=prompt
    )

    return result

# ä½¿ç”¨ç¤ºä¾‹
llm = OpenAI(model="gpt-4", temperature=0)

entity_a = {
    "name": "å¼ ä¼Ÿ",
    "occupation": "å·¥ç¨‹å¸ˆ",
    "social_media": "@zhangwei123",
    "company": "ç§‘æŠ€å…¬å¸A"
}

entity_b = {
    "name": "Zhang Wei",
    "occupation": "Software Engineer",
    "social_media": "@zhangwei123",
    "company": "Tech Company A"
}

match_result = resolve_entities(entity_a, entity_b, llm)

print(f"æ˜¯å¦åŒä¸€å®ä½“: {match_result.is_same_entity}")
print(f"ç½®ä¿¡åº¦: {match_result.confidence:.2f}")
print(f"ç†ç”±: {match_result.reason}")
print(f"åˆå¹¶ç­–ç•¥: {match_result.merge_strategy}")
```

**è¾“å‡ºç¤ºä¾‹**:
```
æ˜¯å¦åŒä¸€å®ä½“: True
ç½®ä¿¡åº¦: 0.95
ç†ç”±: ä¸¤ä¸ªå®ä½“çš„ç¤¾åª’è´¦å·å®Œå…¨ç›¸åŒ(@zhangwei123),ä¸”èŒä¸šå’Œå…¬å¸ä¿¡æ¯è¯­ä¹‰ä¸€è‡´(å·¥ç¨‹å¸ˆ=Software Engineer, ç§‘æŠ€å…¬å¸A=Tech Company A),åªæ˜¯ä¸­è‹±æ–‡è¡¨è¿°ä¸åŒã€‚ç¤¾åª’è´¦å·ä½œä¸ºå¼ºåŒ¹é…ä¿¡å·,å¯ä»¥ç¡®å®šæ˜¯åŒä¸€äººã€‚
åˆå¹¶ç­–ç•¥: merge_to_first
```

#### 6.7.3 æ„å»º"å¯èƒ½ç›¸åŒ"å…³ç³»

ä¸ç›´æ¥åˆå¹¶å®ä½“,è€Œæ˜¯æ„å»º `POSSIBLY_SAME_AS` å…³ç³»,ä¿ç•™åŸå§‹æ•°æ®:

```python
def create_possibly_same_relation(
    graph_store: Neo4jPropertyGraphStore,
    entity_a_id: str,
    entity_b_id: str,
    confidence: float,
    reason: str
):
    """æ„å»º"å¯èƒ½ç›¸åŒ"å…³ç³»"""

    cypher_create = """
    MATCH (a {id: $entity_a_id}), (b {id: $entity_b_id})
    CREATE (a)-[:POSSIBLY_SAME_AS {
        confidence: $confidence,
        reason: $reason,
        created_at: datetime(),
        status: 'pending'  // å¾…äººå·¥ç¡®è®¤
    }]->(b)
    """

    graph_store.structured_query(
        cypher_create,
        param_map={
            "entity_a_id": entity_a_id,
            "entity_b_id": entity_b_id,
            "confidence": confidence,
            "reason": reason
        }
    )

    print(f"âœ… åˆ›å»º POSSIBLY_SAME_AS å…³ç³»: {entity_a_id} <-> {entity_b_id} (ç½®ä¿¡åº¦: {confidence:.2f})")

# ä½¿ç”¨ç¤ºä¾‹
create_possibly_same_relation(
    graph_store=graph_store,
    entity_a_id="person:001",
    entity_b_id="person:002",
    confidence=0.95,
    reason="ç¤¾åª’è´¦å·ç›¸åŒ"
)
```

#### 6.7.4 æŸ¥è¯¢æ—¶åŠ¨æ€è§£æ

åœ¨æŸ¥è¯¢æ—¶è€ƒè™‘"å¯èƒ½ç›¸åŒ"çš„å®ä½“:

```python
def query_with_entity_resolution(
    graph_store: Neo4jPropertyGraphStore,
    entity_name: str,
    confidence_threshold: float = 0.8
):
    """æŸ¥è¯¢æ—¶åŠ¨æ€è§£æå®ä½“,åŒ…å«å¯èƒ½ç›¸åŒçš„å®ä½“"""

    cypher_query = f"""
    MATCH (start {{name: '{entity_name}'}})

    // æ‰¾åˆ°æ‰€æœ‰å¯èƒ½ç›¸åŒçš„å®ä½“ (åŒå‘)
    OPTIONAL MATCH (start)-[same:POSSIBLY_SAME_AS]-(equivalent)
    WHERE same.confidence >= {confidence_threshold}

    // åˆå¹¶æ‰€æœ‰ç­‰ä»·å®ä½“
    WITH collect(DISTINCT equivalent) + [start] as all_entities

    // å±•å¼€å¹¶æŸ¥è¯¢æ¯ä¸ªå®ä½“çš„å…³ç³»
    UNWIND all_entities as entity
    MATCH (entity)-[r]->(target)

    RETURN DISTINCT
        entity.name as æ¥æºå®ä½“,
        type(r) as å…³ç³»ç±»å‹,
        target.name as ç›®æ ‡å®ä½“
    """

    results = graph_store.structured_query(cypher_query)

    print(f"æŸ¥è¯¢å®ä½“ '{entity_name}' (åŒ…å«ç­‰ä»·å®ä½“):")
    for r in results:
        print(f"  {r['æ¥æºå®ä½“']} --[{r['å…³ç³»ç±»å‹']}]--> {r['ç›®æ ‡å®ä½“']}")

    return results

# ä½¿ç”¨ç¤ºä¾‹
query_with_entity_resolution(
    graph_store=graph_store,
    entity_name="å¼ ä¼Ÿ",
    confidence_threshold=0.8
)
```

**è¾“å‡º**:
```
æŸ¥è¯¢å®ä½“ 'å¼ ä¼Ÿ' (åŒ…å«ç­‰ä»·å®ä½“):
  å¼ ä¼Ÿ --[å·¥ä½œäº]--> ç§‘æŠ€å…¬å¸A
  Zhang Wei --[å‘è¡¨]--> è®ºæ–‡X
  å¼ ä¼Ÿ --[è®¤è¯†]--> æå››
```

#### 6.7.5 å®ä½“åˆå¹¶ç­–ç•¥

å®šä¹‰ä¸åŒçš„åˆå¹¶ç­–ç•¥:

```python
from enum import Enum

class MergeStrategy(str, Enum):
    """å®ä½“åˆå¹¶ç­–ç•¥"""
    KEEP_BOTH = "keep_both"              # ä¿ç•™ä¸¤ä¸ªç‹¬ç«‹å®ä½“
    MERGE_TO_FIRST = "merge_to_first"    # åˆå¹¶åˆ°ç¬¬ä¸€ä¸ª,åˆ é™¤ç¬¬äºŒä¸ª
    MERGE_TO_SECOND = "merge_to_second"  # åˆå¹¶åˆ°ç¬¬äºŒä¸ª,åˆ é™¤ç¬¬ä¸€ä¸ª
    CREATE_ALIAS = "create_alias"        # åˆ›å»ºåˆ«åå…³ç³»
    MANUAL_REVIEW = "manual_review"      # äººå·¥å®¡æ ¸

def execute_merge_strategy(
    graph_store: Neo4jPropertyGraphStore,
    entity_a_id: str,
    entity_b_id: str,
    strategy: MergeStrategy
):
    """æ‰§è¡Œåˆå¹¶ç­–ç•¥"""

    if strategy == MergeStrategy.KEEP_BOTH:
        # åªåˆ›å»ºPOSSIBLY_SAME_ASå…³ç³»,ä¸åˆå¹¶
        print(f"ä¿ç•™ä¸¤ä¸ªå®ä½“: {entity_a_id}, {entity_b_id}")

    elif strategy == MergeStrategy.MERGE_TO_FIRST:
        # å°†Bçš„æ‰€æœ‰å…³ç³»è½¬ç§»åˆ°A,ç„¶ååˆ é™¤B
        cypher_merge = """
        MATCH (a {id: $entity_a_id}), (b {id: $entity_b_id})

        // 1. è½¬ç§»Bçš„æ‰€æœ‰å‡ºè¾¹åˆ°A
        OPTIONAL MATCH (b)-[r]->(target)
        WHERE NOT (a)-[]->(target)  // é¿å…é‡å¤
        CREATE (a)-[new_r:SAME_TYPE_AS_r]->(target)
        SET new_r = properties(r)

        // 2. è½¬ç§»Bçš„æ‰€æœ‰å…¥è¾¹åˆ°A
        OPTIONAL MATCH (source)-[r]->(b)
        WHERE NOT (source)-[]->(a)
        CREATE (source)-[new_r:SAME_TYPE_AS_r]->(a)
        SET new_r = properties(r)

        // 3. åˆ é™¤B
        DETACH DELETE b
        """

        graph_store.structured_query(
            cypher_merge,
            param_map={
                "entity_a_id": entity_a_id,
                "entity_b_id": entity_b_id
            }
        )
        print(f"âœ… åˆå¹¶å®Œæˆ: {entity_b_id} â†’ {entity_a_id}")

    elif strategy == MergeStrategy.CREATE_ALIAS:
        # åˆ›å»ºåˆ«åå…³ç³»: A --[HAS_ALIAS]--> B
        cypher_alias = """
        MATCH (a {id: $entity_a_id}), (b {id: $entity_b_id})
        CREATE (a)-[:HAS_ALIAS]->(b)
        """
        graph_store.structured_query(
            cypher_alias,
            param_map={
                "entity_a_id": entity_a_id,
                "entity_b_id": entity_b_id
            }
        )
        print(f"âœ… åˆ›å»ºåˆ«åå…³ç³»: {entity_a_id} --[HAS_ALIAS]--> {entity_b_id}")

    elif strategy == MergeStrategy.MANUAL_REVIEW:
        # æ ‡è®°ä¸ºå¾…äººå·¥å®¡æ ¸
        cypher_mark = """
        MATCH (a {id: $entity_a_id})-[r:POSSIBLY_SAME_AS]-(b {id: $entity_b_id})
        SET r.status = 'manual_review_required'
        SET r.flagged_at = datetime()
        """
        graph_store.structured_query(
            cypher_mark,
            param_map={
                "entity_a_id": entity_a_id,
                "entity_b_id": entity_b_id
            }
        )
        print(f"âš ï¸ æ ‡è®°ä¸ºäººå·¥å®¡æ ¸: {entity_a_id} <-> {entity_b_id}")

# ä½¿ç”¨ç¤ºä¾‹
execute_merge_strategy(
    graph_store=graph_store,
    entity_a_id="person:001",
    entity_b_id="person:002",
    strategy=MergeStrategy.CREATE_ALIAS
)
```

#### 6.7.6 ç”Ÿäº§çº§å®ä½“æ¶ˆæ­§Pipeline

å®Œæ•´çš„å®ä½“æ¶ˆæ­§æµç¨‹:

```python
class EntityResolutionPipeline:
    """å®ä½“æ¶ˆæ­§Pipeline"""

    def __init__(self, graph_store: Neo4jPropertyGraphStore, llm: OpenAI):
        self.graph_store = graph_store
        self.llm = llm

    def find_duplicate_candidates(self, similarity_threshold: float = 0.7):
        """æŸ¥æ‰¾å¯èƒ½é‡å¤çš„å®ä½“å€™é€‰"""

        # æ–¹æ³•1: åŸºäºåå­—ç›¸ä¼¼åº¦
        cypher_similar_names = """
        MATCH (a:Entity), (b:Entity)
        WHERE a.id < b.id  // é¿å…é‡å¤æ¯”è¾ƒ
        AND apoc.text.levenshteinSimilarity(a.name, b.name) > $threshold
        RETURN a.id as entity_a, b.id as entity_b, a.name as name_a, b.name as name_b
        """

        # æ–¹æ³•2: åŸºäºå…±äº«å±æ€§(ç¤¾åª’è´¦å·ã€é‚®ç®±ã€ç”µè¯ç­‰)
        cypher_shared_attrs = """
        MATCH (a:Entity), (b:Entity)
        WHERE a.id < b.id
        AND (
            a.social_media = b.social_media OR
            a.email = b.email OR
            a.phone = b.phone
        )
        RETURN a.id as entity_a, b.id as entity_b
        """

        candidates = []

        # æ‰§è¡ŒæŸ¥è¯¢
        similar_names = self.graph_store.structured_query(
            cypher_similar_names,
            param_map={"threshold": similarity_threshold}
        )
        candidates.extend(similar_names)

        shared_attrs = self.graph_store.structured_query(cypher_shared_attrs)
        candidates.extend(shared_attrs)

        return candidates

    def batch_resolve(self, candidates: list, auto_merge_threshold: float = 0.9):
        """æ‰¹é‡æ¶ˆæ­§"""

        results = {
            "merged": [],
            "flagged_for_review": [],
            "kept_separate": []
        }

        for candidate in candidates:
            entity_a_id = candidate['entity_a']
            entity_b_id = candidate['entity_b']

            # è·å–å®Œæ•´å®ä½“ä¿¡æ¯
            entity_a = self._get_entity(entity_a_id)
            entity_b = self._get_entity(entity_b_id)

            # LLMåˆ¤æ–­
            match_result = resolve_entities(entity_a, entity_b, self.llm)

            # æ ¹æ®ç½®ä¿¡åº¦å†³å®šç­–ç•¥
            if match_result.is_same_entity:
                if match_result.confidence >= auto_merge_threshold:
                    # è‡ªåŠ¨åˆå¹¶
                    execute_merge_strategy(
                        self.graph_store,
                        entity_a_id,
                        entity_b_id,
                        MergeStrategy.MERGE_TO_FIRST
                    )
                    results["merged"].append({
                        "entity_a": entity_a_id,
                        "entity_b": entity_b_id,
                        "confidence": match_result.confidence
                    })
                else:
                    # æ ‡è®°ä¸ºäººå·¥å®¡æ ¸
                    execute_merge_strategy(
                        self.graph_store,
                        entity_a_id,
                        entity_b_id,
                        MergeStrategy.MANUAL_REVIEW
                    )
                    results["flagged_for_review"].append({
                        "entity_a": entity_a_id,
                        "entity_b": entity_b_id,
                        "confidence": match_result.confidence,
                        "reason": match_result.reason
                    })
            else:
                # ç¡®è®¤æ˜¯ä¸åŒå®ä½“,ä¿æŒåˆ†ç¦»
                results["kept_separate"].append({
                    "entity_a": entity_a_id,
                    "entity_b": entity_b_id
                })

        return results

    def _get_entity(self, entity_id: str) -> dict:
        """è·å–å®ä½“å®Œæ•´ä¿¡æ¯"""
        cypher = f"MATCH (e {{id: '{entity_id}'}}) RETURN properties(e) as props"
        result = self.graph_store.structured_query(cypher)
        return result[0]['props'] if result else {}

# ä½¿ç”¨ç¤ºä¾‹
pipeline = EntityResolutionPipeline(graph_store, llm)

# 1. æŸ¥æ‰¾å€™é€‰
candidates = pipeline.find_duplicate_candidates(similarity_threshold=0.7)
print(f"æ‰¾åˆ° {len(candidates)} ç»„å¯èƒ½é‡å¤çš„å®ä½“")

# 2. æ‰¹é‡æ¶ˆæ­§
results = pipeline.batch_resolve(candidates, auto_merge_threshold=0.9)

print(f"\nâœ… è‡ªåŠ¨åˆå¹¶: {len(results['merged'])} ç»„")
print(f"âš ï¸ å¾…äººå·¥å®¡æ ¸: {len(results['flagged_for_review'])} ç»„")
print(f"ğŸ“Œ ä¿æŒåˆ†ç¦»: {len(results['kept_separate'])} ç»„")
```

#### 6.7.7 å®é™…åº”ç”¨æ¡ˆä¾‹

**æ¡ˆä¾‹1: ä¼ä¸šçŸ¥è¯†å›¾è°±å»é‡**

```python
# åœºæ™¯: å‘˜å·¥ä¿¡æ¯ä»å¤šä¸ªç³»ç»Ÿå¯¼å…¥,å­˜åœ¨é‡å¤
employees = [
    {"id": "emp001", "name": "å¼ ä¸‰", "email": "zhangsan@company.com", "dept": "æŠ€æœ¯éƒ¨"},
    {"id": "emp002", "name": "Zhang San", "email": "zhangsan@company.com", "dept": "Engineering"},
    {"id": "emp003", "name": "å¼ ä¸‰", "email": "zs@company.com", "dept": "å¸‚åœºéƒ¨"}
]

# emp001 å’Œ emp002: é‚®ç®±ç›¸åŒ â†’ åŒä¸€äºº
# emp001 å’Œ emp003: åå­—ç›¸åŒä½†é‚®ç®±ã€éƒ¨é—¨ä¸åŒ â†’ å¯èƒ½æ˜¯ä¸åŒäºº(åŒå)
```

**æ¡ˆä¾‹2: å­¦æœ¯ç½‘ç»œä½œè€…æ¶ˆæ­§**

```python
# åœºæ™¯: åŒåä½œè€…æ¶ˆæ­§
authors = [
    {"name": "æä¼Ÿ", "institution": "æ¸…åå¤§å­¦", "field": "è®¡ç®—æœº"},
    {"name": "Li Wei", "institution": "Tsinghua University", "field": "Computer Science"},
    {"name": "æä¼Ÿ", "institution": "åŒ—äº¬å¤§å­¦", "field": "ç‰©ç†"}
]

# å‰ä¸¤ä¸ª: æœºæ„å’Œé¢†åŸŸä¸€è‡´ â†’ åŒä¸€äºº
# ç¬¬ä¸‰ä¸ª: ä¸åŒæœºæ„å’Œé¢†åŸŸ â†’ ä¸åŒäºº
```

**æ¡ˆä¾‹3: ç”µå•†å•†å“å»é‡**

```python
# åœºæ™¯: å•†å“ä¿¡æ¯å»é‡
products = [
    {"name": "iPhone 15 Pro", "sku": "A2848", "price": 7999},
    {"name": "è‹¹æœiPhone15Pro", "sku": "A2848", "price": 7999},
    {"name": "iPhone 15 Pro Max", "sku": "A2849", "price": 8999}
]

# å‰ä¸¤ä¸ª: SKUç›¸åŒ â†’ åŒä¸€å•†å“
# ç¬¬ä¸‰ä¸ª: ä¸åŒSKU â†’ ä¸åŒå•†å“
```

#### 6.7.8 å°ç»“

**å®ä½“æ¶ˆæ­§çš„æ ¸å¿ƒä»·å€¼**:
- âœ… æé«˜çŸ¥è¯†å›¾è°±è´¨é‡ (å»é™¤é‡å¤)
- âœ… é¿å…ä¿¡æ¯åˆ†æ•£ (åŒä¸€å®ä½“çš„çŸ¥è¯†é›†ä¸­)
- âœ… æ”¯æŒç²¾ç¡®æŸ¥è¯¢ (é¿å…é—æ¼)
- âœ… é€‚åº”çœŸå®ä¸–ç•Œçš„æ•°æ®æ··ä¹±

**å…³é”®æŠ€æœ¯**:
- **LLMç»“æ„åŒ–è¾“å‡º**: `llm.structured_predict()` + Pydantic
- **ç›¸ä¼¼åº¦è®¡ç®—**: Levenshteinè·ç¦», å±æ€§åŒ¹é…
- **å…³ç³»å»ºæ¨¡**: `POSSIBLY_SAME_AS`, `HAS_ALIAS`
- **åˆå¹¶ç­–ç•¥**: ä¿ç•™/åˆå¹¶/äººå·¥å®¡æ ¸

**æœ€ä½³å®è·µ**:
1. **ä¸è¦æ€¥äºåˆå¹¶**: å…ˆå»ºç«‹ `POSSIBLY_SAME_AS` å…³ç³»
2. **è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼**: é«˜ç½®ä¿¡åº¦è‡ªåŠ¨åˆå¹¶,ä½ç½®ä¿¡åº¦äººå·¥å®¡æ ¸
3. **ä¿ç•™å®¡è®¡æ—¥å¿—**: è®°å½•åˆå¹¶åŸå› å’Œæ—¶é—´
4. **æ”¯æŒå›æ»š**: å…è®¸æ’¤é”€é”™è¯¯çš„åˆå¹¶
5. **å¢é‡å¤„ç†**: æ–°å¢å®ä½“æ—¶å®æ—¶æ£€æµ‹é‡å¤

**å¸¸ç”¨è§„åˆ™**:
- ç¤¾åª’è´¦å·/é‚®ç®±/ç”µè¯ç›¸åŒ â†’ å¼ºåŒ¹é…ä¿¡å· (ç½®ä¿¡åº¦0.9+)
- åå­—å®Œå…¨ç›¸åŒ + å…¶ä»–å±æ€§ç›¸ä¼¼ â†’ ä¸­ç­‰ä¿¡å· (ç½®ä¿¡åº¦0.7-0.9)
- åå­—ç›¸ä¼¼ä½†å…¶ä»–å±æ€§çŸ›ç›¾ â†’ å¯èƒ½åŒåå¼‚äºº (ç½®ä¿¡åº¦<0.5)

---

### 6.8 å¯è§†åŒ–çŸ¥è¯†å›¾è°±

ä½¿ç”¨NetworkXå’ŒMatplotlibå¯è§†åŒ–PropertyGraphIndex:

```python
import networkx as nx
import matplotlib.pyplot as plt

def visualize_graph(pg_index, max_triplets: int = 50):
    """å¯è§†åŒ–çŸ¥è¯†å›¾è°±

    Args:
        pg_index: PropertyGraphIndexå®ä¾‹
        max_triplets: æœ€å¤šæ˜¾ç¤ºçš„ä¸‰å…ƒç»„æ•°é‡ (é¿å…å›¾è¿‡äºå¤æ‚)
    """
    # è·å–æ‰€æœ‰ä¸‰å…ƒç»„
    triplets = pg_index.property_graph_store.get_triplets()

    # åˆ›å»º NetworkX æœ‰å‘å›¾
    G = nx.DiGraph()

    for triplet in triplets[:max_triplets]:  # é™åˆ¶èŠ‚ç‚¹æ•°é‡
        subject, relation, obj = triplet
        G.add_edge(
            subject.name,
            obj.name,
            label=relation.label
        )

    # ç»˜åˆ¶å›¾
    plt.figure(figsize=(15, 10))
    pos = nx.spring_layout(G, k=0.5, iterations=50)

    # ç»˜åˆ¶èŠ‚ç‚¹
    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=500)

    # ç»˜åˆ¶èŠ‚ç‚¹æ ‡ç­¾
    nx.draw_networkx_labels(G, pos, font_size=8)

    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=15)

    # ç»˜åˆ¶è¾¹æ ‡ç­¾ (å…³ç³»ç±»å‹)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6)

    plt.title("Knowledge Graph Visualization", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("knowledge_graph.png", dpi=300, bbox_inches='tight')
    plt.show()

    print(f"âœ… å¯è§†åŒ–å®Œæˆ! æ˜¾ç¤ºäº† {len(triplets[:max_triplets])} ä¸ªä¸‰å…ƒç»„")
    print(f"   å›¾ä¸­åŒ…å« {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")

# ä½¿ç”¨ç¤ºä¾‹
visualize_graph(pg_index, max_triplets=50)
```

**é«˜çº§å¯è§†åŒ–: æŒ‰å®ä½“ç±»å‹ç€è‰²**

```python
def visualize_graph_by_type(pg_index, max_triplets: int = 50):
    """æŒ‰å®ä½“ç±»å‹ç€è‰²çš„çŸ¥è¯†å›¾è°±å¯è§†åŒ–"""
    triplets = pg_index.property_graph_store.get_triplets()
    G = nx.DiGraph()

    # å­˜å‚¨èŠ‚ç‚¹ç±»å‹
    node_types = {}

    for triplet in triplets[:max_triplets]:
        subject, relation, obj = triplet
        G.add_edge(subject.name, obj.name, label=relation.label)

        # è®°å½•èŠ‚ç‚¹ç±»å‹ (å¦‚æœæœ‰categoryå±æ€§)
        if hasattr(subject, 'category'):
            node_types[subject.name] = subject.category
        if hasattr(obj, 'category'):
            node_types[obj.name] = obj.category

    # æ ¹æ®ç±»å‹åˆ†é…é¢œè‰²
    type_colors = {
        'PERSON': '#FF6B6B',
        'ORGANIZATION': '#4ECDC4',
        'LOCATION': '#45B7D1',
        'TECHNOLOGY': '#FFA07A',
    }

    node_colors = [
        type_colors.get(node_types.get(node, 'UNKNOWN'), '#CCCCCC')
        for node in G.nodes()
    ]

    # ç»˜åˆ¶
    plt.figure(figsize=(16, 12))
    pos = nx.spring_layout(G, k=0.7, iterations=50)

    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600, alpha=0.9)
    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')
    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=20, alpha=0.6)

    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=7)

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=color, label=entity_type)
        for entity_type, color in type_colors.items()
    ]
    plt.legend(handles=legend_elements, loc='upper right', fontsize=10)

    plt.title("Knowledge Graph with Entity Type Coloring", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("knowledge_graph_colored.png", dpi=300, bbox_inches='tight')
    plt.show()

visualize_graph_by_type(pg_index)
```

**äº¤äº’å¼å¯è§†åŒ–: ä½¿ç”¨Pyvis**

```python
from pyvis.network import Network

def visualize_graph_interactive(pg_index, max_triplets: int = 100):
    """åˆ›å»ºäº¤äº’å¼çŸ¥è¯†å›¾è°±å¯è§†åŒ– (HTML)"""
    triplets = pg_index.property_graph_store.get_triplets()

    # åˆ›å»ºPyvisç½‘ç»œ
    net = Network(height='800px', width='100%', notebook=False, directed=True)

    # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
    for triplet in triplets[:max_triplets]:
        subject, relation, obj = triplet

        # æ·»åŠ èŠ‚ç‚¹
        net.add_node(subject.name, title=subject.name, color='#97C2FC')
        net.add_node(obj.name, title=obj.name, color='#FFAB91')

        # æ·»åŠ è¾¹
        net.add_edge(
            subject.name,
            obj.name,
            label=relation.label,
            title=relation.label
        )

    # è®¾ç½®ç‰©ç†å¸ƒå±€
    net.set_options("""
    {
        "physics": {
            "enabled": true,
            "barnesHut": {
                "gravitationalConstant": -8000,
                "springLength": 150,
                "springConstant": 0.04
            }
        }
    }
    """)

    # ä¿å­˜ä¸ºHTMLæ–‡ä»¶
    net.save_graph("knowledge_graph_interactive.html")
    print("âœ… äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ° knowledge_graph_interactive.html")
    print("   åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥æ–‡ä»¶å³å¯æŸ¥çœ‹äº¤äº’å¼å›¾è°±")

# éœ€è¦å…ˆå®‰è£…: pip install pyvis
visualize_graph_interactive(pg_index)
```

**å¯è§†åŒ–çš„åº”ç”¨åœºæ™¯**:

1. **è°ƒè¯•çŸ¥è¯†æŠ½å–**: æ£€æŸ¥æå–çš„ä¸‰å…ƒç»„æ˜¯å¦æ­£ç¡®
2. **æ¼”ç¤ºæ–‡ç¨¿**: å±•ç¤ºçŸ¥è¯†å›¾è°±çš„ç»“æ„
3. **å‘ç°æ¨¡å¼**: ç›´è§‚å‘ç°å®ä½“é—´çš„å…³ç³»æ¨¡å¼
4. **éªŒè¯åˆå¹¶**: æŸ¥çœ‹å®ä½“æ¶ˆæ­§åçš„å›¾ç»“æ„

---

### 6.9 GraphRAGç¤¾åŒºæ£€æµ‹ (Community Detection)

**GraphRAG (Graph Retrieval-Augmented Generation)** æ˜¯å¾®è½¯æå‡ºçš„ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„RAGæ–¹æ³•è®º,æ ¸å¿ƒåˆ›æ–°æ˜¯é€šè¿‡**ç¤¾åŒºæ£€æµ‹**å°†å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±åˆ†å±‚ç»„ç»‡,å¹¶ä¸ºæ¯ä¸ªç¤¾åŒºç”ŸæˆLLMæ‘˜è¦,å®ç°é«˜æ•ˆçš„å…¨å±€æ€§é—®é¢˜å›ç­”ã€‚

#### 6.9.1 ç¤¾åŒºæ£€æµ‹æ¦‚å¿µä¸åŠ¨æœº

**ä»€ä¹ˆæ˜¯ç¤¾åŒºæ£€æµ‹?**

ç¤¾åŒºæ£€æµ‹(Community Detection)æ˜¯å›¾è®ºä¸­çš„ç»å…¸é—®é¢˜,ç›®æ ‡æ˜¯å°†å›¾ä¸­çš„èŠ‚ç‚¹åˆ’åˆ†ä¸ºå¤šä¸ª**ç´§å¯†è¿æ¥çš„ç¾¤ç»„(ç¤¾åŒº)**,ä½¿å¾—:
- ç¤¾åŒºå†…éƒ¨çš„èŠ‚ç‚¹è¿æ¥å¯†é›†
- ç¤¾åŒºä¹‹é—´çš„è¿æ¥ç¨€ç–

```
åŸå§‹çŸ¥è¯†å›¾è°± (1000ä¸ªå®ä½“):
  å®ä½“1 --å…³ç³»--> å®ä½“2 --å…³ç³»--> å®ä½“3 ...

ç¤¾åŒºæ£€æµ‹å:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç¤¾åŒº1: AIæŠ€æœ¯ (120ä¸ªå®ä½“)       â”‚
  â”‚ æ‘˜è¦: è®¨è®ºæ·±åº¦å­¦ä¹ ã€NLPç­‰æŠ€æœ¯   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç¤¾åŒº2: ç§‘æŠ€å…¬å¸ (85ä¸ªå®ä½“)      â”‚
  â”‚ æ‘˜è¦: OpenAIã€Googleç­‰å…¬å¸åŠ¨æ€  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç¤¾åŒº3: äº§å“å‘å¸ƒ (60ä¸ªå®ä½“)      â”‚
  â”‚ æ‘˜è¦: GPT-4ã€Claudeç­‰äº§å“ä¿¡æ¯   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆéœ€è¦ç¤¾åŒºæ£€æµ‹?**

| ä¼ ç»Ÿæ–¹æ³• | GraphRAGæ–¹æ³• |
|---------|-------------|
| å‘é‡æ£€ç´¢: æ£€ç´¢æ–‡æ¡£å— â†’ ç”Ÿæˆç­”æ¡ˆ | ç¤¾åŒºæ£€æµ‹: å»ºå›¾ â†’ ç¤¾åŒºåˆ’åˆ† â†’ ç¤¾åŒºæ‘˜è¦ â†’ åŸºäºæ‘˜è¦å›ç­” |
| é€‚åˆäº‹å®æ€§é—®é¢˜ | é€‚åˆå…¨å±€æ€§ã€ç»¼è¿°æ€§é—®é¢˜ |
| éš¾ä»¥å›ç­”"æ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆ?" | å¯ä»¥é«˜æ•ˆå›ç­”å…¨å±€æ€§é—®é¢˜ |
| æ¯æ¬¡æ£€ç´¢éœ€è¦éå†å¤§é‡æ–‡æ¡£å— | åªéœ€å¤„ç†å°‘é‡ç¤¾åŒºæ‘˜è¦ |

**åº”ç”¨åœºæ™¯**:
- âœ… "è¿™ç¯‡æ–‡æ¡£è®¨è®ºäº†å“ªäº›ä¸»è¦ä¸»é¢˜?" (å…¨å±€æ€§)
- âœ… "æ€»ç»“æ–‡æ¡£ä¸­çš„å…³é”®äº‹ä»¶" (ç»¼è¿°æ€§)
- âœ… "æ–‡æ¡£ä¸­æåˆ°çš„å…¬å¸æœ‰å“ªäº›æˆ˜ç•¥æ–¹å‘?" (å®è§‚åˆ†æ)

**Leidenç®—æ³•ç®€ä»‹**:

LlamaIndexå®˜æ–¹ä½¿ç”¨çš„æ˜¯ `hierarchical_leiden` ç®—æ³•(æ¥è‡ª`graspologic`åº“):
- **Leidenç®—æ³•**: ç¤¾åŒºæ£€æµ‹çš„ç»å…¸ç®—æ³•,æ”¹è¿›è‡ªLouvainç®—æ³•
- **Hierarchicalç‰ˆæœ¬**: æ”¯æŒå±‚æ¬¡åŒ–ç¤¾åŒºåˆ’åˆ†
- **max_cluster_sizeå‚æ•°**: æ§åˆ¶ç¤¾åŒºå¤§å°ä¸Šé™,é¿å…æŸä¸ªç¤¾åŒºè¿‡å¤§

---

#### 6.9.2 GraphRAGExtractor: å¸¦æè¿°çš„å®ä½“å…³ç³»æå–

å®˜æ–¹GraphRAGçš„ç¬¬ä¸€æ­¥æ˜¯æå–**å¸¦è¯¦ç»†æè¿°**çš„å®ä½“å’Œå…³ç³»,è¿™ä¸SimpleLLMPathExtractoræœ‰æ˜¾è‘—åŒºåˆ«ã€‚

**æ ¸å¿ƒåŒºåˆ«å¯¹æ¯”**:

| ç‰¹æ€§ | SimpleLLMPathExtractor | GraphRAGExtractor (å®˜æ–¹) |
|------|----------------------|------------------------|
| å®ä½“ä¿¡æ¯ | ä»…name, type | name, type, **description** |
| å…³ç³»ä¿¡æ¯ | ä»…subject, relation, object | subject, relation, object, **description** |
| æè¿°å­˜å‚¨ | âŒ ä¸å­˜å‚¨ | âœ… å­˜å‚¨åœ¨metadata |
| æå–æ ¼å¼ | ä¸‰å…ƒç»„ | JSON (entities + relationships) |
| ç”¨é€” | åŸºç¡€å›¾æŸ¥è¯¢ | ç¤¾åŒºæ‘˜è¦ç”Ÿæˆ |

**GraphRAGExtractorå®ç°**:

```python
from llama_index.core.schema import TransformComponent, BaseNode
from llama_index.core.graph_stores.types import (
    EntityNode,
    Relation,
    KG_NODES_KEY,
    KG_RELATIONS_KEY
)
from llama_index.core.llms import ChatMessage
from llama_index.llms.openai import OpenAI
import json
import re
from typing import Any, Callable

class GraphRAGExtractor(TransformComponent):
    """å®˜æ–¹GraphRAGå®ä½“å…³ç³»æå–å™¨

    æå–:
      - å®ä½“: name, type, description
      - å…³ç³»: source, target, relation, relationship_description
    """

    llm: OpenAI
    extract_prompt: str
    parse_fn: Callable
    max_paths_per_chunk: int = 10
    num_workers: int = 4

    def __init__(
        self,
        llm: OpenAI,
        extract_prompt: str,
        parse_fn: Callable,
        max_paths_per_chunk: int = 10,
        num_workers: int = 4
    ):
        self.llm = llm
        self.extract_prompt = extract_prompt
        self.parse_fn = parse_fn
        self.max_paths_per_chunk = max_paths_per_chunk
        self.num_workers = num_workers

    async def _aextract(self, node: BaseNode) -> BaseNode:
        """æå–å•ä¸ªèŠ‚ç‚¹çš„å®ä½“å’Œå…³ç³»"""
        text = node.get_content(metadata_mode="llm")

        # 1. LLMæå–
        llm_response = await self.llm.acomplete(
            self.extract_prompt.format(
                text=text,
                max_knowledge_triplets=self.max_paths_per_chunk
            )
        )

        # 2. è§£æå“åº”
        entities, relationships = self.parse_fn(str(llm_response))

        # 3. æ„å»ºEntityNodeå’ŒRelation
        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])
        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])

        # å­˜å‚¨å®ä½“ (å¸¦description)
        metadata = node.metadata.copy()
        for entity, entity_type, description in entities:
            entity_metadata = metadata.copy()
            entity_metadata["entity_description"] = description  # å…³é”®: å­˜å‚¨æè¿°

            entity_node = EntityNode(
                name=entity,
                label=entity_type,
                properties=entity_metadata
            )
            existing_nodes.append(entity_node)

        # å­˜å‚¨å…³ç³» (å¸¦relationship_description)
        for subj, obj, rel, description in relationships:
            rel_metadata = metadata.copy()
            rel_metadata["relationship_description"] = description  # å…³é”®: å­˜å‚¨æè¿°

            subj_node = EntityNode(name=subj, properties=metadata)
            obj_node = EntityNode(name=obj, properties=metadata)

            rel_node = Relation(
                label=rel,
                source_id=subj_node.id,
                target_id=obj_node.id,
                properties=rel_metadata
            )

            existing_nodes.extend([subj_node, obj_node])
            existing_relations.append(rel_node)

        node.metadata[KG_NODES_KEY] = existing_nodes
        node.metadata[KG_RELATIONS_KEY] = existing_relations

        return node

    def __call__(self, nodes, **kwargs):
        """æ‰¹é‡å¤„ç†èŠ‚ç‚¹"""
        import asyncio

        async def _process_all():
            tasks = [self._aextract(node) for node in nodes]
            return await asyncio.gather(*tasks)

        return asyncio.run(_process_all())
```

**å®˜æ–¹æå–Prompt**:

```python
KG_TRIPLET_EXTRACT_TMPL = """
-Goal-
Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- entity_name: Name of the entity, capitalized
- entity_type: Type of the entity (one of: PERSON, ORGANIZATION, LOCATION, TECHNOLOGY, EVENT, CONCEPT)
- entity_description: Comprehensive description of the entity's attributes and activities

Format each entity as a JSON object.

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.
For each pair of related entities, extract the following information:
- source_entity: name of the source entity, as identified in step 1
- target_entity: name of the target entity, as identified in step 1
- relation: relationship between source_entity and target_entity (e.g., WORKS_AT, LOCATED_IN, DEVELOPED, USES)
- relationship_description: explanation as to why you think the source entity and the target entity are related to each other

Format each relationship as a JSON object.

3. Output Formatting:
When you finish, output a single JSON object with two keys:
- "entities": list of entity objects
- "relationships": list of relationship objects

-Real Data-
text: {text}
output:
"""
```

**è§£æå‡½æ•°**:

```python
def parse_fn(response_str: str) -> tuple:
    """è§£æLLMå“åº”,æå–å®ä½“å’Œå…³ç³»

    Returns:
        entities: [(name, type, description), ...]
        relationships: [(source, target, relation, description), ...]
    """
    # æå–JSON
    json_pattern = r"\{.*\}"
    match = re.search(json_pattern, response_str, re.DOTALL)

    entities = []
    relationships = []

    if not match:
        return entities, relationships

    try:
        data = json.loads(match.group(0))

        # è§£æå®ä½“
        for entity in data.get("entities", []):
            entities.append((
                entity["entity_name"],
                entity["entity_type"],
                entity["entity_description"]
            ))

        # è§£æå…³ç³»
        for relation in data.get("relationships", []):
            relationships.append((
                relation["source_entity"],
                relation["target_entity"],
                relation["relation"],
                relation["relationship_description"]
            ))

    except json.JSONDecodeError:
        print(f"Failed to parse JSON: {match.group(0)[:100]}...")

    return entities, relationships
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter

# 1. åˆå§‹åŒ–LLM
llm = OpenAI(model="gpt-4", temperature=0)

# 2. åˆ›å»ºæå–å™¨
kg_extractor = GraphRAGExtractor(
    llm=llm,
    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,
    parse_fn=parse_fn,
    max_paths_per_chunk=10,
    num_workers=4
)

# 3. åŠ è½½å¹¶åˆ†å—æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()
splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
nodes = splitter.get_nodes_from_documents(documents)

# 4. æå–å®ä½“å’Œå…³ç³»
enriched_nodes = kg_extractor(nodes)

# 5. æŸ¥çœ‹æå–ç»“æœ
for node in enriched_nodes[:1]:
    entities = node.metadata.get(KG_NODES_KEY, [])
    relations = node.metadata.get(KG_RELATIONS_KEY, [])

    print(f"æå–äº† {len(entities)} ä¸ªå®ä½“, {len(relations)} ä¸ªå…³ç³»\n")

    # å®ä½“ç¤ºä¾‹
    if entities:
        entity = entities[0]
        print(f"å®ä½“ç¤ºä¾‹: {entity.name} ({entity.label})")
        print(f"æè¿°: {entity.properties.get('entity_description', 'N/A')}\n")

    # å…³ç³»ç¤ºä¾‹
    if relations:
        rel = relations[0]
        print(f"å…³ç³»ç¤ºä¾‹: {rel.label}")
        print(f"æè¿°: {rel.properties.get('relationship_description', 'N/A')}")
```

---

#### 6.9.3 GraphRAGStore: ç¤¾åŒºæ„å»ºä¸æ‘˜è¦

**GraphRAGStore** æ˜¯å®˜æ–¹å®ç°çš„æ ¸å¿ƒç±»,ç»§æ‰¿è‡ª`SimplePropertyGraphStore`,æ–°å¢äº†ç¤¾åŒºæ£€æµ‹å’Œæ‘˜è¦åŠŸèƒ½ã€‚

**å®Œæ•´å®ç°**:

```python
from llama_index.core.graph_stores import SimplePropertyGraphStore
from llama_index.core.llms import ChatMessage
from llama_index.llms.openai import OpenAI
import networkx as nx
from graspologic.partition import hierarchical_leiden
import re

class GraphRAGStore(SimplePropertyGraphStore):
    """å®˜æ–¹GraphRAGå­˜å‚¨,æ”¯æŒç¤¾åŒºæ£€æµ‹å’Œæ‘˜è¦"""

    community_summary: dict = {}
    entity_info: dict = {}  # å®ä½“ -> ç¤¾åŒºIDæ˜ å°„
    max_cluster_size: int = 5

    def build_communities(self):
        """æ„å»ºç¤¾åŒºå¹¶ç”Ÿæˆæ‘˜è¦

        æµç¨‹:
          1. è½¬æ¢ä¸ºNetworkXå›¾
          2. Leidenç¤¾åŒºæ£€æµ‹
          3. æ”¶é›†ç¤¾åŒºä¿¡æ¯
          4. ç”Ÿæˆç¤¾åŒºæ‘˜è¦
        """
        print("ğŸ” å¼€å§‹æ„å»ºç¤¾åŒº...")

        # 1. åˆ›å»ºNetworkXå›¾
        nx_graph = self._create_nx_graph()
        print(f"  å›¾åŒ…å« {nx_graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {nx_graph.number_of_edges()} æ¡è¾¹")

        # 2. ç¤¾åŒºæ£€æµ‹
        print(f"  æ‰§è¡ŒLeidenç¤¾åŒºæ£€æµ‹ (max_cluster_size={self.max_cluster_size})...")
        community_hierarchical_clusters = hierarchical_leiden(
            nx_graph,
            max_cluster_size=self.max_cluster_size
        )

        # 3. æ”¶é›†ç¤¾åŒºä¿¡æ¯
        self.entity_info, community_info = self._collect_community_info(
            nx_graph,
            community_hierarchical_clusters
        )

        num_communities = len(community_info)
        print(f"  æ£€æµ‹åˆ° {num_communities} ä¸ªç¤¾åŒº")

        # 4. ç”Ÿæˆæ‘˜è¦
        print(f"  ä¸º {num_communities} ä¸ªç¤¾åŒºç”ŸæˆLLMæ‘˜è¦...")
        self._summarize_communities(community_info)

        print("âœ… ç¤¾åŒºæ„å»ºå®Œæˆ!")

        # æ‰“å°ç¤¾åŒºç»Ÿè®¡
        for cid, summary in self.community_summary.items():
            entity_count = sum(1 for eid, c in self.entity_info.items() if c == cid)
            print(f"\n  ç¤¾åŒº {cid}: {entity_count} ä¸ªå®ä½“")
            print(f"  æ‘˜è¦: {summary[:100]}...")

    def _create_nx_graph(self) -> nx.Graph:
        """å°†å†…éƒ¨å›¾è¡¨ç¤ºè½¬æ¢ä¸ºNetworkXå›¾"""
        nx_graph = nx.Graph()

        # æ·»åŠ èŠ‚ç‚¹
        for node in self.graph.nodes.values():
            nx_graph.add_node(str(node))

        # æ·»åŠ è¾¹ (å¸¦å…³ç³»æè¿°)
        for relation in self.graph.relations.values():
            nx_graph.add_edge(
                relation.source_id,
                relation.target_id,
                relationship=relation.label,
                description=relation.properties.get("relationship_description", "")
            )

        return nx_graph

    def _collect_community_info(
        self,
        nx_graph: nx.Graph,
        clusters
    ) -> tuple[dict, dict]:
        """æ”¶é›†æ¯ä¸ªç¤¾åŒºçš„è¯¦ç»†ä¿¡æ¯

        Returns:
            entity_info: {entity_name: community_id}
            community_info: {community_id: [relationship_strings]}
        """
        # æ„å»ºå®ä½“ -> ç¤¾åŒºæ˜ å°„
        entity_info = {}
        for item in clusters:
            entity_info[item.node] = item.cluster

        # æ”¶é›†ç¤¾åŒºå†…éƒ¨çš„å…³ç³»
        community_info = {}

        for item in clusters:
            cluster_id = item.cluster
            node = item.node

            if cluster_id not in community_info:
                community_info[cluster_id] = []

            # éå†é‚»å±…èŠ‚ç‚¹
            for neighbor in nx_graph.neighbors(node):
                # åªæ”¶é›†åŒä¸€ç¤¾åŒºå†…çš„å…³ç³»
                if entity_info[neighbor] == cluster_id:
                    edge_data = nx_graph.get_edge_data(node, neighbor)

                    if edge_data:
                        # æ ¼å¼: "entity1 -> entity2 -> relation -> description"
                        detail = (
                            f"{node} -> {neighbor} -> "
                            f"{edge_data['relationship']} -> "
                            f"{edge_data['description']}"
                        )
                        community_info[cluster_id].append(detail)

        return entity_info, community_info

    def _summarize_communities(self, community_info: dict):
        """ä¸ºæ¯ä¸ªç¤¾åŒºç”ŸæˆLLMæ‘˜è¦"""
        for community_id, details in community_info.items():
            # æ‹¼æ¥ç¤¾åŒºå†…æ‰€æœ‰å…³ç³»
            details_text = "\n".join(set(details)) + "."  # å»é‡

            # ç”Ÿæˆæ‘˜è¦
            summary = self.generate_community_summary(details_text)
            self.community_summary[community_id] = summary

    def generate_community_summary(self, text: str) -> str:
        """ä½¿ç”¨LLMä¸ºç¤¾åŒºç”Ÿæˆæ‘˜è¦"""
        llm = OpenAI(model="gpt-4", temperature=0)

        messages = [
            ChatMessage(
                role="system",
                content=(
                    "You are provided with a set of relationships from a knowledge graph, "
                    "each represented as entity1->entity2->relation->relationship_description. "
                    "Your task is to create a summary of these relationships. "
                    "The summary should include the names of the entities involved and "
                    "a concise synthesis of the relationship descriptions. "
                    "The summary should be 2-3 sentences."
                )
            ),
            ChatMessage(
                role="user",
                content=text
            )
        ]

        response = llm.chat(messages)
        clean_summary = re.sub(r"^assistant:\s*", "", str(response)).strip()

        return clean_summary

    def get_community_summaries(self) -> dict:
        """è·å–æ‰€æœ‰ç¤¾åŒºæ‘˜è¦"""
        return self.community_summary
```

**å…³é”®å‚æ•° `max_cluster_size`**:

| max_cluster_size | ç¤¾åŒºæ•°é‡ | ç¤¾åŒºå¤§å° | é€‚ç”¨åœºæ™¯ |
|-----------------|---------|---------|---------|
| 3 | å¤š | å° | æ–‡æ¡£è¾ƒå°,éœ€è¦ç»†ç²’åº¦ |
| 5 (é»˜è®¤) | ä¸­ç­‰ | ä¸­ç­‰ | é€šç”¨åœºæ™¯ |
| 10 | å°‘ | å¤§ | æ–‡æ¡£è¾ƒå¤§,éœ€è¦å®è§‚è§†è§’ |

**å®‰è£…ä¾èµ–**:

```bash
pip install graspologic networkx
```

---

#### 6.9.4 å®Œæ•´ç¤ºä¾‹: ä»æ–‡æ¡£åˆ°ç¤¾åŒºå›¾è°±

**ç«¯åˆ°ç«¯æµç¨‹**:

```python
from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter

# ========== é…ç½® ==========
Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# ========== 1. åŠ è½½æ–‡æ¡£ ==========
documents = SimpleDirectoryReader("./data").load_data()
print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# ========== 2. åˆ†å— ==========
splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
nodes = splitter.get_nodes_from_documents(documents)
print(f"åˆ†æˆ {len(nodes)} ä¸ªæ–‡æœ¬å—")

# ========== 3. åˆ›å»ºGraphRAGæå–å™¨ ==========
kg_extractor = GraphRAGExtractor(
    llm=Settings.llm,
    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,
    parse_fn=parse_fn,
    max_paths_per_chunk=10
)

# ========== 4. åˆ›å»ºGraphRAGStore ==========
graph_store = GraphRAGStore()

# ========== 5. æ„å»ºPropertyGraphIndex ==========
print("\nğŸ—ï¸  æ„å»ºçŸ¥è¯†å›¾è°±...")
pg_index = PropertyGraphIndex(
    nodes=nodes,
    property_graph_store=graph_store,
    kg_extractors=[kg_extractor],
    show_progress=True
)

# ========== 6. æ„å»ºç¤¾åŒº ==========
print("\nğŸ” æ„å»ºç¤¾åŒºå¹¶ç”Ÿæˆæ‘˜è¦...")
pg_index.property_graph_store.build_communities()

# ========== 7. æŸ¥çœ‹ç»“æœ ==========
print("\n" + "="*60)
print("ç¤¾åŒºæ‘˜è¦:")
print("="*60)

for community_id, summary in graph_store.community_summary.items():
    # ç»Ÿè®¡ç¤¾åŒºå®ä½“æ•°é‡
    entity_count = sum(
        1 for entity, cid in graph_store.entity_info.items()
        if cid == community_id
    )

    print(f"\nã€ç¤¾åŒº {community_id}ã€‘ ({entity_count} ä¸ªå®ä½“)")
    print(f"æ‘˜è¦: {summary}")

# ========== 8. ä¿å­˜ ==========
pg_index.storage_context.persist(persist_dir="./storage/graphrag")
print("\nâœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ° ./storage/graphrag")
```

**è¾“å‡ºç¤ºä¾‹**:

```
åŠ è½½äº† 3 ä¸ªæ–‡æ¡£
åˆ†æˆ 15 ä¸ªæ–‡æœ¬å—

ğŸ—ï¸  æ„å»ºçŸ¥è¯†å›¾è°±...
Processing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:45<00:00,  3.2s/it]

ğŸ” æ„å»ºç¤¾åŒºå¹¶ç”Ÿæˆæ‘˜è¦...
ğŸ” å¼€å§‹æ„å»ºç¤¾åŒº...
  å›¾åŒ…å« 45 ä¸ªèŠ‚ç‚¹, 67 æ¡è¾¹
  æ‰§è¡ŒLeidenç¤¾åŒºæ£€æµ‹ (max_cluster_size=5)...
  æ£€æµ‹åˆ° 8 ä¸ªç¤¾åŒº
  ä¸º 8 ä¸ªç¤¾åŒºç”ŸæˆLLMæ‘˜è¦...
âœ… ç¤¾åŒºæ„å»ºå®Œæˆ!

  ç¤¾åŒº 0: 12 ä¸ªå®ä½“
  æ‘˜è¦: This community discusses AI technologies including GPT-4, DALL-E...

  ç¤¾åŒº 1: 8 ä¸ªå®ä½“
  æ‘˜è¦: This community focuses on tech companies like OpenAI, Microsoft...

============================================================
ç¤¾åŒºæ‘˜è¦:
============================================================

ã€ç¤¾åŒº 0ã€‘ (12 ä¸ªå®ä½“)
æ‘˜è¦: This community discusses AI technologies including GPT-4, DALL-E, and their applications in natural language processing and image generation.

ã€ç¤¾åŒº 1ã€‘ (8 ä¸ªå®ä½“)
æ‘˜è¦: This community focuses on tech companies like OpenAI, Microsoft, and Google, highlighting their investments and product releases in the AI sector.

...
```

---

#### 6.9.5 å¯è§†åŒ–ç¤¾åŒºç»“æ„

ä½¿ç”¨ä¸åŒé¢œè‰²æ ‡è®°ä¸åŒç¤¾åŒº:

```python
import networkx as nx
import matplotlib.pyplot as plt

def visualize_communities(graph_store: GraphRAGStore):
    """å¯è§†åŒ–ç¤¾åŒºç»“æ„"""
    nx_graph = graph_store._create_nx_graph()

    # ä¸ºæ¯ä¸ªç¤¾åŒºåˆ†é…é¢œè‰²
    community_colors = {}
    color_palette = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A',
                     '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E2']

    node_colors = []
    for node in nx_graph.nodes():
        community_id = graph_store.entity_info.get(node, -1)

        if community_id not in community_colors:
            community_colors[community_id] = color_palette[
                len(community_colors) % len(color_palette)
            ]

        node_colors.append(community_colors[community_id])

    # ç»˜åˆ¶
    plt.figure(figsize=(16, 12))
    pos = nx.spring_layout(nx_graph, k=0.5, iterations=50)

    nx.draw_networkx_nodes(
        nx_graph, pos,
        node_color=node_colors,
        node_size=600,
        alpha=0.9
    )
    nx.draw_networkx_labels(nx_graph, pos, font_size=8)
    nx.draw_networkx_edges(nx_graph, pos, alpha=0.3, arrows=True)

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=color, label=f"ç¤¾åŒº {cid}")
        for cid, color in community_colors.items()
    ]
    plt.legend(handles=legend_elements, loc='upper right')

    plt.title("Knowledge Graph Communities", fontsize=16)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("graphrag_communities.png", dpi=300)
    plt.show()

# ä½¿ç”¨
visualize_communities(graph_store)
```

---

#### 6.9.6 å°ç»“

**GraphRAGç¤¾åŒºæ£€æµ‹çš„æ ¸å¿ƒä»·å€¼**:
- âœ… å°†å¤§è§„æ¨¡å›¾è°±åˆ†å±‚ç»„ç»‡,é™ä½æŸ¥è¯¢å¤æ‚åº¦
- âœ… ç¤¾åŒºæ‘˜è¦æä¾›é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£
- âœ… æ”¯æŒå…¨å±€æ€§ã€ç»¼è¿°æ€§é—®é¢˜å›ç­”
- âœ… é€‚åˆå¤„ç†é•¿æ–‡æ¡£ã€å¤šæ–‡æ¡£åœºæ™¯

**å…³é”®ç»„ä»¶**:
- **GraphRAGExtractor**: æå–å¸¦æè¿°çš„å®ä½“å’Œå…³ç³»
- **GraphRAGStore**: Leidenç¤¾åŒºæ£€æµ‹ + LLMæ‘˜è¦ç”Ÿæˆ
- **hierarchical_leiden**: å±‚æ¬¡åŒ–ç¤¾åŒºåˆ’åˆ†ç®—æ³•
- **max_cluster_size**: æ§åˆ¶ç¤¾åŒºç²’åº¦

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„åŒºåˆ«**:
- **SimpleLLMPathExtractor**: ä»…æå–ä¸‰å…ƒç»„,æ— æè¿° â†’ é€‚åˆåŸºç¡€å›¾æŸ¥è¯¢
- **GraphRAGExtractor**: æå–å¸¦æè¿°çš„ç»“æ„åŒ–çŸ¥è¯† â†’ é€‚åˆç¤¾åŒºæ‘˜è¦ç”Ÿæˆ

**ä¸‹ä¸€æ­¥**: 6.10å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨**GraphRAGQueryEngine**åŸºäºç¤¾åŒºæ‘˜è¦å›ç­”é—®é¢˜ã€‚

---

### 6.10 GraphRAGæŸ¥è¯¢å¼•æ“ (GraphRAGQueryEngine)

å®Œæˆç¤¾åŒºæ„å»ºå,GraphRAGçš„æœ€åä¸€æ­¥æ˜¯ä½¿ç”¨**GraphRAGQueryEngine**åŸºäºç¤¾åŒºæ‘˜è¦å›ç­”é—®é¢˜ã€‚è¿™æ˜¯GraphRAGæ–¹æ³•è®ºçš„æ ¸å¿ƒæŸ¥è¯¢èŒƒå¼ã€‚

#### 6.10.1 GraphRAGæŸ¥è¯¢èŒƒå¼

**ä¼ ç»ŸRAG vs GraphRAGæŸ¥è¯¢æµç¨‹å¯¹æ¯”**:

```
ã€ä¼ ç»Ÿå‘é‡RAGã€‘:
ç”¨æˆ·æŸ¥è¯¢ â†’ Embeddingç¼–ç  â†’ å‘é‡æ£€ç´¢Top-Kæ–‡æ¡£å— â†’ LLMç”Ÿæˆç­”æ¡ˆ

ã€å¤šè·³å›¾æŸ¥è¯¢ (6.6èŠ‚)ã€‘:
ç”¨æˆ·æŸ¥è¯¢ â†’ è¯†åˆ«èµ·å§‹å®ä½“ â†’ Cypherå¤šè·³éå† â†’ åŸºäºè·¯å¾„LLMç”Ÿæˆç­”æ¡ˆ

ã€GraphRAG (æœ¬èŠ‚)ã€‘:
ç”¨æˆ·æŸ¥è¯¢ â†’ [v1: è·å–æ‰€æœ‰ç¤¾åŒºæ‘˜è¦ / v2: Embeddingæ£€ç´¢ç›¸å…³å®ä½“â†’å®šä½ç¤¾åŒº]
         â†’ æ¯ä¸ªç¤¾åŒºç”Ÿæˆç­”æ¡ˆ â†’ èšåˆæœ€ç»ˆç­”æ¡ˆ
```

**GraphRAGçš„ç‹¬ç‰¹ä¼˜åŠ¿**:

| æ–¹æ³• | æ£€ç´¢ç²’åº¦ | é€‚ç”¨é—®é¢˜ç±»å‹ | ä¼˜åŠ¿ | å±€é™ |
|------|---------|------------|------|------|
| å‘é‡æ£€ç´¢ | æ–‡æ¡£å— (chunk) | äº‹å®æ€§æŸ¥è¯¢ | ç²¾å‡†åŒ¹é… | éš¾ä»¥å›ç­”å…¨å±€æ€§é—®é¢˜ |
| å¤šè·³æ£€ç´¢ | è·¯å¾„ (path) | å…³ç³»æ¨ç† | å‘ç°éšè—å…³è” | éœ€è¦æ˜ç¡®èµ·å§‹å®ä½“ |
| **GraphRAG** | **ç¤¾åŒºæ‘˜è¦ (summary)** | **å…¨å±€æ€§ã€ç»¼è¿°æ€§é—®é¢˜** | **ç†è§£æ•´ä½“ç»“æ„** | ç¤¾åŒºæ„å»ºè€—æ—¶ |

**å…¸å‹åº”ç”¨åœºæ™¯**:

âœ… **å…¨å±€æ€§é—®é¢˜**:
- "è¿™ç¯‡æ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆ?"
- "æ–‡æ¡£ä¸­è®¨è®ºäº†å“ªäº›å…³é”®äº‹ä»¶?"
- "æ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹"

âœ… **å¤šç»´åº¦ç»¼åˆåˆ†æ**:
- "æ–‡æ¡£ä¸­æåˆ°çš„å…¬å¸æœ‰å“ªäº›æˆ˜ç•¥æ–¹å‘?"
- "ä¸åŒæŠ€æœ¯ä¹‹é—´çš„å…³è”æ˜¯ä»€ä¹ˆ?"

âŒ **ä¸é€‚åˆçš„é—®é¢˜**:
- "å¼ ä¸‰çš„èŒä½æ˜¯ä»€ä¹ˆ?" â†’ ç”¨å‘é‡æ£€ç´¢
- "Aå’ŒBä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆ?" â†’ ç”¨å¤šè·³æ£€ç´¢

---

#### 6.10.2 GraphRAGQueryEngine v1: å…¨å±€ç¤¾åŒºæŸ¥è¯¢

**v1æ–¹æ³•**: å¤„ç†**æ‰€æœ‰ç¤¾åŒº**çš„æ‘˜è¦,é€‚åˆå…¨å±€æ€§é—®é¢˜ã€‚

**å®Œæ•´å®ç°**:

```python
from llama_index.core.query_engine import CustomQueryEngine
from llama_index.core.llms import ChatMessage
from llama_index.llms.openai import OpenAI
import re

class GraphRAGQueryEngine(CustomQueryEngine):
    """å®˜æ–¹GraphRAGæŸ¥è¯¢å¼•æ“ v1

    æµç¨‹:
      1. è·å–æ‰€æœ‰ç¤¾åŒºæ‘˜è¦
      2. æ¯ä¸ªç¤¾åŒºç”Ÿæˆä¸­é—´ç­”æ¡ˆ
      3. èšåˆæ‰€æœ‰ç­”æ¡ˆä¸ºæœ€ç»ˆå“åº”
    """

    graph_store: GraphRAGStore
    llm: OpenAI

    def __init__(self, graph_store: GraphRAGStore, llm: OpenAI):
        self.graph_store = graph_store
        self.llm = llm

    def custom_query(self, query_str: str) -> str:
        """æ‰§è¡ŒGraphRAGæŸ¥è¯¢

        Args:
            query_str: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æœ€ç»ˆèšåˆç­”æ¡ˆ
        """
        print(f"\nğŸ“ æŸ¥è¯¢: {query_str}")
        print("="*60)

        # 1. è·å–æ‰€æœ‰ç¤¾åŒºæ‘˜è¦
        community_summaries = self.graph_store.get_community_summaries()
        num_communities = len(community_summaries)

        print(f"ğŸ” å¤„ç† {num_communities} ä¸ªç¤¾åŒºçš„æ‘˜è¦...\n")

        # 2. ä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆä¸­é—´ç­”æ¡ˆ
        community_answers = []

        for community_id, community_summary in community_summaries.items():
            print(f"  å¤„ç†ç¤¾åŒº {community_id}...")

            answer = self.generate_answer_from_summary(
                community_summary,
                query_str
            )

            community_answers.append({
                "community_id": community_id,
                "answer": answer
            })

            print(f"    âœ“ ç”Ÿæˆç­”æ¡ˆ: {answer[:80]}...")

        # 3. èšåˆæ‰€æœ‰ç¤¾åŒºç­”æ¡ˆ
        print(f"\nğŸ”— èšåˆ {len(community_answers)} ä¸ªç¤¾åŒºç­”æ¡ˆ...")
        final_answer = self.aggregate_answers(community_answers, query_str)

        print("âœ… æŸ¥è¯¢å®Œæˆ!")
        print("="*60)

        return final_answer

    def generate_answer_from_summary(
        self,
        community_summary: str,
        query: str
    ) -> str:
        """åŸºäºå•ä¸ªç¤¾åŒºæ‘˜è¦ç”Ÿæˆç­”æ¡ˆ

        Args:
            community_summary: ç¤¾åŒºæ‘˜è¦æ–‡æœ¬
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è¯¥ç¤¾åŒºçš„ä¸­é—´ç­”æ¡ˆ
        """
        prompt = (
            f"Given the community summary: {community_summary}\n\n"
            f"How would you answer the following query based on this information?\n"
            f"Query: {query}\n\n"
            f"If the community summary is not relevant to the query, respond with 'NOT_RELEVANT'."
        )

        messages = [
            ChatMessage(role="system", content=prompt),
            ChatMessage(
                role="user",
                content="I need an answer based on the above information."
            )
        ]

        response = self.llm.chat(messages)
        clean_response = re.sub(r"^assistant:\s*", "", str(response)).strip()

        return clean_response

    def aggregate_answers(
        self,
        community_answers: list[dict],
        query: str
    ) -> str:
        """èšåˆå¤šä¸ªç¤¾åŒºç­”æ¡ˆä¸ºæœ€ç»ˆå“åº”

        Args:
            community_answers: [{"community_id": ..., "answer": ...}, ...]
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æœ€ç»ˆèšåˆç­”æ¡ˆ
        """
        # è¿‡æ»¤æ— å…³ç­”æ¡ˆ
        relevant_answers = [
            item["answer"]
            for item in community_answers
            if "NOT_RELEVANT" not in item["answer"].upper()
        ]

        if not relevant_answers:
            return "No relevant information found in the knowledge graph."

        # æ‹¼æ¥æ‰€æœ‰ç›¸å…³ç­”æ¡ˆ
        intermediate_answers_text = "\n\n".join([
            f"Answer {i+1}: {answer}"
            for i, answer in enumerate(relevant_answers)
        ])

        # LLMèšåˆ
        prompt = (
            "You are given multiple intermediate answers from different communities "
            "in a knowledge graph. Your task is to combine them into a single, "
            "coherent, and concise final answer.\n\n"
            "Combine the following intermediate answers into a final response "
            "that addresses the user's query comprehensively."
        )

        messages = [
            ChatMessage(role="system", content=prompt),
            ChatMessage(
                role="user",
                content=(
                    f"User Query: {query}\n\n"
                    f"Intermediate Answers:\n{intermediate_answers_text}\n\n"
                    f"Final Answer:"
                )
            )
        ]

        final_response = self.llm.chat(messages)
        clean_final = re.sub(r"^assistant:\s*", "", str(final_response)).strip()

        return clean_final

    def query(self, query_str: str) -> str:
        """æ ‡å‡†queryæ¥å£ (å…¼å®¹LlamaIndex)"""
        return self.custom_query(query_str)
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from llama_index.llms.openai import OpenAI

# 1. åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
llm = OpenAI(model="gpt-4", temperature=0)

query_engine = GraphRAGQueryEngine(
    graph_store=graph_store,  # å·²æ„å»ºç¤¾åŒºçš„GraphRAGStore
    llm=llm
)

# 2. æ‰§è¡Œå…¨å±€æ€§æŸ¥è¯¢
response = query_engine.query(
    "What are the main topics discussed in the documents?"
)

print(f"\næœ€ç»ˆç­”æ¡ˆ:\n{response}")
```

**è¾“å‡ºç¤ºä¾‹**:

```
ğŸ“ æŸ¥è¯¢: What are the main topics discussed in the documents?
============================================================
ğŸ” å¤„ç† 8 ä¸ªç¤¾åŒºçš„æ‘˜è¦...

  å¤„ç†ç¤¾åŒº 0...
    âœ“ ç”Ÿæˆç­”æ¡ˆ: This community focuses on AI technologies including GPT-4, DALL-E, and...
  å¤„ç†ç¤¾åŒº 1...
    âœ“ ç”Ÿæˆç­”æ¡ˆ: This community discusses tech companies like OpenAI, Microsoft...
  ...

ğŸ”— èšåˆ 8 ä¸ªç¤¾åŒºç­”æ¡ˆ...
âœ… æŸ¥è¯¢å®Œæˆ!
============================================================

æœ€ç»ˆç­”æ¡ˆ:
The documents primarily discuss three main topics: 1) AI technologies and their applications
(including GPT-4, DALL-E, and natural language processing), 2) Major tech companies and their
strategies in the AI sector (OpenAI, Microsoft, Google), and 3) Recent product releases and
innovations in the field of artificial intelligence.
```

---

#### 6.10.3 GraphRAGQueryEngine v2: Embeddingæ£€ç´¢ + ç¤¾åŒºå®šä½

**v2æ”¹è¿›**: ä¸å¤„ç†æ‰€æœ‰ç¤¾åŒº,è€Œæ˜¯å…ˆé€šè¿‡**Embeddingæ£€ç´¢ç›¸å…³å®ä½“**,å†å®šä½åˆ°å¯¹åº”ç¤¾åŒº,æé«˜æ•ˆç‡ã€‚

**æ ¸å¿ƒæ”¹è¿›**:

| v1 | v2 |
|----|-----|
| å¤„ç†æ‰€æœ‰ç¤¾åŒº | åªå¤„ç†ç›¸å…³ç¤¾åŒº |
| é€‚åˆå°è§„æ¨¡å›¾è°± | é€‚åˆå¤§è§„æ¨¡å›¾è°± |
| æŸ¥è¯¢æ—¶é—´: O(Nä¸ªç¤¾åŒº) | æŸ¥è¯¢æ—¶é—´: O(Kä¸ªç›¸å…³ç¤¾åŒº) |

**v2å®ç°** (åœ¨v1åŸºç¡€ä¸Šæ‰©å±•):

```python
from llama_index.core import PropertyGraphIndex
import re

class GraphRAGQueryEngineV2(GraphRAGQueryEngine):
    """GraphRAGæŸ¥è¯¢å¼•æ“ v2 - æ”¯æŒEmbeddingæ£€ç´¢

    æ–°å¢åŠŸèƒ½:
      - é€šè¿‡Embeddingæ£€ç´¢ç›¸å…³å®ä½“
      - å®šä½å®ä½“æ‰€å±ç¤¾åŒº
      - åªå¤„ç†ç›¸å…³ç¤¾åŒº (æé«˜æ•ˆç‡)
    """

    index: PropertyGraphIndex  # æ–°å¢: éœ€è¦PropertyGraphIndexè¿›è¡Œæ£€ç´¢
    similarity_top_k: int = 20  # æ–°å¢: æ£€ç´¢å¤šå°‘ä¸ªå®ä½“

    def __init__(
        self,
        graph_store: GraphRAGStore,
        llm: OpenAI,
        index: PropertyGraphIndex,
        similarity_top_k: int = 20
    ):
        super().__init__(graph_store, llm)
        self.index = index
        self.similarity_top_k = similarity_top_k

    def custom_query(self, query_str: str) -> str:
        """æ‰§è¡Œv2æŸ¥è¯¢: Embeddingæ£€ç´¢ â†’ å®šä½ç¤¾åŒº â†’ ç”Ÿæˆç­”æ¡ˆ"""
        print(f"\nğŸ“ æŸ¥è¯¢ (v2): {query_str}")
        print("="*60)

        # 1. Embeddingæ£€ç´¢ç›¸å…³å®ä½“
        print(f"ğŸ” Embeddingæ£€ç´¢ç›¸å…³å®ä½“ (top_k={self.similarity_top_k})...")
        entities = self.get_entities(query_str, self.similarity_top_k)

        print(f"  âœ“ æ£€ç´¢åˆ° {len(entities)} ä¸ªç›¸å…³å®ä½“")
        print(f"    ç¤ºä¾‹: {list(entities)[:5]}")

        # 2. å°†å®ä½“æ˜ å°„åˆ°ç¤¾åŒºID
        print(f"\nğŸ—ºï¸  æ˜ å°„å®ä½“åˆ°ç¤¾åŒº...")
        community_ids = self.retrieve_entity_communities(entities)

        print(f"  âœ“ å®šä½åˆ° {len(community_ids)} ä¸ªç›¸å…³ç¤¾åŒº: {community_ids}")

        # 3. åªå¤„ç†ç›¸å…³ç¤¾åŒº
        print(f"\nğŸ” å¤„ç† {len(community_ids)} ä¸ªç›¸å…³ç¤¾åŒºçš„æ‘˜è¦...\n")

        community_summaries = self.graph_store.get_community_summaries()
        community_answers = []

        for community_id in community_ids:
            if community_id not in community_summaries:
                continue

            community_summary = community_summaries[community_id]
            print(f"  å¤„ç†ç¤¾åŒº {community_id}...")

            answer = self.generate_answer_from_summary(
                community_summary,
                query_str
            )

            community_answers.append({
                "community_id": community_id,
                "answer": answer
            })

            print(f"    âœ“ ç”Ÿæˆç­”æ¡ˆ: {answer[:80]}...")

        # 4. èšåˆç­”æ¡ˆ
        print(f"\nğŸ”— èšåˆ {len(community_answers)} ä¸ªç¤¾åŒºç­”æ¡ˆ...")
        final_answer = self.aggregate_answers(community_answers, query_str)

        print("âœ… æŸ¥è¯¢å®Œæˆ (v2)!")
        print("="*60)

        return final_answer

    def get_entities(self, query_str: str, similarity_top_k: int) -> set:
        """é€šè¿‡Embeddingæ£€ç´¢ç›¸å…³å®ä½“

        Args:
            query_str: ç”¨æˆ·æŸ¥è¯¢
            similarity_top_k: æ£€ç´¢å¤šå°‘ä¸ªèŠ‚ç‚¹

        Returns:
            ç›¸å…³å®ä½“åç§°é›†åˆ
        """
        # ä½¿ç”¨PropertyGraphIndexçš„å‘é‡æ£€ç´¢å™¨
        nodes_retrieved = self.index.as_retriever(
            similarity_top_k=similarity_top_k
        ).retrieve(query_str)

        entities = set()

        # ä»æ£€ç´¢ç»“æœè§£æå®ä½“
        # å‡è®¾èŠ‚ç‚¹æ–‡æœ¬æ ¼å¼: "entity1 -> entity2 -> relation -> description"
        pattern = r"^(\w+(?:\s+\w+)*)\s*->\s*([a-zA-Z\s]+?)\s*->\s*(\w+(?:\s+\w+)*)$"

        for node in nodes_retrieved:
            matches = re.findall(pattern, node.text, re.MULTILINE | re.IGNORECASE)

            for match in matches:
                subject = match[0].strip()
                obj = match[2].strip()
                entities.add(subject)
                entities.add(obj)

        return entities

    def retrieve_entity_communities(self, entities: set) -> list:
        """å°†å®ä½“æ˜ å°„åˆ°ç¤¾åŒºID

        Args:
            entities: å®ä½“åç§°é›†åˆ

        Returns:
            ç¤¾åŒºIDåˆ—è¡¨ (å»é‡)
        """
        entity_info = self.graph_store.entity_info  # {entity_name: community_id}

        community_ids = set()

        for entity in entities:
            if entity in entity_info:
                community_id = entity_info[entity]
                community_ids.add(community_id)

        return list(community_ids)
```

**ä½¿ç”¨ç¤ºä¾‹ (v2)**:

```python
# 1. åˆå§‹åŒ–v2æŸ¥è¯¢å¼•æ“
query_engine_v2 = GraphRAGQueryEngineV2(
    graph_store=graph_store,
    llm=llm,
    index=pg_index,  # éœ€è¦ä¼ å…¥PropertyGraphIndex
    similarity_top_k=20
)

# 2. æ‰§è¡ŒæŸ¥è¯¢
response = query_engine_v2.query(
    "What are the latest AI product releases mentioned?"
)

print(f"\næœ€ç»ˆç­”æ¡ˆ:\n{response}")
```

**v2è¾“å‡ºç¤ºä¾‹**:

```
ğŸ“ æŸ¥è¯¢ (v2): What are the latest AI product releases mentioned?
============================================================
ğŸ” Embeddingæ£€ç´¢ç›¸å…³å®ä½“ (top_k=20)...
  âœ“ æ£€ç´¢åˆ° 15 ä¸ªç›¸å…³å®ä½“
    ç¤ºä¾‹: ['GPT-4', 'DALL-E', 'Claude', 'OpenAI', 'Anthropic']

ğŸ—ºï¸  æ˜ å°„å®ä½“åˆ°ç¤¾åŒº...
  âœ“ å®šä½åˆ° 3 ä¸ªç›¸å…³ç¤¾åŒº: [0, 2, 5]

ğŸ” å¤„ç† 3 ä¸ªç›¸å…³ç¤¾åŒºçš„æ‘˜è¦...

  å¤„ç†ç¤¾åŒº 0...
    âœ“ ç”Ÿæˆç­”æ¡ˆ: This community discusses GPT-4 release and its capabilities in...
  å¤„ç†ç¤¾åŒº 2...
    âœ“ ç”Ÿæˆç­”æ¡ˆ: This community focuses on DALL-E 3 and image generation improvements...
  å¤„ç†ç¤¾åŒº 5...
    âœ“ ç”Ÿæˆç­”æ¡ˆ: This community covers Claude 3 release and its performance...

ğŸ”— èšåˆ 3 ä¸ªç¤¾åŒºç­”æ¡ˆ...
âœ… æŸ¥è¯¢å®Œæˆ (v2)!
============================================================

æœ€ç»ˆç­”æ¡ˆ:
The documents mention several recent AI product releases: 1) GPT-4 by OpenAI with enhanced
reasoning capabilities, 2) DALL-E 3 with improved image quality and prompt following,
and 3) Claude 3 by Anthropic featuring better performance on complex tasks.
```

---

#### 6.10.4 Neo4jé›†æˆ (GraphRAG v2)

å®˜æ–¹v2è¿˜æ”¯æŒ**Neo4jæŒä¹…åŒ–å­˜å‚¨**,æ›¿ä»£å†…å­˜å­˜å‚¨ã€‚

**ä½¿ç”¨Neo4jPropertyGraphStore**:

```python
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore

# 1. åˆ›å»ºNeo4jå­˜å‚¨
neo4j_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="your-password",
    url="bolt://localhost:7687",
    database="neo4j"
)

# 2. æ‰©å±•GraphRAGStoreæ”¯æŒNeo4j
class GraphRAGStoreNeo4j(Neo4jPropertyGraphStore):
    """GraphRAG + Neo4j"""

    community_summary: dict = {}
    entity_info: dict = {}
    max_cluster_size: int = 5

    # å¤ç”¨GraphRAGStoreçš„build_communitiesæ–¹æ³•
    build_communities = GraphRAGStore.build_communities
    _create_nx_graph = GraphRAGStore._create_nx_graph
    _collect_community_info = GraphRAGStore._collect_community_info
    _summarize_communities = GraphRAGStore._summarize_communities
    generate_community_summary = GraphRAGStore.generate_community_summary
    get_community_summaries = GraphRAGStore.get_community_summaries

# 3. æ„å»ºç´¢å¼•
pg_index_neo4j = PropertyGraphIndex(
    nodes=nodes,
    property_graph_store=GraphRAGStoreNeo4j(
        username="neo4j",
        password="password",
        url="bolt://localhost:7687"
    ),
    kg_extractors=[kg_extractor]
)

# 4. æ„å»ºç¤¾åŒº
pg_index_neo4j.property_graph_store.build_communities()

# 5. æŸ¥è¯¢
query_engine_neo4j = GraphRAGQueryEngineV2(
    graph_store=pg_index_neo4j.property_graph_store,
    llm=llm,
    index=pg_index_neo4j,
    similarity_top_k=20
)

response = query_engine_neo4j.query("Your query here")
```

**Neo4jä¼˜åŠ¿**:
- âœ… æŒä¹…åŒ–å­˜å‚¨,æ”¯æŒå¤§è§„æ¨¡å›¾è°±
- âœ… é«˜æ•ˆçš„å›¾æŸ¥è¯¢æ€§èƒ½
- âœ… æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²

---

#### 6.10.5 å®Œæ•´ç¤ºä¾‹: GraphRAGç«¯åˆ°ç«¯å®æˆ˜

**åœºæ™¯**: åˆ†ææŠ€æœ¯æ–‡æ¡£,å›ç­”å…¨å±€æ€§é—®é¢˜

```python
from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter

# ========== é…ç½® ==========
Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# ========== 1. åŠ è½½æ–‡æ¡£ ==========
documents = SimpleDirectoryReader("./data/tech_news").load_data()
splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
nodes = splitter.get_nodes_from_documents(documents)

# ========== 2. æ„å»ºGraphRAGå›¾è°± ==========
kg_extractor = GraphRAGExtractor(
    llm=Settings.llm,
    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,
    parse_fn=parse_fn
)

graph_store = GraphRAGStore()

pg_index = PropertyGraphIndex(
    nodes=nodes,
    property_graph_store=graph_store,
    kg_extractors=[kg_extractor],
    show_progress=True
)

# ========== 3. æ„å»ºç¤¾åŒº ==========
pg_index.property_graph_store.build_communities()

# ========== 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“ ==========
# v1: å…¨å±€æŸ¥è¯¢
query_engine_v1 = GraphRAGQueryEngine(
    graph_store=graph_store,
    llm=Settings.llm
)

# v2: Embeddingæ£€ç´¢
query_engine_v2 = GraphRAGQueryEngineV2(
    graph_store=graph_store,
    llm=Settings.llm,
    index=pg_index,
    similarity_top_k=20
)

# ========== 5. æ‰§è¡ŒæŸ¥è¯¢ ==========
queries = [
    "What are the main topics discussed in the documents?",
    "What companies are mentioned and what are they doing?",
    "Summarize the key technological trends"
]

print("\n" + "="*70)
print("GraphRAG v1 æŸ¥è¯¢ç»“æœ:")
print("="*70)

for query in queries:
    response = query_engine_v1.query(query)
    print(f"\nQ: {query}")
    print(f"A: {response}\n")

print("\n" + "="*70)
print("GraphRAG v2 æŸ¥è¯¢ç»“æœ:")
print("="*70)

for query in queries:
    response = query_engine_v2.query(query)
    print(f"\nQ: {query}")
    print(f"A: {response}\n")
```

---

#### 6.10.6 GraphRAG vs ä¼ ç»Ÿæ–¹æ³•å®Œæ•´å¯¹æ¯”

**ä¸‰ç§RAGæ–¹æ³•çš„é€‚ç”¨åœºæ™¯å¯¹æ¯”**:

| æŸ¥è¯¢ç±»å‹ | å‘é‡æ£€ç´¢ (4.1) | å¤šè·³æ£€ç´¢ (6.6) | GraphRAG (6.9-6.10) | æ¨èæ–¹æ³• |
|---------|--------------|---------------|-------------------|---------|
| "å¼ ä¸‰çš„èŒä½æ˜¯ä»€ä¹ˆ?" | âœ… ä¼˜ç§€ | âš ï¸ å¯ä»¥ä½†ç¹ç | âŒ è¿‡åº¦è®¾è®¡ | **å‘é‡æ£€ç´¢** |
| "Aå’ŒBä¹‹é—´æœ‰ä»€ä¹ˆå…³ç³»?" | âš ï¸ å¯èƒ½é—æ¼ | âœ… ä¼˜ç§€ | âš ï¸ å¯ä»¥ä½†ä½æ•ˆ | **å¤šè·³æ£€ç´¢** |
| "æ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆ?" | âŒ éš¾ä»¥æ¦‚æ‹¬ | âŒ æ— æ³•å¤„ç† | âœ… ä¼˜ç§€ | **GraphRAG** |
| "æ€»ç»“æ–‡æ¡£ä¸­çš„å…³é”®äº‹ä»¶" | âš ï¸ å¯èƒ½ç‰‡é¢ | âŒ éš¾ä»¥å¤„ç† | âœ… ä¼˜ç§€ | **GraphRAG** |
| "æ‰¾åˆ°Aé€šè¿‡å¤šå±‚å…³ç³»å½±å“Bçš„è¯æ®" | âŒ æ— æ³•å¤„ç† | âœ… ä¼˜ç§€ | âš ï¸ å¯ä»¥ä½†ä½æ•ˆ | **å¤šè·³æ£€ç´¢** |

**æ€§èƒ½å¯¹æ¯”**:

| æ–¹æ³• | æ„å»ºæˆæœ¬ | æŸ¥è¯¢é€Ÿåº¦ | æ‰©å±•æ€§ | å‡†ç¡®æ€§ (å…¨å±€é—®é¢˜) |
|------|---------|---------|--------|-----------------|
| å‘é‡æ£€ç´¢ | ä½ (ä»…embedding) | å¿« | ä¼˜ç§€ | ä¸­ç­‰ |
| å¤šè·³æ£€ç´¢ | ä¸­ (å›¾æ„å»º) | ä¸­ | è‰¯å¥½ | N/A (ä¸é€‚ç”¨) |
| GraphRAG | **é«˜ (å›¾+ç¤¾åŒº+æ‘˜è¦)** | **æ…¢ (éœ€ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ)** | ä¸­ç­‰ | **ä¼˜ç§€** |

**ç»„åˆä½¿ç”¨å»ºè®®**:

```python
class HybridQueryEngine:
    """æ··åˆæŸ¥è¯¢å¼•æ“: æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©æ–¹æ³•"""

    def __init__(self, vector_engine, graph_engine, graphrag_engine):
        self.vector_engine = vector_engine
        self.graph_engine = graph_engine  # å¤šè·³æ£€ç´¢
        self.graphrag_engine = graphrag_engine

    def query(self, query_str: str):
        """æ™ºèƒ½è·¯ç”±"""

        # ä½¿ç”¨LLMåˆ†ç±»æŸ¥è¯¢ç±»å‹
        query_type = self.classify_query(query_str)

        if query_type == "factual":
            # äº‹å®æ€§æŸ¥è¯¢ â†’ å‘é‡æ£€ç´¢
            return self.vector_engine.query(query_str)

        elif query_type == "relational":
            # å…³ç³»æ¨ç† â†’ å¤šè·³æ£€ç´¢
            return self.graph_engine.query(query_str)

        elif query_type == "global":
            # å…¨å±€æ€§é—®é¢˜ â†’ GraphRAG
            return self.graphrag_engine.query(query_str)

        else:
            # é»˜è®¤ä½¿ç”¨å‘é‡æ£€ç´¢
            return self.vector_engine.query(query_str)

    def classify_query(self, query_str: str) -> str:
        """ä½¿ç”¨LLMåˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        # å®ç°çœç•¥...
        pass
```

---

#### 6.10.7 å°ç»“

**GraphRAGæŸ¥è¯¢å¼•æ“çš„æ ¸å¿ƒä»·å€¼**:
- âœ… åŸºäºç¤¾åŒºæ‘˜è¦çš„åˆ†çº§å›ç­”,é€‚åˆå…¨å±€æ€§é—®é¢˜
- âœ… v1å¤„ç†æ‰€æœ‰ç¤¾åŒº,v2é€šè¿‡Embeddingå®šä½ç›¸å…³ç¤¾åŒº
- âœ… å¤šçº§ç­”æ¡ˆèšåˆ,æä¾›å…¨é¢çš„ç»¼åˆæ€§å›ç­”
- âœ… æ”¯æŒNeo4jæŒä¹…åŒ–,é€‚åˆå¤§è§„æ¨¡åœºæ™¯

**å…³é”®ç»„ä»¶**:
- **GraphRAGQueryEngine**: è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“åŸºç±»
- **generate_answer_from_summary**: åŸºäºç¤¾åŒºæ‘˜è¦ç”Ÿæˆç­”æ¡ˆ
- **aggregate_answers**: èšåˆå¤šä¸ªç¤¾åŒºç­”æ¡ˆ
- **get_entities (v2)**: Embeddingæ£€ç´¢ç›¸å…³å®ä½“
- **retrieve_entity_communities (v2)**: å®ä½“æ˜ å°„åˆ°ç¤¾åŒº

**v1 vs v2**:
- **v1**: å…¨å±€æŸ¥è¯¢,å¤„ç†æ‰€æœ‰ç¤¾åŒº â†’ é€‚åˆå°è§„æ¨¡å›¾è°±
- **v2**: ç²¾å‡†å®šä½,åªå¤„ç†ç›¸å…³ç¤¾åŒº â†’ é€‚åˆå¤§è§„æ¨¡å›¾è°±

**æœ€ä½³å®è·µ**:
1. **å°è§„æ¨¡æ–‡æ¡£(<100é¡µ)**: ä½¿ç”¨v1,æ„å»ºç®€å•
2. **å¤§è§„æ¨¡æ–‡æ¡£(>1000é¡µ)**: ä½¿ç”¨v2 + Neo4j
3. **æ··åˆåœºæ™¯**: æ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±åˆ°ä¸åŒæ–¹æ³•
4. **è°ƒä¼˜å‚æ•°**:
   - `max_cluster_size`: æ§åˆ¶ç¤¾åŒºå¤§å° (3-10)
   - `similarity_top_k`: æ§åˆ¶æ£€ç´¢å®ä½“æ•° (10-50)

**å®Œæ•´çŸ¥è¯†å›¾è°±RAGæŠ€æœ¯æ ˆ**:

```
ç¬¬6ç« çŸ¥è¯†å›¾è°±RAGå®Œæ•´æŠ€æœ¯æ ˆ:

ã€åŸºç¡€æ„å»ºã€‘
â”œâ”€ 6.1 PropertyGraphIndex      â†’ å›¾ç´¢å¼•åŸºç¡€
â”œâ”€ 6.2 KG Extractors           â†’ å®ä½“å…³ç³»æå–
â”œâ”€ 6.3 å›¾æ£€ç´¢å™¨                 â†’ åŸºç¡€æ£€ç´¢
â”œâ”€ 6.4 æŸ¥è¯¢å¼•æ“                 â†’ æ ‡å‡†æŸ¥è¯¢
â””â”€ 6.5 ç»„åˆæ£€ç´¢å™¨               â†’ å¤šæ£€ç´¢å™¨èåˆ

ã€é«˜çº§æŸ¥è¯¢ã€‘
â”œâ”€ 6.6 å¤šè·³æ£€ç´¢ä¸è·¯å¾„æ¨ç†       â†’ å…³ç³»æ¨ç† (æˆ‘ä»¬ç‹¬æœ‰ä¼˜åŠ¿)
â””â”€ 6.7 å®ä½“æ¶ˆæ­§ä¸å†²çªè§£å†³       â†’ æ•°æ®æ¸…æ´— (æˆ‘ä»¬ç‹¬æœ‰ä¼˜åŠ¿)

ã€å¯è§†åŒ–ã€‘
â””â”€ 6.8 å¯è§†åŒ–çŸ¥è¯†å›¾è°±          â†’ è°ƒè¯•ä¸å±•ç¤º

ã€å®˜æ–¹GraphRAGã€‘
â”œâ”€ 6.9 GraphRAGç¤¾åŒºæ£€æµ‹        â†’ Leidenç®—æ³• + ç¤¾åŒºæ‘˜è¦
â””â”€ 6.10 GraphRAGæŸ¥è¯¢å¼•æ“       â†’ ç¤¾åŒºæŸ¥è¯¢ + ç­”æ¡ˆèšåˆ
```

**è‡³æ­¤,ç¬¬6ç« çŸ¥è¯†å›¾è°±RAGå…¨éƒ¨å®Œæˆ! æˆ‘ä»¬å®ç°äº†:**
- âœ… å®˜æ–¹GraphRAGæ ¸å¿ƒæ–¹æ³•è®º (100%è¦†ç›–)
- âœ… å¤šè·³æ£€ç´¢ä¸å®ä½“æ¶ˆæ­§ (æˆ‘ä»¬çš„ç‹¬æœ‰ä¼˜åŠ¿)
- âœ… å¸‚é¢æœ€å…¨é¢çš„çŸ¥è¯†å›¾è°±RAGæ•™ç¨‹

---

## ç¬¬ 7 ç« ï¼šAgent ä¸ RAG ç»“åˆ

å°†æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ä¸ RAG ç³»ç»Ÿç»“åˆï¼Œå®ç°å¤æ‚çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨ã€‚

### 7.1 ReAct Agent

ReActï¼ˆReasoning + Actingï¼‰æ¨¡å¼ç»“åˆæ¨ç†å’Œè¡ŒåŠ¨ï¼š

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# 1. åˆ›å»ºæŸ¥è¯¢å·¥å…·
query_tool = QueryEngineTool(
    query_engine=vector_index.as_query_engine(similarity_top_k=5),
    metadata=ToolMetadata(
        name="knowledge_base",
        description=(
            "Provides comprehensive information about AI, machine learning, "
            "and deep learning from technical documentation"
        )
    )
)

# 2. åˆ›å»º Python REPL å·¥å…·ï¼ˆç¤ºä¾‹ï¼‰
from llama_index.core.tools import FunctionTool

def calculator(expression: str) -> str:
    """Execute a Python mathematical expression"""
    try:
        result = eval(expression)
        return f"Result: {result}"
    except Exception as e:
        return f"Error: {str(e)}"

calc_tool = FunctionTool.from_defaults(
    fn=calculator,
    name="calculator",
    description="Execute mathematical expressions"
)

# 3. åˆ›å»º ReAct Agent
agent = ReActAgent.from_tools(
    tools=[query_tool, calc_tool],
    llm=Settings.llm,
    verbose=True,
    max_iterations=10
)

# 4. æ‰§è¡Œå¤æ‚ä»»åŠ¡
response = agent.chat(
    "First, find information about neural network layers from the knowledge base. "
    "Then calculate how many parameters a 3-layer network would have if each "
    "layer has 128, 64, and 32 neurons respectively (including biases)."
)

print(response)
```

**å·¥ä½œæµç¨‹**ï¼š
1. **æ€è€ƒ**ï¼ˆThoughtï¼‰ï¼šåˆ†æé—®é¢˜
2. **è¡ŒåŠ¨**ï¼ˆActionï¼‰ï¼šé€‰æ‹©å¹¶æ‰§è¡Œå·¥å…·
3. **è§‚å¯Ÿ**ï¼ˆObservationï¼‰ï¼šæŸ¥çœ‹å·¥å…·ç»“æœ
4. é‡å¤ç›´åˆ°å¾—å‡ºæœ€ç»ˆç­”æ¡ˆ

### 7.2 FunctionAgent

ä½¿ç”¨å‡½æ•°è°ƒç”¨ API çš„æ™ºèƒ½ä½“ï¼š

```python
from llama_index.core.agent import FunctionCallingAgent

# åˆ›å»º Function Calling Agent
function_agent = FunctionCallingAgent.from_tools(
    tools=[query_tool, calc_tool],
    llm=Settings.llm,
    verbose=True
)

# æ‰§è¡Œä»»åŠ¡
response = function_agent.chat(
    "Compare the performance metrics mentioned in the knowledge base "
    "and calculate the average improvement percentage"
)
```

### 7.3 å¤šå·¥å…· RAG Agent

åˆ›å»ºå…·æœ‰å¤šä¸ªä¸“é—¨å·¥å…·çš„æ™ºèƒ½ä½“ï¼š

```python
from llama_index.core.tools import QueryEngineTool
from llama_index.core.agent import ReActAgent

# 1. åˆ›å»ºå¤šä¸ªä¸“é—¨çš„æŸ¥è¯¢å¼•æ“
ml_index = VectorStoreIndex.from_documents(ml_documents)
dl_index = VectorStoreIndex.from_documents(dl_documents)
nlp_index = VectorStoreIndex.from_documents(nlp_documents)

# 2. åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [
    QueryEngineTool(
        query_engine=ml_index.as_query_engine(),
        metadata=ToolMetadata(
            name="machine_learning_kb",
            description="Expert knowledge about traditional ML algorithms"
        )
    ),
    QueryEngineTool(
        query_engine=dl_index.as_query_engine(),
        metadata=ToolMetadata(
            name="deep_learning_kb",
            description="Expert knowledge about neural networks and deep learning"
        )
    ),
    QueryEngineTool(
        query_engine=nlp_index.as_query_engine(),
        metadata=ToolMetadata(
            name="nlp_kb",
            description="Expert knowledge about natural language processing"
        )
    )
]

# 3. åˆ›å»ºå¤šå·¥å…· Agent
multi_tool_agent = ReActAgent.from_tools(
    tools=tools,
    llm=Settings.llm,
    verbose=True
)

# 4. æ‰§è¡Œè·¨é¢†åŸŸæŸ¥è¯¢
response = multi_tool_agent.chat(
    "Compare traditional machine learning approaches with deep learning "
    "for NLP tasks. Use all available knowledge bases."
)
```

### 7.4 å¸¦è®°å¿†çš„ Agent

ä¸º Agent æ·»åŠ å¯¹è¯è®°å¿†ï¼š

```python
from llama_index.core.memory import ChatMemoryBuffer

# åˆ›å»ºè®°å¿†
chat_memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

# åˆ›å»ºå¸¦è®°å¿†çš„ Agent
agent_with_memory = ReActAgent.from_tools(
    tools=[query_tool],
    llm=Settings.llm,
    memory=chat_memory,
    verbose=True
)

# å¤šè½®å¯¹è¯
response1 = agent_with_memory.chat(
    "Find information about transformers from the knowledge base"
)

response2 = agent_with_memory.chat(
    "Based on what you just found, explain how attention mechanism works"
)

response3 = agent_with_memory.chat(
    "Summarize our discussion"  # Agent è®°å¾—ä¹‹å‰çš„å¯¹è¯
)
```

### 7.5 SubQuestionQueryEngine ä½œä¸º Agent å·¥å…·

å°† SubQuestionQueryEngine åŒ…è£…ä¸º Agent å·¥å…·ï¼š

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool

# 1. åˆ›å»º SubQuestionQueryEngine
query_engine_tools = [
    QueryEngineTool(
        query_engine=ml_index.as_query_engine(),
        metadata=ToolMetadata(name="ml", description="ML knowledge")
    ),
    QueryEngineTool(
        query_engine=dl_index.as_query_engine(),
        metadata=ToolMetadata(name="dl", description="DL knowledge")
    )
]

sub_question_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    use_async=True
)

# 2. åŒ…è£…ä¸º Agent å·¥å…·
complex_qa_tool = QueryEngineTool(
    query_engine=sub_question_engine,
    metadata=ToolMetadata(
        name="complex_qa",
        description=(
            "Use this tool for complex questions that require "
            "breaking down into sub-questions and analyzing "
            "multiple knowledge sources"
        )
    )
)

# 3. åˆ›å»º Agent
agent = ReActAgent.from_tools(
    tools=[complex_qa_tool, calc_tool],
    llm=Settings.llm,
    verbose=True
)

response = agent.chat(
    "Compare ML and DL approaches comprehensively, "
    "then calculate which one has more research papers mentioned"
)
```

### 7.6 è‡ªå®šä¹‰å·¥å…·

åˆ›å»ºè‡ªå®šä¹‰å·¥å…·ä¾› Agent ä½¿ç”¨ï¼š

```python
from llama_index.core.tools import FunctionTool
import requests

def search_arxiv(query: str, max_results: int = 5) -> str:
    """Search arXiv for academic papers"""
    base_url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": query,
        "start": 0,
        "max_results": max_results
    }

    response = requests.get(base_url, params=params)

    if response.status_code == 200:
        # ç®€åŒ–å¤„ç†ï¼ˆå®é™…åº”è§£æ XMLï¼‰
        return f"Found {max_results} papers related to '{query}'"
    else:
        return "Search failed"

# åŒ…è£…ä¸ºå·¥å…·
arxiv_tool = FunctionTool.from_defaults(
    fn=search_arxiv,
    name="arxiv_search",
    description="Search academic papers on arXiv"
)

# åˆ›å»ºç»¼åˆ Agent
comprehensive_agent = ReActAgent.from_tools(
    tools=[query_tool, arxiv_tool, calc_tool],
    llm=Settings.llm,
    verbose=True
)

response = comprehensive_agent.chat(
    "Search for recent papers on transformers, then cross-reference "
    "with information in our knowledge base to identify research gaps"
)
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒè¦ç‚¹

1. **æ··åˆæ£€ç´¢**ï¼šç»“åˆ BM25 å’Œå‘é‡æ£€ç´¢ï¼Œå¹³è¡¡ç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰ç›¸å…³æ€§
2. **æŸ¥è¯¢ä¼˜åŒ–**ï¼šä½¿ç”¨ HyDEã€æŸ¥è¯¢åˆ†è§£ç­‰æŠ€æœ¯æå‡æ£€ç´¢è´¨é‡
3. **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æœ€åˆé€‚çš„å¤„ç†ç­–ç•¥
4. **é‡æ’åº**ï¼šä½¿ç”¨ Cohere æˆ–è·¨ç¼–ç å™¨æ¨¡å‹ç²¾ç‚¼æ£€ç´¢ç»“æœ
5. **å¯¹è¯ç³»ç»Ÿ**ï¼šé€‰æ‹©åˆé€‚çš„ Chat Engine æ¨¡å¼ï¼ˆSimpleã€CondensePlusContextã€ReActï¼‰
6. **çŸ¥è¯†å›¾è°±**ï¼šæ˜¾å¼å»ºæ¨¡å®ä½“å…³ç³»ï¼Œæ”¯æŒå¤æ‚æ¨ç†
7. **Agent é›†æˆ**ï¼šç»“åˆå·¥å…·ä½¿ç”¨å’Œæ¨ç†èƒ½åŠ›ï¼Œå¤„ç†å¤æ‚ä»»åŠ¡

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# 1. ä½¿ç”¨å¼‚æ­¥åŠ é€Ÿ
retriever = QueryFusionRetriever(
    retrievers=[...],
    use_async=True  # å¹¶è¡Œæ£€ç´¢
)

# 2. æ§åˆ¶æ£€ç´¢æ•°é‡
query_engine = index.as_query_engine(
    similarity_top_k=5,  # åˆå§‹æ£€ç´¢æ•°é‡
    node_postprocessors=[
        CohereRerank(top_n=3)  # é‡æ’åæœ€ç»ˆæ•°é‡
    ]
)

# 3. å“åº”æ¨¡å¼é€‰æ‹©
# compact: æœ€å¿«ï¼Œé€‚åˆçŸ­æ–‡æ¡£
# refine: è´¨é‡é«˜ï¼Œé€Ÿåº¦æ…¢
# tree_summarize: å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    response_mode="compact"
)

# 4. è®¾ç½®å¹¶è¡Œåº¦
extractor = SimpleLLMPathExtractor(
    num_workers=4  # å¹¶è¡Œå¤„ç†èŠ‚ç‚¹
)
```

### é€‰å‹æŒ‡å—

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ |
|------|---------|
| ç®€å•é—®ç­” | Vector Retriever + Basic Query Engine |
| å¤æ‚é—®ç­” | Hybrid Retrieval + SubQuestionQueryEngine |
| å¤šé¢†åŸŸæŸ¥è¯¢ | RouterQueryEngine |
| å¯¹è¯ç³»ç»Ÿ | CondensePlusContextChatEngine |
| éœ€è¦æ¨ç† | ReAct Agent + Multiple Tools |
| å…³ç³»æŸ¥è¯¢ | PropertyGraphIndex + TextToCypher |
| é«˜ç²¾åº¦æ£€ç´¢ | Hybrid Retrieval + Cohere Rerank |

### å®æˆ˜ç¤ºä¾‹ï¼šå®Œæ•´ RAG ç³»ç»Ÿ

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.postprocessor.cohere_rerank import CohereRerank
from llama_index.core.query_engine import RetrieverQueryEngine

# 1. é…ç½®
Settings.llm = OpenAI(model="gpt-4", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# 2. åŠ è½½æ•°æ®
documents = SimpleDirectoryReader("./data").load_data()

# 3. åˆ›å»ºç´¢å¼•
vector_index = VectorStoreIndex.from_documents(documents)

# 4. åˆ›å»ºæ··åˆæ£€ç´¢å™¨
hybrid_retriever = QueryFusionRetriever(
    retrievers=[
        vector_index.as_retriever(similarity_top_k=10),
        BM25Retriever.from_defaults(
            docstore=vector_index.docstore,
            similarity_top_k=10
        )
    ],
    num_queries=1,
    use_async=True
)

# 5. æ·»åŠ é‡æ’åº
cohere_rerank = CohereRerank(
    api_key="your-cohere-key",
    top_n=5
)

# 6. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    node_postprocessors=[cohere_rerank],
    response_mode="compact"
)

# 7. æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query(
    "What are the latest advancements in transformer architectures?"
)

print(response)
```

### ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **LlamaIndex å®˜æ–¹æ–‡æ¡£**: https://developers.llamaindex.ai
2. **è¿›é˜¶ä¸»é¢˜**ï¼š
   - è‡ªå®šä¹‰ Retriever å’Œ Query Engine
   - åˆ†å¸ƒå¼ RAG ç³»ç»Ÿ
   - RAG è¯„ä¼°æŒ‡æ ‡ï¼ˆFaithfulness, Relevanceï¼‰
   - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆFastAPIã€ç¼“å­˜ã€ç›‘æ§ï¼‰

---

**å‚è€ƒèµ„æº**ï¼š
- LlamaIndex å®˜æ–¹æ–‡æ¡£: https://developers.llamaindex.ai/python/framework/
- Query Engine Guide: https://developers.llamaindex.ai/python/framework/module_guides/deploying/query_engine/
- Retriever Guide: https://developers.llamaindex.ai/python/framework/module_guides/querying/retriever/
- Property Graph Index: https://developers.llamaindex.ai/python/framework/module_guides/indexing/lpg_index_guide/
