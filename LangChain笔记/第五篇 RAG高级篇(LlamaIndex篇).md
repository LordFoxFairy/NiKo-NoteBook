# ç¬¬äº”ç¯‡ RAGé«˜çº§ç¯‡ (LlamaIndex)

> **ç›®æ ‡**: åœ¨æŒæ¡ LlamaIndex åŸºç¡€ç»„ä»¶ï¼ˆIndex, Retriever, QueryEngineï¼‰çš„åŸºç¡€ä¸Šï¼Œæ·±å…¥å­¦ä¹ å…¶"æ€æ‰‹é”"çº§çš„é«˜çº§æ£€ç´¢ç­–ç•¥ä¸ Agent é›†æˆèƒ½åŠ›ã€‚æœ¬ç¯‡å°†å¸¦ä½ ä»"èƒ½ç”¨"è¿›åŒ–åˆ°"å¥½ç”¨"ã€‚

## ğŸ“‹ å‰ç½®å‡†å¤‡

æœ¬ç¯‡åŸºäº LlamaIndex v0.10+ ç‰ˆæœ¬ï¼Œå»ºè®®å…ˆå®Œæˆç¬¬å››ç¯‡çš„ç¯å¢ƒé…ç½®ã€‚

```bash
# å®‰è£…é«˜çº§ç»„ä»¶ä¾èµ–
pip install llama-index-retrievers-bm25
pip install llama-index-postprocessor-cohere-rerank
pip install llama-index-graph-stores-neo4j
pip install llama-parse
```

---

## ç¬¬1ç« ï¼šæ··åˆæ£€ç´¢ (Hybrid Retrieval)

å•ä¸€çš„å‘é‡æ£€ç´¢ï¼ˆSemantic Searchï¼‰åœ¨å¤„ç†ç²¾ç¡®å…³é”®è¯åŒ¹é…ï¼ˆå¦‚äº§å“å‹å·ã€ä¸“æœ‰åè¯ï¼‰æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ã€‚æ··åˆæ£€ç´¢é€šè¿‡ç»“åˆ **BM25ï¼ˆå…³é”®è¯ï¼‰** å’Œ **Vectorï¼ˆè¯­ä¹‰ï¼‰**ï¼Œäº’è¡¥é•¿çŸ­ã€‚

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦æ··åˆæ£€ç´¢ï¼Ÿ

*   **å‘é‡æ£€ç´¢**ï¼šæ“…é•¿ç†è§£"æ„å›¾"å’Œ"æ¦‚å¿µ"ã€‚ä¾‹å¦‚æœ"è‹¹æœæ‰‹æœº"ï¼Œèƒ½åŒ¹é…åˆ°"iPhone"ã€‚
*   **å…³é”®è¯æ£€ç´¢**ï¼šæ“…é•¿ç²¾ç¡®åŒ¹é…ã€‚ä¾‹å¦‚æœ"é”™è¯¯ç  502"ï¼Œå‘é‡å¯èƒ½ä¼šåŒ¹é…åˆ°"ç½‘ç»œé”™è¯¯"ï¼Œä½† BM25 èƒ½ç²¾ç¡®å‘½ä¸­åŒ…å«"502"çš„æ–‡æ¡£ã€‚

### 1.2 å®æˆ˜ï¼šæ„å»ºæ··åˆæ£€ç´¢å™¨

LlamaIndex æä¾›äº† `QueryFusionRetriever` æ¥ä¼˜é›…åœ°èåˆå¤šç§æ£€ç´¢ç»“æœã€‚

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core import Settings

# 1. å‡†å¤‡æ•°æ®ä¸å‘é‡ç´¢å¼•
documents = SimpleDirectoryReader("./data").load_data()
vector_index = VectorStoreIndex.from_documents(documents)

# 2. åˆ›å»º BM25 æ£€ç´¢å™¨ (åŸºäºå…³é”®è¯)
# æ³¨æ„ï¼šBM25 éœ€è¦ docstore æ¥æ„å»ºå€’æ’ç´¢å¼•
bm25_retriever = BM25Retriever.from_defaults(
    docstore=vector_index.docstore,
    similarity_top_k=5
)

# 3. åˆ›å»º Vector æ£€ç´¢å™¨ (åŸºäºè¯­ä¹‰)
vector_retriever = vector_index.as_retriever(similarity_top_k=5)

# 4. åˆ›å»ºèåˆæ£€ç´¢å™¨ (Hybrid)
hybrid_retriever = QueryFusionRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    num_queries=1,  # ä¸ç”Ÿæˆæ‰©å±•æŸ¥è¯¢ï¼Œä»…èåˆå½“å‰ç»“æœ
    mode="reciprocal_rerank",  # ä½¿ç”¨ RRF (å€’æ•°æ’åèåˆ) ç®—æ³•
    similarity_top_k=5,
    use_async=True
)

# 5. æµ‹è¯•æ£€ç´¢
nodes = hybrid_retriever.retrieve("LlamaIndex çš„è‡ªåŠ¨åˆå¹¶æ£€ç´¢åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ")
for node in nodes:
    print(f"å¾—åˆ†: {node.score:.4f} | å†…å®¹: {node.text[:50]}...")
```

---

## ç¬¬2ç« ï¼šæŸ¥è¯¢ä¼˜åŒ–ä¸è·¯ç”± (Routing & Transformation)

ç”¨æˆ·çš„ Query å¾€å¾€æ˜¯ä¸å®Œç¾çš„ï¼ˆæ¨¡ç³Šã€å¤æ‚ã€ç¼ºå¤±ä¸Šä¸‹æ–‡ï¼‰ã€‚LlamaIndex æä¾›äº†ä¸€ç³»åˆ—å·¥å…·æ¥"ä¿®å¤"æˆ–"åˆ†å‘"ç”¨æˆ·çš„æŸ¥è¯¢ã€‚

### 2.1 Router Query Engine (è‡ªåŠ¨è·¯ç”±)

è¿™æ˜¯ LlamaIndex æœ€å¼ºå¤§çš„ Pattern ä¹‹ä¸€ï¼šè®© LLM å†³å®šæŸ¥å“ªä¸ªåº“ã€‚

*   **åœºæ™¯**ï¼š
    *   Query: "æ€»ç»“å…¨æ–‡" -> è·¯ç”±åˆ° **SummaryIndex**
    *   Query: "ä½œè€…æ˜¯è°ï¼Ÿ" -> è·¯ç”±åˆ° **VectorStoreIndex**

```python
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector
from llama_index.core import SummaryIndex

# 1. æ„å»ºä¸¤ä¸ªç´¢å¼•
summary_index = SummaryIndex.from_documents(documents)
vector_index = VectorStoreIndex.from_documents(documents)

# 2. å°è£…ä¸ºå·¥å…·
summary_tool = QueryEngineTool.from_defaults(
    query_engine=summary_index.as_query_engine(response_mode="tree_summarize"),
    description="ç”¨äºå›ç­”å…³äºæ–‡æ¡£æ•´ä½“æ‘˜è¦ã€å¤§çº²ã€æ€»ç»“ç±»çš„é—®é¢˜"
)

vector_tool = QueryEngineTool.from_defaults(
    query_engine=vector_index.as_query_engine(),
    description="ç”¨äºå›ç­”å…³äºæ–‡æ¡£ä¸­å…·ä½“äº‹å®ã€ç»†èŠ‚ã€å®šä¹‰çš„ç²¾ç¡®é—®é¢˜"
)

# 3. æ„å»º Router (å¤§è„‘)
router_query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[summary_tool, vector_tool],
    verbose=True
)

# 4. æµ‹è¯•è‡ªé€‚åº”è·¯ç”±
response = router_query_engine.query("è¿™ç¯‡æ–‡ç« ä¸»è¦è®²äº†ä»€ä¹ˆï¼Ÿ")
print(f"Used Tool: {response.metadata['selector_result']}")
```

### 2.2 HyDE (å‡è®¾æ€§æ–‡æ¡£åµŒå…¥)

åœ¨æ£€ç´¢å‰ï¼Œè®© LLM å…ˆ"å¹»è§‰"ä¸€ä¸ªç­”æ¡ˆï¼Œç”¨è¿™ä¸ªå‡è®¾æ€§ç­”æ¡ˆå»æ£€ç´¢æ–‡æ¡£ã€‚

*   **åŸç†**ï¼šæŸ¥è¯¢ "å¦‚ä½•ä¼˜åŒ–ç´¢å¼•ï¼Ÿ" -> LLM ç”Ÿæˆä¸€æ®µå…³äºç´¢å¼•ä¼˜åŒ–çš„æ–‡æœ¬ -> è®¡ç®—è¿™æ®µæ–‡æœ¬çš„å‘é‡ -> æ£€ç´¢æœ€ç›¸ä¼¼çš„çœŸå®æ–‡æ¡£ã€‚
*   **ä¼˜åŠ¿**ï¼šè§£å†³äº† Query å’Œ Document è¯­ä¹‰ç©ºé—´ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

```python
from llama_index.core.indices.query.query_transform import HyDEQueryTransform
from llama_index.core.query_engine import TransformQueryEngine

# 1. å®šä¹‰ HyDE å˜æ¢
hyde = HyDEQueryTransform(include_original=True)

# 2. åŒ…è£…åŸå§‹æŸ¥è¯¢å¼•æ“
vector_query_engine = vector_index.as_query_engine()
hyde_query_engine = TransformQueryEngine(vector_query_engine, query_transform=hyde)

# 3. æŸ¥è¯¢
response = hyde_query_engine.query("å¦‚ä½•æé«˜æ£€ç´¢çš„å‡†ç¡®ç‡ï¼Ÿ")
```

---

## ç¬¬3ç« ï¼šé‡æ’åºæŠ€æœ¯ (Reranking)

**"æ£€ç´¢-ç²¾æ’"**æ˜¯ç°ä»£ RAG çš„æ ‡å‡†èŒƒå¼ã€‚
*   **Retriever**: å¿«é€Ÿå¬å› Top-50ï¼ˆæ­¤æ—¶ç²¾åº¦å¯èƒ½ä¸é«˜ï¼‰ã€‚
*   **Reranker**: ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹ï¼ˆå¦‚ Cross-Encoderï¼‰å¯¹ Top-50 è¿›è¡Œé‡æ–°æ‰“åˆ†ï¼Œé€‰å‡º Top-5ã€‚

### 3.1 Cohere Rerank å®æˆ˜

Cohere æä¾›äº†ç›®å‰ä¸šç•Œå…¬è®¤æ•ˆæœæœ€å¥½çš„å•†ä¸š Rerank æ¨¡å‹ã€‚

```python
from llama_index.postprocessor.cohere_rerank import CohereRerank

# 1.é…ç½® API Key
# os.environ["COHERE_API_KEY"] = "your_key"

# 2. å®šä¹‰ Reranker
cohere_rerank = CohereRerank(
    model="rerank-english-v3.0",
    top_n=3  # æœ€ç»ˆåªä¿ç•™å‰3å
)

# 3. æ³¨å…¥åˆ°æŸ¥è¯¢å¼•æ“ (ä½œä¸º Postprocessor)
query_engine = vector_index.as_query_engine(
    similarity_top_k=20,  # ç¬¬ä¸€æ­¥ï¼šå…ˆå®½æ³›å¬å›20ä¸ª
    node_postprocessors=[cohere_rerank]  # ç¬¬äºŒæ­¥ï¼šç²¾æ’é€‰å‡º3ä¸ª
)

response = query_engine.query("LlamaIndex å’Œ LangChain çš„åŒºåˆ«ï¼Ÿ")

# æŸ¥çœ‹é‡æ’åçš„å¾—åˆ†
for node in response.source_nodes:
    print(f"Rerank Score: {node.score:.4f} - {node.node.get_text()[:30]}...")
```

---

## ç¬¬4ç« ï¼šChat Engine (å¤šè½®å¯¹è¯)

åŸºç¡€çš„ `query_engine` æ˜¯æ— çŠ¶æ€çš„ï¼ˆä¸€é—®ä¸€ç­”ï¼‰ã€‚è€Œ `ChatEngine` ç»´æŠ¤äº†å¯¹è¯å†å²ï¼ˆMemoryï¼‰ï¼Œè®© RAG å…·å¤‡äº†èŠå¤©çš„èƒ½åŠ›ã€‚

### 4.1 Condense Plus Context Mode (æœ€ä½³å®è·µ)

è¿™æ˜¯å¤„ç†å¤šè½® RAG å¯¹è¯æœ€ç¨³å¥çš„æ¨¡å¼ï¼š
1.  **Condense**: å°†"å½“å‰é—®é¢˜" + "å†å²å¯¹è¯" é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€å®Œæ•´çš„æŸ¥è¯¢è¯­å¥ã€‚
2.  **Retrieve**: ç”¨é‡å†™åçš„æŸ¥è¯¢å»æ£€ç´¢ã€‚
3.  **Context**: å°†æ£€ç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå›ç­”ç”¨æˆ·ã€‚

```python
from llama_index.core.memory import ChatMemoryBuffer

# 1. è®¾ç½®è®°å¿†ç¼“å†²åŒº (Token é™åˆ¶)
memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

# 2. åˆ›å»ºèŠå¤©å¼•æ“
chat_engine = vector_index.as_chat_engine(
    chat_mode="condense_plus_context",
    memory=memory,
    similarity_top_k=3,
    verbose=True  # å¼€å¯åå¯ä»¥çœ‹åˆ°"é‡å†™åçš„æŸ¥è¯¢"æ˜¯ä»€ä¹ˆ
)

# 3. å¤šè½®å¯¹è¯
response = chat_engine.chat("LlamaIndex æœ‰å“ªäº›ç´¢å¼•ç±»å‹ï¼Ÿ")
print(response)

# ä¸‹ä¸€ä¸ªé—®é¢˜éšå«äº†ä¸Šä¸‹æ–‡ ("å®ƒä»¬")
response = chat_engine.chat("å®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ")
# å†…éƒ¨ä¼šé‡å†™ä¸º: "LlamaIndex çš„ä¸åŒç´¢å¼•ç±»å‹ä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
print(response)
```

---

## ç¬¬5ç« ï¼šçŸ¥è¯†å›¾è°± RAG (GraphRAG)

å‘é‡æ£€ç´¢éš¾ä»¥å¤„ç†"é•¿ç¨‹å…³ç³»"æˆ–"å…¨å±€ç†è§£"ã€‚çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰é€šè¿‡æ˜¾å¼çš„å®ä½“å…³ç³»å»ºæ¨¡ï¼Œå¼¥è¡¥äº†è¿™ä¸€çŸ­æ¿ã€‚

### 5.1 PropertyGraphIndex æ„å»º

LlamaIndex v0.10 æ¨å‡ºçš„ `PropertyGraphIndex` æ˜¯ç›®å‰æœ€æ˜“ç”¨çš„ GraphRAG å®ç°ï¼Œæ”¯æŒ"å›¾+å‘é‡"çš„åŒé‡æ£€ç´¢ã€‚

```python
from llama_index.core import PropertyGraphIndex
from llama_index.core.indices.property_graph import (
    ImplicitPathExtractor,
    SimpleLLMPathExtractor
)

# 1. å®šä¹‰æå–å™¨ (å¦‚ä½•ä»æ–‡æœ¬å˜å›¾)
# è‡ªåŠ¨æå–å®ä½“å’Œå…³ç³»ï¼Œå¦‚ (Steve Jobs)-[FOUNDED]->(Apple)
kg_extractor = SimpleLLMPathExtractor(
    llm=Settings.llm,
    max_paths_per_chunk=10,
    num_workers=4
)

# 2. æ„å»ºå›¾ç´¢å¼• (åŒæ—¶ç”Ÿæˆ Embedding)
graph_index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=[kg_extractor],
    embed_kg_nodes=True,  # å…³é”®ï¼šå¼€å¯å‘é‡åŒ–ï¼Œæ”¯æŒæ··åˆæ£€ç´¢
    show_progress=True
)

# 3. æ··åˆæ£€ç´¢æŸ¥è¯¢ (Graph + Vector)
# æ—¢èƒ½åŒ¹é…å…³é”®è¯å®ä½“ï¼Œé€šè¿‡å›¾æ¸¸èµ°æ‰¾åˆ°å…³ç³»ï¼Œä¹Ÿèƒ½é€šè¿‡å‘é‡åŒ¹é…è¯­ä¹‰
query_engine = graph_index.as_query_engine(
    include_text=True,
    similarity_top_k=5
)

response = query_engine.query("Steve Jobs å’Œ Pixar æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ")
```

---

## ç¬¬6ç« ï¼šAgent ä¸ RAG çš„ç»ˆæèåˆ

RAG ä¸åº”åªæ˜¯ä¸€ä¸ªè¢«åŠ¨çš„æŸ¥è¯¢æ¥å£ï¼Œå®ƒåº”è¯¥æˆä¸º **Agent** æ‰‹ä¸­çš„ä¸€æŠŠåˆ©å™¨ï¼ˆToolï¼‰ã€‚

### 6.1 RAG as a Tool

é€šè¿‡ `FunctionCallingAgent`ï¼Œæˆ‘ä»¬è®© LLM è‡ªä¸»å†³å®šä½•æ—¶æŸ¥æ–‡æ¡£ã€æŸ¥å“ªä»½æ–‡æ¡£ï¼Œç”šè‡³è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚

```python
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner

# 1. å°† QueryEngine åŒ…è£…ä¸º Tool
rag_tool = QueryEngineTool(
    query_engine=vector_index.as_query_engine(),
    metadata=ToolMetadata(
        name="company_wiki",
        description="ç”¨äºæŸ¥è¯¢å…¬å¸å†…éƒ¨è§„ç« åˆ¶åº¦ã€æŠ¥é”€æµç¨‹ã€å‡æœŸæ”¿ç­–ç­‰ã€‚"
    )
)

# 2. åˆ›å»º Agent
agent_worker = FunctionCallingAgentWorker.from_tools(
    tools=[rag_tool],
    llm=Settings.llm,
    verbose=True
)
agent = AgentRunner(agent_worker)

# 3. å¤æ‚ä»»åŠ¡ (ReAct æ¨¡å¼)
# ç”¨æˆ·é—®ï¼š"æˆ‘ä¸‹å‘¨æƒ³è¯·å¹´å‡ï¼Œæµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿå¦‚æœæˆ‘è¿˜æœ‰5å¤©å¹´å‡ï¼Œå¤Ÿå—ï¼Ÿ"
# Agent ä¼šï¼š
# 1. è°ƒç”¨ company_wiki æŸ¥"è¯·å‡æµç¨‹"
# 2. è¿™é‡Œæ¼”ç¤ºçš„æ˜¯å•æ­¥ï¼Œå¦‚æœæ¥äº†æ•°æ®åº“å·¥å…·ï¼Œå®ƒè¿˜èƒ½å»æŸ¥"å‰©ä½™å¹´å‡"
# 3. ç»¼åˆå›ç­”
response = agent.chat("è¯·å‡æµç¨‹æ€ä¹ˆèµ°ï¼Ÿ")
print(response)
```

---

## ç¬¬7ç« ï¼šæ€»ç»“ä¸å±•æœ›

### 7.1 LlamaIndex æ ¸å¿ƒèƒ½åŠ›å¤§å›¾

| æ¨¡å— | æ ¸å¿ƒç»„ä»¶ | è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ |
| :--- | :--- | :--- |
| **Indexing** | VectorStoreIndex, SummaryIndex, PropertyGraphIndex | å¦‚ä½•é«˜æ•ˆã€ç»“æ„åŒ–åœ°å­˜å‚¨æ•°æ®ï¼Ÿ |
| **Retrieval** | Hybrid Retrieval, RouterRetriever | å¦‚ä½•ä»æµ·é‡æ•°æ®ä¸­ç²¾å‡†æå‡ºç›¸å…³ç‰‡æ®µï¼Ÿ |
| **Reranking** | CohereRerank | å¦‚ä½•åœ¨å¬å›ç»“æœä¸­å»ç²—å–ç²¾ï¼Œæå‡ Top-1 å‡†ç¡®ç‡ï¼Ÿ |
| **Querying** | SubQuestion, MultiStep, HyDE | å¦‚ä½•å¤„ç†å¤æ‚ã€æ¨¡ç³Šã€å¤šè·³çš„ç”¨æˆ·é—®é¢˜ï¼Ÿ |
| **Integration** | ChatEngine, AgentRunner | å¦‚ä½•å°†é™æ€æŸ¥è¯¢å˜ä¸ºåŠ¨æ€äº¤äº’ä¸æ™ºèƒ½ä½“ï¼Ÿ |

### 7.2 è¿›é˜¶å­¦ä¹ è·¯çº¿

1.  **Fine-tuning Embeddings**: å¦‚æœä½ çš„é¢†åŸŸéå¸¸å‚ç›´ï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ï¼‰ï¼Œé€šç”¨ Embedding æ¨¡å‹æ•ˆæœä¸ä½³ï¼Œè¯•ç€ä½¿ç”¨ LlamaIndex çš„ `SentenceTransformerFineTuning` æ¨¡å—å¾®è°ƒè‡ªå·±çš„ Embeddingã€‚
2.  **RAG Evaluation**: ä¸è¦å‡­æ„Ÿè§‰ä¼˜åŒ–ã€‚å¼•å…¥ `Ragas` æˆ– `DeepEval`ï¼Œå»ºç«‹ç”± `Faithfulness`ï¼ˆå¿ å®åº¦ï¼‰å’Œ `Answer Relevance`ï¼ˆç›¸å…³åº¦ï¼‰æ„æˆçš„è‡ªåŠ¨åŒ–æµ‹è¯•é›†ã€‚
3.  **Local RAG**: å°è¯•ä½¿ç”¨ `Ollama` + `LlamaIndex`ï¼Œæ„å»ºå®Œå…¨è¿è¡Œåœ¨æœ¬åœ°éšç§ç¯å¢ƒä¸‹çš„ RAG ç³»ç»Ÿã€‚

LlamaIndex çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå…¶**æé«˜çš„æ¨¡å—åŒ–**å’Œ**æ•°æ®ä¼˜å…ˆ**çš„è®¾è®¡å“²å­¦ã€‚æŒæ¡äº†æœ¬ç¯‡çš„é«˜çº§æŠ€å·§ï¼Œä½ å·²ç»å…·å¤‡äº†æ„å»ºä¼ä¸šçº§ RAG åº”ç”¨çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

---
