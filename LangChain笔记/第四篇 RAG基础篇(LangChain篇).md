# ç¬¬å››ç¯‡ï¼šRAGåŸºç¡€ç¯‡ï¼ˆLangChainç”Ÿäº§å®æˆ˜ï¼‰

## ğŸ“‹ å‰ç½®å‡†å¤‡

### ç¯å¢ƒé…ç½®

```bash
# æ ¸å¿ƒä¾èµ–ï¼ˆLangChain 1.0+ï¼‰
pip install langchain>=1.0.7
pip install langchain-openai>=1.0.3
pip install langchain-core>=1.0.0
pip install langchain-community>=0.4.1
pip install langchain-text-splitters>=0.4.0

# å‘é‡æ•°æ®åº“
pip install langchain-chroma>=0.2.0
pip install chromadb>=0.5.0

# å¯é€‰ä¾èµ–
pip install pypdf              # PDFæ–‡æ¡£æ”¯æŒ
pip install python-dotenv      # ç¯å¢ƒå˜é‡ç®¡ç†
```

### ç¯å¢ƒå˜é‡è®¾ç½®

```python
# .env
OPENAI_API_KEY=sk-your-api-key

# å¯é€‰ï¼šå¯ç”¨LangSmithè¿½è¸ª
LANGSMITH_API_KEY=your-langsmith-key
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=rag-tutorial
```

---

## ç¬¬ 1 ç« ï¼šRAGæ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ

#### 1.1 ä»€ä¹ˆæ˜¯RAGï¼Ÿ

**RAGï¼ˆRetrieval-Augmented Generationï¼‰**æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºLLMçš„å›ç­”èƒ½åŠ›ã€‚

##### 1.1.1 ä¸ºä»€ä¹ˆéœ€è¦RAGï¼Ÿ

**LLMçš„ä¸¤å¤§é™åˆ¶**ï¼š
1. **æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£** - æ— æ³•ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªæ–‡æ¡£åº“
2. **é™æ€çŸ¥è¯†** - è®­ç»ƒæ•°æ®å›ºåŒ–åœ¨æŸä¸ªæ—¶é—´ç‚¹

**RAGçš„è§£å†³æ–¹æ¡ˆ**ï¼š
- åœ¨æŸ¥è¯¢æ—¶åŠ¨æ€æ£€ç´¢ç›¸å…³å¤–éƒ¨çŸ¥è¯†
- å°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°LLMæç¤ºä¸­
- ç”ŸæˆåŸºäºå®æ—¶æ•°æ®çš„å‡†ç¡®å›ç­”

##### 1.1.2 RAGå®Œæ•´æ¶æ„

```mermaid
graph TB
    subgraph "ç¦»çº¿ç´¢å¼•é˜¶æ®µ Indexing"
        A[ğŸ“„ åŸå§‹æ–‡æ¡£<br/>Documents] --> B[ğŸ“¥ æ–‡æ¡£åŠ è½½<br/>Document Loaders]
        B --> C[âœ‚ï¸ æ–‡æœ¬åˆ†å‰²<br/>Text Splitters]
        C --> D[ğŸ”¢ å‘é‡åŒ–<br/>Embeddings]
        D --> E[(ğŸ—„ï¸ å‘é‡å­˜å‚¨<br/>Vector Store)]
    end

    subgraph "åœ¨çº¿æ£€ç´¢é˜¶æ®µ Retrieval"
        F[â“ ç”¨æˆ·æŸ¥è¯¢<br/>User Query] --> G[ğŸ”¢ æŸ¥è¯¢å‘é‡åŒ–<br/>Query Embedding]
        G --> H[ğŸ” ç›¸ä¼¼åº¦æ£€ç´¢<br/>Similarity Search]
        H --> E
        E --> I[ğŸ“‘ Top-Kæ–‡æ¡£<br/>Retrieved Docs]
    end

    subgraph "ç”Ÿæˆé˜¶æ®µ Generation"
        F --> J[ğŸ’¬ æç¤ºæ¨¡æ¿<br/>Prompt Template]
        I --> J
        J --> K[ğŸ¤– LLMç”Ÿæˆ<br/>Chat Model]
        K --> L[âœ… æœ€ç»ˆç­”æ¡ˆ<br/>Response]
    end

    style A fill:#FFE4E1
    style E fill:#E3F2FD
    style L fill:#C8E6C9
```

##### 1.1.3 RAGå·¥ä½œæµç¨‹

**é˜¶æ®µä¸€ï¼šç¦»çº¿ç´¢å¼•ï¼ˆIndexingï¼‰**
```python
æ–‡æ¡£ â†’ åŠ è½½ â†’ åˆ†å‰² â†’ å‘é‡åŒ– â†’ å­˜å‚¨
```

**é˜¶æ®µäºŒï¼šåœ¨çº¿æ£€ç´¢ä¸ç”Ÿæˆï¼ˆRetrieval & Generationï¼‰**
```python
ç”¨æˆ·æŸ¥è¯¢ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ æ„å»ºæç¤º â†’ LLMç”Ÿæˆç­”æ¡ˆ
```

---

#### 1.2 LangChain RAGçš„ä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **LCELç»„åˆ** | ä½¿ç”¨ç®¡é“è¯­æ³•ï¼ˆ`\|`ï¼‰ä¸²è”ç»„ä»¶ | `retriever \| prompt \| llm` |
| **æ¨¡å—åŒ–** | æ¯ä¸ªç»„ä»¶å¯ç‹¬ç«‹æ›¿æ¢ | è½»æ¾åˆ‡æ¢å‘é‡æ•°æ®åº“æˆ–LLM |
| **ç”Ÿäº§çº§** | å†…ç½®è¿½è¸ªã€ç›‘æ§ã€è¯„ä¼° | LangSmithé›†æˆ |
| **çµæ´»æ€§** | æ”¯æŒå¤šç§RAGæ¨¡å¼ | Agent RAGã€2-Step RAG |
| **ä¸°å¯Œé›†æˆ** | 100+ å‘é‡åº“ã€LLMé›†æˆ | Chromaã€FAISSã€Pineconeç­‰ |

---

#### 1.3 ä¸¤ç§RAGå®ç°æ¨¡å¼

##### 1.3.1 RAG Agentï¼ˆæ™ºèƒ½çµæ´»ï¼‰

**ç‰¹ç‚¹**ï¼šLLMè‡ªä¸»å†³å®šä½•æ—¶æ£€ç´¢

```python
from langchain_core.tools import tool
from langchain.agents import create_agent

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£ä»¥å¸®åŠ©å›ç­”é—®é¢˜"""
    docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        f"æ¥æº: {doc.metadata}\nå†…å®¹: {doc.page_content}"
        for doc in docs
    )
    return serialized, docs

# åˆ›å»ºAgentï¼ˆLLMå†³å®šæ˜¯å¦è°ƒç”¨æ£€ç´¢å·¥å…·ï¼‰
agent = create_agent(model, tools=[retrieve_context])
```

**ä¼˜åŠ¿ä¸åŠ£åŠ¿**ï¼š
- âœ… **æŒ‰éœ€æ£€ç´¢** - LLMå¯å¤„ç†é—²èŠã€è¿½é—®ï¼Œæ— éœ€æ¯æ¬¡éƒ½æ£€ç´¢
- âœ… **ä¸Šä¸‹æ–‡æŸ¥è¯¢** - LLMå¯æ ¹æ®å¯¹è¯å†å²æ„å»ºæ›´å¥½çš„æ£€ç´¢æŸ¥è¯¢
- âœ… **å¤šæ¬¡æ£€ç´¢** - å¯æ‰§è¡Œå¤šè½®æ£€ç´¢ä»¥è·å¾—æ›´å…¨é¢çš„ä¿¡æ¯
- âš ï¸ **ä¸¤æ¬¡æ¨ç†** - éœ€è¦ä¸€æ¬¡ç”ŸæˆæŸ¥è¯¢ï¼Œä¸€æ¬¡ç”Ÿæˆç­”æ¡ˆï¼ˆå»¶è¿Ÿæ›´é«˜ï¼‰
- âš ï¸ **æ§åˆ¶åŠ›å¼±** - LLMå¯èƒ½è·³è¿‡å¿…è¦çš„æ£€ç´¢æˆ–æ‰§è¡Œä¸å¿…è¦çš„æ£€ç´¢

##### 1.3.2 2-Step RAG Chainï¼ˆå¿«é€Ÿç®€æ´ï¼‰

**ç‰¹ç‚¹**ï¼šæ¯æ¬¡æŸ¥è¯¢éƒ½æ‰§è¡Œæ£€ç´¢ï¼Œå•æ¬¡LLMè°ƒç”¨

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# æ ¼å¼åŒ–æ£€ç´¢æ–‡æ¡£
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# æ„å»º2-Step Chainï¼ˆLCELè¯­æ³•ï¼‰
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

**ä¼˜åŠ¿ä¸åŠ£åŠ¿**ï¼š
- âœ… **ä½å»¶è¿Ÿ** - å•æ¬¡LLMè°ƒç”¨ï¼Œå“åº”æ›´å¿«
- âœ… **å¯é¢„æµ‹** - æ¯æ¬¡éƒ½æ‰§è¡Œæ£€ç´¢ï¼Œè¡Œä¸ºä¸€è‡´
- âœ… **æ˜“è°ƒè¯•** - æµç¨‹å›ºå®šï¼Œå®¹æ˜“è¿½è¸ªå’Œä¼˜åŒ–
- âš ï¸ **çµæ´»æ€§ä½** - æ— æ³•å¤„ç†ä¸éœ€è¦æ£€ç´¢çš„ç®€å•æŸ¥è¯¢
- âš ï¸ **å›ºå®šæ¨¡å¼** - æ€»æ˜¯æ£€ç´¢å›ºå®šæ•°é‡çš„æ–‡æ¡£

> **é€‰æ‹©å»ºè®®**ï¼šå¯¹äºå¤§å¤šæ•°åº”ç”¨ï¼Œæ¨èä»**2-Step RAG Chain**å¼€å§‹ï¼Œå› ä¸ºå®ƒç®€å•ã€å¿«é€Ÿã€æ˜“äºè°ƒè¯•ã€‚åªæœ‰åœ¨éœ€è¦åŠ¨æ€å†³ç­–æ—¶æ‰ä½¿ç”¨RAG Agentã€‚

---

## ç¬¬ 2 ç« ï¼šç´¢å¼•æµç¨‹ - ä»æ–‡æ¡£åˆ°å‘é‡åº“

### 2.1 æ–‡æ¡£åŠ è½½ï¼ˆDocument Loadersï¼‰

##### 2.1.1 åŸºç¡€åŠ è½½å™¨

```python
from langchain_community.document_loaders import TextLoader, DirectoryLoader

# åŠ è½½å•ä¸ªæ–‡æœ¬æ–‡ä»¶
loader = TextLoader("document.txt")
docs = loader.load()

# æ‰¹é‡åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
loader = DirectoryLoader(
    "./data",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()

print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
for doc in documents[:2]:
    print(f"å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
```

##### 2.1.2 å¸¸ç”¨åŠ è½½å™¨

```python
# PDFåŠ è½½å™¨
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("report.pdf")
pages = loader.load()
print(f"PDFå…± {len(pages)} é¡µ")

# ç½‘é¡µåŠ è½½å™¨
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader(web_paths=("https://example.com",))
web_docs = loader.load()

# CSVåŠ è½½å™¨
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader(file_path="data.csv")
csv_docs = loader.load()

# MarkdownåŠ è½½å™¨
from langchain_community.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader("README.md")
md_docs = loader.load()
```

> **æç¤º**ï¼šæ‰€æœ‰åŠ è½½å™¨è¿”å›çš„æ–‡æ¡£éƒ½åŒ…å« `page_content`ï¼ˆæ–‡æœ¬å†…å®¹ï¼‰å’Œ `metadata`ï¼ˆå…ƒæ•°æ®ï¼Œå¦‚æ–‡ä»¶åã€é¡µç ç­‰ï¼‰ã€‚

---

### 2.2 æ–‡æœ¬åˆ†å‰²ï¼ˆText Splittersï¼‰

##### 2.2.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²ï¼Ÿ

**æŒ‘æˆ˜**ï¼š
- LLMæœ‰ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
- å‘é‡æ£€ç´¢éœ€è¦è¯­ä¹‰ç‹¬ç«‹çš„æ–‡æœ¬å—
- å—å¤ªå¤§ä¼šé™ä½æ£€ç´¢ç²¾åº¦ï¼Œå—å¤ªå°ä¼šä¸¢å¤±ä¸Šä¸‹æ–‡

**è§£å†³æ–¹æ¡ˆ**ï¼šå°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚å½“å¤§å°çš„å—ï¼ˆchunksï¼‰ï¼Œå¹¶ä¿ç•™é‡å ï¼ˆoverlapï¼‰ä»¥ç»´æŒä¸Šä¸‹æ–‡ã€‚

##### 2.2.2 RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºæ™ºèƒ½åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,      # å—é—´é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
    length_function=len,    # è®¡ç®—é•¿åº¦çš„å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # ä¼˜å…ˆåœ¨æ®µè½/å¥å­è¾¹ç•Œåˆ†å‰²
)

# åˆ†å‰²æ–‡æ¡£
chunks = text_splitter.split_documents(documents)

print(f"âœ… åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
for i, chunk in enumerate(chunks[:3]):
    print(f"\nå— {i+1} (é•¿åº¦: {len(chunk.page_content)}):")
    print(chunk.page_content[:150])
```

**å·¥ä½œåŸç†**ï¼š
1. å°è¯•ç”¨ `\n\n`ï¼ˆæ®µè½ï¼‰åˆ†å‰²
2. å¦‚æœå—ä»å¤ªå¤§ï¼Œç”¨ `\n`ï¼ˆæ¢è¡Œï¼‰åˆ†å‰²
3. ç»§ç»­ç”¨ `ã€‚`ã€`.`ï¼ˆå¥å­ï¼‰åˆ†å‰²
4. æœ€åç”¨ç©ºæ ¼å’Œå­—ç¬¦åˆ†å‰²

##### 2.2.3 ä¸åŒåœºæ™¯çš„åˆ†å—ç­–ç•¥

```python
# åœºæ™¯1ï¼šçŸ­æ–‡æœ¬FAQï¼ˆå°å—ï¼‰
faq_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# åœºæ™¯2ï¼šé•¿æ–‡æ¡£ï¼ˆå¤§å—ï¼Œæ›´å¤šä¸Šä¸‹æ–‡ï¼‰
long_doc_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=400
)

# åœºæ™¯3ï¼šä»£ç æ–‡æ¡£ï¼ˆä¿ç•™ä»£ç ç»“æ„ï¼‰
from langchain_text_splitters import Language

code_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=100
)

# åœºæ™¯4ï¼šä¸­æ–‡æ–‡æ¡£ï¼ˆä¼˜åŒ–åˆ†éš”ç¬¦ï¼‰
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""]
)
```

> **æœ€ä½³å®è·µ**ï¼š
> - `chunk_overlap` é€šå¸¸è®¾ç½®ä¸º `chunk_size` çš„ 10-20%
> - æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼Œé€šè¿‡è¯„ä¼°æ‰¾åˆ°æœ€ä½³å€¼
> - ä¸­æ–‡å»ºè®®å—å¤§å° 500-1000 å­—ç¬¦

---

### 2.3 å‘é‡åŒ–ï¼ˆEmbeddingsï¼‰

##### 2.3.1 OpenAI Embeddingsï¼ˆæ¨èï¼‰

```python
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºEmbeddingsæ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",  # 3072ç»´ï¼Œæ•ˆæœæœ€å¥½
    # model="text-embedding-3-small",  # 1536ç»´ï¼Œæ€§ä»·æ¯”é«˜
)

# å‘é‡åŒ–å•ä¸ªæŸ¥è¯¢
query_vector = embeddings.embed_query("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
print(f"æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")

# æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£ï¼ˆæ›´é«˜æ•ˆï¼‰
doc_texts = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
doc_vectors = embeddings.embed_documents(doc_texts)
print(f"æ‰¹é‡å‘é‡åŒ–äº† {len(doc_vectors)} ä¸ªæ–‡æ¡£")
```

##### 2.3.2 æ¨¡å‹é€‰æ‹©æŒ‡å—

| æ¨¡å‹ | ç»´åº¦ | æ€§èƒ½ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|---------|
| `text-embedding-3-large` | 3072 | æœ€ä½³ | é«˜ | ç”Ÿäº§ç¯å¢ƒã€é«˜ç²¾åº¦éœ€æ±‚ |
| `text-embedding-3-small` | 1536 | è‰¯å¥½ | ä½ | å¼€å‘æµ‹è¯•ã€æ€§ä»·æ¯”ä¼˜å…ˆ |
| `text-embedding-ada-002` | 1536 | è‰¯å¥½ | ä¸­ | æ—§ç‰ˆæœ¬ï¼Œä¸æ¨èæ–°é¡¹ç›®ä½¿ç”¨ |

> **æç¤º**ï¼š`text-embedding-3-*` ç³»åˆ—æ€§èƒ½æ›´ä¼˜ï¼Œä»·æ ¼æ›´ä½ï¼Œæ˜¯æ¨èé€‰æ‹©ã€‚

---

### 2.4 å‘é‡å­˜å‚¨ï¼ˆVector Storesï¼‰

##### 2.4.1 Chromaï¼ˆæœ¬åœ°å¼€å‘æ¨èï¼‰

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# æ–¹å¼1: ä»æ–‡æ¡£ç›´æ¥åˆ›å»ºå‘é‡åº“
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–å­˜å‚¨
)

# æ–¹å¼2: åŠ è½½å·²æœ‰å‘é‡åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# æ·»åŠ æ–‡æ¡£
vectorstore.add_documents(documents=new_chunks)

# ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search(
    query="ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    k=3  # è¿”å›Top-3
)

for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.page_content[:100]}...")
```

##### 2.4.2 InMemoryVectorStoreï¼ˆå¿«é€ŸåŸå‹ï¼‰

```python
from langchain_core.vectorstores import InMemoryVectorStore

# è½»é‡çº§å†…å­˜å‘é‡åº“ï¼ˆæ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰
vector_store = InMemoryVectorStore(embeddings)
ids = vector_store.add_documents(documents=chunks)

# ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
results = vector_store.similarity_search_with_score(
    query="ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
    k=3
)

for doc, score in results:
    print(f"ç›¸ä¼¼åº¦: {score:.4f}")
    print(f"å†…å®¹: {doc.page_content[:100]}...\n")
```

##### 2.4.3 å‘é‡æ•°æ®åº“é€‰æ‹©æŒ‡å—

| æ•°æ®åº“ | ç±»å‹ | æ€§èƒ½ | éƒ¨ç½²éš¾åº¦ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|---------|---------|
| **InMemoryVectorStore** | å†…å­˜ | å¿« | â­ | å¿«é€ŸåŸå‹ã€æµ‹è¯• |
| **Chroma** | åµŒå…¥å¼ | ä¸­ | â­ | æœ¬åœ°å¼€å‘ã€ä¸­å°å‹åº”ç”¨ |
| **FAISS** | åº“ | é«˜ | â­â­ | å•æœºé«˜æ€§èƒ½ã€å¤§è§„æ¨¡æ£€ç´¢ |
| **Qdrant** | æœåŠ¡ | é«˜ | â­â­â­ | ç”Ÿäº§ç¯å¢ƒã€åˆ†å¸ƒå¼ |
| **Pinecone** | äº‘æœåŠ¡ | é«˜ | â­ | äº‘åŸç”Ÿã€æ— éœ€è¿ç»´ |
| **Weaviate** | æœåŠ¡ | é«˜ | â­â­â­ | ä¼ä¸šçº§ã€GraphRAG |

---

### 2.5 æ£€ç´¢å™¨ï¼ˆRetrieversï¼‰

##### 2.5.1 åŸºç¡€æ£€ç´¢å™¨

```python
# æ–¹å¼1ï¼šç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Top-5
)

# æ–¹å¼2ï¼šMMRæ£€ç´¢ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,           # è¿”å›5ä¸ªç»“æœ
        "fetch_k": 20,    # ä»20ä¸ªå€™é€‰ä¸­é€‰æ‹©
        "lambda_mult": 0.5  # 0=å¤šæ ·æ€§, 1=ç›¸å…³æ€§
    }
)

# æ–¹å¼3ï¼šç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # åªè¿”å›ç›¸ä¼¼åº¦>0.7çš„æ–‡æ¡£
        "k": 5
    }
)

# ä½¿ç”¨æ£€ç´¢å™¨
docs = retriever.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
for doc in docs:
    print(doc.page_content[:100])
```

> **æç¤º**ï¼šæ£€ç´¢å™¨å®ç°äº† `Runnable` æ¥å£ï¼Œå¯ç›´æ¥ç”¨äºLCELé“¾ä¸­ã€‚

---

## ç¬¬ 3 ç« ï¼šæ£€ç´¢ä¸ç”Ÿæˆ - æ„å»ºRAGé“¾

### 3.1 æ ‡å‡†RAG Chainï¼ˆLCELï¼‰

##### 3.1.1 å®Œæ•´å®ç°

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. åˆ›å»ºæ£€ç´¢å™¨
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 2. åˆ›å»ºPromptæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œåªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å†…å®¹

å›ç­”ï¼š
""")

# 3. åˆ›å»ºLLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 4. æ ¼å¼åŒ–å‡½æ•°
def format_docs(docs: list) -> str:
    """å°†æ–‡æ¡£åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
    return "\n\n".join(doc.page_content for doc in docs)

# 5. æ„å»ºRAGé“¾ï¼ˆLCELè¯­æ³•ï¼‰
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 6. æ‰§è¡ŒæŸ¥è¯¢
response = rag_chain.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
print(response)
```

##### 3.1.2 LCELè¯­æ³•è¯¦è§£

```python
# LCELä½¿ç”¨ç®¡é“ï¼ˆ|ï¼‰æ“ä½œç¬¦ä¸²è”ç»„ä»¶

# æ­¥éª¤1: å¹¶è¡Œæ‰§è¡Œæ£€ç´¢å’Œé—®é¢˜ä¼ é€’
{"context": retriever | format_docs, "question": RunnablePassthrough()}
# è¾“å‡º: {"context": "æ£€ç´¢åˆ°çš„æ–‡æ¡£", "question": "ç”¨æˆ·é—®é¢˜"}

# æ­¥éª¤2: å°†å­—å…¸ä¼ é€’ç»™Promptæ¨¡æ¿
| prompt
# è¾“å‡º: ChatPromptValueï¼ˆæ ¼å¼åŒ–åçš„æç¤ºï¼‰

# æ­¥éª¤3: LLMç”Ÿæˆ
| llm
# è¾“å‡º: AIMessageï¼ˆåŒ…å«answerå’Œmetadataï¼‰

# æ­¥éª¤4: æå–æ–‡æœ¬å†…å®¹
| StrOutputParser()
# è¾“å‡º: strï¼ˆçº¯æ–‡æœ¬ç­”æ¡ˆï¼‰
```

---

### 3.2 å¸¦æ¥æºçš„RAG Chain

##### 3.2.1 è¿”å›æ£€ç´¢æ–‡æ¡£

```python
from langchain_core.runnables import RunnableParallel

# æ„å»ºè¿”å›æ¥æºçš„é“¾
rag_chain_with_source = RunnableParallel(
    {
        "context": retriever | format_docs,
        "source_documents": retriever,  # ä¿ç•™åŸå§‹æ–‡æ¡£
        "question": RunnablePassthrough()
    }
).assign(
    answer=lambda x: (
        prompt
        | llm
        | StrOutputParser()
    ).invoke({"context": x["context"], "question": x["question"]})
)

# æ‰§è¡ŒæŸ¥è¯¢
result = rag_chain_with_source.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")

print(f"å›ç­”: {result['answer']}\n")
print("ğŸ“š æ¥æºæ–‡æ¡£:")
for i, doc in enumerate(result['source_documents'], 1):
    print(f"{i}. {doc.page_content[:100]}...")
    print(f"   å…ƒæ•°æ®: {doc.metadata}\n")
```

##### 3.2.2 å¢å¼ºæ ¼å¼åŒ–å‡½æ•°

```python
def format_docs_with_metadata(docs: list) -> str:
    """æ ¼å¼åŒ–æ–‡æ¡£ï¼ŒåŒ…å«æ¥æºä¿¡æ¯"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'æœªçŸ¥')
        page = doc.metadata.get('page', 'N/A')
        formatted.append(
            f"[æ–‡æ¡£{i}] (æ¥æº: {source}, é¡µç : {page})\n{doc.page_content}"
        )
    return "\n\n".join(formatted)

# ä½¿ç”¨å¢å¼ºæ ¼å¼åŒ–
rag_chain = (
    {"context": retriever | format_docs_with_metadata, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

---

### 3.3 æµå¼RAG Chain

##### 3.3.1 å®ç°æµå¼è¾“å‡º

```python
# åˆ›å»ºæ”¯æŒæµå¼çš„LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    streaming=True  # å¯ç”¨æµå¼
)

# æ„å»ºæµå¼RAGé“¾
rag_chain_stream = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# æµå¼è¾“å‡ºï¼ˆå®æ—¶æ‰“å°ï¼‰
print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
for chunk in rag_chain_stream.stream("è¯¦ç»†è§£é‡ŠRAGçš„å·¥ä½œåŸç†"):
    print(chunk, end="", flush=True)
print("\n")
```

##### 3.3.2 æµå¼è¾“å‡ºçš„ä¼˜åŠ¿

```python
import time

# å¯¹æ¯”ï¼šéæµå¼ vs æµå¼

# éæµå¼ï¼ˆç”¨æˆ·ç­‰å¾…å®Œæ•´å“åº”ï¼‰
start = time.time()
response = rag_chain.invoke("è§£é‡Šä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿ")
end = time.time()
print(f"éæµå¼è€—æ—¶: {end-start:.2f}ç§’")
print(response)

# æµå¼ï¼ˆç”¨æˆ·ç«‹å³çœ‹åˆ°è¾“å‡ºï¼‰
start = time.time()
for chunk in rag_chain_stream.stream("è§£é‡Šä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿ"):
    print(chunk, end="", flush=True)
end = time.time()
print(f"\næµå¼è€—æ—¶: {end-start:.2f}ç§’")
```

> **ç”¨æˆ·ä½“éªŒæå‡**ï¼šæµå¼è¾“å‡ºå¯æ˜¾è‘—æ”¹å–„é•¿å›ç­”çš„ç”¨æˆ·ä½“éªŒï¼Œç”¨æˆ·æ— éœ€ç­‰å¾…å®Œæ•´ç”Ÿæˆå³å¯å¼€å§‹é˜…è¯»ã€‚

---

## ç¬¬ 4 ç« ï¼šç”Ÿäº§çº§RAGç³»ç»Ÿ

### 4.1 å®Œæ•´ç”Ÿäº§çº§å®ç°

```python
"""
ç”Ÿäº§çº§RAGç³»ç»Ÿ - å®Œæ•´å®ç°
åŒ…å«ï¼šé”™è¯¯å¤„ç†ã€æ—¥å¿—ã€ç›‘æ§ã€é…ç½®ç®¡ç†
"""
from typing import List, Dict, Any
from pathlib import Path
import logging

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""

    def __init__(
        self,
        data_dir: str = "./data",
        persist_dir: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-large",
        llm_model: str = "gpt-4o-mini",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        top_k: int = 3
    ):
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k

        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.llm = ChatOpenAI(model=llm_model, temperature=0)
        self.vectorstore = None
        self.retriever = None
        self.rag_chain = None

        logger.info(f"âœ… ProductionRAGåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"   - å‘é‡åº“: {self.persist_dir}")
        logger.info(f"   - Embeddingæ¨¡å‹: {embedding_model}")
        logger.info(f"   - LLMæ¨¡å‹: {llm_model}")

    def build_vectorstore(self, force_rebuild: bool = False) -> None:
        """æ„å»ºæˆ–åŠ è½½å‘é‡åº“"""
        if not force_rebuild and self.persist_dir.exists():
            logger.info("ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡åº“...")
            self.vectorstore = Chroma(
                persist_directory=str(self.persist_dir),
                embedding_function=self.embeddings
            )
            logger.info("âœ… å‘é‡åº“åŠ è½½æˆåŠŸ")
            return

        logger.info("ğŸ”¨ å¼€å§‹æ„å»ºæ–°å‘é‡åº“...")

        # 1. åŠ è½½æ–‡æ¡£
        logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")
        loader = DirectoryLoader(
            str(self.data_dir),
            glob="**/*.txt",
            loader_cls=TextLoader
        )
        documents = loader.load()
        logger.info(f"   âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        if not documents:
            raise ValueError(f"æœªåœ¨ {self.data_dir} ä¸­æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")

        # 2. åˆ†å‰²æ–‡æ¡£
        logger.info("âœ‚ï¸  æ­¥éª¤2: åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        logger.info(f"   âœ… åˆ›å»ºäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # 3. åˆ›å»ºå‘é‡åº“
        logger.info("ğŸ—„ï¸  æ­¥éª¤3: åˆ›å»ºå‘é‡åº“...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=str(self.persist_dir)
        )
        logger.info("   âœ… å‘é‡åº“åˆ›å»ºå®Œæˆ")

    def setup_rag_chain(self) -> None:
        """è®¾ç½®RAGé“¾"""
        if self.vectorstore is None:
            raise ValueError("å‘é‡åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ build_vectorstore()")

        # åˆ›å»ºæ£€ç´¢å™¨
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.top_k}
        )

        # åˆ›å»ºPromptæ¨¡æ¿
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œåªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å†…å®¹
4. å¦‚æœå¯ä»¥ï¼Œè¯·å¼•ç”¨å…·ä½“çš„æ¥æº

å›ç­”ï¼š
""")

        # æ ¼å¼åŒ–å‡½æ•°
        def format_docs(docs: List[Document]) -> str:
            return "\n\n".join(
                f"[æ–‡æ¡£{i+1}]\n{doc.page_content}"
                for i, doc in enumerate(docs)
            )

        # æ„å»ºRAGé“¾ï¼ˆå¸¦æ¥æºï¼‰
        self.rag_chain = RunnableParallel(
            {
                "context": self.retriever | format_docs,
                "source_documents": self.retriever,
                "question": RunnablePassthrough()
            }
        ).assign(
            answer=lambda x: (
                prompt
                | self.llm
                | StrOutputParser()
            ).invoke({"context": x["context"], "question": x["question"]})
        )

        logger.info("âœ… RAGé“¾è®¾ç½®å®Œæˆ")

    def query(
        self,
        question: str,
        show_sources: bool = True,
        stream: bool = False
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        if self.rag_chain is None:
            self.setup_rag_chain()

        logger.info(f"\nâ“ é—®é¢˜: {question}")

        if stream:
            # æµå¼è¾“å‡º
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            full_response = ""
            for chunk in self.rag_chain.stream(question):
                if isinstance(chunk, dict) and "answer" in chunk:
                    print(chunk["answer"], end="", flush=True)
                    full_response = chunk["answer"]
            print("\n")
            result = {"answer": full_response}
        else:
            # æ ‡å‡†è¾“å‡º
            result = self.rag_chain.invoke(question)
            print(f"\nğŸ’¡ å›ç­”:\n{result['answer']}\n")

        if show_sources and "source_documents" in result:
            print("ğŸ“š æ¥æºæ–‡æ¡£:")
            for i, doc in enumerate(result['source_documents'], 1):
                print(f"  {i}. {doc.page_content[:100]}...")
                print(f"     å…ƒæ•°æ®: {doc.metadata}")

        return result

    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŸ¥è¯¢"""
        if self.rag_chain is None:
            self.setup_rag_chain()

        logger.info(f"ğŸ”„ æ‰¹é‡æŸ¥è¯¢ {len(questions)} ä¸ªé—®é¢˜...")
        results = self.rag_chain.batch(questions)

        for i, (q, r) in enumerate(zip(questions, results), 1):
            print(f"\né—®é¢˜ {i}: {q}")
            print(f"å›ç­”: {r['answer'][:200]}...\n")

        return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–
    rag = ProductionRAG(
        data_dir="./data",
        persist_dir="./chroma_db",
        embedding_model="text-embedding-3-large",
        llm_model="gpt-4o-mini",
        chunk_size=1000,
        chunk_overlap=200,
        top_k=3
    )

    # æ„å»ºå‘é‡åº“ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–å¼ºåˆ¶é‡å»ºï¼‰
    rag.build_vectorstore(force_rebuild=False)

    # å•æ¬¡æŸ¥è¯¢
    result = rag.query(
        "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        show_sources=True
    )

    # æµå¼æŸ¥è¯¢
    rag.query(
        "è¯¦ç»†è§£é‡Šå…³é”®æŠ€æœ¯",
        show_sources=False,
        stream=True
    )

    # æ‰¹é‡æŸ¥è¯¢
    questions = [
        "æœ‰å“ªäº›ä¸»è¦ç‰¹ç‚¹ï¼Ÿ",
        "å¦‚ä½•å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ",
        "æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    rag.batch_query(questions)
```

---

### 4.2 æ€§èƒ½ä¼˜åŒ–æŒ‡å—

##### 4.2.1 åˆ†å—ä¼˜åŒ–

```python
# âŒ ä¸ä½³çš„åˆ†å—ç­–ç•¥
bad_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,    # å¤ªå°ï¼Œä¸¢å¤±ä¸Šä¸‹æ–‡
    chunk_overlap=0    # æ— é‡å ï¼Œå¯èƒ½åˆ‡æ–­è¯­ä¹‰
)

# âœ… ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥
good_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,   # é€‚ä¸­å¤§å°
    chunk_overlap=200, # 20%é‡å 
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # ä¼˜å…ˆåœ¨è‡ªç„¶è¾¹ç•Œåˆ†å‰²
)

# âœ… é’ˆå¯¹ä¸­æ–‡çš„ä¼˜åŒ–
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""]
)
```

##### 4.2.2 Embeddingä¼˜åŒ–

```python
# 1. æ‰¹é‡å¤„ç†ï¼ˆèŠ‚çœæ—¶é—´å’Œæˆæœ¬ï¼‰
batch_size = 100
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    vectorstore.add_documents(batch)

# 2. ä½¿ç”¨åˆé€‚çš„æ¨¡å‹
# å¼€å‘/æµ‹è¯•é˜¶æ®µ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")  # å¿«é€Ÿã€ä¾¿å®œ

# ç”Ÿäº§ç¯å¢ƒ
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")  # é«˜è´¨é‡
```

##### 4.2.3 æ£€ç´¢ä¼˜åŒ–

```python
# 1. è°ƒæ•´Top-Kï¼ˆé€šè¿‡è¯„ä¼°æ‰¾åˆ°æœ€ä½³å€¼ï¼‰
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5}  # å®éªŒ3-10ä¹‹é—´çš„å€¼
)

# 2. ä½¿ç”¨MMRå¢åŠ å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,      # ä»20ä¸ªå€™é€‰ä¸­é€‰æ‹©5ä¸ª
        "lambda_mult": 0.5  # å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
    }
)

# 3. è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆè¿‡æ»¤ä½è´¨é‡ç»“æœï¼‰
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # åªè¿”å›>0.7çš„ç»“æœ
        "k": 5
    }
)
```

##### 4.2.4 Promptä¼˜åŒ–

```python
# âœ… ä¼˜åŒ–çš„Promptæ¨¡æ¿
optimized_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”æŒ‡å—ï¼š
1. **æœ‰ç­”æ¡ˆæ—¶**ï¼šåŸºäºä¸Šä¸‹æ–‡ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”
2. **æ— ç­”æ¡ˆæ—¶**ï¼šæ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. **éƒ¨åˆ†ç­”æ¡ˆæ—¶**ï¼šè¯´æ˜å“ªäº›éƒ¨åˆ†æœ‰ä¾æ®ï¼Œå“ªäº›éƒ¨åˆ†ä¸ç¡®å®š
4. **å¼•ç”¨æ¥æº**ï¼šå¦‚æœå¯ä»¥ï¼Œæ ‡æ³¨ä¿¡æ¯æ¥è‡ªå“ªä¸ªæ–‡æ¡£ç‰‡æ®µ

æ³¨æ„äº‹é¡¹ï¼š
- ä¸è¦ç¼–é€ ä¿¡æ¯
- ä¸è¦ä½¿ç”¨ä¸Šä¸‹æ–‡ä¹‹å¤–çš„çŸ¥è¯†
- å¦‚æœ‰ä¸ç¡®å®šï¼Œæ˜ç¡®è¯´æ˜

å›ç­”ï¼š
""")
```

---

### 4.3 LangSmithç›‘æ§ä¸è¿½è¸ª

##### 4.3.1 å¯ç”¨LangSmith

```python
import os

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["LANGSMITH_API_KEY"] = "your-langsmith-key"
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "rag-production"

# æˆ–åœ¨ä»£ç ä¸­è®¾ç½®
import getpass

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("LangSmith API Key: ")
```

##### 4.3.2 ä½¿ç”¨LangSmith

```python
# æ­£å¸¸ä½¿ç”¨RAGï¼ˆè‡ªåŠ¨è¿½è¸ªï¼‰
result = rag_chain.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")

# LangSmith Dashboardä¼šè‡ªåŠ¨è®°å½•ï¼š
# - å®Œæ•´çš„è°ƒç”¨trace
# - æ¯ä¸ªç»„ä»¶çš„è¾“å…¥/è¾“å‡º
# - Tokenä½¿ç”¨é‡
# - å»¶è¿Ÿæ—¶é—´
# - æˆæœ¬ä¼°ç®—
# - é”™è¯¯æ—¥å¿—
```

#### 4.3.3 è‡ªå®šä¹‰è¿½è¸ª

```python
from langsmith import traceable

@traceable(
    run_type="chain",
    name="custom_rag_chain",
    tags=["production", "rag"]
)
def custom_rag(question: str) -> str:
    """è‡ªå®šä¹‰RAGå‡½æ•°ï¼ˆå¸¦è¿½è¸ªï¼‰"""
    return rag_chain.invoke(question)

# è°ƒç”¨ä¼šè‡ªåŠ¨è¿½è¸ª
response = custom_rag("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
```

---

## ç¬¬ 5 ç« ï¼šé«˜çº§RAGæŠ€æœ¯

### 5.1 å…ƒæ•°æ®è¿‡æ»¤

```python
from typing import Optional

# æ·»åŠ å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£
documents_with_metadata = [
    Document(
        page_content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
        metadata={"category": "ç¼–ç¨‹", "level": "å…¥é—¨", "language": "Python"}
    ),
    Document(
        page_content="æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        metadata={"category": "AI", "level": "é«˜çº§", "language": "é€šç”¨"}
    )
]

vectorstore.add_documents(documents_with_metadata)

# åˆ›å»ºå¸¦è¿‡æ»¤çš„æ£€ç´¢å™¨
def create_filtered_retriever(category: Optional[str] = None):
    """åˆ›å»ºå¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢å™¨"""
    if category:
        # åªæ£€ç´¢ç‰¹å®šç±»åˆ«çš„æ–‡æ¡£
        retriever = vectorstore.as_retriever(
            search_kwargs={
                "k": 3,
                "filter": {"category": category}  # å…ƒæ•°æ®è¿‡æ»¤
            }
        )
    else:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    return retriever

# ä½¿ç”¨
retriever_ai = create_filtered_retriever(category="AI")
docs = retriever_ai.invoke("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
```

---

### 5.2 å¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆMulti-Query Retrievalï¼‰

```python
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

# åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼ˆè‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢è§’åº¦ï¼‰
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# å•ä¸ªæŸ¥è¯¢ä¼šè¢«æ‰©å±•ä¸ºå¤šä¸ªæŸ¥è¯¢
# ä¾‹å¦‚ï¼š"ä»€ä¹ˆæ˜¯RAGï¼Ÿ" å¯èƒ½æ‰©å±•ä¸ºï¼š
# - "RAGçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
# - "æ£€ç´¢å¢å¼ºç”Ÿæˆå¦‚ä½•å·¥ä½œï¼Ÿ"
# - "RAGçš„åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ"

docs = multi_query_retriever.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
```

---

### 5.3 ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆContextual Compressionï¼‰

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

# åˆ›å»ºå‹ç¼©å™¨ï¼ˆæå–æœ€ç›¸å…³çš„ç‰‡æ®µï¼‰
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
compressor = LLMChainExtractor.from_llm(llm)

# åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)

# ä½¿ç”¨ï¼ˆä¼šå‹ç¼©æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œåªä¿ç•™ç›¸å…³ç‰‡æ®µï¼‰
compressed_docs = compression_retriever.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
```

---

## ç¬¬ 6 ç« ï¼šè¯„ä¼°ä¸ä¼˜åŒ–

### 6.1 è¯„ä¼°æŒ‡æ ‡

##### 6.1.1 æ£€ç´¢è´¨é‡è¯„ä¼°

```python
def evaluate_retrieval(retriever, test_cases: list) -> dict:
    """è¯„ä¼°æ£€ç´¢è´¨é‡

    test_cases: [
        {"query": "é—®é¢˜", "relevant_doc_ids": ["doc1", "doc2"]},
        ...
    ]
    """
    total_precision = 0
    total_recall = 0

    for case in test_cases:
        query = case["query"]
        relevant_ids = set(case["relevant_doc_ids"])

        # æ£€ç´¢
        retrieved_docs = retriever.invoke(query)
        retrieved_ids = set(doc.metadata.get("id") for doc in retrieved_docs)

        # è®¡ç®—æŒ‡æ ‡
        relevant_retrieved = retrieved_ids & relevant_ids

        precision = len(relevant_retrieved) / len(retrieved_ids) if retrieved_ids else 0
        recall = len(relevant_retrieved) / len(relevant_ids) if relevant_ids else 0

        total_precision += precision
        total_recall += recall

    avg_precision = total_precision / len(test_cases)
    avg_recall = total_recall / len(test_cases)

    return {
        "precision": avg_precision,
        "recall": avg_recall,
        "f1": 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)
    }

# ä½¿ç”¨
test_cases = [
    {"query": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ", "relevant_doc_ids": ["doc1", "doc2"]},
    {"query": "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢ï¼Ÿ", "relevant_doc_ids": ["doc3", "doc4"]}
]

metrics = evaluate_retrieval(retriever, test_cases)
print(f"Precision: {metrics['precision']:.2f}")
print(f"Recall: {metrics['recall']:.2f}")
print(f"F1: {metrics['f1']:.2f}")
```

##### 6.1.2 ç«¯åˆ°ç«¯RAGè¯„ä¼°ï¼ˆä½¿ç”¨LangSmithï¼‰

```python
from langsmith import evaluate

# 1. å‡†å¤‡æµ‹è¯•æ•°æ®é›†
test_dataset = [
    {
        "question": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
        "expected_answer": "RAGæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯..."
    },
    {
        "question": "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢ï¼Ÿ",
        "expected_answer": "å¯ä»¥é€šè¿‡è°ƒæ•´chunk_size..."
    }
]

# 2. å®šä¹‰è¯„ä¼°å‡½æ•°
def rag_evaluator(inputs: dict, outputs: dict, reference: dict) -> dict:
    """è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°"""
    # è¿™é‡Œå¯ä»¥ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤
    # æˆ–è€…ä½¿ç”¨BLEUã€ROUGEç­‰æŒ‡æ ‡
    pass

# 3. è¿è¡Œè¯„ä¼°
results = evaluate(
    lambda x: rag_chain.invoke(x["question"]),
    data=test_dataset,
    evaluators=[rag_evaluator],
    experiment_prefix="rag-v1"
)
```

---

### 6.2 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

##### 6.2.1 æ£€ç´¢ä¸åˆ°ç›¸å…³æ–‡æ¡£

**åŸå› **ï¼š
- æ–‡æ¡£åˆ†å—ä¸åˆç†
- Embeddingæ¨¡å‹ä¸åŒ¹é…
- Top-Kè®¾ç½®è¿‡å°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ä¼˜åŒ–åˆ†å—ç­–ç•¥
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,    # å¢å¤§å—å¤§å°
    chunk_overlap=300   # å¢åŠ é‡å 
)

# 2. è°ƒæ•´Top-K
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 10}  # å¢åŠ å€™é€‰æ•°é‡
)

# 3. ä½¿ç”¨MMRå¢åŠ å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)
```

##### 6.2.2 ç”Ÿæˆçš„ç­”æ¡ˆä¸å‡†ç¡®

**åŸå› **ï¼š
- Promptä¸å¤Ÿæ¸…æ™°
- LLMæ¸©åº¦è®¾ç½®è¿‡é«˜
- æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸ç›¸å…³

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ä¼˜åŒ–Prompt
prompt = ChatPromptTemplate.from_template("""
ä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜æ— æ³•å›ç­”ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

# 2. è®¾ç½®æ¸©åº¦ä¸º0ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7}
)
```

##### 6.2.3 å“åº”å»¶è¿Ÿé«˜

**åŸå› **ï¼š
- æ£€ç´¢Top-Kè¿‡å¤§
- LLMæ¨¡å‹è¿‡å¤§
- æœªä½¿ç”¨æµå¼è¾“å‡º

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°‘Top-K
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # å‡å°‘æ£€ç´¢æ•°é‡
)

# 2. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini")  # æ›´å¿«

# 3. å¯ç”¨æµå¼è¾“å‡º
llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

# 4. ä½¿ç”¨2-Step RAGè€ŒéAgent RAG
# 2-Step RAGåªéœ€1æ¬¡LLMè°ƒç”¨ï¼ŒAgent RAGéœ€è¦2æ¬¡
```

---

## å…¨æ–‡æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

##### ç¬¬1ç« ï¼šRAGæ¶æ„
- âœ… RAGè§£å†³LLMçš„ä¸Šä¸‹æ–‡é™åˆ¶å’ŒçŸ¥è¯†é™ˆæ—§é—®é¢˜
- âœ… å®Œæ•´æµç¨‹ï¼šç´¢å¼•ï¼ˆLoad â†’ Split â†’ Embed â†’ Storeï¼‰â†’ æ£€ç´¢ â†’ ç”Ÿæˆ
- âœ… ä¸¤ç§æ¨¡å¼ï¼šAgent RAGï¼ˆçµæ´»ï¼‰vs 2-Step RAGï¼ˆå¿«é€Ÿï¼‰

##### ç¬¬2ç« ï¼šç´¢å¼•æµç¨‹
- âœ… æ–‡æ¡£åŠ è½½ï¼šæ”¯æŒPDFã€ç½‘é¡µã€CSVç­‰å¤šç§æ ¼å¼
- âœ… æ–‡æœ¬åˆ†å‰²ï¼šRecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰
- âœ… å‘é‡åŒ–ï¼šOpenAI Embeddingsï¼ˆtext-embedding-3-largeï¼‰
- âœ… å‘é‡å­˜å‚¨ï¼šChromaï¼ˆå¼€å‘ï¼‰ã€Pineconeï¼ˆç”Ÿäº§ï¼‰

##### ç¬¬3ç« ï¼šæ£€ç´¢ä¸ç”Ÿæˆ
- âœ… LCELè¯­æ³•ï¼šä½¿ç”¨ç®¡é“ï¼ˆ`|`ï¼‰ä¸²è”ç»„ä»¶
- âœ… æ ‡å‡†RAGé“¾ï¼š`retriever | prompt | llm | parser`
- âœ… æµå¼è¾“å‡ºï¼šæå‡ç”¨æˆ·ä½“éªŒ

##### ç¬¬4ç« ï¼šç”Ÿäº§çº§ç³»ç»Ÿ
- âœ… å®Œæ•´å®ç°ï¼šé”™è¯¯å¤„ç†ã€æ—¥å¿—ã€é…ç½®ç®¡ç†
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼šåˆ†å—ã€Embeddingã€æ£€ç´¢ã€Prompt
- âœ… LangSmithç›‘æ§ï¼šè¿½è¸ªã€è°ƒè¯•ã€è¯„ä¼°

##### ç¬¬5ç« ï¼šé«˜çº§æŠ€æœ¯
- âœ… å…ƒæ•°æ®è¿‡æ»¤ï¼šç²¾å‡†æ£€ç´¢
- âœ… å¤šæŸ¥è¯¢æ£€ç´¢ï¼šæ‰©å±•æŸ¥è¯¢è§’åº¦
- âœ… ä¸Šä¸‹æ–‡å‹ç¼©ï¼šæå–ç›¸å…³ç‰‡æ®µ

##### ç¬¬6ç« ï¼šè¯„ä¼°ä¸ä¼˜åŒ–
- âœ… æ£€ç´¢è¯„ä¼°ï¼šPrecisionã€Recallã€F1
- âœ… ç«¯åˆ°ç«¯è¯„ä¼°ï¼šä½¿ç”¨LangSmith
- âœ… å¸¸è§é—®é¢˜ï¼šæ£€ç´¢ä¸å‡†ã€ç­”æ¡ˆé”™è¯¯ã€å»¶è¿Ÿé«˜

---

### æœ€ä½³å®è·µæ¸…å•

##### å¼€å‘é˜¶æ®µ
- [ ] ä½¿ç”¨InMemoryVectorStoreæˆ–Chromaå¿«é€ŸåŸå‹
- [ ] ä½¿ç”¨text-embedding-3-smallé™ä½æˆæœ¬
- [ ] è®¾ç½®`chunk_size=500-1000`ï¼Œ`chunk_overlap=100-200`
- [ ] ä½¿ç”¨gpt-4o-miniè¿›è¡Œæµ‹è¯•

##### ç”Ÿäº§é˜¶æ®µ
- [ ] åˆ‡æ¢åˆ°æŒä¹…åŒ–å‘é‡åº“ï¼ˆChromaã€Qdrantã€Pineconeï¼‰
- [ ] ä½¿ç”¨text-embedding-3-largeæå‡è´¨é‡
- [ ] å¯ç”¨LangSmithè¿½è¸ªå’Œç›‘æ§
- [ ] å®ç°æµå¼è¾“å‡ºæ”¹å–„ç”¨æˆ·ä½“éªŒ
- [ ] æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤å’Œé”™è¯¯å¤„ç†
- [ ] è®¾ç½®åˆç†çš„Top-Kï¼ˆé€šå¸¸3-10ï¼‰
- [ ] ä½¿ç”¨MMRå¢åŠ æ£€ç´¢å¤šæ ·æ€§

##### ä¼˜åŒ–é˜¶æ®µ
- [ ] è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆPrecisionã€Recallï¼‰
- [ ] è°ƒæ•´åˆ†å—ç­–ç•¥ï¼ˆé€šè¿‡å®éªŒæ‰¾åˆ°æœ€ä½³å€¼ï¼‰
- [ ] ä¼˜åŒ–Promptæ¨¡æ¿
- [ ] ä½¿ç”¨ä¸Šä¸‹æ–‡å‹ç¼©å‡å°‘Tokenæ¶ˆè€—
- [ ] A/Bæµ‹è¯•ä¸åŒé…ç½®

---

### å‚è€ƒèµ„æº

##### å®˜æ–¹æ–‡æ¡£
- [LangChain Pythonæ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/)
- [RAGæ•™ç¨‹](https://docs.langchain.com/oss/python/langchain/rag)
- [Semantic Searchæ•™ç¨‹](https://docs.langchain.com/oss/python/langchain/knowledge-base)
- [LangSmithæ–‡æ¡£](https://docs.langchain.com/langsmith/)

##### APIå‚è€ƒ
- [LangChain Python API](https://python.langchain.com/api_reference/)
- [langchain-core](https://python.langchain.com/api_reference/core/)
- [langchain-openai](https://python.langchain.com/api_reference/openai/)
- [langchain-chroma](https://python.langchain.com/api_reference/chroma/)

##### ç¤¾åŒºèµ„æº
- [LangChain GitHub](https://github.com/langchain-ai/langchain)
- [LangChain Discord](https://discord.gg/langchain)
