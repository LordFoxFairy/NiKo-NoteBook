# ç¬¬å››ç¯‡ï¼šRAGåŸºç¡€ç¯‡

---

## ğŸ“‹ å‰ç½®å‡†å¤‡

### ç¯å¢ƒé…ç½®

```bash
# æ ¸å¿ƒä¾èµ–
pip install langchain>=1.0.7
pip install langchain-openai>=1.0.3
pip install langchain-community>=0.4.1
pip install langchain-text-splitters>=0.4.0

# å‘é‡æ•°æ®åº“
pip install langchain-chroma>=0.2.0
pip install chromadb>=0.5.0

# å¯é€‰ä¾èµ–
pip install pypdf  # PDFæ”¯æŒ
pip install python-dotenv  # ç¯å¢ƒå˜é‡ç®¡ç†
```

### ç¯å¢ƒå˜é‡

```python
# .env
OPENAI_API_KEY=sk-your-api-key
```

---

# ç¬¬ 1 ç« ï¼šLangChain RAGæ ¸å¿ƒæ¦‚å¿µ

## 1.1 ä»€ä¹ˆæ˜¯RAG

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºLLMçš„å›ç­”èƒ½åŠ›ã€‚

### 1.1.1 RAGçš„æ ¸å¿ƒæŒ‘æˆ˜

```mermaid
graph TD
    A[æ–‡æ¡£] --> B{æŒ‘æˆ˜1: å¦‚ä½•åŠ è½½?}
    B --> C[å¤šç§æ ¼å¼]
    C --> D[PDF/DOCX/Web...]

    A --> E{æŒ‘æˆ˜2: å¦‚ä½•åˆ†å—?}
    E --> F[è¯­ä¹‰å®Œæ•´æ€§]
    F --> G[å—å¤§å°å¹³è¡¡]

    A --> H{æŒ‘æˆ˜3: å¦‚ä½•æ£€ç´¢?}
    H --> I[å‡†ç¡®æ€§]
    I --> J[ç›¸å…³æ€§æ’åº]

    style C fill:#FFE4E1
    style F fill:#E3F2FD
    style I fill:#FFF9C4
```

**å¸¸è§é—®é¢˜**ï¼š
- ğŸ“„ **æ–‡æ¡£åŠ è½½**ï¼šéœ€è¦æ”¯æŒPDFã€DOCXã€HTMLã€Markdownç­‰å¤šç§æ ¼å¼
- âœ‚ï¸ **æ™ºèƒ½åˆ†å—**ï¼šå¦‚ä½•ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Ÿå›ºå®šå¤§å° vs è¯­ä¹‰åˆ†å—ï¼Ÿ
- ğŸ” **ç²¾ç¡®æ£€ç´¢**ï¼šå¦‚ä½•æé«˜æ£€ç´¢å‡†ç¡®ç‡ï¼Ÿå‘é‡æ£€ç´¢ vs å…³é”®è¯æ£€ç´¢ï¼Ÿ
- ğŸ¯ **ç›¸å…³æ€§æ’åº**ï¼šå¦‚ä½•ç¡®ä¿æœ€ç›¸å…³çš„å†…å®¹æ’åœ¨å‰é¢ï¼Ÿ
- ğŸ’¾ **å­˜å‚¨ç®¡ç†**ï¼šå¦‚ä½•é«˜æ•ˆå­˜å‚¨å’ŒæŸ¥è¯¢å¤§è§„æ¨¡æ–‡æ¡£ï¼Ÿ

### 1.1.2 LangChain RAGçš„ä¼˜åŠ¿

**LangChain RAGçš„ç‰¹ç‚¹**ï¼š

| ç»´åº¦ | ç‰¹ç‚¹ | è¯´æ˜ |
|------|------|------|
| **çµæ´»æ€§** | é«˜åº¦å¯å®šåˆ¶ | å¯ç²¾ç»†æ§åˆ¶æ¯ä¸ªæ­¥éª¤ |
| **ç»„åˆæ€§** | LCELé“¾å¼ç»„åˆ | ä½¿ç”¨ç®¡é“ç»„åˆå„ç»„ä»¶ |
| **æ‰©å±•æ€§** | ä¸°å¯Œçš„é›†æˆ | æ”¯æŒå¤šç§å‘é‡åº“å’Œæ¨¡å‹ |
| **ç”Ÿæ€** | å®Œæ•´çš„å·¥å…·é“¾ | Agentã€Memoryã€Toolsç­‰ |
| **ç”Ÿäº§çº§** | LangSmithç›‘æ§ | è¿½è¸ªã€è°ƒè¯•ã€ä¼˜åŒ– |

---

## 1.2 LangChain RAGæ ¸å¿ƒç»„ä»¶

### 1.2.1 æ–‡æ¡£åŠ è½½å™¨ï¼ˆDocument Loadersï¼‰

LangChainæä¾›ä¸°å¯Œçš„æ–‡æ¡£åŠ è½½å™¨ï¼š

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
loader = DirectoryLoader(
    "./data",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
for doc in documents[:2]:
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
```

**å¸¸ç”¨åŠ è½½å™¨**ï¼š

```python
# PDFåŠ è½½å™¨
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("document.pdf")
pages = loader.load()

# ç½‘é¡µåŠ è½½å™¨
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com")
docs = loader.load()

# CSVåŠ è½½å™¨
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader("data.csv")
docs = loader.load()

# MarkdownåŠ è½½å™¨
from langchain_community.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader("README.md")
docs = loader.load()
```

### 1.2.2 æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆText Splittersï¼‰

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºåˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # å—å¤§å°
    chunk_overlap=200,      # é‡å å¤§å°
    length_function=len,    # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # åˆ†éš”ç¬¦ä¼˜å…ˆçº§
)

# åˆ†å‰²æ–‡æ¡£
chunks = text_splitter.split_documents(documents)

print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
for i, chunk in enumerate(chunks[:3]):
    print(f"\nå— {i+1}:")
    print(f"å†…å®¹: {chunk.page_content[:100]}...")
    print(f"é•¿åº¦: {len(chunk.page_content)}")
```

**å…¶ä»–åˆ†å‰²å™¨**ï¼š

```python
# å­—ç¬¦åˆ†å‰²å™¨ï¼ˆç®€å•ï¼‰
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0,
    separator="\n"
)

# Tokenåˆ†å‰²å™¨ï¼ˆç²¾ç¡®æ§åˆ¶tokenæ•°ï¼‰
from langchain_text_splitters import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=1000,
    chunk_overlap=0
)

# Markdownåˆ†å‰²å™¨ï¼ˆä¿ç•™ç»“æ„ï¼‰
from langchain_text_splitters import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
```

### 1.2.3 Embeddingsï¼ˆå‘é‡åŒ–ï¼‰

```python
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºembeddingsæ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",  # æˆ– text-embedding-3-small
    api_key="your-api-key"
)

# å‘é‡åŒ–å•ä¸ªæ–‡æœ¬
vector = embeddings.embed_query("è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬")
print(f"å‘é‡ç»´åº¦: {len(vector)}")

# æ‰¹é‡å‘é‡åŒ–
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
vectors = embeddings.embed_documents(texts)
print(f"æ‰¹é‡å‘é‡åŒ–äº† {len(vectors)} ä¸ªæ–‡æœ¬")
```

### 1.2.4 å‘é‡å­˜å‚¨ï¼ˆVector Storesï¼‰

#### Chromaå‘é‡åº“

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# æ–¹å¼1: ä»æ–‡æ¡£åˆ›å»º
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# æ–¹å¼2: åŠ è½½å·²æœ‰å‘é‡åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# æ·»åŠ æ–‡æ¡£
vectorstore.add_documents(documents=new_docs)

# ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search("æŸ¥è¯¢æ–‡æœ¬", k=3)
for doc in results:
    print(doc.page_content)
```

#### å…¶ä»–å‘é‡åº“

```python
# FAISS
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embeddings)
vectorstore.save_local("faiss_index")
vectorstore = FAISS.load_local("faiss_index", embeddings)

# Qdrant
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient

client = QdrantClient(host="localhost", port=6333)
vectorstore = Qdrant(
    client=client,
    collection_name="my_documents",
    embeddings=embeddings
)

# Pinecone
from langchain_community.vectorstores import Pinecone
import pinecone

pinecone.init(api_key="your-key", environment="your-env")
vectorstore = Pinecone.from_documents(chunks, embeddings, index_name="my-index")
```

---

## 1.3 å¿«é€Ÿå¼€å§‹ï¼šç¬¬ä¸€ä¸ªRAGåº”ç”¨

### 1.3.1 5åˆ†é’Ÿå®ç°å®Œæ•´RAG

```python
"""
å®Œæ•´çš„RAGåº”ç”¨ - LangChainç‰ˆæœ¬
"""
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import os

# è®¾ç½®API Key
os.environ["OPENAI_API_KEY"] = "sk-your-key"

# æ­¥éª¤1: åŠ è½½æ–‡æ¡£
print("ğŸ“„ åŠ è½½æ–‡æ¡£...")
loader = DirectoryLoader(
    "./data",
    glob="**/*.txt",
    loader_cls=TextLoader
)
documents = loader.load()
print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# æ­¥éª¤2: åˆ†å‰²æ–‡æ¡£
print("âœ‚ï¸  åˆ†å‰²æ–‡æ¡£...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
print(f"âœ… åˆ›å»ºäº† {len(chunks)} ä¸ªå—")

# æ­¥éª¤3: åˆ›å»ºå‘é‡åº“
print("ğŸ”¨ åˆ›å»ºå‘é‡åº“...")
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
print("âœ… å‘é‡åº“åˆ›å»ºå®Œæˆ")

# æ­¥éª¤4: åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# æ­¥éª¤5: åˆ›å»ºPromptæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("""
è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

# æ­¥éª¤6: åˆ›å»ºLLM
llm = ChatOpenAI(model="gpt-4", temperature=0)

# æ­¥éª¤7: æ„å»ºRAGé“¾
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    RunnablePassthrough.assign(
        context=retriever | format_docs
    )
    | prompt
    | llm
    | StrOutputParser()
)

# æ­¥éª¤8: æŸ¥è¯¢
questions = [
    "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æœ‰å“ªäº›å…³é”®æ¦‚å¿µï¼Ÿ",
    "å¦‚ä½•å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ"
]

for question in questions:
    print(f"\nâ“ é—®é¢˜: {question}")
    response = rag_chain.invoke({"question": question})
    print(f"ğŸ’¡ å›ç­”: {response}")
    print("-" * 80)
```

---

# ç¬¬ 2 ç« ï¼šRAGæ ¸å¿ƒç»„ä»¶æ·±å…¥

## 2.1 æ–‡æœ¬åˆ†å‰²ç­–ç•¥

### 2.1.1 RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æ™ºèƒ½é€’å½’åˆ†å‰²
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
    length_function=len,
)

chunks = splitter.split_documents(documents)

# æŸ¥çœ‹åˆ†å‰²æ•ˆæœ
for i, chunk in enumerate(chunks[:3]):
    print(f"\nå— {i+1} (é•¿åº¦: {len(chunk.page_content)}):")
    print(chunk.page_content[:200])
```

### 2.1.2 åˆ†å—å‚æ•°ä¼˜åŒ–

```python
# ä¸åŒåœºæ™¯çš„åˆ†å—ç­–ç•¥

# åœºæ™¯1: çŸ­æ–‡æœ¬é—®ç­”ï¼ˆå¦‚FAQï¼‰
short_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# åœºæ™¯2: é•¿æ–‡æ¡£åˆ†æï¼ˆå¦‚æŠ€æœ¯æ–‡æ¡£ï¼‰
long_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=400
)

# åœºæ™¯3: ä»£ç æ–‡æ¡£
code_splitter = RecursiveCharacterTextSplitter.from_language(
    language="python",
    chunk_size=1000,
    chunk_overlap=100
)
```

---

## 2.2 å‘é‡å­˜å‚¨æ·±å…¥

### 2.2.1 Chromaè¿›é˜¶ç”¨æ³•

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# åˆ›å»ºæŒä¹…åŒ–å‘é‡åº“
vectorstore = Chroma(
    collection_name="my_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

# æ·»åŠ æ–‡æ¡£ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
vectorstore.add_documents(
    documents=chunks,
    ids=[f"doc_{i}" for i in range(len(chunks))]
)

# é«˜çº§æ£€ç´¢ï¼šä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
results = vectorstore.similarity_search(
    query="æŸ¥è¯¢æ–‡æœ¬",
    k=3,
    filter={"source": "document.pdf"}
)

# MMRæ£€ç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼Œå¢åŠ å¤šæ ·æ€§ï¼‰
results = vectorstore.max_marginal_relevance_search(
    query="æŸ¥è¯¢æ–‡æœ¬",
    k=5,
    fetch_k=20,
    lambda_mult=0.5  # 0=å¤šæ ·æ€§, 1=ç›¸å…³æ€§
)
```

### 2.2.2 å‘é‡æ•°æ®åº“é€‰æ‹©æŒ‡å—

| æ•°æ®åº“ | ç±»å‹ | æ€§èƒ½ | éƒ¨ç½²éš¾åº¦ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|---------|---------|
| **Chroma** | åµŒå…¥å¼ | ä¸­ | â­ | å¼€å‘æµ‹è¯•ã€ä¸­å°å‹åº”ç”¨ |
| **FAISS** | åº“ | é«˜ | â­â­ | å•æœºé«˜æ€§èƒ½ã€å¤§è§„æ¨¡æ£€ç´¢ |
| **Qdrant** | æœåŠ¡ | é«˜ | â­â­â­ | ç”Ÿäº§ç¯å¢ƒã€åˆ†å¸ƒå¼ |
| **Pinecone** | äº‘æœåŠ¡ | é«˜ | â­ | äº‘åŸç”Ÿã€æ— éœ€è¿ç»´ |
| **Weaviate** | æœåŠ¡ | é«˜ | â­â­â­ | ä¼ä¸šçº§ã€GraphRAG |

---

## 2.3 æ£€ç´¢å™¨ï¼ˆRetrieversï¼‰

### 2.3.1 åŸºç¡€æ£€ç´¢å™¨

```python
# ç›¸ä¼¼åº¦æ£€ç´¢
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# MMRæ£€ç´¢
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,
        "lambda_mult": 0.5
    }
)

# ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,
        "k": 5
    }
)
```

### 2.3.2 è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from typing import List

class CustomRetriever(BaseRetriever):
    vectorstore: object
    top_k: int = 5

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘"""
        # 1. å‘é‡æ£€ç´¢
        vector_results = self.vectorstore.similarity_search(query, k=self.top_k)

        # 2. è‡ªå®šä¹‰åå¤„ç†ï¼ˆå¦‚é‡æ’åºã€è¿‡æ»¤ç­‰ï¼‰
        filtered_results = [
            doc for doc in vector_results
            if len(doc.page_content) > 100
        ]

        return filtered_results

# ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨
custom_retriever = CustomRetriever(vectorstore=vectorstore, top_k=5)
results = custom_retriever.get_relevant_documents("æŸ¥è¯¢æ–‡æœ¬")
```

---

## 2.4 RAGé“¾æ„å»º

### 2.4.1 åŸºç¡€RAGé“¾

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Promptæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}
""")

# æ ¼å¼åŒ–å‡½æ•°
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# æ„å»ºé“¾
rag_chain = (
    RunnablePassthrough.assign(
        context=retriever | format_docs
    )
    | prompt
    | ChatOpenAI(model="gpt-4", temperature=0)
    | StrOutputParser()
)

# æ‰§è¡Œ
response = rag_chain.invoke({"question": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"})
print(response)
```

### 2.4.2 å¸¦æ¥æºçš„RAGé“¾

```python
from langchain_core.runnables import RunnableParallel

# ä¿ç•™æ£€ç´¢çš„æ–‡æ¡£
rag_chain_with_source = RunnableParallel(
    {
        "context": retriever | format_docs,
        "source_documents": retriever,
        "question": RunnablePassthrough()
    }
).assign(
    answer=lambda x: (
        prompt
        | ChatOpenAI(model="gpt-4", temperature=0)
        | StrOutputParser()
    ).invoke({"context": x["context"], "question": x["question"]})
)

# æ‰§è¡Œ
result = rag_chain_with_source.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
print(f"å›ç­”: {result['answer']}")
print(f"\næ¥æºæ–‡æ¡£:")
for i, doc in enumerate(result['source_documents'], 1):
    print(f"{i}. {doc.page_content[:100]}...")
```

### 2.4.3 æµå¼RAGé“¾

```python
# å¯ç”¨æµå¼è¾“å‡º
rag_chain_stream = (
    RunnablePassthrough.assign(
        context=retriever | format_docs
    )
    | prompt
    | ChatOpenAI(model="gpt-4", temperature=0, streaming=True)
    | StrOutputParser()
)

# æµå¼è¾“å‡º
for chunk in rag_chain_stream.stream({"question": "è¯¦ç»†è§£é‡ŠRAGçš„å·¥ä½œåŸç†"}):
    print(chunk, end="", flush=True)
```

---

# ç¬¬ 3 ç« ï¼šRAGå®Œæ•´åº”ç”¨å®æˆ˜

## 3.1 ç”Ÿäº§çº§RAGåº”ç”¨

```python
"""
ç”Ÿäº§çº§RAGåº”ç”¨ - å®Œæ•´å®ç°
"""
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
import os
from pathlib import Path

class ProductionRAG:
    def __init__(self, data_dir="./data", persist_dir="./chroma_db"):
        self.data_dir = data_dir
        self.persist_dir = persist_dir
        self.vectorstore = None
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    def build_vectorstore(self, force_rebuild=False):
        """æ„å»ºæˆ–åŠ è½½å‘é‡åº“"""
        if not force_rebuild and Path(self.persist_dir).exists():
            print("ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡åº“...")
            self.vectorstore = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
            print("âœ… å‘é‡åº“åŠ è½½æˆåŠŸ")
            return

        print("ğŸ“„ 1. åŠ è½½æ–‡æ¡£...")
        loader = DirectoryLoader(
            self.data_dir,
            glob="**/*.txt",
            loader_cls=TextLoader
        )
        documents = loader.load()
        print(f"   âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        print("âœ‚ï¸  2. åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"   âœ… åˆ›å»ºäº† {len(chunks)} ä¸ªå—")

        print("ğŸ”¨ 3. åˆ›å»ºå‘é‡åº“...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )
        print("   âœ… å‘é‡åº“åˆ›å»ºå®Œæˆ")

    def create_rag_chain(self, model="gpt-4", temperature=0, top_k=3):
        """åˆ›å»ºRAGé“¾"""
        if self.vectorstore is None:
            raise ValueError("å‘é‡åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ build_vectorstore()")

        # æ£€ç´¢å™¨
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": top_k}
        )

        # Prompt
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. ä¸è¦ç¼–é€ ä¿¡æ¯
4. å¦‚æœå¯ä»¥ï¼Œè¯·å¼•ç”¨å…·ä½“çš„æ¥æº

å›ç­”ï¼š
""")

        # æ ¼å¼åŒ–å‡½æ•°
        def format_docs(docs):
            return "\n\n".join(
                f"[æ–‡æ¡£{i+1}]\n{doc.page_content}"
                for i, doc in enumerate(docs)
            )

        # æ„å»ºé“¾
        rag_chain = RunnableParallel(
            {
                "context": retriever | format_docs,
                "source_documents": retriever,
                "question": RunnablePassthrough()
            }
        ).assign(
            answer=lambda x: (
                prompt
                | ChatOpenAI(model=model, temperature=temperature)
                | StrOutputParser()
            ).invoke({"context": x["context"], "question": x["question"]})
        )

        return rag_chain

    def query(self, question, model="gpt-4", temperature=0, top_k=3, show_sources=True):
        """æŸ¥è¯¢"""
        chain = self.create_rag_chain(model=model, temperature=temperature, top_k=top_k)

        print(f"\nâ“ é—®é¢˜: {question}")
        result = chain.invoke(question)

        print(f"\nğŸ’¡ å›ç­”:\n{result['answer']}\n")

        if show_sources:
            print("ğŸ“š æ¥æº:")
            for i, doc in enumerate(result['source_documents'], 1):
                print(f"  {i}. {doc.page_content[:100]}...")
                print(f"     å…ƒæ•°æ®: {doc.metadata}")

        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–
    rag = ProductionRAG()
    rag.build_vectorstore()

    # æŸ¥è¯¢
    questions = [
        "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æœ‰å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿ",
        "å¦‚ä½•å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ"
    ]

    for q in questions:
        rag.query(q)
        print("-" * 80)
```

---

## 3.2 RAGæ€§èƒ½ä¼˜åŒ–

### 3.2.1 åˆ†å—ä¼˜åŒ–

```python
# é—®é¢˜ï¼šå›ºå®šå¤§å°åˆ†å—å¯èƒ½åˆ‡æ–­è¯­ä¹‰

# âŒ å·®çš„åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,   # å¤ªå°
    chunk_overlap=0   # æ— é‡å 
)

# âœ… ä¼˜åŒ–çš„åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # é€‚ä¸­
    chunk_overlap=200,  # 20%é‡å 
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # ä¼˜å…ˆåœ¨æ®µè½/å¥å­è¾¹ç•Œåˆ†å‰²
)

# âœ… é’ˆå¯¹ä¸­æ–‡çš„ä¼˜åŒ–
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""]
)
```

### 3.2.2 Embeddingä¼˜åŒ–

```python
from langchain_openai import OpenAIEmbeddings

# 1. æ¨¡å‹é€‰æ‹©
# å¹³è¡¡æ–¹æ¡ˆï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"  # 1536ç»´
)

# é«˜è´¨é‡æ–¹æ¡ˆï¼ˆæ•ˆæœæœ€å¥½ï¼‰
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large"  # 3072ç»´
)

# 2. æ‰¹é‡å¤„ç†ï¼ˆèŠ‚çœæˆæœ¬å’Œæ—¶é—´ï¼‰
from langchain_chroma import Chroma

# åˆ†æ‰¹å‘é‡åŒ–
batch_size = 100
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    vectorstore.add_documents(batch)
```

### 3.2.3 æ£€ç´¢ä¼˜åŒ–

```python
# 1. è°ƒæ•´Top-K
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5}  # å®éªŒæœ€ä½³å€¼ï¼ˆé€šå¸¸3-10ï¼‰
)

# 2. ä½¿ç”¨MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰- å¢åŠ å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,      # å€™é€‰æ± 
        "lambda_mult": 0.5  # 0=å¤šæ ·æ€§, 1=ç›¸å…³æ€§
    }
)

# 3. ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # åªè¿”å›>0.7çš„ç»“æœ
        "k": 5
    }
)
```

---

# ç¬¬ 4 ç« ï¼šRAGè¯„ä¼°ä¸ç›‘æ§

## 4.1 ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°ï¼Ÿ

**å¸¸è§RAGé—®é¢˜**ï¼š
- âŒ æ£€ç´¢ä¸åˆ°ç›¸å…³æ–‡æ¡£
- âŒ æ£€ç´¢åˆ°ä¸ç›¸å…³æ–‡æ¡£
- âŒ ç”Ÿæˆçš„ç­”æ¡ˆä¸å‡†ç¡®
- âŒ ç”Ÿæˆçš„ç­”æ¡ˆæœ‰å¹»è§‰

**è¯„ä¼°çš„é‡è¦æ€§**ï¼š
```mermaid
graph LR
    A[RAGç³»ç»Ÿ] --> B[è¯„ä¼°æŒ‡æ ‡]
    B --> C[å‘ç°é—®é¢˜]
    C --> D[ä¼˜åŒ–æ”¹è¿›]
    D --> A
```

---

## 4.2 ä½¿ç”¨LangSmithç›‘æ§

```python
"""
ä½¿ç”¨LangSmithè¿½è¸ªå’Œç›‘æ§RAG
"""
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. å¯ç”¨LangSmith
os.environ["LANGSMITH_API_KEY"] = "your-key"
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "rag-project"

# 2. æ­£å¸¸ä½¿ç”¨ï¼ˆè‡ªåŠ¨è¿½è¸ªï¼‰
rag_chain = create_rag_chain()
result = rag_chain.invoke({"question": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"})

# 3. åœ¨LangSmith DashboardæŸ¥çœ‹ï¼š
# - æ¯æ¬¡è°ƒç”¨çš„è¯¦ç»†trace
# - Tokenä½¿ç”¨é‡
# - å»¶è¿Ÿ
# - æˆæœ¬
# - é”™è¯¯è¿½è¸ª
```

---

## 4.3 è¯„ä¼°æŒ‡æ ‡

### 4.3.1 æ£€ç´¢è´¨é‡æŒ‡æ ‡

```python
"""
æ‰‹åŠ¨è¯„ä¼°æ£€ç´¢è´¨é‡
"""
def evaluate_retrieval(retriever, test_queries):
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""
    results = []

    for query, relevant_docs in test_queries:
        # æ£€ç´¢
        retrieved = retriever.get_relevant_documents(query)
        retrieved_ids = [doc.metadata.get('id') for doc in retrieved]

        # è®¡ç®—æŒ‡æ ‡
        relevant_ids = [doc.metadata.get('id') for doc in relevant_docs]

        # Precision@K
        relevant_retrieved = len(set(retrieved_ids) & set(relevant_ids))
        precision = relevant_retrieved / len(retrieved_ids) if retrieved_ids else 0

        # Recall@K
        recall = relevant_retrieved / len(relevant_ids) if relevant_ids else 0

        results.append({
            'query': query,
            'precision': precision,
            'recall': recall
        })

    return results

# æµ‹è¯•æ•°æ®
test_queries = [
    ("ä»€ä¹ˆæ˜¯RAGï¼Ÿ", [doc1, doc2]),
    ("å¦‚ä½•ä¼˜åŒ–æ£€ç´¢ï¼Ÿ", [doc3, doc4, doc5])
]

# è¯„ä¼°
metrics = evaluate_retrieval(retriever, test_queries)
for m in metrics:
    print(f"æŸ¥è¯¢: {m['query']}")
    print(f"  Precision: {m['precision']:.2f}")
    print(f"  Recall: {m['recall']:.2f}")
```

---

## æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **è¯„ä¼°çš„é‡è¦æ€§**ï¼šå‘ç°é—®é¢˜ã€æŒç»­ä¼˜åŒ–
2. **LangSmithç›‘æ§**ï¼šè¿½è¸ªã€è°ƒè¯•ã€ä¼˜åŒ–
3. **è¯„ä¼°æŒ‡æ ‡**ï¼šPrecisionã€Recallç­‰

---

## å…¨æ–‡æ€»ç»“

**ç¬¬1ç« å›é¡¾**ï¼š
- âœ… RAGæ ¸å¿ƒæ¦‚å¿µ
- âœ… LangChain RAGä¼˜åŠ¿
- âœ… æ ¸å¿ƒç»„ä»¶ï¼ˆLoadersã€Splittersã€Embeddingsã€VectorStoresï¼‰
- âœ… 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

**ç¬¬2ç« å›é¡¾**ï¼š
- âœ… æ–‡æœ¬åˆ†å‰²ç­–ç•¥
- âœ… å‘é‡å­˜å‚¨æ·±å…¥
- âœ… æ£€ç´¢å™¨è¿›é˜¶
- âœ… RAGé“¾æ„å»º

**ç¬¬3ç« å›é¡¾**ï¼š
- âœ… ç”Ÿäº§çº§RAGåº”ç”¨
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼ˆåˆ†å—ã€Embeddingã€æ£€ç´¢ï¼‰

**ç¬¬4ç« å›é¡¾**ï¼š
- âœ… è¯„ä¼°ä¸ç›‘æ§
- âœ… LangSmithä½¿ç”¨

---

## æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹ 1**ï¼šä½¿ç”¨LangChainæ„å»ºä¸€ä¸ªæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
2. **ç»ƒä¹ 2**ï¼šå¯¹æ¯”ä¸åŒåˆ†å—ç­–ç•¥çš„æ•ˆæœ
3. **ç»ƒä¹ 3**ï¼šå®ç°ä¸€ä¸ªå¸¦å…ƒæ•°æ®è¿‡æ»¤çš„RAGç³»ç»Ÿ
4. **ç»ƒä¹ 4**ï¼šä½¿ç”¨LangSmithç›‘æ§å’Œä¼˜åŒ–ä½ çš„RAGç³»ç»Ÿ

---

## å‚è€ƒèµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/)
- [LangChain Python API Reference](https://python.langchain.com/api_reference/)
- [LangSmithæ–‡æ¡£](https://docs.smith.langchain.com/)

---

**ç‰ˆæœ¬ä¿¡æ¯**ï¼š
- LangChain: 1.0.7+
- langchain-community: 0.4.1+
- langchain-openai: 1.0.3+
- langchain-chroma: 0.2.0+
- æœ€åæ›´æ–°: 2025-11-23
