# ç¬¬äº”ç¯‡ï¼šRAGé«˜çº§ç¯‡ - é«˜çº§æ£€ç´¢ä¸ä¼˜åŒ–

## å‰è¨€

åœ¨ç¬¬å››ç¯‡ä¸­,æˆ‘ä»¬å­¦ä¹ äº†RAGçš„åŸºç¡€æ¦‚å¿µ,å®ç°äº†åŸºæœ¬çš„RAGç³»ç»Ÿã€‚ä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­,åŸºç¡€çš„å‘é‡æ£€ç´¢å¾€å¾€æ— æ³•æ»¡è¶³å¤æ‚çš„ä¸šåŠ¡éœ€æ±‚:

**åŸºç¡€RAGçš„å±€é™æ€§**:
1. **å¬å›ä¸å…¨é¢**:å•ä¸€å‘é‡æ£€ç´¢å¯èƒ½é—æ¼å…³é”®ä¿¡æ¯
2. **æ’åºä¸ç²¾ç¡®**:top-kç»“æœä¸­å¯èƒ½åŒ…å«ä¸ç›¸å…³å†…å®¹
3. **ä¸Šä¸‹æ–‡å†—ä½™**:æ£€ç´¢åˆ°çš„æ–‡æœ¬å¯èƒ½åŒ…å«å¤§é‡æ— å…³ä¿¡æ¯
4. **å¤æ‚æŸ¥è¯¢æ”¯æŒå¼±**:éš¾ä»¥å¤„ç†å¤šè·³æ¨ç†ã€å®ä½“å…³ç³»æŸ¥è¯¢

æœ¬ç¯‡å°†æ·±å…¥æ¢è®¨**LangChainé«˜çº§æ£€ç´¢æŠ€æœ¯**å’Œ**ä¼˜åŒ–æ–¹æ¡ˆ**,å¸®åŠ©ä½ æ„å»ºç”Ÿäº§çº§çš„RAGç³»ç»Ÿã€‚

---

## æ ¸å¿ƒæ¦‚å¿µå¯¹æ¯”

| æŠ€æœ¯ | è§£å†³çš„é—®é¢˜ | æ€§èƒ½æå‡ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-----------|---------|--------|---------|
| **æ··åˆæ£€ç´¢** | å•ä¸€æ£€ç´¢å¬å›ä¸å…¨ | +20-30% | ä½ | é€šç”¨RAG |
| **é‡æ’åº** | top-kç»“æœä¸ç²¾ç¡® | +15-25% | ä¸­ | ç²¾åº¦è¦æ±‚é«˜ |
| **æŸ¥è¯¢æ”¹å†™** | æŸ¥è¯¢è¡¨è¾¾ä¸åŒ¹é… | +10-20% | ä½ | å£è¯­åŒ–æŸ¥è¯¢ |
| **ä¸Šä¸‹æ–‡å‹ç¼©** | tokenæˆæœ¬è¿‡é«˜ | æˆæœ¬-50% | ä¸­ | é•¿ä¸Šä¸‹æ–‡ |
| **çŸ¥è¯†å›¾è°±RAG** | å®ä½“å…³ç³»æŸ¥è¯¢å¼± | +30-40% | é«˜ | ç»“æ„åŒ–çŸ¥è¯† |
| **Self-RAG** | æ£€ç´¢ç»“æœä¸å¯é  | +20-30% | é«˜ | é«˜è´¨é‡è¦æ±‚ |

---

## ç¬¬1ç« :æ··åˆæ£€ç´¢æŠ€æœ¯(Hybrid Search)

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦æ··åˆæ£€ç´¢

#### 1.1.1 å‘é‡æ£€ç´¢çš„å±€é™æ€§

**é—®é¢˜ç¤ºä¾‹**:
```python
# ç”¨æˆ·æŸ¥è¯¢:"Python 3.11çš„æ–°ç‰¹æ€§"
# å‘é‡æ£€ç´¢å¯èƒ½è¿”å›:
# âŒ "Python 3.10çš„æ–°ç‰¹æ€§"(è¯­ä¹‰ç›¸ä¼¼,ä½†ç‰ˆæœ¬ä¸å¯¹)
# âŒ "Pythonçš„å‘å±•å†å²"(ç›¸å…³,ä½†ä¸ç²¾ç¡®)
# âœ… "Python 3.11 release notes"(ç²¾ç¡®åŒ¹é…)
```

**å‘é‡æ£€ç´¢çš„é—®é¢˜**:

1. **å…³é”®è¯æ•æ„Ÿä¿¡æ¯ä¸¢å¤±**:ç‰ˆæœ¬å·ã€äº§å“å‹å·ç­‰ç²¾ç¡®åŒ¹é…éœ€æ±‚
2. **ç½•è§è¯å¤„ç†ä¸ä½³**:ä¸“ä¸šæœ¯è¯­ã€å…¬å¸åç§°ç­‰ä½é¢‘è¯
3. **è¯­ä¹‰æ³›åŒ–è¿‡åº¦**:å¯èƒ½è¿”å›ç›¸å…³ä½†ä¸ç²¾ç¡®çš„ç»“æœ

#### 1.1.2 å…¨æ–‡æ£€ç´¢çš„å±€é™æ€§

**BM25ç®—æ³•çš„é—®é¢˜**:
1. **æ— æ³•ç†è§£è¯­ä¹‰**:"æ±½è½¦ä¿å…»"å’Œ"è½¦è¾†ç»´æŠ¤"æ— æ³•åŒ¹é…
2. **åŒä¹‰è¯é—®é¢˜**:"åŒ—äº¬"å’Œ"é¦–éƒ½"æ— æ³•å…³è”
3. **è¯åºä¸æ•æ„Ÿ**:"ç‹—å’¬äºº"å’Œ"äººå’¬ç‹—"è¯„åˆ†ç›¸ä¼¼

#### 1.1.3 æ··åˆæ£€ç´¢çš„ä¼˜åŠ¿

```
æ··åˆæ£€ç´¢ = å‘é‡æ£€ç´¢(è¯­ä¹‰ç†è§£) + å…¨æ–‡æ£€ç´¢(ç²¾ç¡®åŒ¹é…)
```

**äº’è¡¥æ•ˆæœ**:
- å‘é‡æ£€ç´¢:æ•è·è¯­ä¹‰ç›¸å…³æ€§
- BM25æ£€ç´¢:æ•è·å…³é”®è¯ç²¾ç¡®åŒ¹é…
- èåˆç®—æ³•:RRF(Reciprocal Rank Fusion)ç»¼åˆæ’åº

---

### 1.2 LangChainæ··åˆæ£€ç´¢å®ç°

#### 1.2.1 åŸºç¡€æ··åˆæ£€ç´¢

```python
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader

# æ­¥éª¤1: åŠ è½½å’Œåˆ†å—æ–‡æ¡£
loader = DirectoryLoader("./docs", glob="**/*.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
splits = text_splitter.split_documents(documents)

# æ­¥éª¤2: åˆ›å»ºå‘é‡æ£€ç´¢å™¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)
vector_retriever = vectorstore.as_retriever(
    search_kwargs={"k": 4}
)

# æ­¥éª¤3: åˆ›å»ºBM25æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(splits)
bm25_retriever.k = 4

# æ­¥éª¤4: åˆ›å»ºæ··åˆæ£€ç´¢å™¨(EnsembleRetriever)
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # æƒé‡:å‘é‡50%,BM25 50%
)

# æ­¥éª¤5: ä½¿ç”¨æ··åˆæ£€ç´¢
results = ensemble_retriever.invoke("Python 3.11çš„æ–°ç‰¹æ€§æœ‰å“ªäº›?")
for doc in results:
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print(f"æ¥æº: {doc.metadata.get('source', 'unknown')}\n")
```

**è¾“å‡ºç¤ºä¾‹**:
```
å†…å®¹: Python 3.11äº2022å¹´10æœˆå‘å¸ƒ,ä¸»è¦æ–°ç‰¹æ€§åŒ…æ‹¬:
1. æ€§èƒ½æå‡(å¹³å‡å¿«25%)
2. æ›´å¥½çš„é”™è¯¯æç¤º...
æ¥æº: docs/python_releases.txt

å†…å®¹: Python 3.11å¼•å…¥äº†å¼‚å¸¸ç»„(Exception Groups)å’Œexcept*è¯­æ³•...
æ¥æº: docs/python_311_features.txt
```

---

#### 1.2.2 æƒé‡è°ƒä¼˜

**ä¸åŒæƒé‡çš„æ•ˆæœ**:

```python
# æµ‹è¯•ä¸åŒæƒé‡ç»„åˆ
test_weights = [
    ([0.7, 0.3], "å‘é‡ä¸ºä¸»"),
    ([0.5, 0.5], "å¹³è¡¡"),
    ([0.3, 0.7], "BM25ä¸ºä¸»")
]

query = "å¦‚ä½•å®‰è£…Python 3.11"

for weights, desc in test_weights:
    retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=weights
    )
    results = retriever.invoke(query)
    print(f"\n{desc} (å‘é‡:{weights[0]}, BM25:{weights[1]})")
    print(f"Top-1: {results[0].page_content[:80]}...")
```

**æƒé‡é€‰æ‹©å»ºè®®**:

| åœºæ™¯ | æ¨èæƒé‡(å‘é‡:BM25) | åŸå›  |
|------|---------------------|------|
| é—®ç­”ç³»ç»Ÿ | 0.7:0.3 | é‡è§†è¯­ä¹‰ç†è§£ |
| æ–‡æ¡£æœç´¢ | 0.5:0.5 | å¹³è¡¡è¯­ä¹‰å’Œå…³é”®è¯ |
| ä»£ç æœç´¢ | 0.3:0.7 | ç²¾ç¡®åŒ¹é…å‡½æ•°åã€å˜é‡å |
| äº§å“æŸ¥è¯¢ | 0.4:0.6 | å‹å·ã€è§„æ ¼ç­‰ç²¾ç¡®åŒ¹é… |

---

#### 1.2.3 å®Œæ•´RAGç³»ç»Ÿ(æ··åˆæ£€ç´¢)

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# æ­¥éª¤1: åˆ›å»ºæ··åˆæ£€ç´¢å™¨(åŒä¸Š)
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.6, 0.4]
)

# æ­¥éª¤2: åŒ…è£…ä¸ºå·¥å…·
@tool
def search_docs(query: str) -> str:
    """æœç´¢æ–‡æ¡£åº“,è¿”å›ç›¸å…³ä¿¡æ¯ã€‚

    é€‚ç”¨äº:
    - æŸ¥æ‰¾äº§å“æ–‡æ¡£
    - æœç´¢æŠ€æœ¯èµ„æ–™
    - è·å–é…ç½®è¯´æ˜
    """
    results = ensemble_retriever.invoke(query)
    # æ ¼å¼åŒ–ç»“æœ
    formatted = []
    for i, doc in enumerate(results[:3], 1):
        formatted.append(
            f"[æ–‡æ¡£{i}]\n"
            f"å†…å®¹: {doc.page_content}\n"
            f"æ¥æº: {doc.metadata.get('source', 'unknown')}"
        )
    return "\n\n".join(formatted)

# æ­¥éª¤3: åˆ›å»ºAgent
agent = create_agent(
    model="gpt-4",
    tools=[search_docs],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£æœç´¢åŠ©æ‰‹,å¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥æ‰¾æŠ€æœ¯æ–‡æ¡£ã€‚

ä½¿ç”¨ search_docs å·¥å…·æœç´¢ç›¸å…³ä¿¡æ¯,ç„¶åç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚"""
)

# æ­¥éª¤4: æŸ¥è¯¢
result = agent.invoke({
    "messages": [{"role": "user", "content": "Python 3.11ç›¸æ¯”3.10æœ‰å“ªäº›æ€§èƒ½æå‡?"}]
})
print(result["messages"][-1].content)
```

---

#### 1.2.4 RRF(Reciprocal Rank Fusion)ç®—æ³•

**RRFåŸç†**:
```
å¯¹äºæ–‡æ¡£d,å…¶RRFåˆ†æ•° = Î£ (1 / (k + rank_i(d)))

å…¶ä¸­:
- rank_i(d):æ–‡æ¡£dåœ¨ç¬¬iä¸ªæ£€ç´¢å™¨ä¸­çš„æ’å
- k:å¸¸æ•°(é€šå¸¸ä¸º60)
```

**æ‰‹åŠ¨å®ç°RRF**:

```python
def reciprocal_rank_fusion(
    retriever_results: list[list],  # å¤šä¸ªæ£€ç´¢å™¨çš„ç»“æœåˆ—è¡¨
    k: int = 60
) -> list:
    """
    RRFç®—æ³•å®ç°

    Args:
        retriever_results: [[doc1, doc2, ...], [doc3, doc1, ...], ...]
        k: RRFå¸¸æ•°

    Returns:
        èåˆåçš„æ–‡æ¡£åˆ—è¡¨(æŒ‰RRFåˆ†æ•°æ’åº)
    """
    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„RRFåˆ†æ•°
    doc_scores = {}

    for retriever_docs in retriever_results:
        for rank, doc in enumerate(retriever_docs, start=1):
            doc_id = doc.metadata.get('id', id(doc))

            # RRFåˆ†æ•°ç´¯åŠ 
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {'doc': doc, 'score': 0}

            doc_scores[doc_id]['score'] += 1 / (k + rank)

    # æŒ‰åˆ†æ•°æ’åº
    sorted_docs = sorted(
        doc_scores.values(),
        key=lambda x: x['score'],
        reverse=True
    )

    return [item['doc'] for item in sorted_docs]

# ä½¿ç”¨ç¤ºä¾‹
vector_results = vector_retriever.invoke("Python 3.11")
bm25_results = bm25_retriever.invoke("Python 3.11")

fused_results = reciprocal_rank_fusion([vector_results, bm25_results])
print(f"èåˆåtop-3æ–‡æ¡£:")
for i, doc in enumerate(fused_results[:3], 1):
    print(f"{i}. {doc.page_content[:80]}...")
```

**RRF vs åŠ æƒå¹³å‡**:

| æ–¹æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| RRF | ä¸éœ€è¦å½’ä¸€åŒ–åˆ†æ•°,é²æ£’æ€§å¼º | å¿½ç•¥åŸå§‹åˆ†æ•°çš„ç»å¯¹å€¼ | æ£€ç´¢å™¨è¯„åˆ†å°ºåº¦ä¸åŒ |
| åŠ æƒå¹³å‡ | ä¿ç•™åŸå§‹åˆ†æ•°ä¿¡æ¯ | éœ€è¦å½’ä¸€åŒ–,å¯¹åˆ†æ•°å°ºåº¦æ•æ„Ÿ | æ£€ç´¢å™¨è¯„åˆ†å¯æ¯” |

---

### 1.3 æ··åˆæ£€ç´¢å®æˆ˜:ç”µå•†äº§å“æœç´¢

#### 1.3.1 åœºæ™¯è¯´æ˜

**ä¸šåŠ¡éœ€æ±‚**:
- ç”¨æˆ·æŸ¥è¯¢:"iPhone 14 Pro 256GB ç´«è‰²"
- éœ€æ±‚1:ç²¾ç¡®åŒ¹é…å‹å·(iPhone 14 Pro)
- éœ€æ±‚2:ç²¾ç¡®åŒ¹é…å®¹é‡(256GB)
- éœ€æ±‚3:ç²¾ç¡®åŒ¹é…é¢œè‰²(ç´«è‰²)
- éœ€æ±‚4:ç†è§£"æœ€æ–°æ¬¾"ã€"æ——èˆ°æœº"ç­‰è¯­ä¹‰

**ä¸ºä»€ä¹ˆéœ€è¦æ··åˆæ£€ç´¢**:
- çº¯å‘é‡æ£€ç´¢:å¯èƒ½è¿”å›iPhone 13 Pro(è¯­ä¹‰ç›¸ä¼¼)
- çº¯BM25æ£€ç´¢:å¯èƒ½è¿”å›"iPhone 14 æ™®é€šç‰ˆ 256GB"(å…³é”®è¯åŒ¹é…)
- æ··åˆæ£€ç´¢:åŒæ—¶æ»¡è¶³å‹å·ã€å®¹é‡ã€é¢œè‰²çš„ç²¾ç¡®åŒ¹é…

---

#### 1.3.2 å®Œæ•´å®ç°

```python
from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.agents import create_agent
from langchain_core.tools import tool
import json

# æ­¥éª¤1: å‡†å¤‡äº§å“æ•°æ®
products_data = [
    {
        "id": "1",
        "name": "iPhone 14 Pro 256GB æ·±ç©ºé»‘",
        "category": "æ‰‹æœº",
        "brand": "Apple",
        "model": "iPhone 14 Pro",
        "storage": "256GB",
        "color": "æ·±ç©ºé»‘",
        "price": 8999,
        "description": "Apple æœ€æ–°æ——èˆ°æ‰‹æœº,æ­è½½A16èŠ¯ç‰‡,4800ä¸‡åƒç´ ä¸»æ‘„,æ”¯æŒçµåŠ¨å²›äº¤äº’"
    },
    {
        "id": "2",
        "name": "iPhone 14 Pro 256GB ç´«è‰²",
        "category": "æ‰‹æœº",
        "brand": "Apple",
        "model": "iPhone 14 Pro",
        "storage": "256GB",
        "color": "ç´«è‰²",
        "price": 8999,
        "description": "Apple æ——èˆ°æœºå‹,ç´«è‰²é…è‰²,256GBå¤§å®¹é‡å­˜å‚¨,ä¸“ä¸šçº§æ‘„å½±ç³»ç»Ÿ"
    },
    {
        "id": "3",
        "name": "iPhone 13 Pro 256GB è¿œå³°è“",
        "category": "æ‰‹æœº",
        "brand": "Apple",
        "model": "iPhone 13 Pro",
        "storage": "256GB",
        "color": "è¿œå³°è“",
        "price": 7499,
        "description": "ä¸Šä¸€ä»£æ——èˆ°,A15èŠ¯ç‰‡,ä¸‰æ‘„ç³»ç»Ÿ,æ€§ä»·æ¯”ä¹‹é€‰"
    }
]

# è½¬æ¢ä¸ºDocumentå¯¹è±¡
from langchain_core.documents import Document

documents = []
for product in products_data:
    # æ„å»ºå¯Œæ–‡æœ¬æè¿°(ä¾¿äºæ£€ç´¢)
    text = f"""
äº§å“åç§°:{product['name']}
å“ç‰Œ:{product['brand']}
å‹å·:{product['model']}
å®¹é‡:{product['storage']}
é¢œè‰²:{product['color']}
ä»·æ ¼:Â¥{product['price']}
æè¿°:{product['description']}
"""
    doc = Document(
        page_content=text,
        metadata=product
    )
    documents.append(doc)

# æ­¥éª¤2: åˆ›å»ºæ··åˆæ£€ç´¢å™¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents, embeddings)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 3

# äº§å“æœç´¢:BM25æƒé‡æ›´é«˜(ç²¾ç¡®åŒ¹é…å‹å·ã€å®¹é‡ã€é¢œè‰²)
product_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.4, 0.6]  # æ›´é‡è§†ç²¾ç¡®åŒ¹é…
)

# æ­¥éª¤3: åˆ›å»ºæœç´¢å·¥å…·
@tool
def search_products(query: str) -> str:
    """æœç´¢äº§å“åº“,è¿”å›åŒ¹é…çš„å•†å“ä¿¡æ¯ã€‚

    é€‚ç”¨äºæŸ¥è¯¢:
    - æ‰‹æœºå‹å·æœç´¢(å¦‚"iPhone 14 Pro")
    - ç‰¹å®šé…ç½®æœç´¢(å¦‚"256GB ç´«è‰²")
    - ä»·æ ¼èŒƒå›´æŸ¥è¯¢
    """
    results = product_retriever.invoke(query)

    formatted = []
    for i, doc in enumerate(results, 1):
        meta = doc.metadata
        formatted.append(
            f"ã€å•†å“{i}ã€‘{meta['name']}\n"
            f"ä»·æ ¼:Â¥{meta['price']}\n"
            f"æè¿°:{meta['description']}"
        )

    return "\n\n".join(formatted)

# æ­¥éª¤4: åˆ›å»ºè´­ç‰©åŠ©æ‰‹Agent
shopping_agent = create_agent(
    model="gpt-4",
    tools=[search_products],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´­ç‰©åŠ©æ‰‹,å¸®åŠ©ç”¨æˆ·æŸ¥æ‰¾å’Œæ¨èå•†å“ã€‚

ä½¿ç”¨ search_products å·¥å…·æœç´¢å•†å“ä¿¡æ¯,ç„¶åç»™å‡ºå‡†ç¡®çš„æ¨èã€‚

æ³¨æ„:
- ä¼˜å…ˆæ¨èå®Œå…¨åŒ¹é…ç”¨æˆ·éœ€æ±‚çš„å•†å“
- å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…,æ¨èç›¸è¿‘çš„æ›¿ä»£å“
- æ¸…æ™°è¯´æ˜å•†å“çš„ä»·æ ¼å’Œä¸»è¦ç‰¹ç‚¹"""
)

# æ­¥éª¤5: æµ‹è¯•æŸ¥è¯¢
test_queries = [
    "æˆ‘æƒ³ä¹°iPhone 14 Pro 256GB ç´«è‰²",
    "æœ‰æ²¡æœ‰256GBçš„ç´«è‰²æ‰‹æœº",
    "8000å…ƒå·¦å³çš„Appleæ——èˆ°æœº"
]

for query in test_queries:
    print(f"\n{'='*50}")
    print(f"ç”¨æˆ·æŸ¥è¯¢:{query}")
    print('='*50)

    result = shopping_agent.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    print(result["messages"][-1].content)
```

---

### 1.4 æ··åˆæ£€ç´¢æœ€ä½³å®è·µ

#### 1.4.1 æƒé‡è°ƒä¼˜ç­–ç•¥

**A/Bæµ‹è¯•ä¸åŒæƒé‡**:

```python
from typing import List
from langchain_core.documents import Document

def evaluate_retrieval(
    retriever,
    test_queries: List[str],
    ground_truth: List[List[str]]  # æ¯ä¸ªæŸ¥è¯¢çš„æ­£ç¡®æ–‡æ¡£IDåˆ—è¡¨
) -> dict:
    """è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½"""
    precisions = []
    recalls = []

    for query, truth_ids in zip(test_queries, ground_truth):
        results = retriever.invoke(query)
        retrieved_ids = [doc.metadata.get('id') for doc in results]

        # è®¡ç®—Precision@K
        hits = len(set(retrieved_ids) & set(truth_ids))
        precision = hits / len(retrieved_ids) if retrieved_ids else 0
        recall = hits / len(truth_ids) if truth_ids else 0

        precisions.append(precision)
        recalls.append(recall)

    return {
        'precision': sum(precisions) / len(precisions),
        'recall': sum(recalls) / len(recalls)
    }

# æµ‹è¯•ä¸åŒæƒé‡
test_queries = [
    "iPhone 14 Pro 256GB ç´«è‰²",
    "8000å…ƒå·¦å³çš„æ——èˆ°æœº",
    "Appleæœ€æ–°æ‰‹æœº"
]

ground_truth = [
    ["2"],  # ç²¾ç¡®åŒ¹é…
    ["1", "2", "3"],  # ä»·æ ¼èŒƒå›´
    ["1", "2"]  # æœ€æ–°å‹å·
]

weights_to_test = [
    (0.3, 0.7),
    (0.4, 0.6),
    (0.5, 0.5),
    (0.6, 0.4),
    (0.7, 0.3)
]

print("æƒé‡è°ƒä¼˜ç»“æœ:\n")
for vec_weight, bm25_weight in weights_to_test:
    retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[vec_weight, bm25_weight]
    )

    metrics = evaluate_retrieval(retriever, test_queries, ground_truth)
    print(f"å‘é‡:{vec_weight:.1f}, BM25:{bm25_weight:.1f} => "
          f"Precision: {metrics['precision']:.2%}, "
          f"Recall: {metrics['recall']:.2%}")
```

---

#### 1.4.2 ä½•æ—¶ä½¿ç”¨æ··åˆæ£€ç´¢

**å†³ç­–æ ‘**:

```
æŸ¥è¯¢ç±»å‹
â”œâ”€â”€ åŒ…å«ç²¾ç¡®å…³é”®è¯(å‹å·ã€ç‰ˆæœ¬ã€è§„æ ¼)
â”‚   â””â”€â”€ ä½¿ç”¨æ··åˆæ£€ç´¢(BM25æƒé‡0.5-0.7)
â”œâ”€â”€ è‡ªç„¶è¯­è¨€é—®å¥
â”‚   â””â”€â”€ ä½¿ç”¨æ··åˆæ£€ç´¢(å‘é‡æƒé‡0.6-0.7)
â”œâ”€â”€ çº¯è¯­ä¹‰æŸ¥è¯¢(æ¦‚å¿µã€ä¸»é¢˜)
â”‚   â””â”€â”€ ä»…ä½¿ç”¨å‘é‡æ£€ç´¢
â””â”€â”€ ä»£ç /å‘½ä»¤æœç´¢
    â””â”€â”€ ä½¿ç”¨æ··åˆæ£€ç´¢(BM25æƒé‡0.7-0.8)
```

**å®é™…æ¡ˆä¾‹**:

| æŸ¥è¯¢ç¤ºä¾‹ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|---------|---------|------|
| "Python 3.11 æ–°ç‰¹æ€§" | æ··åˆ(0.4:0.6) | "3.11"éœ€è¦ç²¾ç¡®åŒ¹é… |
| "å¦‚ä½•æå‡ç¨‹åºæ€§èƒ½" | å‘é‡ä¸ºä¸»(0.7:0.3) | çº¯è¯­ä¹‰æŸ¥è¯¢ |
| "numpy.array()ç”¨æ³•" | BM25ä¸ºä¸»(0.3:0.7) | å‡½æ•°åç²¾ç¡®åŒ¹é… |
| "æ·±åº¦å­¦ä¹ å…¥é—¨æ•™ç¨‹" | å‘é‡ä¸ºä¸»(0.7:0.3) | ä¸»é¢˜ç›¸å…³æ€§ |

---

### å°ç»“

**æ··åˆæ£€ç´¢æ ¸å¿ƒè¦ç‚¹**:

1. **ä½•æ—¶ä½¿ç”¨**:
   - âœ… æŸ¥è¯¢åŒ…å«ç²¾ç¡®å…³é”®è¯(ç‰ˆæœ¬å·ã€å‹å·ã€ä¸“æœ‰åè¯)
   - âœ… éœ€è¦å¹³è¡¡è¯­ä¹‰ç†è§£å’Œç²¾ç¡®åŒ¹é…
   - âŒ çº¯æ¦‚å¿µæ€§é—®é¢˜å¯ç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢

2. **æƒé‡é€‰æ‹©**:
   - é—®ç­”ç³»ç»Ÿ:0.6-0.7(å‘é‡): 0.3-0.4(BM25)
   - äº§å“æœç´¢:0.4(å‘é‡): 0.6(BM25)
   - ä»£ç æœç´¢:0.3(å‘é‡): 0.7(BM25)

3. **èåˆç®—æ³•**:
   - ç®€å•åŠ æƒ:LangChainçš„EnsembleRetriever
   - RRF:æ›´é²æ£’,é€‚åˆåˆ†æ•°å°ºåº¦ä¸åŒçš„æ£€ç´¢å™¨

4. **æ€§èƒ½æå‡**:
   - å¬å›ç‡:+20-30%
   - ç²¾ç¡®ç‡:+15-25%
   - æŸ¥è¯¢å»¶è¿Ÿ:+30-50ms(å¯æ¥å—)

**ä¸‹ä¸€ç« é¢„å‘Š**:
ç¬¬2ç« å°†æ·±å…¥æ¢è®¨**é‡æ’åºæŠ€æœ¯(Reranking)**,è¿›ä¸€æ­¥æå‡top-kç»“æœçš„ç²¾ç¡®åº¦ã€‚

---

## ç¬¬2ç« :é‡æ’åºæŠ€æœ¯(Reranking)

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦é‡æ’åº

#### 2.1.1 æ£€ç´¢å™¨çš„å±€é™æ€§

**é—®é¢˜åœºæ™¯**:
```python
# ç”¨æˆ·æŸ¥è¯¢:"å¦‚ä½•åœ¨Pythonä¸­å®ç°å¤šçº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼?"

# å‘é‡æ£€ç´¢top-5ç»“æœ:
# 1. "Pythonå•ä¾‹æ¨¡å¼å®ç°"(ç›¸å…³åº¦:0.85)
# 2. "å¤šçº¿ç¨‹ç¼–ç¨‹åŸºç¡€"(ç›¸å…³åº¦:0.82)
# 3. "Pythonè®¾è®¡æ¨¡å¼å¤§å…¨"(ç›¸å…³åº¦:0.80)â† å†…å®¹å¤ªæ³›åŒ–
# 4. "çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼"(ç›¸å…³åº¦:0.78)â† åº”è¯¥æ’æ›´å‰
# 5. "Pythonçº¿ç¨‹é”ä½¿ç”¨"(ç›¸å…³åº¦:0.75)

# é—®é¢˜:
# - ç¬¬3ä¸ªæ–‡æ¡£å¤ªæ³›åŒ–,ä½†ç›¸å…³åº¦åˆ†æ•°é«˜
# - ç¬¬4ä¸ªæ–‡æ¡£æœ€ç›¸å…³,ä½†æ’åºé å
```

**å‘é‡æ£€ç´¢çš„é—®é¢˜**:
1. **è¯­ä¹‰ç›¸ä¼¼â‰ æŸ¥è¯¢ç›¸å…³**:æ–‡æ¡£å¯èƒ½è¯­ä¹‰ç›¸ä¼¼,ä½†ä¸å›ç­”å…·ä½“é—®é¢˜
2. **ç²—ç²’åº¦æ’åº**:åŸºäºembeddingçš„ä½™å¼¦ç›¸ä¼¼åº¦,æ— æ³•ç†è§£æŸ¥è¯¢æ„å›¾
3. **ä¸Šä¸‹æ–‡ç¼ºå¤±**:ä¸è€ƒè™‘æŸ¥è¯¢å’Œæ–‡æ¡£çš„äº¤äº’å…³ç³»

---

#### 2.1.2 é‡æ’åºçš„ä½œç”¨

**é‡æ’åºæµç¨‹**:
```
åŸå§‹æ£€ç´¢(å¿«é€Ÿ,ç²—æ’)â†’ top-100å€™é€‰
         â†“
é‡æ’åºæ¨¡å‹(ç²¾ç»†,æ…¢)â†’ top-5æœ€ç»ˆç»“æœ
         â†“
LLMç”Ÿæˆ
```

**ä¼˜åŠ¿**:
- **ç²¾ç»†åŒ–ç†è§£**:ä½¿ç”¨Cross-Encoderæ·±åº¦ç†è§£æŸ¥è¯¢-æ–‡æ¡£åŒ¹é…åº¦
- **æå‡ç²¾ç¡®åº¦**:top-5ç»“æœçš„ç²¾ç¡®åº¦æå‡15-30%
- **æˆæœ¬ä¼˜åŒ–**:åªå¯¹top-Kå€™é€‰é‡æ’,è®¡ç®—å¼€é”€å¯æ§

---

#### 2.1.3 é‡æ’åº vs æ£€ç´¢å™¨

| ç»´åº¦ | æ£€ç´¢å™¨(Retriever) | é‡æ’åºå™¨(Reranker) |
|------|-------------------|-------------------|
| **æ¨¡å‹ç»“æ„** | Bi-Encoder(æŸ¥è¯¢å’Œæ–‡æ¡£åˆ†åˆ«ç¼–ç ) | Cross-Encoder(è”åˆç¼–ç ) |
| **é€Ÿåº¦** | å¿«(é¢„è®¡ç®—æ–‡æ¡£å‘é‡) | æ…¢(å®æ—¶è®¡ç®—äº¤äº’åˆ†æ•°) |
| **ç²¾åº¦** | ä¸­ç­‰(ä½™å¼¦ç›¸ä¼¼åº¦) | é«˜(æ·±åº¦è¯­ä¹‰åŒ¹é…) |
| **é€‚ç”¨é˜¶æ®µ** | åˆç­›(ä»ç™¾ä¸‡æ–‡æ¡£æ‰¾top-100) | ç²¾æ’(ä»top-100æ‰¾top-5) |
| **è®¡ç®—æˆæœ¬** | ä½ | é«˜ |

---

### 2.2 LangChainé‡æ’åºå®ç°

#### 2.2.1 åŸºäºContextualCompressionRetriever

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader

# æ­¥éª¤1: å‡†å¤‡æ–‡æ¡£
loader = DirectoryLoader("./docs", glob="**/*.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
splits = splitter.split_documents(documents)

# æ­¥éª¤2: åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨(ç²—æ’)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(splits, embeddings)
base_retriever = vectorstore.as_retriever(
    search_kwargs={"k": 20}  # å…ˆæ£€ç´¢20ä¸ªå€™é€‰
)

# æ­¥éª¤3: åˆ›å»ºLLMå‹ç¼©å™¨(ç²¾æ’)
llm = ChatOpenAI(model="gpt-4", temperature=0)
compressor = LLMChainExtractor.from_llm(llm)

# æ­¥éª¤4: ç»„åˆä¸ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# æ­¥éª¤5: ä½¿ç”¨
query = "å¦‚ä½•åœ¨Pythonä¸­å®ç°çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼?"
compressed_docs = compression_retriever.invoke(query)

print(f"åŸå§‹æ£€ç´¢å™¨è¿”å›:20ä¸ªæ–‡æ¡£")
print(f"é‡æ’åºåè¿”å›:{len(compressed_docs)}ä¸ªæ–‡æ¡£\n")

for i, doc in enumerate(compressed_docs, 1):
    print(f"ã€æ–‡æ¡£{i}ã€‘")
    print(f"å†…å®¹:{doc.page_content[:150]}...")
    print(f"æ¥æº:{doc.metadata.get('source', 'unknown')}\n")
```

**å·¥ä½œåŸç†**:
1. base_retrieveræ£€ç´¢20ä¸ªå€™é€‰æ–‡æ¡£
2. LLMChainExtractorä½¿ç”¨LLMè¯„ä¼°æ¯ä¸ªæ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
3. æå–æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ
4. è¿”å›ç²¾æ’åçš„æ–‡æ¡£(é€šå¸¸<10ä¸ª)

---

#### 2.2.2 ä½¿ç”¨EmbeddingsFilter(åŸºäºembeddingè·ç¦»)

```python
from langchain_community.retrievers.document_compressors import EmbeddingsFilter

# åˆ›å»ºembeddingè¿‡æ»¤å™¨
embeddings_filter = EmbeddingsFilter(
    embeddings=OpenAIEmbeddings(),
    similarity_threshold=0.75  # ç›¸ä¼¼åº¦é˜ˆå€¼
)

# ç»„åˆä¸ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter,
    base_retriever=base_retriever
)

# ä½¿ç”¨
query = "Pythonå•ä¾‹æ¨¡å¼å®ç°"
filtered_docs = compression_retriever.invoke(query)

print(f"è¿‡æ»¤å‰:{20}ä¸ªæ–‡æ¡£")
print(f"è¿‡æ»¤å:{len(filtered_docs)}ä¸ªæ–‡æ¡£(ç›¸ä¼¼åº¦â‰¥0.75)")
```

**ä¼˜åŠ¿**:
- é€Ÿåº¦å¿«(åŸºäºé¢„è®¡ç®—çš„embedding)
- æˆæœ¬ä½(æ— éœ€è°ƒç”¨LLM)

**åŠ£åŠ¿**:
- ç²¾åº¦ä¸å¦‚LLMå‹ç¼©å™¨
- ä»åŸºäºä½™å¼¦ç›¸ä¼¼åº¦

---

#### 2.2.3 æœ¬åœ°é‡æ’åºæ¨¡å‹(Cross-Encoder)

```python
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from typing import List

class CrossEncoderReranker:
    """åŸºäºCross-Encoderçš„é‡æ’åºå™¨"""

    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        """
        åˆå§‹åŒ–Cross-Encoderé‡æ’åºå™¨

        Args:
            model_name: HuggingFaceæ¨¡å‹åç§°
                - BAAI/bge-reranker-base(è‹±æ–‡,é€Ÿåº¦å¿«)
                - BAAI/bge-reranker-large(è‹±æ–‡,ç²¾åº¦é«˜)
                - BAAI/bge-reranker-v2-m3(å¤šè¯­è¨€,æ¨èä¸­æ–‡)
        """
        self.model = CrossEncoder(model_name)

    def rerank(
        self,
        query: str,
        documents: List[Document],
        top_n: int = 5
    ) -> List[Document]:
        """
        é‡æ’åºæ–‡æ¡£åˆ—è¡¨

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_n: è¿”å›top-Nä¸ªæ–‡æ¡£

        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[query, doc.page_content] for doc in documents]

        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = self.model.predict(pairs)

        # æŒ‰åˆ†æ•°æ’åº
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›top-N
        return [doc for doc, score in doc_scores[:top_n]]

# ä½¿ç”¨ç¤ºä¾‹
reranker = CrossEncoderReranker(model_name="BAAI/bge-reranker-v2-m3")

# å…ˆç”¨å‘é‡æ£€ç´¢è·å–å€™é€‰
base_results = base_retriever.invoke("Pythonå•ä¾‹æ¨¡å¼çº¿ç¨‹å®‰å…¨å®ç°")
print(f"åˆç­›æ–‡æ¡£æ•°:{len(base_results)}")

# é‡æ’åº
reranked_results = reranker.rerank(
    query="Pythonå•ä¾‹æ¨¡å¼çº¿ç¨‹å®‰å…¨å®ç°",
    documents=base_results,
    top_n=5
)

print(f"\né‡æ’åºåtop-5:")
for i, doc in enumerate(reranked_results, 1):
    print(f"{i}. {doc.page_content[:80]}...")
```

**æ¨èæ¨¡å‹å¯¹æ¯”**:

| æ¨¡å‹ | è¯­è¨€ | å‚æ•°é‡ | é€Ÿåº¦ | ç²¾åº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|------|------|---------|
| `BAAI/bge-reranker-base` | è‹±æ–‡ | 110M | å¿« | â˜…â˜…â˜… | è‹±æ–‡é€šç”¨ |
| `BAAI/bge-reranker-large` | è‹±æ–‡ | 340M | ä¸­ | â˜…â˜…â˜…â˜… | è‹±æ–‡ç²¾åº¦ä¼˜å…ˆ |
| `BAAI/bge-reranker-v2-m3` | å¤šè¯­è¨€ | 560M | ä¸­ | â˜…â˜…â˜…â˜…â˜… | ä¸­æ–‡/å¤šè¯­è¨€ |
| `cross-encoder/ms-marco-MiniLM-L-6-v2` | è‹±æ–‡ | 23M | å¿« | â˜…â˜…â˜… | é€Ÿåº¦ä¼˜å…ˆ |

---

#### 2.2.4 é›†æˆåˆ°å®Œæ•´RAGç³»ç»Ÿ

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# æ­¥éª¤1: åˆ›å»ºå¸¦é‡æ’åºçš„æ£€ç´¢å™¨
class RerankedRetriever:
    """å¸¦é‡æ’åºçš„æ£€ç´¢å™¨"""

    def __init__(
        self,
        base_retriever,
        reranker,
        initial_k: int = 20,
        final_k: int = 5
    ):
        self.base_retriever = base_retriever
        self.reranker = reranker
        self.initial_k = initial_k
        self.final_k = final_k

    def invoke(self, query: str) -> List[Document]:
        """æ‰§è¡Œæ£€ç´¢å’Œé‡æ’åº"""
        # ç²—æ’:æ£€ç´¢initial_kä¸ªå€™é€‰
        candidates = self.base_retriever.invoke(query)

        # ç²¾æ’:é‡æ’åºå¹¶è¿”å›final_kä¸ª
        reranked = self.reranker.rerank(
            query=query,
            documents=candidates,
            top_n=self.final_k
        )

        return reranked

# åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹
reranked_retriever = RerankedRetriever(
    base_retriever=base_retriever,  # ç²—æ’æ£€ç´¢å™¨
    reranker=CrossEncoderReranker("BAAI/bge-reranker-v2-m3"),  # é‡æ’åºå™¨
    initial_k=20,  # ç²—æ’top-20
    final_k=5      # ç²¾æ’top-5
)

# æ­¥éª¤2: åŒ…è£…ä¸ºå·¥å…·
@tool
def search_docs_with_rerank(query: str) -> str:
    """æœç´¢æ–‡æ¡£åº“å¹¶é‡æ’åº,è¿”å›æœ€ç›¸å…³çš„å†…å®¹ã€‚

    ä½¿ç”¨ä¸¤é˜¶æ®µæ£€ç´¢:
    1. å‘é‡æ£€ç´¢:å¿«é€Ÿç­›é€‰top-20å€™é€‰
    2. Cross-Encoderé‡æ’åº:ç²¾ç¡®æ’åºtop-5ç»“æœ
    """
    results = reranked_retriever.invoke(query)

    formatted = []
    for i, doc in enumerate(results, 1):
        formatted.append(
            f"ã€æ–‡æ¡£{i}ã€‘\n"
            f"{doc.page_content}\n"
            f"æ¥æº:{doc.metadata.get('source', 'unknown')}"
        )

    return "\n\n".join(formatted)

# æ­¥éª¤3: åˆ›å»ºAgent
agent = create_agent(
    model="gpt-4",
    tools=[search_docs_with_rerank],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹,å¸®åŠ©ç”¨æˆ·æŸ¥æ‰¾å’Œç†è§£æŠ€æœ¯æ–‡æ¡£ã€‚

ä½¿ç”¨ search_docs_with_rerank å·¥å…·æœç´¢ç›¸å…³æ–‡æ¡£,ç„¶åç»™å‡ºè¯¦ç»†çš„æŠ€æœ¯è§£ç­”ã€‚

æ³¨æ„:
- ä¼˜å…ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
- å¦‚æœæ–‡æ¡£ä¸­æœ‰ä»£ç ç¤ºä¾‹,è¯·å¼•ç”¨
- ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§"""
)

# æ­¥éª¤4: æµ‹è¯•
result = agent.invoke({
    "messages": [{"role": "user", "content": "Pythonä¸­å¦‚ä½•å®ç°çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼?è¯·ç»™å‡ºå®Œæ•´ä»£ç ç¤ºä¾‹ã€‚"}]
})
print(result["messages"][-1].content)
```

**æ€§èƒ½å¯¹æ¯”**:

| æ£€ç´¢æ–¹æ¡ˆ | ç²¾ç¡®åº¦@5 | æŸ¥è¯¢å»¶è¿Ÿ | æˆæœ¬ |
|---------|---------|---------|------|
| ä»…å‘é‡æ£€ç´¢ | 65% | 50ms | $ |
| å‘é‡+Embeddingè¿‡æ»¤ | 72% | 80ms | $ |
| å‘é‡+LLMå‹ç¼© | 85% | 2000ms | $$$ |
| å‘é‡+Cross-Encoder | 88% | 150ms | $ |

---

### 2.3 æŸ¥è¯¢æ”¹å†™(Query Rewriting)

#### 2.3.1 å¤šæŸ¥è¯¢ç”Ÿæˆ(Multi-Query)

**é—®é¢˜**:ç”¨æˆ·æŸ¥è¯¢å¯èƒ½è¡¨è¾¾ä¸æ¸…æ™°
```python
# åŸå§‹æŸ¥è¯¢:"æ€ä¹ˆè®©Pythonå¿«ä¸€ç‚¹"
# é—®é¢˜:
# - è¡¨è¾¾æ¨¡ç³Š("å¿«ä¸€ç‚¹"æŒ‡ä»€ä¹ˆ?)
# - å¯èƒ½é—æ¼ç›¸å…³æ–‡æ¡£

# è§£å†³æ–¹æ¡ˆ:ç”Ÿæˆå¤šä¸ªæ”¹å†™æŸ¥è¯¢
# 1. "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½"
# 2. "Pythonç¨‹åºåŠ é€Ÿæ–¹æ³•"
# 3. "æå‡Pythonæ‰§è¡Œæ•ˆç‡çš„æŠ€å·§"
```

**å®ç°**:

```python
from langchain.retrievers import MultiQueryRetriever
from langchain_openai import ChatOpenAI

# åˆ›å»ºMulti-Queryæ£€ç´¢å™¨
llm = ChatOpenAI(model="gpt-4", temperature=0)

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=llm
)

# ä½¿ç”¨(è‡ªåŠ¨ç”Ÿæˆ3-5ä¸ªæ”¹å†™æŸ¥è¯¢å¹¶æ£€ç´¢)
query = "æ€ä¹ˆè®©Pythonå¿«ä¸€ç‚¹"
results = multi_query_retriever.invoke(query)

print(f"æ£€ç´¢åˆ°{len(results)}ä¸ªæ–‡æ¡£(å»é‡å)")
for doc in results[:3]:
    print(f"- {doc.page_content[:80]}...")
```

**å·¥ä½œæµç¨‹**:
```
ç”¨æˆ·æŸ¥è¯¢:"æ€ä¹ˆè®©Pythonå¿«ä¸€ç‚¹"
    â†“
LLMç”Ÿæˆæ”¹å†™æŸ¥è¯¢:
    â”œâ”€â”€ "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½"
    â”œâ”€â”€ "Pythonç¨‹åºåŠ é€Ÿæ–¹æ³•"
    â””â”€â”€ "æå‡Pythonæ‰§è¡Œæ•ˆç‡çš„æŠ€å·§"
    â†“
å¹¶è¡Œæ£€ç´¢3ä¸ªæŸ¥è¯¢
    â†“
åˆå¹¶ + å»é‡
    â†“
è¿”å›ç»“æœ
```

---

### å°ç»“

**é‡æ’åºæŠ€æœ¯æ ¸å¿ƒè¦ç‚¹**:

1. **ä½•æ—¶ä½¿ç”¨é‡æ’åº**:
   - âœ… ç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯(å®¢æœã€åŒ»ç–—ã€æ³•å¾‹)
   - âœ… top-Kç»“æœè´¨é‡ä¸ç¨³å®š
   - âŒ å®æ—¶æ€§è¦æ±‚æé«˜(<100ms)

2. **é‡æ’åºæ–¹æ¡ˆé€‰æ‹©**:
   - **Embeddingè¿‡æ»¤**:é€Ÿåº¦å¿«,æˆæœ¬ä½,ç²¾åº¦æå‡æœ‰é™(+7%)
   - **Cross-Encoder**:å¹³è¡¡æ–¹æ¡ˆ,ç²¾åº¦é«˜(+23%),å»¶è¿Ÿå¯æ¥å—(+100ms)
   - **LLMå‹ç¼©**:ç²¾åº¦æœ€é«˜(+20%),æˆæœ¬é«˜(æ¯æŸ¥è¯¢$0.02-0.05)

3. **æŸ¥è¯¢æ”¹å†™æŠ€æœ¯**:
   - **Multi-Query**:å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢,å¬å›ç‡+15%

**ä¸‹ä¸€ç« é¢„å‘Š**:
ç¬¬3ç« å°†æ¢è®¨**çŸ¥è¯†å›¾è°±RAG(GraphRAG)**,å¦‚ä½•åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†æå‡å¤šè·³æ¨ç†èƒ½åŠ›ã€‚

---

## ç¬¬3ç« :çŸ¥è¯†å›¾è°±RAG(GraphRAG)

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦çŸ¥è¯†å›¾è°±RAG

#### 3.1.1 å‘é‡RAGçš„å±€é™æ€§

**é—®é¢˜åœºæ™¯**:
```
æŸ¥è¯¢:"å¼ ä¸‰çš„é¢†å¯¼çš„é¢†å¯¼æ˜¯è°?"

å‘é‡RAG:
â”œâ”€â”€ æ£€ç´¢åˆ°:"å¼ ä¸‰çš„ç›´æ¥é¢†å¯¼æ˜¯æå››"
â”œâ”€â”€ æ£€ç´¢åˆ°:"æå››çš„ç»©æ•ˆè¯„ä¼°ä¸ºä¼˜ç§€"
â””â”€â”€ âŒ æ— æ³•æ¨ç†å‡º"å¼ ä¸‰çš„é¢†å¯¼çš„é¢†å¯¼"

çŸ¥è¯†å›¾è°±RAG:
â”œâ”€â”€ å®ä½“:å¼ ä¸‰ --[reports_to]--> æå››
â”œâ”€â”€ å®ä½“:æå›› --[reports_to]--> ç‹äº”
â””â”€â”€ âœ… æ¨ç†:å¼ ä¸‰ --> æå›› --> ç‹äº”(å¤šè·³æŸ¥è¯¢)
```

**å‘é‡RAG vs çŸ¥è¯†å›¾è°±RAG**:

| ç»´åº¦ | å‘é‡RAG | çŸ¥è¯†å›¾è°±RAG |
|------|---------|------------|
| **æ•°æ®è¡¨ç¤º** | éç»“æ„åŒ–æ–‡æœ¬ | ç»“æ„åŒ–ä¸‰å…ƒç»„(ä¸»-è°“-å®¾) |
| **æ£€ç´¢æ–¹å¼** | è¯­ä¹‰ç›¸ä¼¼åº¦ | å›¾éå†+è¯­ä¹‰åŒ¹é… |
| **å¤šè·³æ¨ç†** | âŒ å¼± | âœ… å¼º |
| **å®ä½“å…³ç³»** | âŒ éšå¼ | âœ… æ˜¾å¼ |
| **é€‚ç”¨åœºæ™¯** | æ–‡æ¡£é—®ç­” | å¤æ‚å…³ç³»æŸ¥è¯¢ |

---

### 3.2 Neo4j + LangChainå®ç°

#### 3.2.1 ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…Neo4j(ä½¿ç”¨Docker)
docker run \
    --name neo4j \
    -p 7474:7474 -p 7687:7687 \
    -e NEO4J_AUTH=neo4j/password \
    neo4j:latest

# å®‰è£…Pythonä¾èµ–
pip install langchain-community langchain-neo4j neo4j
```

#### 3.2.2 åŸºç¡€çŸ¥è¯†å›¾è°±RAG

```python
from langchain_community.graphs import Neo4jGraph
from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain
from langchain_openai import ChatOpenAI

# æ­¥éª¤1: è¿æ¥Neo4j
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# æ­¥éª¤2: æ„å»ºç¤ºä¾‹çŸ¥è¯†å›¾è°±
# åˆ›å»ºç»„ç»‡æ¶æ„å›¾
graph.query("""
// åˆ›å»ºå‘˜å·¥èŠ‚ç‚¹
CREATE (å¼ ä¸‰:Employee {name: 'å¼ ä¸‰', position: 'å·¥ç¨‹å¸ˆ', department: 'ç ”å‘éƒ¨'})
CREATE (æå››:Employee {name: 'æå››', position: 'æŠ€æœ¯ç»ç†', department: 'ç ”å‘éƒ¨'})
CREATE (ç‹äº”:Employee {name: 'ç‹äº”', position: 'æŠ€æœ¯æ€»ç›‘', department: 'ç ”å‘éƒ¨'})
CREATE (èµµå…­:Employee {name: 'èµµå…­', position: 'äº§å“ç»ç†', department: 'äº§å“éƒ¨'})

// åˆ›å»ºæ±‡æŠ¥å…³ç³»
CREATE (å¼ ä¸‰)-[:REPORTS_TO]->(æå››)
CREATE (æå››)-[:REPORTS_TO]->(ç‹äº”)
CREATE (èµµå…­)-[:REPORTS_TO]->(ç‹äº”)

// åˆ›å»ºé¡¹ç›®èŠ‚ç‚¹å’Œå‚ä¸å…³ç³»
CREATE (é¡¹ç›®A:Project {name: 'é¡¹ç›®A', status: 'è¿›è¡Œä¸­'})
CREATE (å¼ ä¸‰)-[:WORKS_ON]->(é¡¹ç›®A)
CREATE (èµµå…­)-[:WORKS_ON]->(é¡¹ç›®A)
""")

# æ­¥éª¤3: åˆ›å»ºCypher QAé“¾
cypher_chain = GraphCypherQAChain.from_llm(
    llm=ChatOpenAI(model="gpt-4", temperature=0),
    graph=graph,
    verbose=True
)

# æ­¥éª¤4: æŸ¥è¯¢(è‡ªåŠ¨ç”ŸæˆCypherè¯­å¥)
queries = [
    "å¼ ä¸‰çš„é¢†å¯¼æ˜¯è°?",
    "å¼ ä¸‰çš„é¢†å¯¼çš„é¢†å¯¼æ˜¯è°?",  # å¤šè·³æŸ¥è¯¢
    "ç ”å‘éƒ¨æœ‰å“ªäº›äºº?",
    "é¡¹ç›®Aæœ‰å“ªäº›å‚ä¸è€…?"
]

for query in queries:
    print(f"\næŸ¥è¯¢:{query}")
    result = cypher_chain.invoke({"query": query})
    print(f"ç­”æ¡ˆ:{result['result']}")
```

**è¾“å‡ºç¤ºä¾‹**:
```
æŸ¥è¯¢:å¼ ä¸‰çš„é¢†å¯¼æ˜¯è°?
ç”ŸæˆCypher:
MATCH (e:Employee {name: 'å¼ ä¸‰'})-[:REPORTS_TO]->(manager)
RETURN manager.name

ç­”æ¡ˆ:å¼ ä¸‰çš„é¢†å¯¼æ˜¯æå››

æŸ¥è¯¢:å¼ ä¸‰çš„é¢†å¯¼çš„é¢†å¯¼æ˜¯è°?
ç”ŸæˆCypher:
MATCH (e:Employee {name: 'å¼ ä¸‰'})-[:REPORTS_TO*2]->(manager)
RETURN manager.name

ç­”æ¡ˆ:å¼ ä¸‰çš„é¢†å¯¼çš„é¢†å¯¼æ˜¯ç‹äº”
```

---

#### 3.2.3 GraphRAG: å‘é‡ + å›¾éå† (Neo4jVector)

çœŸæ­£çš„ GraphRAG ä¸ä»…ä»…æ˜¯è®© Agent å¤šä¸€ä¸ªæŸ¥å›¾çš„å·¥å…·ï¼Œè€Œæ˜¯åˆ©ç”¨"å‘é‡æœç´¢"ä½œä¸ºå›¾å…¥å£ï¼Œç»“åˆå›¾éå†è·å–ä¸Šä¸‹æ–‡ã€‚

**å·¥ä½œæµç¨‹**:
1. **Indexing**: æå–æ–‡æ¡£ä¸­çš„å®ä½“(Nodes)å’Œå…³ç³»(Relationships)å­˜å…¥Neo4jï¼Œå¹¶å¯¹å®ä½“æ–‡æœ¬æˆ–æ–‡æ¡£å—è¿›è¡ŒEmbeddingã€‚
2. **Retrieval**:
   - Step 1: Query -> Vector Search -> æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å®ä½“èŠ‚ç‚¹(Entry Points)ã€‚
   - Step 2: Graph Traversal -> ä»å…¥å£èŠ‚ç‚¹å‡ºå‘ï¼Œéå†è·å–é‚»å±…èŠ‚ç‚¹(Context)ã€‚
3. **Generation**: å°†ç»“æ„åŒ–ä¸Šä¸‹æ–‡(é‚»å±…å…³ç³»)æäº¤ç»™LLMå›ç­”ã€‚

```python
from langchain_community.vectorstores import Neo4jVector
from langchain_openai import OpenAIEmbeddings

# ä½¿ç”¨Neo4jVectorå®ç°"å‘é‡å…¥å£ + å›¾éå†"çš„æ£€ç´¢
# å‡è®¾Graphä¸­å·²ç»å­˜åœ¨ Employee èŠ‚ç‚¹, ä¸”åŒ…å« "name", "position" ç­‰å±æ€§

# æ­¥éª¤1: åˆ›å»ºå‘é‡æ£€ç´¢å™¨ (è¿æ¥ç°æœ‰çš„å›¾)
vector_store = Neo4jVector.from_existing_graph(
    embedding=OpenAIEmbeddings(),
    url="bolt://localhost:7687",
    username="neo4j",
    password="password",
    index_name="employee_index",
    node_label="Employee",
    text_node_properties=["name", "position", "department"], # è¿™äº›å±æ€§å†…å®¹ä¼šè¢«å‘é‡åŒ–
    embedding_node_property="embedding", # å‘é‡å­˜æ”¾åœ¨èŠ‚ç‚¹çš„embeddingå±æ€§ä¸­

    # ã€æ ¸å¿ƒMagicã€‘retrieval_query: å‘é‡æ£€ç´¢æ‰¾åˆ°èŠ‚ç‚¹å, æ‰§è¡Œæ­¤Cypherè·å–ä¸Šä¸‹æ–‡
    # è¿™é‡Œçš„ 'node' æ˜¯å‘é‡åŒ¹é…åˆ°çš„èŠ‚ç‚¹
    retrieval_query="""
    // æ‰¾åˆ°è¯¥å‘˜å·¥çš„ç›´æ¥ä¸‹å± (1è·³å…³ç³»)
    MATCH (node)<-[:REPORTS_TO]-(subordinate)
    RETURN "å‘˜å·¥: " + node.name + " (" + node.position + ")" +
           " ç®¡ç†ç€: " + subordinate.name + " (" + subordinate.position + ")" AS text,
           score,
           {} AS metadata
    """
)

# æ­¥éª¤2: æ‰§è¡Œæ£€ç´¢
# æŸ¥è¯¢: "è°æ˜¯æå››çš„ä¸‹å±?"
# 1. å‘é‡æ£€ç´¢æ‰¾åˆ° "æå››" èŠ‚ç‚¹
# 2. æ‰§è¡Œ retrieval_query æ‰¾åˆ°æå››çš„ä¸‹å±
results = vector_store.similarity_search("æå››", k=1)

print(f"GraphRAGæ£€ç´¢ç»“æœ:")
for doc in results:
    print(doc.page_content)

# è¾“å‡ºç¤ºä¾‹:
# å‘˜å·¥: æå›› (æŠ€æœ¯ç»ç†) ç®¡ç†ç€: å¼ ä¸‰ (å·¥ç¨‹å¸ˆ)
```

---

### 3.3 GraphRAG vs ä¼ ç»ŸRAGæ€§èƒ½å¯¹æ¯”

| æŸ¥è¯¢ç±»å‹ | ä¼ ç»ŸRAGå‡†ç¡®ç‡ | GraphRAGå‡†ç¡®ç‡ | æå‡ |
|---------|-------------|---------------|------|
| å•è·³å…³ç³»æŸ¥è¯¢ | 75% | 95% | +20% |
| å¤šè·³å…³ç³»æŸ¥è¯¢ | 30% | 88% | +58% |
| æè¿°æ€§é—®ç­” | 85% | 87% | +2% |
| æ··åˆæŸ¥è¯¢ | 55% | 82% | +27% |

**é€‚ç”¨åœºæ™¯**:
- âœ… ç»„ç»‡æ¶æ„ã€å®¶æ—å…³ç³»ç­‰å±‚æ¬¡ç»“æ„
- âœ… ä¾›åº”é“¾ã€çŸ¥è¯†ç½‘ç»œç­‰å¤æ‚å…³ç³»ç½‘
- âœ… éœ€è¦å¤šè·³æ¨ç†çš„æŸ¥è¯¢
- âŒ çº¯æ–‡æœ¬é—®ç­”(ä¼ ç»ŸRAGæ›´ç®€å•)

---

## ç¬¬4ç« :å‰æ²¿RAGæ–¹æ¡ˆ

### 4.1 Self-RAG(è‡ªæˆ‘åæ€æ£€ç´¢)

#### 4.1.1 æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»ŸRAG:
æŸ¥è¯¢ â†’ æ£€ç´¢ â†’ ç”Ÿæˆ

Self-RAG:
æŸ¥è¯¢ â†’ æ£€ç´¢ â†’ è¯„ä¼°ç›¸å…³æ€§ â†’ ç”Ÿæˆ â†’ éªŒè¯ç­”æ¡ˆ â†’ (é‡æ–°æ£€ç´¢)
```

**å…³é”®æ­¥éª¤**:
1. **æ£€ç´¢å†³ç­–**:åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
2. **ç›¸å…³æ€§è¯„ä¼°**:è¯„ä¼°æ£€ç´¢æ–‡æ¡£æ˜¯å¦ç›¸å…³
3. **ç­”æ¡ˆç”Ÿæˆ**:åŸºäºæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
4. **ç­”æ¡ˆéªŒè¯**:éªŒè¯ç­”æ¡ˆæ˜¯å¦è¢«æ–‡æ¡£æ”¯æŒ
5. **è¿­ä»£æ”¹è¿›**:å¦‚æœä¸æ»¡æ„,é‡æ–°æ£€ç´¢

---

#### 4.1.2 ç®€åŒ–å®ç°

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

class SelfRAG:
    """Self-RAGå®ç°"""

    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm

        # ç›¸å…³æ€§è¯„ä¼°æç¤ºè¯
        self.relevance_prompt = ChatPromptTemplate.from_template("""
è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

æŸ¥è¯¢:{query}

æ–‡æ¡£:
{document}

ç›¸å…³æ€§(0-10åˆ†):
ç†ç”±:
""")

        # ç­”æ¡ˆéªŒè¯æç¤ºè¯
        self.verification_prompt = ChatPromptTemplate.from_template("""
éªŒè¯ç­”æ¡ˆæ˜¯å¦è¢«æ–‡æ¡£æ”¯æŒã€‚

æŸ¥è¯¢:{query}
ç­”æ¡ˆ:{answer}

æ–‡æ¡£:
{documents}

éªŒè¯ç»“æœ(æ”¯æŒ/ä¸æ”¯æŒ):
ç†ç”±:
""")

    def invoke(self, query: str, max_iterations: int = 2) -> dict:
        """Self-RAGæŸ¥è¯¢"""
        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            print(f"\nç¬¬{iteration}è½®æ£€ç´¢...")

            # æ­¥éª¤1: æ£€ç´¢æ–‡æ¡£
            docs = self.retriever.invoke(query)

            # æ­¥éª¤2: è¯„ä¼°ç›¸å…³æ€§
            relevant_docs = []
            for doc in docs[:5]:
                relevance_result = (self.relevance_prompt | self.llm).invoke({
                    "query": query,
                    "document": doc.page_content
                })

                # ç®€å•è§£æè¯„åˆ†(å®é™…åº”ç”¨ä¸­å¯ç”¨structured output)
                if "ç›¸å…³æ€§" in relevance_result.content:
                    score_line = [
                        line for line in relevance_result.content.split('\n')
                        if 'ç›¸å…³æ€§' in line or 'åˆ†' in line
                    ][0]

                    try:
                        score = int(''.join(filter(str.isdigit, score_line))[:2])
                        if score >= 7:
                            relevant_docs.append(doc)
                    except:
                        continue

            if not relevant_docs:
                print("  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£,é‡æ–°æ£€ç´¢...")
                continue

            print(f"  æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")

            # æ­¥éª¤3: ç”Ÿæˆç­”æ¡ˆ
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            answer_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:

{context}

é—®é¢˜:{query}

ç­”æ¡ˆ:
"""
            answer = self.llm.invoke(answer_prompt).content

            # æ­¥éª¤4: éªŒè¯ç­”æ¡ˆ
            verification = (self.verification_prompt | self.llm).invoke({
                "query": query,
                "answer": answer,
                "documents": context
            })

            if "æ”¯æŒ" in verification.content:
                return {
                    "query": query,
                    "answer": answer,
                    "documents": relevant_docs,
                    "iterations": iteration,
                    "verified": True
                }

            print("  ç­”æ¡ˆæœªé€šè¿‡éªŒè¯,é‡æ–°æ£€ç´¢...")

        # æœ€å¤§è¿­ä»£æ¬¡æ•°åä»æœªéªŒè¯é€šè¿‡
        return {
            "query": query,
            "answer": answer,
            "documents": relevant_docs,
            "iterations": iteration,
            "verified": False
        }

# ä½¿ç”¨ç¤ºä¾‹
self_rag = SelfRAG(
    retriever=vectorstore.as_retriever(),
    llm=ChatOpenAI(model="gpt-4", temperature=0)
)

result = self_rag.invoke("Python 3.11çš„ä¸»è¦æ–°ç‰¹æ€§æ˜¯ä»€ä¹ˆ?")
print(f"\næœ€ç»ˆç­”æ¡ˆ:\n{result['answer']}")
print(f"\nè¿­ä»£æ¬¡æ•°:{result['iterations']}")
print(f"éªŒè¯é€šè¿‡:{result['verified']}")
```

---

### 4.2 Corrective RAG (CRAG)

#### 4.2.1 æ ¸å¿ƒæµç¨‹

```mermaid
graph TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[æ£€ç´¢æ–‡æ¡£]
    B --> C{è¯„ä¼°ç›¸å…³æ€§}
    C -->|é«˜ç›¸å…³| D[ç›´æ¥ç”Ÿæˆ]
    C -->|ä½ç›¸å…³| E[ç½‘ç»œæœç´¢]
    C -->|ä¸­ç­‰ç›¸å…³| F[çŸ¥è¯†ç²¾ç‚¼]
    E --> D
    F --> D
    D --> G[æœ€ç»ˆç­”æ¡ˆ]
```

---

#### 4.2.2 ç®€åŒ–å®ç°

```python
from langchain_community.tools import DuckDuckGoSearchRun

class CorrectiveRAG:
    """Corrective RAGå®ç°"""

    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
        self.web_search = DuckDuckGoSearchRun()

    def invoke(self, query: str) -> str:
        """CRAGæŸ¥è¯¢"""
        # æ­¥éª¤1: æœ¬åœ°æ£€ç´¢
        docs = self.retriever.invoke(query)

        # æ­¥éª¤2: è¯„ä¼°ç›¸å…³æ€§
        relevance_score = self._evaluate_relevance(query, docs)

        if relevance_score >= 0.8:
            # é«˜ç›¸å…³:ç›´æ¥ä½¿ç”¨
            context = "\n\n".join([doc.page_content for doc in docs[:3]])
            return self._generate_answer(query, context)

        elif relevance_score < 0.4:
            # ä½ç›¸å…³:ç½‘ç»œæœç´¢
            print("æœ¬åœ°æ–‡æ¡£ç›¸å…³æ€§ä½,å¯åŠ¨ç½‘ç»œæœç´¢...")
            web_results = self.web_search.invoke(query)
            return self._generate_answer(query, web_results)

        else:
            # ä¸­ç­‰ç›¸å…³:çŸ¥è¯†ç²¾ç‚¼
            print("æ–‡æ¡£éœ€è¦ç²¾ç‚¼...")
            refined_context = self._refine_knowledge(query, docs)
            return self._generate_answer(query, refined_context)

    def _evaluate_relevance(self, query: str, docs: list) -> float:
        """è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§(ç®€åŒ–ç‰ˆ)"""
        # å®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨ä¸“é—¨çš„è¯„ä¼°æ¨¡å‹
        prompt = f"""
è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„å¹³å‡ç›¸å…³æ€§(0-1ä¹‹é—´çš„åˆ†æ•°)ã€‚

æŸ¥è¯¢:{query}

æ–‡æ¡£:
{docs[0].page_content[:200]}...

ç›¸å…³æ€§åˆ†æ•°(0-1):
"""
        response = self.llm.invoke(prompt).content
        try:
            score = float(''.join(filter(lambda x: x.isdigit() or x == '.', response)))
            return min(1.0, score)
        except:
            return 0.5

    def _refine_knowledge(self, query: str, docs: list) -> str:
        """çŸ¥è¯†ç²¾ç‚¼:æå–å…³é”®ä¿¡æ¯"""
        context = "\n\n".join([doc.page_content for doc in docs[:5]])
        prompt = f"""
ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯(å»é™¤æ— å…³å†…å®¹)ã€‚

æŸ¥è¯¢:{query}

æ–‡æ¡£:
{context}

å…³é”®ä¿¡æ¯:
"""
        return self.llm.invoke(prompt).content

    def _generate_answer(self, query: str, context: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

{context}

é—®é¢˜:{query}

ç­”æ¡ˆ:
"""
        return self.llm.invoke(prompt).content

# ä½¿ç”¨
crag = CorrectiveRAG(
    retriever=vectorstore.as_retriever(),
    llm=ChatOpenAI(model="gpt-4", temperature=0)
)

answer = crag.invoke("é‡å­è®¡ç®—æœºçš„æœ€æ–°è¿›å±•")
print(answer)
```

---

### 4.3 Agentic RAG

**æ ¸å¿ƒæ€æƒ³**:è®©Agentè‡ªä¸»å†³å®šä½•æ—¶æ£€ç´¢ã€æ£€ç´¢ä»€ä¹ˆã€å¦‚ä½•ç»„åˆä¿¡æ¯

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# å®šä¹‰å¤šä¸ªæ£€ç´¢å·¥å…·
@tool
def search_technical_docs(query: str) -> str:
    """æœç´¢æŠ€æœ¯æ–‡æ¡£"""
    # ä½¿ç”¨å‘é‡æ£€ç´¢å™¨
    results = tech_retriever.invoke(query)
    return "\n".join([doc.page_content for doc in results[:3]])

@tool
def search_company_policies(query: str) -> str:
    """æœç´¢å…¬å¸æ”¿ç­–æ–‡æ¡£"""
    results = policy_retriever.invoke(query)
    return "\n".join([doc.page_content for doc in results[:3]])

@tool
def query_database(sql_query: str) -> str:
    """æŸ¥è¯¢æ•°æ®åº“è·å–ç»Ÿè®¡æ•°æ®"""
    # ç®€åŒ–ç¤ºä¾‹
    return "æŸ¥è¯¢ç»“æœ:..."

# åˆ›å»ºAgentic RAG
agentic_rag = create_agent(
    model="gpt-4",
    tools=[search_technical_docs, search_company_policies, query_database],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹,å¯ä»¥æŸ¥è¯¢æŠ€æœ¯æ–‡æ¡£ã€å…¬å¸æ”¿ç­–å’Œæ•°æ®åº“ã€‚

æ ¹æ®ç”¨æˆ·é—®é¢˜è‡ªä¸»å†³å®š:
1. éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·
2. ä»¥ä»€ä¹ˆé¡ºåºä½¿ç”¨å·¥å…·
3. å¦‚ä½•ç»„åˆä¸åŒæ¥æºçš„ä¿¡æ¯

ä¿æŒå›ç­”å‡†ç¡®ã€å®Œæ•´ã€ä¸“ä¸šã€‚"""
)

# ä½¿ç”¨
result = agentic_rag.invoke({
    "messages": [{"role": "user", "content": "å…¬å¸çš„è¿œç¨‹åŠå…¬æ”¿ç­–æ˜¯ä»€ä¹ˆ?éœ€è¦æäº¤å“ªäº›ç”³è¯·?"}]
})
print(result["messages"][-1].content)
```

---

### å°ç»“

**ç¬¬3-4ç« æ ¸å¿ƒè¦ç‚¹**:

#### çŸ¥è¯†å›¾è°±RAG(ç¬¬3ç« )

| ç‰¹æ€§ | ä»·å€¼ |
|------|------|
| å¤šè·³æ¨ç† | å‡†ç¡®ç‡æå‡30-60% |
| æ˜¾å¼å…³ç³» | å¯è§£é‡Šæ€§å¼º |
| ç»“æ„åŒ–è¡¨ç¤º | é€‚åˆç»„ç»‡æ¶æ„ã€ä¾›åº”é“¾ç­‰åœºæ™¯ |

**å®æ–½å»ºè®®**:
- âœ… æ··åˆæ–¹æ¡ˆ:å‘é‡RAG(æ–‡æ¡£) + å›¾RAG(å…³ç³»)
- âœ… å·¥å…·:Neo4j + LangChain
- âš ï¸ æˆæœ¬:éœ€è¦æ„å»ºå’Œç»´æŠ¤çŸ¥è¯†å›¾è°±

---

#### å‰æ²¿RAGæ–¹æ¡ˆ(ç¬¬4ç« )

| æ–¹æ¡ˆ | æ ¸å¿ƒç‰¹ç‚¹ | æ€§èƒ½æå‡ | æˆæœ¬ |
|------|---------|---------|------|
| **Self-RAG** | è‡ªæˆ‘åæ€ã€è¿­ä»£æ£€ç´¢ | +15-20% | LLMè°ƒç”¨2-3å€ |
| **Corrective RAG** | ç›¸å…³æ€§è¯„ä¼°ã€ç½‘ç»œè¡¥å…… | +20-25% | +Webæœç´¢æˆæœ¬ |
| **Agentic RAG** | Agentè‡ªä¸»å†³ç­– | +25-30% | é«˜(å¤šæ¬¡LLMè°ƒç”¨) |

**é€‰æ‹©å»ºè®®**:
- **é«˜å‡†ç¡®ç‡éœ€æ±‚** â†’ Self-RAGæˆ–Corrective RAG
- **å¤æ‚ä¿¡æ¯æ•´åˆ** â†’ Agentic RAG
- **æˆæœ¬æ•æ„Ÿ** â†’ ä¼ ç»ŸRAG + é‡æ’åº(ç¬¬2ç« )

---

## ç¬¬5ç« ï¼šæ··åˆæ£€ç´¢ç”Ÿäº§å®è·µå®Œæ•´æŒ‡å—

> **å…³æ³¨ç‚¹**: å°†æ··åˆæ£€ç´¢ä»demoæå‡åˆ°ä¼ä¸šçº§ç”Ÿäº§ç³»ç»Ÿ

åœ¨å‰é¢çš„ç« èŠ‚ä¸­,æˆ‘ä»¬å­¦ä¹ äº†æ··åˆæ£€ç´¢çš„åŸºæœ¬åŸç†å’Œå®ç°ã€‚ä½†**ç”Ÿäº§ç¯å¢ƒ**æœ‰æ›´é«˜çš„è¦æ±‚:
- ğŸ¯ **æ€§èƒ½**: å»¶è¿Ÿ < 300ms
- ğŸ“Š **è´¨é‡**: æ£€ç´¢å‡†ç¡®ç‡ > 90%
- ğŸ’° **æˆæœ¬**: Tokenä½¿ç”¨ä¼˜åŒ–
- ğŸ”„ **å¯ç»´æŠ¤**: å¯è¯„ä¼°ã€å¯ä¼˜åŒ–ã€å¯ç›‘æ§

æœ¬ç« å°†å±•ç¤ºå¦‚ä½•æ„å»ºã€è¯„ä¼°ã€ä¼˜åŒ–ã€éƒ¨ç½²ä¸€ä¸ª**ç”Ÿäº§çº§æ··åˆæ£€ç´¢ç³»ç»Ÿ**ã€‚

---

#### 5.1 ç”Ÿäº§ç³»ç»Ÿæ¶æ„è®¾è®¡

##### 5.1.1 å®Œæ•´æ¶æ„

```
ç”¨æˆ·æŸ¥è¯¢
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æŸ¥è¯¢å¤„ç†å±‚ (Query Processing)      â”‚
â”‚  - æŸ¥è¯¢æ”¹å†™                         â”‚
â”‚  - æ„å›¾è¯†åˆ«                         â”‚
â”‚  - å‚æ•°æå–                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¹¶å‘æ£€ç´¢å±‚ (Parallel Retrieval)    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ å‘é‡æ£€ç´¢ â”‚      â”‚ BM25æ£€ç´¢ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  èåˆæ’åºå±‚ (Hybrid Ranking)        â”‚
â”‚  - RRFèåˆ                          â”‚
â”‚  - æƒé‡è°ƒæ•´                         â”‚
â”‚  - å»é‡                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é‡æ’åºå±‚ (Reranking) [å¯é€‰]        â”‚
â”‚  - Cross-Encoderç²¾æ’                â”‚
â”‚  - å¤šæ ·æ€§ä¼˜åŒ–                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¼“å­˜å±‚ (Caching)                   â”‚
â”‚  - æŸ¥è¯¢ç¼“å­˜                         â”‚
â”‚  - ç»“æœç¼“å­˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç›‘æ§å±‚ (Monitoring)                â”‚
â”‚  - å»¶è¿Ÿç›‘æ§                         â”‚
â”‚  - è´¨é‡ç›‘æ§                         â”‚
â”‚  - æˆæœ¬ç›‘æ§                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 5.1.2 ç”Ÿäº§çº§å®ç°

```python
# production_hybrid_retriever.py
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from functools import lru_cache
import asyncio
import time
import logging
from dataclasses import dataclass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RetrievalMetrics:
    """æ£€ç´¢æŒ‡æ ‡"""
    latency_ms: float
    num_results: int
    cache_hit: bool
    retriever_used: str

class ProductionHybridRetriever:
    """ç”Ÿäº§çº§æ··åˆæ£€ç´¢å™¨"""

    def __init__(
        self,
        documents: List[Document],
        vector_weight: float = 0.5,
        bm25_weight: float = 0.5,
        top_k: int = 4,
        enable_cache: bool = True,
        enable_rerank: bool = False,
        enable_monitoring: bool = True
    ):
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.top_k = top_k
        self.enable_cache = enable_cache
        self.enable_rerank = enable_rerank
        self.enable_monitoring = enable_monitoring

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        logger.info("åˆå§‹åŒ–ç”Ÿäº§çº§æ··åˆæ£€ç´¢å™¨...")

        # å‘é‡æ£€ç´¢å™¨
        embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings
        )
        self.vector_retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": top_k}
        )

        # BM25æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = top_k

        # æ··åˆæ£€ç´¢å™¨
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=[vector_weight, bm25_weight]
        )

        # ç›‘æ§æ•°æ®
        self.metrics_history: List[RetrievalMetrics] = []

        logger.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    @lru_cache(maxsize=1000)
    def _cached_retrieve(self, query: str, k: int) -> tuple:
        """ç¼“å­˜çš„æ£€ç´¢ï¼ˆä½¿ç”¨tupleå› ä¸ºListä¸å¯hashï¼‰"""
        results = self._retrieve_internal(query, k)
        # è½¬ä¸ºtupleä»¥æ”¯æŒç¼“å­˜
        return tuple((doc.page_content, doc.metadata) for doc in results)

    def _retrieve_internal(self, query: str, k: int) -> List[Document]:
        """å†…éƒ¨æ£€ç´¢å®ç°"""
        return self.ensemble_retriever.invoke(query)

    async def _async_retrieve(self, query: str) -> List[Document]:
        """å¼‚æ­¥å¹¶å‘æ£€ç´¢"""
        # å¹¶å‘æ‰§è¡Œå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢
        vector_task = asyncio.create_task(
            asyncio.to_thread(self.vector_retriever.invoke, query)
        )
        bm25_task = asyncio.create_task(
            asyncio.to_thread(self.bm25_retriever.invoke, query)
        )

        vector_results, bm25_results = await asyncio.gather(vector_task, bm25_task)

        # æ‰‹åŠ¨èåˆï¼ˆä½¿ç”¨RRFï¼‰
        return self._rrf_fusion(vector_results, bm25_results)

    def _rrf_fusion(
        self,
        vector_results: List[Document],
        bm25_results: List[Document],
        k: int = 60
    ) -> List[Document]:
        """Reciprocal Rank Fusionèåˆ"""
        scores = {}

        # å‘é‡æ£€ç´¢è¯„åˆ†
        for rank, doc in enumerate(vector_results):
            doc_id = id(doc)
            rrf_score = 1 / (k + rank + 1)
            scores[doc_id] = scores.get(doc_id, 0) + self.vector_weight * rrf_score

        # BM25æ£€ç´¢è¯„åˆ†
        for rank, doc in enumerate(bm25_results):
            doc_id = id(doc)
            rrf_score = 1 / (k + rank + 1)
            scores[doc_id] = scores.get(doc_id, 0) + self.bm25_weight * rrf_score

        # åˆå¹¶å¹¶å»é‡
        all_docs = {id(doc): doc for doc in vector_results + bm25_results}

        # æŒ‰åˆ†æ•°æ’åº
        sorted_doc_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

        return [all_docs[doc_id] for doc_id in sorted_doc_ids[:self.top_k]]

    def retrieve(
        self,
        query: str,
        use_async: bool = False,
        bypass_cache: bool = False
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ£€ç´¢å¹¶è¿”å›ç»“æœ+æŒ‡æ ‡"""

        start_time = time.time()
        cache_hit = False

        # ç¼“å­˜æ£€ç´¢
        if self.enable_cache and not bypass_cache:
            try:
                cached_results = self._cached_retrieve(query, self.top_k)
                documents = [
                    Document(page_content=content, metadata=metadata)
                    for content, metadata in cached_results
                ]
                cache_hit = True
            except Exception as e:
                logger.warning(f"ç¼“å­˜å¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥æ£€ç´¢: {e}")
                documents = self._retrieve_internal(query, self.top_k)
        elif use_async:
            # å¼‚æ­¥å¹¶å‘æ£€ç´¢
            documents = asyncio.run(self._async_retrieve(query))
        else:
            # åŒæ­¥æ£€ç´¢
            documents = self._retrieve_internal(query, self.top_k)

        latency_ms = (time.time() - start_time) * 1000

        # è®°å½•æŒ‡æ ‡
        metrics = RetrievalMetrics(
            latency_ms=latency_ms,
            num_results=len(documents),
            cache_hit=cache_hit,
            retriever_used="async" if use_async else "sync"
        )

        if self.enable_monitoring:
            self.metrics_history.append(metrics)
            logger.info(
                f"æ£€ç´¢å®Œæˆ: {latency_ms:.0f}ms | "
                f"ç»“æœæ•°: {len(documents)} | "
                f"ç¼“å­˜: {'å‘½ä¸­' if cache_hit else 'æœªå‘½ä¸­'}"
            )

        return {
            "documents": documents,
            "metrics": metrics
        }

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.metrics_history:
            return {}

        latencies = [m.latency_ms for m in self.metrics_history]
        cache_hits = sum(1 for m in self.metrics_history if m.cache_hit)

        return {
            "total_queries": len(self.metrics_history),
            "avg_latency_ms": sum(latencies) / len(latencies),
            "p50_latency_ms": sorted(latencies)[len(latencies) // 2],
            "p95_latency_ms": sorted(latencies)[int(len(latencies) * 0.95)],
            "p99_latency_ms": sorted(latencies)[int(len(latencies) * 0.99)],
            "cache_hit_rate": cache_hits / len(self.metrics_history),
            "avg_results_per_query": sum(m.num_results for m in self.metrics_history) / len(self.metrics_history)
        }
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½æ–‡æ¡£
loader = DirectoryLoader("./docs", glob="**/*.txt")
documents = loader.load()

# åˆ†å—
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = splitter.split_documents(documents)

# åˆ›å»ºç”Ÿäº§çº§æ£€ç´¢å™¨
retriever = ProductionHybridRetriever(
    documents=splits,
    vector_weight=0.6,
    bm25_weight=0.4,
    top_k=5,
    enable_cache=True,
    enable_monitoring=True
)

# æ‰§è¡Œæ£€ç´¢
result = retriever.retrieve("Python 3.11çš„æ–°ç‰¹æ€§")

print(f"æ£€ç´¢ç»“æœ: {len(result['documents'])}ä¸ª")
print(f"å»¶è¿Ÿ: {result['metrics'].latency_ms:.0f}ms")
print(f"ç¼“å­˜: {'å‘½ä¸­' if result['metrics'].cache_hit else 'æœªå‘½ä¸­'}")

# æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
stats = retriever.get_performance_stats()
print(f"\næ€§èƒ½ç»Ÿè®¡:")
print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg_latency_ms']:.0f}ms")
print(f"  P95å»¶è¿Ÿ: {stats['p95_latency_ms']:.0f}ms")
print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
```

---

#### 5.2 è¯„ä¼°ä¸ä¼˜åŒ–ä½“ç³»

##### 5.2.1 æ„å»ºè¯„ä¼°æ•°æ®é›†

```python
# build_eval_dataset.py
from langsmith import Client
from typing import List, Dict

client = Client()

def build_retrieval_dataset(
    dataset_name: str = "hybrid_retrieval_eval"
) -> None:
    """æ„å»ºæ£€ç´¢è¯„ä¼°æ•°æ®é›†"""

    # æ­¥éª¤1ï¼šåˆ›å»ºDataset
    dataset = client.create_dataset(
        dataset_name=dataset_name,
        description="æ··åˆæ£€ç´¢è´¨é‡è¯„ä¼°æ•°æ®é›†"
    )

    # æ­¥éª¤2ï¼šå®šä¹‰æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "inputs": {
                "query": "Python 3.11çš„æ–°ç‰¹æ€§æœ‰å“ªäº›ï¼Ÿ"
            },
            "outputs": {
                "expected_doc_ids": ["doc_123", "doc_456"],  # æ–‡æ¡£ID
                "expected_keywords": ["Python 3.11", "æ€§èƒ½æå‡", "å¼‚å¸¸ç»„"],
                "relevance_score": 5  # 1-5åˆ†
            },
            "metadata": {
                "category": "æŠ€æœ¯æŸ¥è¯¢",
                "difficulty": "easy"
            }
        },
        {
            "inputs": {
                "query": "å¦‚ä½•ä¼˜åŒ–LangChainçš„æ€§èƒ½ï¼Ÿ"
            },
            "outputs": {
                "expected_doc_ids": ["doc_789", "doc_012"],
                "expected_keywords": ["ç¼“å­˜", "å¹¶å‘", "æ‰¹å¤„ç†"],
                "relevance_score": 4
            },
            "metadata": {
                "category": "æ€§èƒ½ä¼˜åŒ–",
                "difficulty": "medium"
            }
        },
        # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹(å»ºè®®100+ä¸ª)
    ]

    # æ­¥éª¤3ï¼šæ·»åŠ æµ‹è¯•ç”¨ä¾‹
    for case in test_cases:
        client.create_example(
            dataset_id=dataset.id,
            inputs=case["inputs"],
            outputs=case["outputs"],
            metadata=case.get("metadata", {})
        )

    print(f"âœ… åˆ›å»ºDatasetæˆåŠŸ: {dataset_name}")
    print(f"   åŒ…å« {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print(f"   Dataset ID: {dataset.id}")
```

##### 5.2.2 è‡ªå®šä¹‰Evaluators

```python
# evaluators.py
from langsmith.evaluation import evaluator
from typing import List, Dict, Any

@evaluator
def keyword_coverage_evaluator(run, example):
    """å…³é”®è¯è¦†ç›–ç‡è¯„ä¼°å™¨"""
    retrieved_docs = run.outputs.get("documents", [])
    expected_keywords = example.outputs.get("expected_keywords", [])

    # åˆå¹¶æ‰€æœ‰æ£€ç´¢æ–‡æ¡£çš„å†…å®¹
    all_content = " ".join([doc.page_content for doc in retrieved_docs])

    # æ£€æŸ¥å…³é”®è¯è¦†ç›–
    found_keywords = [kw for kw in expected_keywords if kw in all_content]
    coverage = len(found_keywords) / len(expected_keywords) if expected_keywords else 0

    return {
        "key": "keyword_coverage",
        "score": coverage,
        "comment": f"è¦†ç›– {len(found_keywords)}/{len(expected_keywords)} ä¸ªå…³é”®è¯"
    }

@evaluator
def recall_at_k_evaluator(run, example):
    """Recall@Kè¯„ä¼°å™¨"""
    retrieved_docs = run.outputs.get("documents", [])
    expected_doc_ids = example.outputs.get("expected_doc_ids", [])

    # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£ID
    retrieved_ids = [doc.metadata.get("id", "") for doc in retrieved_docs]

    # è®¡ç®—Recall
    hits = len(set(retrieved_ids) & set(expected_doc_ids))
    recall = hits / len(expected_doc_ids) if expected_doc_ids else 0

    return {
        "key": "recall_at_k",
        "score": recall,
        "comment": f"å¬å› {hits}/{len(expected_doc_ids)} ä¸ªç›¸å…³æ–‡æ¡£"
    }

@evaluator
def mrr_evaluator(run, example):
    """MRR (Mean Reciprocal Rank) è¯„ä¼°å™¨"""
    retrieved_docs = run.outputs.get("documents", [])
    expected_doc_ids = example.outputs.get("expected_doc_ids", [])

    retrieved_ids = [doc.metadata.get("id", "") for doc in retrieved_docs]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®
    for rank, doc_id in enumerate(retrieved_ids, start=1):
        if doc_id in expected_doc_ids:
            mrr = 1 / rank
            return {
                "key": "mrr",
                "score": mrr,
                "comment": f"ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£åœ¨ä½ç½® {rank}"
            }

    return {
        "key": "mrr",
        "score": 0.0,
        "comment": "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£"
    }

@evaluator
def latency_evaluator(run, example):
    """å»¶è¿Ÿè¯„ä¼°å™¨"""
    metrics = run.outputs.get("metrics", {})
    latency_ms = metrics.latency_ms if hasattr(metrics, 'latency_ms') else 0

    # è¯„åˆ†æ ‡å‡†: <200ms=1.0, 200-500ms=0.8, 500-1000ms=0.5, >1000ms=0.0
    if latency_ms < 200:
        score = 1.0
    elif latency_ms < 500:
        score = 0.8
    elif latency_ms < 1000:
        score = 0.5
    else:
        score = 0.0

    return {
        "key": "latency_score",
        "score": score,
        "comment": f"å»¶è¿Ÿ {latency_ms:.0f}ms"
    }
```

##### 5.2.3 è¿è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°

```python
# run_evaluation.py
from langsmith.evaluation import evaluate
from langsmith import Client
from production_hybrid_retriever import ProductionHybridRetriever
from evaluators import (
    keyword_coverage_evaluator,
    recall_at_k_evaluator,
    mrr_evaluator,
    latency_evaluator
)

client = Client()

# åŠ è½½æ•°æ®
# ... (åŒå‰é¢çš„æ–‡æ¡£åŠ è½½ä»£ç )

# åˆ›å»ºæ£€ç´¢å™¨
retriever = ProductionHybridRetriever(
    documents=splits,
    vector_weight=0.5,
    bm25_weight=0.5,
    top_k=5
)

# å®šä¹‰é¢„æµ‹å‡½æ•°
def predict(inputs: Dict) -> Dict:
    """æ‰§è¡Œæ£€ç´¢"""
    query = inputs["query"]
    result = retriever.retrieve(query)
    return result

# è¿è¡Œè¯„ä¼°
results = evaluate(
    predict,
    data="hybrid_retrieval_eval",
    evaluators=[
        keyword_coverage_evaluator,
        recall_at_k_evaluator,
        mrr_evaluator,
        latency_evaluator
    ],
    experiment_prefix="baseline_v1.0",
    description="åŸºå‡†ç‰ˆæœ¬è¯„ä¼°",
    max_concurrency=5
)

# è¾“å‡ºç»“æœ
print("\n=== è¯„ä¼°ç»“æœ ===")
print(f"æ€»è®¡: {results['total']}")
print(f"å…³é”®è¯è¦†ç›–ç‡: {results['keyword_coverage_avg']:.2%}")
print(f"Recall@5: {results['recall_at_k_avg']:.2%}")
print(f"MRR: {results['mrr_avg']:.3f}")
print(f"å»¶è¿Ÿè¯„åˆ†: {results['latency_score_avg']:.2f}")
```

##### 5.2.4 æƒé‡ä¼˜åŒ–å®éªŒ

```python
# optimize_weights.py
from langsmith.evaluation import evaluate
import numpy as np
from typing import List, Tuple

def grid_search_weights(
    weight_candidates: List[Tuple[float, float]],
    dataset_name: str = "hybrid_retrieval_eval"
) -> Tuple[float, float, Dict]:
    """ç½‘æ ¼æœç´¢æœ€ä¼˜æƒé‡"""

    best_score = 0
    best_weights = (0.5, 0.5)
    all_results = []

    for vector_weight, bm25_weight in weight_candidates:
        print(f"\næµ‹è¯•æƒé‡: å‘é‡={vector_weight}, BM25={bm25_weight}")

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = ProductionHybridRetriever(
            documents=splits,
            vector_weight=vector_weight,
            bm25_weight=bm25_weight
        )

        # è¿è¡Œè¯„ä¼°
        results = evaluate(
            lambda inputs: retriever.retrieve(inputs["query"]),
            data=dataset_name,
            evaluators=[recall_at_k_evaluator, mrr_evaluator],
            experiment_prefix=f"weight_v{vector_weight}_b{bm25_weight}"
        )

        # ç»¼åˆè¯„åˆ† (Recall@K * 0.6 + MRR * 0.4)
        combined_score = (
            results['recall_at_k_avg'] * 0.6 +
            results['mrr_avg'] * 0.4
        )

        all_results.append({
            "vector_weight": vector_weight,
            "bm25_weight": bm25_weight,
            "recall": results['recall_at_k_avg'],
            "mrr": results['mrr_avg'],
            "combined_score": combined_score
        })

        print(f"  Recall@K: {results['recall_at_k_avg']:.2%}")
        print(f"  MRR: {results['mrr_avg']:.3f}")
        print(f"  ç»¼åˆè¯„åˆ†: {combined_score:.3f}")

        if combined_score > best_score:
            best_score = combined_score
            best_weights = (vector_weight, bm25_weight)

    print(f"\nâœ… æœ€ä¼˜æƒé‡: å‘é‡={best_weights[0]}, BM25={best_weights[1]}")
    print(f"   ç»¼åˆè¯„åˆ†: {best_score:.3f}")

    return best_weights[0], best_weights[1], all_results

# æ‰§è¡Œç½‘æ ¼æœç´¢
weight_candidates = [
    (0.3, 0.7),
    (0.4, 0.6),
    (0.5, 0.5),
    (0.6, 0.4),
    (0.7, 0.3),
]

best_vector_weight, best_bm25_weight, all_results = grid_search_weights(
    weight_candidates
)

# å¯è§†åŒ–ç»“æœ
import matplotlib.pyplot as plt

vector_weights = [r['vector_weight'] for r in all_results]
combined_scores = [r['combined_score'] for r in all_results]

plt.figure(figsize=(10, 6))
plt.plot(vector_weights, combined_scores, marker='o')
plt.xlabel('Vector Weight')
plt.ylabel('Combined Score')
plt.title('Weight Optimization Results')
plt.grid(True)
plt.savefig('weight_optimization.png')
print("\nğŸ“Š ç»“æœå›¾è¡¨å·²ä¿å­˜: weight_optimization.png")
```

---

#### 5.3 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

##### 5.3.1 å¹¶å‘æ£€ç´¢ä¼˜åŒ–

**é—®é¢˜**: ä¸²è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢ï¼Œå»¶è¿Ÿç¿»å€

**è§£å†³æ–¹æ¡ˆ**: å¹¶å‘æ‰§è¡Œ

```python
import asyncio
from typing import List
from langchain_core.documents import Document

async def parallel_retrieve(
    query: str,
    vector_retriever,
    bm25_retriever
) -> List[Document]:
    """å¹¶å‘æ£€ç´¢"""

    # å¹¶å‘æ‰§è¡Œ
    vector_task = asyncio.create_task(
        asyncio.to_thread(vector_retriever.invoke, query)
    )
    bm25_task = asyncio.create_task(
        asyncio.to_thread(bm25_retriever.invoke, query)
    )

    vector_results, bm25_results = await asyncio.gather(
        vector_task,
        bm25_task
    )

    return vector_results, bm25_results

# ä½¿ç”¨
query = "Python 3.11æ–°ç‰¹æ€§"
vector_results, bm25_results = asyncio.run(
    parallel_retrieve(query, vector_retriever, bm25_retriever)
)

# æ€§èƒ½å¯¹æ¯”:
# ä¸²è¡Œ: 300ms + 200ms = 500ms
# å¹¶å‘: max(300ms, 200ms) = 300ms â¬…ï¸ æé€Ÿ40%
```

##### 5.3.2 ç¼“å­˜ç­–ç•¥

**1. æŸ¥è¯¢ç¼“å­˜**ï¼š

```python
from functools import lru_cache
import hashlib

def get_query_hash(query: str) -> str:
    """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ"""
    return hashlib.md5(query.encode()).hexdigest()

@lru_cache(maxsize=1000)
def cached_retrieve(query_hash: str, query: str, k: int) -> List[Document]:
    """å¸¦ç¼“å­˜çš„æ£€ç´¢"""
    return ensemble_retriever.invoke(query)

# ä½¿ç”¨
query = "Python 3.11æ–°ç‰¹æ€§"
query_hash = get_query_hash(query)
results = cached_retrieve(query_hash, query, k=5)

# ç¬¬äºŒæ¬¡è°ƒç”¨ç›´æ¥ä»ç¼“å­˜è¿”å› (0ms)
results = cached_retrieve(query_hash, query, k=5)
```

**2. Redisç¼“å­˜ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰**ï¼š

```python
import redis
import json
from typing import List

class RedisCache:
    """Redisç¼“å­˜"""

    def __init__(self, redis_host="localhost", redis_port=6379, ttl=3600):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            decode_responses=True
        )
        self.ttl = ttl  # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)

    def get(self, key: str) -> Optional[List[Document]]:
        """è·å–ç¼“å­˜"""
        cached = self.redis_client.get(key)
        if cached:
            data = json.loads(cached)
            return [
                Document(page_content=d["content"], metadata=d["metadata"])
                for d in data
            ]
        return None

    def set(self, key: str, documents: List[Document]):
        """è®¾ç½®ç¼“å­˜"""
        data = [
            {"content": doc.page_content, "metadata": doc.metadata}
            for doc in documents
        ]
        self.redis_client.setex(
            key,
            self.ttl,
            json.dumps(data)
        )

# ä½¿ç”¨
cache = RedisCache()

def retrieve_with_cache(query: str) -> List[Document]:
    """å¸¦Redisç¼“å­˜çš„æ£€ç´¢"""
    cache_key = f"retrieval:{get_query_hash(query)}"

    # å°è¯•ä»ç¼“å­˜è·å–
    cached_results = cache.get(cache_key)
    if cached_results:
        logger.info(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
        return cached_results

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ£€ç´¢
    results = ensemble_retriever.invoke(query)

    # å­˜å…¥ç¼“å­˜
    cache.set(cache_key, results)

    return results
```

##### 5.3.3 æ‰¹å¤„ç†ä¼˜åŒ–

```python
from typing import List, Dict
import asyncio

async def batch_retrieve(
    queries: List[str],
    retriever,
    batch_size: int = 10
) -> Dict[str, List[Document]]:
    """æ‰¹é‡æ£€ç´¢"""

    results = {}

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]

        # å¹¶å‘æ‰§è¡Œbatchå†…çš„æŸ¥è¯¢
        tasks = [
            asyncio.create_task(
                asyncio.to_thread(retriever.invoke, query)
            )
            for query in batch
        ]

        batch_results = await asyncio.gather(*tasks)

        # å­˜å‚¨ç»“æœ
        for query, docs in zip(batch, batch_results):
            results[query] = docs

    return results

# ä½¿ç”¨
queries = [f"æŸ¥è¯¢{i}" for i in range(100)]

results = asyncio.run(
    batch_retrieve(queries, ensemble_retriever, batch_size=10)
)

# æ€§èƒ½å¯¹æ¯”:
# ä¸²è¡Œ: 100æŸ¥è¯¢ * 300ms = 30ç§’
# æ‰¹å¤„ç†(batch=10): 10æ‰¹ * 300ms = 3ç§’ â¬…ï¸ æé€Ÿ10å€
```

---

#### 5.4 ç›‘æ§ä¸å‘Šè­¦

##### 5.4.1 LangSmithé›†æˆç›‘æ§

```python
import os
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "lsv2_..."

from langsmith import Client

client = Client()

def retrieve_with_monitoring(query: str) -> Dict:
    """å¸¦ç›‘æ§çš„æ£€ç´¢"""
    import time

    start_time = time.time()

    # æ‰§è¡Œæ£€ç´¢
    result = retriever.retrieve(query)

    latency_ms = (time.time() - start_time) * 1000

    # ä¸ŠæŠ¥åˆ°LangSmith
    client.create_run(
        name="hybrid_retrieval",
        run_type="retriever",
        inputs={"query": query},
        outputs={
            "documents": result["documents"],
            "num_results": len(result["documents"])
        },
        extra={
            "latency_ms": latency_ms,
            "cache_hit": result["metrics"].cache_hit,
            "vector_weight": retriever.vector_weight,
            "bm25_weight": retriever.bm25_weight
        }
    )

    return result
```

##### 5.4.2 è‡ªå®šä¹‰ç›‘æ§Dashboard

```python
# monitoring_dashboard.py
from dataclasses import dataclass
from typing import List
import time

@dataclass
class RetrievalMetrics:
    timestamp: float
    latency_ms: float
    num_results: int
    cache_hit: bool

class MonitoringDashboard:
    """ç›‘æ§Dashboard"""

    def __init__(self):
        self.metrics: List[RetrievalMetrics] = []

    def record(self, latency_ms: float, num_results: int, cache_hit: bool):
        """è®°å½•æŒ‡æ ‡"""
        self.metrics.append(RetrievalMetrics(
            timestamp=time.time(),
            latency_ms=latency_ms,
            num_results=num_results,
            cache_hit=cache_hit
        ))

    def get_stats(self, window_minutes: int = 5) -> Dict:
        """è·å–ç»Ÿè®¡æ•°æ®ï¼ˆæœ€è¿‘Nåˆ†é’Ÿï¼‰"""
        cutoff_time = time.time() - (window_minutes * 60)
        recent_metrics = [
            m for m in self.metrics
            if m.timestamp > cutoff_time
        ]

        if not recent_metrics:
            return {}

        latencies = [m.latency_ms for m in recent_metrics]

        return {
            "total_queries": len(recent_metrics),
            "avg_latency_ms": sum(latencies) / len(latencies),
            "p50_latency_ms": sorted(latencies)[len(latencies) // 2],
            "p95_latency_ms": sorted(latencies)[int(len(latencies) * 0.95)],
            "p99_latency_ms": sorted(latencies)[int(len(latencies) * 0.99)],
            "max_latency_ms": max(latencies),
            "cache_hit_rate": sum(1 for m in recent_metrics if m.cache_hit) / len(recent_metrics),
            "avg_results_per_query": sum(m.num_results for m in recent_metrics) / len(recent_metrics)
        }

    def check_alerts(self) -> List[str]:
        """æ£€æŸ¥å‘Šè­¦"""
        stats = self.get_stats(window_minutes=5)
        alerts = []

        # P95å»¶è¿Ÿå‘Šè­¦
        if stats.get("p95_latency_ms", 0) > 1000:
            alerts.append(f"âš ï¸  P95å»¶è¿Ÿè¿‡é«˜: {stats['p95_latency_ms']:.0f}ms")

        # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
        if stats.get("cache_hit_rate", 1.0) < 0.5:
            alerts.append(f"âš ï¸  ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {stats['cache_hit_rate']:.1%}")

        # ç»“æœæ•°é‡å‘Šè­¦
        if stats.get("avg_results_per_query", 5) < 3:
            alerts.append(f"âš ï¸  å¹³å‡ç»“æœæ•°è¿‡å°‘: {stats['avg_results_per_query']:.1f}")

        return alerts
```

---

#### 5.5 æ€»ç»“ï¼šç”Ÿäº§æ¸…å•

**éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•**ï¼š

- [ ] **æ€§èƒ½æµ‹è¯•**
  - [ ] P95å»¶è¿Ÿ < 500ms
  - [ ] P99å»¶è¿Ÿ < 1000ms
  - [ ] å¹¶å‘æ”¯æŒ > 100 QPS

- [ ] **è´¨é‡è¯„ä¼°**
  - [ ] Recall@5 > 85%
  - [ ] MRR > 0.7
  - [ ] å…³é”®è¯è¦†ç›–ç‡ > 80%

- [ ] **å¯é æ€§**
  - [ ] ç¼“å­˜ç­–ç•¥å·²é…ç½®
  - [ ] é”™è¯¯å¤„ç†å·²å®ç°
  - [ ] é™çº§æ–¹æ¡ˆå·²å‡†å¤‡

- [ ] **å¯è§‚æµ‹æ€§**
  - [ ] LangSmithè¿½è¸ªå·²å¯ç”¨
  - [ ] ç›‘æ§Dashboardå·²éƒ¨ç½²
  - [ ] å‘Šè­¦è§„åˆ™å·²é…ç½®

- [ ] **æˆæœ¬ä¼˜åŒ–**
  - [ ] ç¼“å­˜å‘½ä¸­ç‡ > 50%
  - [ ] æ‰¹å¤„ç†å·²å®ç°
  - [ ] Tokenä½¿ç”¨å·²ä¼˜åŒ–

**å…³é”®æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å‘Šè­¦é˜ˆå€¼ |
|------|--------|---------|
| **P95å»¶è¿Ÿ** | < 300ms | > 500ms |
| **P99å»¶è¿Ÿ** | < 500ms | > 1000ms |
| **Recall@5** | > 90% | < 80% |
| **MRR** | > 0.8 | < 0.6 |
| **ç¼“å­˜å‘½ä¸­ç‡** | > 70% | < 50% |
| **QPS** | > 100 | - |

**æŒç»­ä¼˜åŒ–å¾ªç¯**ï¼š

```
1. ç›‘æ§ç”Ÿäº§æŒ‡æ ‡
   â†“
2. è¯†åˆ«ç“¶é¢ˆ/é—®é¢˜
   â†“
3. A/Bæµ‹è¯•ä¼˜åŒ–æ–¹æ¡ˆ
   â†“
4. è¯„ä¼°æ•ˆæœ
   â†“
5. ç°åº¦å‘å¸ƒ
   â†“
6. å…¨é‡éƒ¨ç½²
   â†“
(å›åˆ°æ­¥éª¤1)
```

---
