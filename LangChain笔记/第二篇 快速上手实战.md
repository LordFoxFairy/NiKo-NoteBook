# ç¬¬äºŒç¯‡ å¿«é€Ÿä¸Šæ‰‹å®æˆ˜

> **ç›®æ ‡**: å°½å¿«æ„å»ºå‡ºç¬¬ä¸€ä¸ªå¯ç”¨çš„ Agent ç³»ç»Ÿ
> **ç‰ˆæœ¬è¦æ±‚**: LangChain 1.0.7+ã€LangGraph 1.0.3+ã€Python 3.10+
> **æ›´æ–°æ—¥æœŸ**: 2025-12

---

## ğŸ“Œ å‰ç½®çŸ¥è¯†è¯´æ˜

æœ¬ç¯‡å°†ä½¿ç”¨ä»¥ä¸‹æ ¸å¿ƒæ¦‚å¿µï¼Œå¦‚éœ€æ·±å…¥ç†è§£è¯·å‚è€ƒç›¸å…³ç« èŠ‚ï¼š

- **StateGraph**: LangGraphçš„çŠ¶æ€å›¾ï¼Œç”¨äºç¼–æ’å¤æ‚æµç¨‹ â†’ æœ¬ç¯‡ä»…ä½¿ç”¨åŸºç¡€åŠŸèƒ½ï¼Œé«˜çº§ç”¨æ³•è¯¦è§**ç¬¬ä¸‰ç¯‡ç¬¬7ç« **
- **Runnable Protocol**: ç»Ÿä¸€æ‰§è¡Œæ¥å£ï¼ˆinvoke/stream/batchï¼‰ â†’ å·²åœ¨**ç¬¬ä¸€ç¯‡ç¬¬2ç« **è®²è§£
- **LCELè¯­æ³•**: ç®¡é“æ“ä½œç¬¦`|`å’Œå¹¶è¡Œ`{}` â†’ å·²åœ¨**ç¬¬ä¸€ç¯‡ç¬¬2.2èŠ‚**è®²è§£

> ğŸ’¡ **å­¦ä¹ å»ºè®®**: åˆå­¦è€…å¯ä»¥å…ˆè·Ÿç€æœ¬ç¯‡ä»£ç å®è·µï¼Œé‡åˆ°ä¸ç†è§£çš„æ¦‚å¿µå†å›çœ‹ç›¸å…³ç« èŠ‚ã€‚

---

## ç¬¬1ç« ï¼šMessage ä¸ Tools åŸºç¡€

### 1.1 Message æ¶ˆæ¯ç³»ç»Ÿ

#### 1.1.1 æ¶ˆæ¯ç±»å‹ï¼šHumanMessageã€AIMessageã€SystemMessageã€ToolMessage

LangChain 1.0 å¼•å…¥äº†ç»Ÿä¸€çš„æ¶ˆæ¯ç±»å‹ç³»ç»Ÿï¼Œç”¨äºè¡¨ç¤ºäººæœºå¯¹è¯ä¸­çš„ä¸åŒè§’è‰²å’Œå†…å®¹ã€‚

**æ ¸å¿ƒæ¶ˆæ¯ç±»å‹**

```mermaid
graph TD
    A[BaseMessage] --> B[HumanMessage]
    A --> C[AIMessage]
    A --> D[SystemMessage]
    A --> E[ToolMessage]
    A --> F[FunctionMessage]

    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style C fill:#FFF9C4
    style D fill:#FFCCBC
    style E fill:#E1BEE7
    style F fill:#B2DFDB
```

**1. HumanMessage - ç”¨æˆ·æ¶ˆæ¯**

è¡¨ç¤ºç”¨æˆ·è¾“å…¥ï¼š

```python
from langchain_core.messages import HumanMessage

# æ–‡æœ¬æ¶ˆæ¯
message = HumanMessage(content="What is LangChain?")

# å¸¦å…ƒæ•°æ®
message = HumanMessage(
    content="Analyze this image",
    metadata={"user_id": "123"}
)
```

**2. AIMessage - AI å“åº”**

è¡¨ç¤º AI æ¨¡å‹çš„å›å¤ï¼š

```python
from langchain_core.messages import AIMessage

# ç®€å•æ–‡æœ¬å›å¤
response = AIMessage(content="LangChain is a framework...")

# å¸¦å·¥å…·è°ƒç”¨ï¼ˆLangChain 1.0 æ ¼å¼ï¼‰
response = AIMessage(
    content="",
    tool_calls=[
        {
            "id": "call_123",
            "name": "search",
            "args": {"query": "LangChain"}  # ç›´æ¥æ˜¯å­—å…¸ï¼Œä¸æ˜¯ JSON å­—ç¬¦ä¸²
        }
    ]
)
```

**3. SystemMessage - ç³»ç»Ÿæ¶ˆæ¯**

å®šä¹‰ AI è¡Œä¸ºå’Œè§’è‰²ï¼š

```python
from langchain_core.messages import SystemMessage

system = SystemMessage(
    content="""You are a helpful AI assistant.
    Always be concise and accurate.
    Use tools when necessary."""
)
```

**4. ToolMessage - å·¥å…·ç»“æœ**

è¡¨ç¤ºå·¥å…·æ‰§è¡Œçš„è¿”å›ç»“æœï¼š

```python
from langchain_core.messages import ToolMessage

tool_result = ToolMessage(
    content="Search results: ...",
    tool_call_id="call_123"
)
```

**æ¶ˆæ¯å¯¹è¯æµç¨‹**

```mermaid
sequenceDiagram
    participant User
    participant System
    participant AI
    participant Tool

    System->>AI: SystemMessage (è§’è‰²å®šä¹‰)
    User->>AI: HumanMessage (ç”¨æˆ·é—®é¢˜)
    AI->>AI: å†³ç­–ï¼šéœ€è¦å·¥å…·?
    AI->>Tool: AIMessage (tool_calls)
    Tool->>AI: ToolMessage (å·¥å…·ç»“æœ)
    AI->>User: AIMessage (æœ€ç»ˆå›ç­”)
```

**å®Œæ•´å¯¹è¯ç¤ºä¾‹**

```python
from langchain_core.messages import (
    SystemMessage, HumanMessage, AIMessage, ToolMessage
)

# æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²ç¤ºä¾‹
# è¿™å±•ç¤ºäº†ä¸€ä¸ªå…¸å‹çš„ Agent å¯¹è¯æµç¨‹ï¼šç³»ç»Ÿæ¶ˆæ¯ â†’ ç”¨æˆ·é—®é¢˜ â†’ AI å·¥å…·è°ƒç”¨ â†’ å·¥å…·ç»“æœ â†’ AI æœ€ç»ˆå›ç­”
conversation = [
    # 1. ç³»ç»Ÿæ¶ˆæ¯ï¼šå®šä¹‰ AI çš„è§’è‰²å’Œè¡Œä¸º
    SystemMessage(content="You are a helpful assistant."),

    # 2. ç”¨æˆ·æ¶ˆæ¯ï¼šç”¨æˆ·æå‡ºé—®é¢˜
    HumanMessage(content="What's the weather in Beijing?"),

    # 3. AI æ¶ˆæ¯ï¼šAI å†³å®šè°ƒç”¨å·¥å…·
    AIMessage(
        content="",  # å·¥å…·è°ƒç”¨æ—¶ content ä¸ºç©º
        tool_calls=[{
            "id": "call_123",  # å·¥å…·è°ƒç”¨çš„å”¯ä¸€ ID
            "name": "get_weather",  # å·¥å…·åç§°
            "args": {"city": "Beijing"}  # å·¥å…·å‚æ•°ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        }]
    ),

    # 4. å·¥å…·æ¶ˆæ¯ï¼šå·¥å…·æ‰§è¡Œç»“æœ
    ToolMessage(
        content="Temperature: 20Â°C, Sunny",  # å·¥å…·è¿”å›çš„ç»“æœ
        tool_call_id="call_123"  # å¯¹åº”ä¸Šé¢çš„å·¥å…·è°ƒç”¨ ID
    ),

    # 5. AI æ¶ˆæ¯ï¼šåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
    AIMessage(content="The weather in Beijing is 20Â°C and sunny.")
]
```

---

#### 1.1.2 Content Blocks æ ¸å¿ƒåˆ›æ–°ï¼ˆTextã€Tool Useã€Thinkingã€Imageï¼‰

**Content Blocks** æ˜¯ LangChain 1.0 çš„é‡å¤§åˆ›æ–°ï¼Œæä¾›äº†è·¨ Provider çš„ç»Ÿä¸€å†…å®¹è¡¨ç¤ºã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ Content Blocksï¼Ÿ**

åœ¨ LangChain 1.0 ä¹‹å‰ï¼š
- âŒ ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼ä¸ä¸€è‡´
- âŒ æ— æ³•ç»Ÿä¸€å¤„ç†å·¥å…·è°ƒç”¨ã€æ€è€ƒè¿‡ç¨‹ã€å¤šæ¨¡æ€å†…å®¹
- âŒ Provider åˆ‡æ¢éœ€è¦é‡å†™ä»£ç 

**Content Blocks è§£å†³æ–¹æ¡ˆ**ï¼š

```mermaid
graph LR
    A[AIMessage] --> B[Content Blocks]
    B --> C[Text Block]
    B --> D[Tool Use Block]
    B --> E[Thinking Block]
    B --> F[Image Block]
    B --> G[Citation Block]

    style B fill:#FFF9C4
    style C fill:#E8F5E9
    style D fill:#E3F2FD
    style E fill:#F3E5F5
    style F fill:#FFF3E0
    style G fill:#FCE4EC
```

**1. Text Block - æ–‡æœ¬å†…å®¹**

æœ€åŸºç¡€çš„æ–‡æœ¬å†…å®¹ï¼š

```python
from langchain_core.messages import AIMessage

response = AIMessage(
    content=[
        {"type": "text", "text": "Here's the answer..."}
    ]
)

# è®¿é—® content blocks
# æ³¨æ„: content æ˜¯åˆ—è¡¨æ—¶ç›´æ¥è¿­ä»£
for block in response.content:
    if block["type"] == "text":
        print(block["text"])
```

**2. Tool Use Block - å·¥å…·è°ƒç”¨**

ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ ¼å¼ï¼š

```python
response = AIMessage(
    content=[
        {
            "type": "tool_use",
            "id": "call_123",
            "name": "search",
            "input": {"query": "LangChain"}
        }
    ]
)

# æå–å·¥å…·è°ƒç”¨
for block in response.content:
    if block["type"] == "tool_use":
        print(f"Tool: {block['name']}")
        print(f"Input: {block['input']}")
```

**3. Thinking Block - æ€è€ƒè¿‡ç¨‹ï¼ˆClaudeï¼‰**

Claude æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼ˆExtended Thinkingï¼‰ï¼š

```python
# Claude è¿”å›çš„æ€è€ƒè¿‡ç¨‹
response = AIMessage(
    content=[
        {
            "type": "thinking",
            "thinking": "Let me analyze this step by step..."
        },
        {
            "type": "text",
            "text": "Based on my analysis..."
        }
    ]
)

# è®¿é—®æ€è€ƒè¿‡ç¨‹
for block in response.content:
    if block["type"] == "thinking":
        print(f"Thinking: {block['thinking']}")
```

**4. Image Block - å›¾åƒå†…å®¹**

å¤šæ¨¡æ€å›¾åƒæ”¯æŒï¼š

```python
from langchain_core.messages import HumanMessage

# å‘é€å›¾åƒ
message = HumanMessage(
    content=[
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image_url",
            "image_url": {"url": "https://example.com/image.jpg"}
        }
    ]
)
```

**5. Citation Block - å¼•ç”¨æ¥æºï¼ˆGeminiï¼‰**

Gemini æ¨¡å‹çš„å¼•ç”¨æ¥æºï¼š

```python
response = AIMessage(
    content=[
        {
            "type": "text",
            "text": "LangChain is a framework..."
        },
        {
            "type": "citation",
            "citation": {
                "source": "https://docs.langchain.com",
                "start_index": 0,
                "end_index": 50
            }
        }
    ]
)
```

---

#### 1.1.3 è·¨ Provider ç»Ÿä¸€å¤„ç†

**ç»Ÿä¸€çš„ content_blocks æ¥å£**

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# 1. åˆ›å»ºä¸åŒ Provider çš„æ¨¡å‹å®ä¾‹
# OpenAI æ¨¡å‹
openai_model = ChatOpenAI(model="gpt-4")
response1 = openai_model.invoke("Hello")

# Anthropic Claude æ¨¡å‹
claude_model = ChatAnthropic(model="claude-3-sonnet")
response2 = claude_model.invoke("Hello")

# 2. ç»Ÿä¸€è®¿é—®æ–¹å¼ - æ— éœ€å…³å¿ƒåº•å±‚ Provider
# Content Blocks æä¾›äº†è·¨ Provider çš„ç»Ÿä¸€æ¥å£
for response in [response1, response2]:
    # éå† content blocksï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼‰
    if isinstance(response.content, str):
        # ç®€å•æ–‡æœ¬å“åº”
        print(response.content)
    else:
        # Content blocks åˆ—è¡¨
        for block in response.content:
            if block["type"] == "text":
                print(block["text"])
            elif block["type"] == "thinking":
                # Claude ç‰¹æœ‰çš„æ€è€ƒè¿‡ç¨‹
                print(f"[Thinking] {block['thinking']}")
```

**Provider å·®å¼‚è‡ªåŠ¨å¤„ç†**

```mermaid
graph TD
    A[User Input] --> B{Provider Type}
    B -->|OpenAI| C[OpenAI Format]
    B -->|Anthropic| D[Anthropic Format]
    B -->|Google| E[Google Format]

    C --> F[Unified Content Blocks]
    D --> F
    E --> F

    F --> G[Application Code]

    style F fill:#C8E6C9
```

---

#### 1.1.4 å¤šæ¨¡æ€æ”¯æŒ

**å›¾åƒè¾“å…¥**

```python
from langchain_core.messages import HumanMessage
import base64

# æ–¹å¼1ï¼šURL
message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe this image"},
        {
            "type": "image_url",
            "image_url": {"url": "https://example.com/image.jpg"}
        }
    ]
)

# æ–¹å¼2ï¼šBase64 ç¼–ç 
with open("image.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

message = HumanMessage(
    content=[
        {"type": "text", "text": "What's in this image?"},
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_data}"
            }
        }
    ]
)
```

**éŸ³é¢‘å¤„ç†**

```python
# éŸ³é¢‘è¾“å…¥ï¼ˆæŸäº›æ¨¡å‹æ”¯æŒï¼‰
message = HumanMessage(
    content=[
        {
            "type": "audio_url",
            "audio_url": {"url": "https://example.com/audio.mp3"}
        }
    ]
)
```

**æ··åˆå†…å®¹**

```python
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

# 1. åˆ›å»ºæ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹å®ä¾‹
model = ChatOpenAI(model="gpt-4o")  # GPT-4o æ”¯æŒè§†è§‰è¾“å…¥

# 2. æ„å»ºæ··åˆå†…å®¹æ¶ˆæ¯ï¼šæ–‡æœ¬ + å›¾åƒ
message = HumanMessage(
    content=[
        {"type": "text", "text": "Analyze this chart and search for related data"},
        {"type": "image_url", "image_url": {"url": "chart.png"}},
    ]
)

# 3. è°ƒç”¨æ¨¡å‹
response = model.invoke(message)

# 4. å¤„ç†å¤šæ¨¡æ€å“åº”
# response.content å¯èƒ½åŒ…å«å¤šç§ç±»å‹çš„ content blocks
if isinstance(response.content, str):
    print(response.content)
else:
    for block in response.content:
        if block["type"] == "text":
            print(f"Text: {block['text']}")
        elif block["type"] == "tool_use":
            print(f"Tool Call: {block['name']}")
```

---

### 1.2 Tools å·¥å…·ä½“ç³»

#### 1.2.1 å·¥å…·å®šä¹‰æ–¹å¼ï¼š@toolã€StructuredToolã€BaseTool

**æ–¹å¼ 1ï¼š@tool è£…é¥°å™¨ï¼ˆæ¨èï¼‰**

æœ€ç®€å•å¿«é€Ÿçš„æ–¹å¼ï¼š

```python
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """Search the web for information.

    Args:
        query: The search query string
    """
    # å®ç°æœç´¢é€»è¾‘
    return f"Search results for: {query}"

# è‡ªåŠ¨æå–ï¼š
# - å‡½æ•°å -> å·¥å…·å
# - æ–‡æ¡£å­—ç¬¦ä¸² -> å·¥å…·æè¿°
# - å‚æ•°ç±»å‹æ³¨è§£ -> Schema
```

**å¸¦å¤æ‚å‚æ•°**ï¼š

```python
from typing import Optional
from pydantic import Field

@tool
def advanced_search(
    query: str = Field(description="æœç´¢æŸ¥è¯¢"),
    max_results: int = Field(default=10, description="æœ€å¤§ç»“æœæ•°"),
    language: Optional[str] = Field(default="en", description="è¯­è¨€")
) -> str:
    """Advanced search with filters."""
    return f"Found {max_results} results for '{query}' in {language}"
```

**æ–¹å¼ 2ï¼šStructuredTool ç±»**

æ›´çµæ´»çš„å®šä¹‰æ–¹å¼ï¼š

```python
from langchain_core.tools import StructuredTool
from pydantic import BaseModel

class SearchInput(BaseModel):
    query: str
    max_results: int = 10

def search_func(query: str, max_results: int = 10) -> str:
    return f"Found {max_results} results for {query}"

search_tool = StructuredTool.from_function(
    func=search_func,
    name="web_search",
    description="Search the web for information",
    args_schema=SearchInput
)
```

**æ–¹å¼ 3ï¼šBaseTool ç»§æ‰¿**

å®Œå…¨è‡ªå®šä¹‰æ§åˆ¶ï¼š

```python
from langchain_core.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    query: str = Field(description="æŸ¥è¯¢å­—ç¬¦ä¸²")
    max_results: int = Field(default=10, description="æœ€å¤§ç»“æœæ•°")

class CustomSearchTool(BaseTool):
    name = "custom_search"
    description = "è‡ªå®šä¹‰æœç´¢å·¥å…·"
    args_schema: Type[BaseModel] = SearchInput

    def _run(self, query: str, max_results: int = 10) -> str:
        """åŒæ­¥æ‰§è¡Œ"""
        return f"Search: {query}, Max: {max_results}"

    async def _arun(self, query: str, max_results: int = 10) -> str:
        """å¼‚æ­¥æ‰§è¡Œ"""
        return f"Async Search: {query}"

tool = CustomSearchTool()
```

**ä¸‰ç§æ–¹å¼å¯¹æ¯”**

| æ–¹å¼ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|
| @tool | ç®€å•å·¥å…· | å¿«é€Ÿã€ç®€æ´ | çµæ´»æ€§ä½ |
| StructuredTool | ä¸­ç­‰å¤æ‚åº¦ | å¹³è¡¡ | éœ€è¦é¢å¤–ç±» |
| BaseTool | å¤æ‚å·¥å…· | å®Œå…¨æ§åˆ¶ | ä»£ç é‡å¤š |

---

#### 1.2.2 å‚æ•° Schemaï¼ˆPydanticï¼‰

**åŸºç¡€ Schema å®šä¹‰**

```python
from pydantic import BaseModel, Field

class WeatherInput(BaseModel):
    """å¤©æ°”æŸ¥è¯¢å‚æ•°"""
    city: str = Field(description="åŸå¸‚åç§°")
    units: str = Field(
        default="celsius",
        description="æ¸©åº¦å•ä½",
        enum=["celsius", "fahrenheit"]
    )

@tool(args_schema=WeatherInput)
def get_weather(city: str, units: str = "celsius") -> str:
    """Get weather information for a city."""
    return f"Weather in {city}: 20Â°{units[0].upper()}"
```

**å¤æ‚åµŒå¥— Schema**

```python
from typing import List, Optional

class Location(BaseModel):
    city: str
    country: str

class SearchFilters(BaseModel):
    locations: List[Location]
    date_range: Optional[tuple] = None
    categories: List[str] = Field(default_factory=list)

@tool(args_schema=SearchFilters)
def filtered_search(
    locations: List[Location],
    date_range: Optional[tuple] = None,
    categories: List[str] = []
) -> str:
    """Search with complex filters."""
    return f"Searching in {len(locations)} locations"
```

**Schema éªŒè¯**

```python
from pydantic import field_validator

class EmailInput(BaseModel):
    email: str = Field(description="é‚®ç®±åœ°å€")

    @field_validator('email')
    @classmethod
    def validate_email(cls, v):
        if '@' not in v:
            raise ValueError('Invalid email address')
        return v

@tool(args_schema=EmailInput)
def send_email(email: str) -> str:
    """Send an email."""
    return f"Email sent to {email}"
```

---

#### 1.2.3 å·¥å…·è°ƒç”¨æœºåˆ¶ä¸å¹¶è¡Œè°ƒç”¨

**å•å·¥å…·è°ƒç”¨æµç¨‹**

```mermaid
sequenceDiagram
    participant Agent
    participant LLM
    participant Tool

    Agent->>LLM: ç”¨æˆ·é—®é¢˜
    LLM->>LLM: åˆ†æï¼šéœ€è¦å·¥å…·?
    LLM->>Agent: tool_calls
    Agent->>Tool: æ‰§è¡Œå·¥å…·
    Tool-->>Agent: å·¥å…·ç»“æœ
    Agent->>LLM: ToolMessage
    LLM-->>Agent: æœ€ç»ˆç­”æ¡ˆ
```

**å¹¶è¡Œå·¥å…·è°ƒç”¨**

LangChain 1.0 è‡ªåŠ¨æ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨ï¼š

```python
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

# 1. å®šä¹‰å¤šä¸ªå·¥å…·
@tool
def get_weather(city: str) -> str:
    """Get weather for a city.

    Args:
        city: The name of the city
    """
    return f"Weather in {city}: Sunny"

@tool
def get_time(city: str) -> str:
    """Get current time in a city.

    Args:
        city: The name of the city
    """
    return f"Time in {city}: 10:00 AM"

# 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
model = ChatOpenAI(model="gpt-4")

# 3. ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
tools = [get_weather, get_time]
model_with_tools = model.bind_tools(tools)

# 4. è°ƒç”¨æ¨¡å‹ï¼ˆLLM å¯èƒ½ä¼šå¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·ï¼‰
response = model_with_tools.invoke("What's the weather and time in Beijing?")

# 5. æŸ¥çœ‹å·¥å…·è°ƒç”¨
# response.tool_calls ä¼šåŒ…å«å¤šä¸ªå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœ LLM å†³å®šå¹¶è¡Œè°ƒç”¨ï¼‰
print(f"å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
for tool_call in response.tool_calls:
    print(f"å·¥å…·: {tool_call['name']}, å‚æ•°: {tool_call['args']}")
```

**å¹¶è¡Œæ‰§è¡Œæµç¨‹**ï¼š

```mermaid
graph TD
    A[LLM Decision] --> B[Tool Call 1: get_weather]
    A --> C[Tool Call 2: get_time]
    A --> D[Tool Call 3: search]

    B --> E[Parallel Execution]
    C --> E
    D --> E

    E --> F[Aggregate Results]
    F --> G[Final Response]

    style E fill:#C8E6C9
```

---

#### 1.2.4 é”™è¯¯å¤„ç†ç­–ç•¥

**åŸºç¡€é”™è¯¯å¤„ç†**

```python
from langchain_core.tools import tool

@tool
def divide(a: float, b: float) -> float:
    """Divide two numbers."""
    try:
        return a / b
    except ZeroDivisionError:
        return "Error: Division by zero"
    except Exception as e:
        return f"Error: {str(e)}"
```

**è¿”å›ç»“æ„åŒ–é”™è¯¯**

```python
from typing import Union, Optional
from pydantic import BaseModel, Field
from langchain_core.tools import tool

# å®šä¹‰ç»“æ„åŒ–çš„å·¥å…·ç»“æœç±»å‹
class ToolResult(BaseModel):
    """å·¥å…·æ‰§è¡Œç»“æœçš„ç»“æ„åŒ–è¡¨ç¤º"""
    success: bool = Field(description="æ‰§è¡Œæ˜¯å¦æˆåŠŸ")
    data: Optional[Union[str, dict]] = Field(default=None, description="æˆåŠŸæ—¶çš„æ•°æ®")
    error: Optional[str] = Field(default=None, description="å¤±è´¥æ—¶çš„é”™è¯¯ä¿¡æ¯")

@tool
def safe_search(query: str) -> str:
    """Safe search with error handling.

    Args:
        query: The search query string

    Returns:
        JSON string of ToolResult
    """
    try:
        # æ‰§è¡Œæœç´¢ï¼ˆç¤ºä¾‹ï¼šå‡è®¾æœ‰ä¸€ä¸ª perform_search å‡½æ•°ï¼‰
        # results = perform_search(query)
        results = f"Search results for: {query}"  # ç¤ºä¾‹å®ç°

        result = ToolResult(success=True, data=results)
        return result.model_dump_json()
    except Exception as e:
        result = ToolResult(success=False, error=str(e))
        return result.model_dump_json()
```

**é‡è¯•æœºåˆ¶**

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@tool
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))
def reliable_api_call(endpoint: str) -> str:
    """API call with automatic retry."""
    # ä¼šè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡
    response = call_api(endpoint)
    return response
```

**è¶…æ—¶æ§åˆ¶**

```python
import asyncio

@tool
async def async_search(query: str) -> str:
    """Search with timeout."""
    try:
        result = await asyncio.wait_for(
            search_async(query),
            timeout=5.0  # 5ç§’è¶…æ—¶
        )
        return result
    except asyncio.TimeoutError:
        return "Search timeout after 5 seconds"
```

**Agent çº§åˆ«é”™è¯¯å¤„ç†æœ€ä½³å®è·µ**

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒAgent è°ƒç”¨å¯èƒ½ä¼šé‡åˆ°å„ç§é”™è¯¯ï¼Œéœ€è¦å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

```python
from langchain_core.exceptions import OutputParserException
from langchain_core.runnables import RunnableConfig
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_agent_invoke(agent, input_data: dict, max_retries: int = 3):
    """å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†çš„ Agent è°ƒç”¨

    Args:
        agent: LangGraph Agent å®ä¾‹
        input_data: è¾“å…¥æ•°æ®å­—å…¸ï¼Œå¦‚ {"messages": [HumanMessage(...)]}
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        Agent è¾“å‡ºç»“æœæˆ–é”™è¯¯ä¿¡æ¯
    """
    for attempt in range(max_retries):
        try:
            # è°ƒç”¨ Agent
            result = agent.invoke(input_data)
            logger.info(f"Agent è°ƒç”¨æˆåŠŸ (å°è¯• {attempt + 1}/{max_retries})")
            return result

        except OutputParserException as e:
            # è¾“å‡ºè§£æé”™è¯¯ï¼ˆLLM è¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼‰
            logger.error(f"è¾“å‡ºè§£æå¤±è´¥: {e}")
            if attempt < max_retries - 1:
                logger.info("å°è¯•ä½¿ç”¨é™çº§æç¤ºè¯é‡è¯•...")
                # å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´æç¤ºè¯ï¼Œè¦æ±‚ LLM è¾“å‡ºæ›´ç®€å•çš„æ ¼å¼
                continue
            return {
                "error": "response_parse_failed",
                "message": "AI è¿”å›æ ¼å¼æ— æ³•è§£æï¼Œè¯·é‡è¯•"
            }

        except Exception as e:
            error_type = type(e).__name__

            # é€Ÿç‡é™åˆ¶é”™è¯¯ - ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
            if "RateLimitError" in error_type or "rate_limit" in str(e).lower():
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿: 1s, 2s, 4s
                    logger.warning(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                return {
                    "error": "rate_limit_exceeded",
                    "message": "API è°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•"
                }

            # API å¯†é’¥é”™è¯¯ - ä¸é‡è¯•
            elif "authentication" in str(e).lower() or "api_key" in str(e).lower():
                logger.error(f"è®¤è¯å¤±è´¥: {e}")
                return {
                    "error": "authentication_failed",
                    "message": "API å¯†é’¥é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡"
                }

            # ç½‘ç»œé”™è¯¯ - é‡è¯•
            elif "connection" in str(e).lower() or "timeout" in str(e).lower():
                if attempt < max_retries - 1:
                    logger.warning(f"ç½‘ç»œé”™è¯¯ï¼Œé‡è¯•ä¸­... ({e})")
                    time.sleep(1)
                    continue
                return {
                    "error": "network_error",
                    "message": "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œçŠ¶æ€"
                }

            # å…¶ä»–æœªçŸ¥é”™è¯¯
            else:
                logger.error(f"æœªçŸ¥é”™è¯¯ ({error_type}): {e}")
                if attempt < max_retries - 1:
                    continue
                return {
                    "error": "unknown_error",
                    "message": f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)[:100]}"
                }

    return {
        "error": "max_retries_exceeded",
        "message": f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})"
    }

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.messages import HumanMessage

result = safe_agent_invoke(
    agent=my_agent,
    input_data={"messages": [HumanMessage(content="å¸®æˆ‘æŸ¥è¯¢å¤©æ°”")]},
    max_retries=3
)

# æ£€æŸ¥ç»“æœ
if isinstance(result, dict) and "error" in result:
    print(f"âŒ è°ƒç”¨å¤±è´¥: {result['message']}")
else:
    print(f"âœ… è°ƒç”¨æˆåŠŸ: {result}")
```

**é”™è¯¯å¤„ç†æœ€ä½³å®è·µæ€»ç»“**ï¼š

| é”™è¯¯ç±»å‹ | æ˜¯å¦é‡è¯• | ç­–ç•¥ |
|---------|---------|------|
| **é€Ÿç‡é™åˆ¶** | âœ… æ˜¯ | æŒ‡æ•°é€€é¿é‡è¯• (1s â†’ 2s â†’ 4s) |
| **ç½‘ç»œè¶…æ—¶** | âœ… æ˜¯ | å›ºå®šé—´éš”é‡è¯• (1s) |
| **è¾“å‡ºè§£æå¤±è´¥** | âœ… æ˜¯ | è°ƒæ•´æç¤ºè¯åé‡è¯• |
| **API å¯†é’¥é”™è¯¯** | âŒ å¦ | ç«‹å³è¿”å›ï¼Œæç¤ºç”¨æˆ·æ£€æŸ¥é…ç½® |
| **å‚æ•°éªŒè¯é”™è¯¯** | âŒ å¦ | ç«‹å³è¿”å›ï¼Œæç¤ºç”¨æˆ·ä¿®æ­£è¾“å…¥ |

> ğŸ’¡ **æç¤º**: æ›´å¤šç”Ÿäº§çº§é”™è¯¯å¤„ç†ã€ç›‘æ§å‘Šè­¦ã€æ•…éšœæ¢å¤ç­–ç•¥ï¼Œè¯¦è§ **ç¬¬åç¯‡ã€Šç”Ÿäº§å®è·µä¸ç›‘æ§è¯„ä¼°ã€‹**ã€‚

---

### æœ¬ç« å°ç»“

**Message ç³»ç»Ÿæ ¸å¿ƒè¦ç‚¹**ï¼š
- âœ… ç»Ÿä¸€æ¶ˆæ¯ç±»å‹ï¼šHumanMessageã€AIMessageã€SystemMessageã€ToolMessage
- âœ… Content Blocksï¼šè·¨ Provider çš„ç»Ÿä¸€å†…å®¹è¡¨ç¤º
- âœ… å¤šæ¨¡æ€æ”¯æŒï¼šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘æ··åˆå¤„ç†

**Tools å·¥å…·ä½“ç³»æ ¸å¿ƒè¦ç‚¹**ï¼š
- âœ… ä¸‰ç§å®šä¹‰æ–¹å¼ï¼š@toolã€StructuredToolã€BaseTool
- âœ… Pydantic Schemaï¼šç±»å‹å®‰å…¨çš„å‚æ•°å®šä¹‰
- âœ… å¹¶è¡Œè°ƒç”¨ï¼šè‡ªåŠ¨ä¼˜åŒ–å·¥å…·æ‰§è¡Œ
- âœ… é”™è¯¯å¤„ç†ï¼šé‡è¯•ã€è¶…æ—¶ã€ç»“æ„åŒ–é”™è¯¯

**è®¾è®¡å“²å­¦**ï¼š
> ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼ + ç±»å‹å®‰å…¨çš„å·¥å…·å®šä¹‰ = å¯é çš„ Agent ç³»ç»Ÿ

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼šæ¶ˆæ¯ç±»å‹**
   æ„å»ºä¸€ä¸ªå®Œæ•´çš„å¯¹è¯æµç¨‹ï¼ŒåŒ…å« SystemMessageã€HumanMessageã€AIMessageï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰ã€ToolMessageã€‚

2. **ç»ƒä¹  2ï¼šå·¥å…·å®šä¹‰**
   ä½¿ç”¨ä¸‰ç§æ–¹å¼åˆ†åˆ«å®šä¹‰ä¸€ä¸ªè®¡ç®—å™¨å·¥å…·ï¼Œå¯¹æ¯”ä»£ç é‡å’Œçµæ´»æ€§ã€‚

3. **ç»ƒä¹  3ï¼šContent Blocks**
   ç¼–å†™ä»£ç æå– AIMessage ä¸­çš„æ‰€æœ‰ text block å’Œ tool_use blockã€‚

4. **ç»ƒä¹  4ï¼šé”™è¯¯å¤„ç†**
   å®ç°ä¸€ä¸ªå¸¦é‡è¯•å’Œè¶…æ—¶çš„ç½‘ç»œè¯·æ±‚å·¥å…·ã€‚

5. **æ€è€ƒé¢˜ï¼š**
   - Content Blocks ç›¸æ¯”ä¼ ç»Ÿå­—ç¬¦ä¸² content æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
   - ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥ä½¿ç”¨ BaseTool è€Œä¸æ˜¯ @toolï¼Ÿ
   - å¹¶è¡Œå·¥å…·è°ƒç”¨åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¼šå¤±æ•ˆï¼Ÿ

---

## ç¬¬2ç« ï¼šcreate_agent å¿«é€Ÿæ„å»º

### 2.1 Agent åŸºæœ¬æ¦‚å¿µ

#### 2.1.1 Agent å®šä¹‰ä¸æ‰§è¡Œå¾ªç¯

**ä»€ä¹ˆæ˜¯ Agentï¼Ÿ**

Agent æ˜¯ä¸€ä¸ªå¯ä»¥**è‡ªä¸»å†³ç­–ã€è°ƒç”¨å·¥å…·ã€è¿­ä»£æ±‚è§£**çš„ AI ç³»ç»Ÿã€‚

**æ ¸å¿ƒç‰¹å¾**ï¼š
- ğŸ¤– **è‡ªä¸»æ€§**ï¼šæ ¹æ®ç¯å¢ƒåŠ¨æ€å†³ç­–
- ğŸ”§ **å·¥å…·ä½¿ç”¨**ï¼šè°ƒç”¨å¤–éƒ¨å·¥å…·è·å–ä¿¡æ¯
- ğŸ”„ **è¿­ä»£æ‰§è¡Œ**ï¼šå¤šè½®æ¨ç†ç›´åˆ°è§£å†³é—®é¢˜
- ğŸ¯ **ç›®æ ‡å¯¼å‘**ï¼šæœç€ç›®æ ‡æŒç»­è¡ŒåŠ¨

```mermaid
graph LR
    A[ç”¨æˆ·é—®é¢˜] --> B[Agent]
    B --> C{éœ€è¦å·¥å…·?}
    C -->|æ˜¯| D[è°ƒç”¨å·¥å…·]
    C -->|å¦| E[ç›´æ¥å›ç­”]
    D --> F[è·å–ç»“æœ]
    F --> B
    E --> G[è¿”å›ç­”æ¡ˆ]

    style B fill:#FFF9C4
    style D fill:#E3F2FD
```

**Agent æ‰§è¡Œå¾ªç¯**

```python
# Agent æ‰§è¡Œå¾ªç¯ä¼ªä»£ç 
while not solved:
    # 1. æ€è€ƒï¼šåˆ†æå½“å‰æƒ…å†µ
    thought = model.think(current_state)

    # 2. å†³ç­–ï¼šé€‰æ‹©è¡ŒåŠ¨
    if need_more_info:
        # è°ƒç”¨å·¥å…·
        action = select_tool(available_tools)
        result = execute_tool(action)
        current_state.update(result)
    else:
        # ç»™å‡ºç­”æ¡ˆ
        final_answer = model.generate(current_state)
        break

return final_answer
```

**æ‰§è¡Œæµç¨‹å›¾**ï¼š

```mermaid
sequenceDiagram
    participant U as User
    participant A as Agent
    participant L as LLM
    participant T as Tools

    U->>A: è¾“å…¥é—®é¢˜
    loop è¿­ä»£å¾ªç¯
        A->>L: å½“å‰çŠ¶æ€ + å†å²
        L->>A: å†³ç­–ï¼šå·¥å…·è°ƒç”¨ / æœ€ç»ˆç­”æ¡ˆ
        alt éœ€è¦å·¥å…·
            A->>T: æ‰§è¡Œå·¥å…·
            T-->>A: å·¥å…·ç»“æœ
        else å¾—åˆ°ç­”æ¡ˆ
            A->>U: è¿”å›æœ€ç»ˆç­”æ¡ˆ
        end
    end
```

---

#### 2.1.2 Agent ä¸ Chain çš„æœ¬è´¨åŒºåˆ«

**Chainï¼ˆé“¾ï¼‰- é¢„å®šä¹‰æµç¨‹**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 1. å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatOpenAI(model="gpt-4")
parser = StrOutputParser()

# 2. Chain: å›ºå®šçš„çº¿æ€§æµç¨‹ï¼ˆä½¿ç”¨ LCEL ç®¡é“æ“ä½œç¬¦ï¼‰
chain = prompt | model | parser

# 3. æ‰§è¡Œï¼ˆæµç¨‹å›ºå®šï¼šprompt â†’ model â†’ parserï¼‰
result = chain.invoke({"topic": "programming"})
print(result)  # ç›´æ¥è¾“å‡ºç¬‘è¯æ–‡æœ¬
```

**ç‰¹ç‚¹**ï¼š
- âœ… æµç¨‹å›ºå®šï¼Œå¯é¢„æµ‹
- âœ… æ€§èƒ½ç¨³å®š
- âŒ ç¼ºä¹çµæ´»æ€§
- âŒ æ— æ³•åŠ¨æ€è°ƒæ•´

**Agentï¼ˆä»£ç†ï¼‰- åŠ¨æ€å†³ç­–**

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# 1. å®šä¹‰å·¥å…·
@tool
def search(query: str) -> str:
    """Search for information."""
    return f"Results for: {query}"

# 2. åˆ›å»ºæ¨¡å‹å’Œå·¥å…·åˆ—è¡¨
model = ChatOpenAI(model="gpt-4")
tools = [search]

# 3. Agent: åŠ¨æ€å†³ç­–æµç¨‹
# create_agent ä¼šè‡ªåŠ¨æ„å»ºä¸€ä¸ªå¯ä»¥å†³ç­–ã€è°ƒç”¨å·¥å…·ã€è¿­ä»£æ‰§è¡Œçš„ Agent
agent = create_agent(model, tools)

# 4. æ‰§è¡Œï¼ˆAgent å†…éƒ¨ä¼šè‡ªåŠ¨å¾ªç¯ï¼šæ€è€ƒ â†’ å†³ç­– â†’ æ‰§è¡Œå·¥å…· â†’ å†æ€è€ƒ...ï¼‰
result = agent.invoke({"messages": [("user", "Search for LangChain documentation")]})

# 5. è·å–æœ€ç»ˆç»“æœ
print(result["messages"][-1].content)
```

**ç‰¹ç‚¹**ï¼š
- âœ… åŠ¨æ€å†³ç­–ï¼Œçµæ´»
- âœ… å¯ä»¥ä½¿ç”¨å·¥å…·
- âŒ è¡Œä¸ºä¸å¯é¢„æµ‹
- âŒ Token æ¶ˆè€—å¯èƒ½è¾ƒé«˜

**å¯¹æ¯”è¡¨**

| ç»´åº¦ | Chain | Agent |
|------|-------|-------|
| æ‰§è¡Œæµç¨‹ | å›ºå®šçº¿æ€§ | åŠ¨æ€å¾ªç¯ |
| å†³ç­–èƒ½åŠ› | æ— å†³ç­– | LLM è‡ªä¸»å†³ç­– |
| å·¥å…·ä½¿ç”¨ | æ— ï¼ˆæˆ–å›ºå®šï¼‰ | åŠ¨æ€é€‰æ‹©å·¥å…· |
| é€‚ç”¨åœºæ™¯ | ç®€å•ã€ç¡®å®šæ€§ä»»åŠ¡ | å¤æ‚ã€éœ€è¦æ¨ç†çš„ä»»åŠ¡ |
| Token æ¶ˆè€— | å¯é¢„æµ‹ | ä¸å¯é¢„æµ‹ |
| å¯é æ€§ | é«˜ | ä¸­ï¼ˆä¾èµ– LLMï¼‰ |

```mermaid
graph TD
    A[ä»»åŠ¡] --> B{æ˜¯å¦éœ€è¦æ¨ç†?}
    B -->|å¦| C[ä½¿ç”¨ Chain]
    B -->|æ˜¯| D{æ˜¯å¦éœ€è¦å·¥å…·?}
    D -->|å¦| C
    D -->|æ˜¯| E[ä½¿ç”¨ Agent]

    style C fill:#C8E6C9
    style E fill:#FFF9C4
```

---

#### 2.1.3 é€‚ç”¨åœºæ™¯åˆ†æ

**ä½¿ç”¨ Agent çš„åœºæ™¯**

âœ… **ä¿¡æ¯æ£€ç´¢**ï¼šéœ€è¦æœç´¢ã€æŸ¥è¯¢æ•°æ®åº“
âœ… **å¤šæ­¥éª¤æ¨ç†**ï¼šéœ€è¦åˆ†è§£ä»»åŠ¡ã€é€æ­¥æ±‚è§£
âœ… **åŠ¨æ€å†³ç­–**ï¼šæ ¹æ®ä¸­é—´ç»“æœè°ƒæ•´ç­–ç•¥
âœ… **å·¥å…·ç»„åˆ**ï¼šéœ€è¦ç»„åˆä½¿ç”¨å¤šä¸ªå·¥å…·

**ç¤ºä¾‹åœºæ™¯**ï¼š
- ğŸ“Š æ•°æ®åˆ†æï¼šæŸ¥è¯¢æ•°æ®åº“ â†’ åˆ†æ â†’ ç”ŸæˆæŠ¥å‘Š
- ğŸ” ç ”ç©¶åŠ©æ‰‹ï¼šæœç´¢ â†’ æ€»ç»“ â†’ å¼•ç”¨æ¥æº
- ğŸ’¼ å®¢æœæœºå™¨äººï¼šæŸ¥è¯¢è®¢å• â†’ æ£€æŸ¥åº“å­˜ â†’ æ¨èè§£å†³æ–¹æ¡ˆ

**ä¸é€‚åˆä½¿ç”¨ Agent çš„åœºæ™¯**

âŒ **ç®€å•è½¬æ¢**ï¼šæ–‡æœ¬ç¿»è¯‘ã€æ‘˜è¦ï¼ˆç”¨ Chainï¼‰
âŒ **ç¡®å®šæ€§æµç¨‹**ï¼šå·²çŸ¥æ­¥éª¤çš„å·¥ä½œæµï¼ˆç”¨ LangGraphï¼‰
âŒ **ä½å»¶è¿Ÿè¦æ±‚**ï¼šå®æ—¶å“åº”ï¼ˆChain æ›´å¿«ï¼‰
âŒ **æˆæœ¬æ•æ„Ÿ**ï¼šToken é¢„ç®—æœ‰é™

---

### 2.2 æ ¸å¿ƒå‚æ•°è¯¦è§£

#### 2.2.1 modelã€toolsã€system_prompt

**model - é€‰æ‹© LLM**

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# OpenAI
model = ChatOpenAI(model="gpt-4", temperature=0)

# Anthropic
model = ChatAnthropic(model="claude-sonnet-4-5-20250929", temperature=0)
```

**æ¨¡å‹é€‰æ‹©å»ºè®®**ï¼š
- **GPT-4**ï¼šæ¨ç†èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
- **GPT-3.5-Turbo**ï¼šé€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- **Claude 3 Sonnet**ï¼šå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
- **Claude 3 Opus**ï¼šæœ€å¼ºæ¨ç†èƒ½åŠ›

**tools - å®šä¹‰å·¥å…·åˆ—è¡¨**

```python
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """Search the web."""
    return f"Results for: {query}"

@tool
def calculate(expression: str) -> str:
    """Calculate mathematical expression."""
    return str(eval(expression))

# å·¥å…·åˆ—è¡¨
tools = [search, calculate]
```

**system_prompt - å®šä¹‰ Agent è¡Œä¸º**

```python
system_prompt = """You are a helpful research assistant.

Your capabilities:
- Search the web for information
- Perform calculations
- Provide accurate citations

Guidelines:
1. Always verify information before answering
2. Use tools when necessary
3. Be concise and accurate
4. Cite your sources
"""
```

**æç¤ºè¯è®¾è®¡åŸåˆ™**ï¼š
- âœ… æ˜ç¡®è§’è‰²å’Œèƒ½åŠ›
- âœ… æä¾›æ¸…æ™°çš„æŒ‡å¯¼
- âœ… è¯´æ˜å·¥å…·ä½¿ç”¨åœºæ™¯
- âœ… å®šä¹‰è¾“å‡ºæ ¼å¼è¦æ±‚

---

#### 2.2.2 è¿›é˜¶é…ç½®ï¼šMiddleware ç®€ä»‹

**æ ¸å¿ƒé—®é¢˜**ï¼š`create_agent` å·²ç»å¾ˆå¥½ç”¨äº†ï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦ Middlewareï¼Ÿ

**çœŸå®åœºæ™¯**ï¼š
1. ğŸ“ **å¯¹è¯å¤ªé•¿ï¼ŒToken è¶…é™** â†’ éœ€è¦è‡ªåŠ¨æ‘˜è¦å†å²æ¶ˆæ¯
2. ğŸ”’ **ç”¨æˆ·è¾“å…¥åŒ…å«æ•æ„Ÿä¿¡æ¯** â†’ éœ€è¦è„±æ•åå†å‘é€ç»™ LLM
3. ğŸ”„ **å·¥å…·è°ƒç”¨å¤±è´¥** â†’ éœ€è¦è‡ªåŠ¨é‡è¯•
4. ğŸ’° **æˆæœ¬æ§åˆ¶** â†’ ç®€å•é—®é¢˜ç”¨ä¾¿å®œæ¨¡å‹ï¼Œå¤æ‚é—®é¢˜ç”¨é«˜çº§æ¨¡å‹

**Middleware æ˜¯ä»€ä¹ˆï¼Ÿ**

Middleware æ˜¯ LangChain 1.0 å¼•å…¥çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç”¨äºåœ¨ Agent æ‰§è¡Œè¿‡ç¨‹ä¸­æ³¨å…¥è‡ªå®šä¹‰é€»è¾‘å’Œæ§åˆ¶æµç¨‹ã€‚

**å¸¸è§ Middleware ç±»å‹**ï¼š

| Middleware | åŠŸèƒ½ | ä½¿ç”¨åœºæ™¯ |
|-----------|------|---------|
| `SummarizationMiddleware` | è‡ªåŠ¨æ‘˜è¦é•¿å¯¹è¯ | Token è¶…é™é£é™© |
| `PIIMiddleware` | æ•æ„Ÿä¿¡æ¯è„±æ• | å¤„ç†ç”¨æˆ·éšç§æ•°æ® |
| `ToolRetryMiddleware` | å·¥å…·è°ƒç”¨é‡è¯• | æé«˜å·¥å…·è°ƒç”¨å¯é æ€§ |
| `LLMToolSelectorMiddleware` | åŠ¨æ€ç­›é€‰å·¥å…· | å·¥å…·å¤ªå¤šå¯¼è‡´æ··ä¹± |

> ğŸ’¡ **æ·±å…¥å­¦ä¹ **ï¼šMiddleware çš„å®Œæ•´ç”¨æ³•ã€Hook ä½“ç³»ã€è‡ªå®šä¹‰ç­–ç•¥ç­‰å†…å®¹ï¼Œè¯·å‚è§ **ç¬¬å…«ç¯‡ã€ŠMiddleware å·¥ç¨‹åŒ–ã€‹**ã€‚

---

**recursion_limit - é˜²æ­¢æ— é™å¾ªç¯**

é€šè¿‡ `config` æ§åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼š

```python
from langgraph.errors import GraphRecursionError
from langchain.agents import create_agent

agent = create_agent(model=model, tools=tools)

try:
    result = agent.invoke(
        {"messages": [("user", "å¤æ‚é—®é¢˜")]},
        config={"recursion_limit": 11}  # æœ€å¤š 5 æ¬¡è¿­ä»£
    )
except GraphRecursionError as e:
    print(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼š{e}")
```

**è®¡ç®—å…¬å¼**ï¼š`recursion_limit = 2 Ã— æœŸæœ›è¿­ä»£æ¬¡æ•° + 1`

| æœŸæœ›è¿­ä»£æ¬¡æ•° | recursion_limit | é€‚ç”¨åœºæ™¯ |
|------------|-----------------|---------|
| 3 | 7 | æŸ¥å¤©æ°”ç­‰ç®€å•ä»»åŠ¡ |
| 5 | 11 | å¤šæ­¥æœç´¢ |
| 10 | 21 | ç ”ç©¶æŠ¥å‘Šç­‰å¤æ‚ä»»åŠ¡ |

> ğŸ’¡ é»˜è®¤å€¼ 10000ï¼Œå¼€å‘æ—¶åŸºæœ¬ä¸ä¼šè§¦å‘

---

**Callback Handler - ç®€å•æ—¥å¿—ï¼ˆé€‰å­¦ï¼‰**

**ç”¨é€”**ï¼šè§‚å¯Ÿ Agent æ‰§è¡Œè¿‡ç¨‹ï¼Œä¸ä¿®æ”¹è¡Œä¸º

**æœ€ç®€ç¤ºä¾‹**ï¼š

```python
from langchain_core.callbacks import BaseCallbackHandler

class SimpleLogger(BaseCallbackHandler):
    """ç®€å•çš„æ—¥å¿—å›è°ƒ"""

    def on_tool_start(self, serialized, input_str, **kwargs):
        print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {serialized.get('name')}")

    def on_tool_end(self, output, **kwargs):
        preview = getattr(output, "content", output)
        if isinstance(preview, str):
            text = preview
        else:
            text = str(preview)
        print(f"âœ… å·¥å…·ç»“æœ: {text[:50]}...")

        
# ä½¿ç”¨
result = agent.invoke(
    {"messages": [("user", "å¤©æ°”å¦‚ä½•ï¼Ÿ")]},
    config={"callbacks": [SimpleLogger()]}
)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ”§ è°ƒç”¨å·¥å…·: get_weather
âœ… å·¥å…·ç»“æœ: Weather in Beijing: Sunny, 20Â°C...
```

> ğŸ’¡ **æç¤º**ï¼šCallback ä¸»è¦ç”¨äºç›‘æ§å’Œè°ƒè¯•ã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ LangSmithï¼ˆç¬¬13ç« è¯¦è§£ï¼‰ã€‚

---

#### 2.2.3 response_format ç»“æ„åŒ–è¾“å‡º

> âš ï¸ **é‡è¦è¯´æ˜**ï¼š`create_agent` æ”¯æŒ `response_format` å‚æ•°ç”¨äºç»“æ„åŒ–è¾“å‡ºã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `model.with_structured_output()` æ–¹æ³•ã€‚

**Pydantic Schema å®šä¹‰**

```python
from pydantic import BaseModel, Field
from typing import List
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

class ResearchReport(BaseModel):
    """ç ”ç©¶æŠ¥å‘Šæ ¼å¼"""
    title: str = Field(description="æŠ¥å‘Šæ ‡é¢˜")
    summary: str = Field(description="æ‰§è¡Œæ‘˜è¦")
    findings: List[str] = Field(description="ä¸»è¦å‘ç°")
    sources: List[str] = Field(description="å¼•ç”¨æ¥æº")
    confidence: float = Field(description="ç½®ä¿¡åº¦ 0-1")

# âœ… æ­£ç¡®æ–¹æ³•ï¼šä½¿ç”¨ with_structured_output
model = ChatOpenAI(model="gpt-4")
structured_model = model.with_structured_output(ResearchReport)

# åˆ›å»º Agentï¼ˆä¸ä½¿ç”¨ response_formatï¼‰
agent = create_agent(
    model=structured_model,  # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹
    tools=tools
)

# è¿”å›ç»“æ„åŒ–å¯¹è±¡
result = agent.invoke({"messages": [("user", "Research about LangChain")]})
# ä»æœ€åä¸€æ¡ AI æ¶ˆæ¯ä¸­æå–ç»“æ„åŒ–è¾“å‡º
last_message = result['messages'][-1]
# æ³¨æ„ï¼šå…·ä½“æå–æ–¹å¼å–å†³äºæ¨¡å‹è¿”å›æ ¼å¼
```

**JSON Schema**

```python
# ä½¿ç”¨ JSON Schema å®šä¹‰è¾“å‡ºæ ¼å¼
response_schema = {
    "title": "SearchResponse",  # âš ï¸ å¿…é¡»åŒ…å« title
    "type": "object",
    "properties": {
        "answer": {"type": "string"},
        "confidence": {"type": "number"},
        "sources": {
            "type": "array",
            "items": {"type": "string"}
        }
    },
    "required": ["answer", "confidence"]
}

# âœ… æ­£ç¡®æ–¹æ³•ï¼šä½¿ç”¨ with_structured_output
from langchain.agents import create_agent

model = ChatOpenAI(model="gpt-4")
structured_model = model.with_structured_output(response_schema)

agent = create_agent(
    model=structured_model,
    tools=tools
)
```

---

### 2.3 ç¬¬ä¸€ä¸ª Agent å®æˆ˜

#### 2.3.1 ç¯å¢ƒå‡†å¤‡ä¸å·¥å…·å®šä¹‰

**å®‰è£…ä¾èµ–**

```bash
pip install langchain langchain-openai langchain-core python-dotenv
```

**ç¯å¢ƒé…ç½®**

```python
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-...
LANGSMITH_API_KEY=ls...
LANGSMITH_TRACING=true
```

```python
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# è¿™ä¼šè¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„å˜é‡åŠ è½½åˆ°ç¯å¢ƒä¸­
load_dotenv()

# éªŒè¯ç¯å¢ƒå˜é‡å·²åŠ è½½
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY not found in environment variables")
```

**å®šä¹‰å·¥å…·**

```python
from langchain_core.tools import tool
from typing import Dict, Any
import requests
import math

@tool
def search_wikipedia(query: str) -> str:
    """Search Wikipedia for information.

    Args:
        query: The search query string

    Returns:
        str: Search result snippet or error message
    """
    # 1. æ„å»º Wikipedia API è¯·æ±‚
    url = "https://en.wikipedia.org/w/api.php"
    params: Dict[str, Any] = {
        "action": "query",
        "list": "search",
        "srsearch": query,
        "format": "json"
    }

    # 2. å‘é€ HTTP è¯·æ±‚
    response = requests.get(url, params=params)
    data = response.json()

    # 3. è§£æå¹¶è¿”å›ç»“æœ
    if data["query"]["search"]:
        # è¿”å›ç¬¬ä¸€ä¸ªæœç´¢ç»“æœçš„æ‘˜è¦
        return data["query"]["search"][0]["snippet"]
    return "No results found"

@tool
def calculate(expression: str) -> str:
    """Calculate a mathematical expression.

    Args:
        expression: Math expression like "2+2" or "sqrt(16)"

    Returns:
        str: Calculation result or error message
    """
    try:
        # å®‰å…¨çš„ evalï¼ˆä»…æ”¯æŒæ•°å­¦å‡½æ•°ï¼‰
        # é€šè¿‡é™åˆ¶ __builtins__ å’Œæä¾›ç™½åå•å‡½æ•°ï¼Œé˜²æ­¢ä»£ç æ³¨å…¥
        result = eval(
            expression,
            {"__builtins__": None},  # ç¦ç”¨å†…ç½®å‡½æ•°
            {
                # åªå…è®¸è¿™äº›å®‰å…¨çš„æ•°å­¦å‡½æ•°
                "sqrt": math.sqrt,
                "sin": math.sin,
                "cos": math.cos,
                "pi": math.pi
            }
        )
        return str(result)
    except Exception as e:
        return f"Error: {str(e)}"

# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [search_wikipedia, calculate]
```

---

#### 2.3.2 åˆ›å»º Agent ä¸è¿è¡Œè°ƒè¯•

**åˆ›å»º Agent**

```python
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from typing import List
from langchain_core.tools import BaseTool

# 1. é…ç½® LLM æ¨¡å‹
model = ChatOpenAI(
    model="gpt-4",  # æ¨¡å‹åç§°
    temperature=0  # æ¸©åº¦è®¾ä¸º 0ï¼Œç¡®ä¿è¾“å‡ºç¨³å®š
)

# 2. å®šä¹‰ç³»ç»Ÿæç¤ºè¯
# ç³»ç»Ÿæç¤ºè¯ç”¨äºæŒ‡å¯¼ Agent çš„è¡Œä¸ºå’Œå†³ç­–é€»è¾‘
system_prompt = """You are a helpful research assistant.

You have access to:
1. Wikipedia search - for factual information
2. Calculator - for mathematical calculations

When answering:
- Use tools to verify facts
- Show your reasoning
- Cite sources when applicable
"""

# 3. åˆ›å»º Agent
# create_agent æ¥æ”¶æ¨¡å‹ã€å·¥å…·åˆ—è¡¨ã€ç³»ç»Ÿæç¤ºè¯ï¼Œè¿”å›ä¸€ä¸ªå¯æ‰§è¡Œçš„ Agent
agent = create_agent(
    model=model,  # LLM æ¨¡å‹å®ä¾‹
    tools=tools,  # å·¥å…·åˆ—è¡¨ï¼š[search_wikipedia, calculate]
    system_prompt=system_prompt  # ç³»ç»Ÿæç¤ºè¯
)

# Agent ç°åœ¨å·²å‡†å¤‡å¥½å¤„ç†ç”¨æˆ·è¯·æ±‚
# å®ƒä¼šè‡ªåŠ¨å†³ç­–æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Œå¹¶è¿›è¡Œå¤šè½®è¿­ä»£ç›´åˆ°å¾—å‡ºç­”æ¡ˆ
```

**è¿è¡Œ Agent**

```python
from typing import Dict, List, Any
from langchain_core.messages import BaseMessage

# 1. ç®€å•é—®é¢˜ï¼ˆå¯èƒ½ä¸éœ€è¦å·¥å…·ï¼‰
result: Dict[str, Any] = agent.invoke({
    "messages": [("user", "What is the capital of France?")]
})
# è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯çš„å†…å®¹
final_answer: str = result['messages'][-1].content
print(final_answer)

# 2. éœ€è¦è®¡ç®—çš„é—®é¢˜ï¼ˆAgent ä¼šè°ƒç”¨ calculate å·¥å…·ï¼‰
result = agent.invoke({
    "messages": [("user", "What is the square root of 144?")]
})
print(result['messages'][-1].content)
# è¾“å‡ºï¼šThe square root of 144 is 12.

# 3. éœ€è¦æœç´¢ + è®¡ç®—ï¼ˆAgent ä¼šè°ƒç”¨å¤šä¸ªå·¥å…·ï¼‰
result = agent.invoke({
    "messages": [("user", "Search for the population of Tokyo and calculate its square root")]
})
print(result['messages'][-1].content)

# 4. æŸ¥çœ‹å®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨ï¼‰
for i, msg in enumerate(result['messages']):
    print(f"[{i}] {msg.type}: {msg.content[:100] if msg.content else '(tool call)'}...")
```

**è°ƒè¯•è¾“å‡º**

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import langchain
langchain.debug = True

# æŸ¥çœ‹æ‰§è¡Œè¿‡ç¨‹
result = agent.invoke({
    "messages": [("user", "Complex question")]
})

# è¾“å‡ºä¼šæ˜¾ç¤ºï¼š
# - LLM æ€è€ƒè¿‡ç¨‹
# - å·¥å…·è°ƒç”¨å†³ç­–
# - å·¥å…·æ‰§è¡Œç»“æœ
# - æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ
```

---

#### 2.3.3 ç»“æ„åŒ–è¾“å‡ºï¼ˆPydantic Schemaï¼‰

**å®šä¹‰è¾“å‡ºæ ¼å¼**

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class FactCheckResult(BaseModel):
    """äº‹å®æ ¸æŸ¥ç»“æœ"""
    claim: str = Field(description="åŸå§‹å£°æ˜")
    verdict: str = Field(description="åˆ¤å®šç»“æœ: True/False/Uncertain")
    explanation: str = Field(description="è¯¦ç»†è§£é‡Š")
    sources: List[str] = Field(description="éªŒè¯æ¥æº")
    confidence: float = Field(ge=0, le=1, description="ç½®ä¿¡åº¦ 0-1")
```

**åˆ›å»ºç»“æ„åŒ– Agent**

```python
fact_checker = create_agent(
    model=model,
    tools=tools,
    response_format=FactCheckResult,
    system_prompt="""You are a fact-checking assistant.

For each claim:
1. Search for credible sources
2. Verify the information
3. Provide verdict and explanation
4. List all sources used
5. Assign confidence score
"""
)
```

**ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º**

```python
# è¾“å…¥å£°æ˜
claim = "The Earth is the third planet from the Sun"

# è·å–ç»“æ„åŒ–ç»“æœ
result = fact_checker.invoke({
    "messages": [("user", claim)]
})

# è®¿é—®ç»“æ„åŒ–å­—æ®µï¼ˆä» structured_response ä¸­è·å–ï¼‰
fact_result = result['structured_response']
print(f"Claim: {fact_result.claim}")
print(f"Verdict: {fact_result.verdict}")
print(f"Explanation: {fact_result.explanation}")
print(f"Sources: {', '.join(fact_result.sources)}")
print(f"Confidence: {fact_result.confidence:.2%}")

# è¾“å‡ºï¼š
# Claim: The Earth is the third planet from the Sun
# Verdict: True
# Explanation: According to astronomical data, Earth orbits the Sun...
# Sources: Wikipedia - Solar System, NASA
# Confidence: 95.00%
```

**æ‰¹é‡å¤„ç†**

```python
claims = [
    "Water boils at 100Â°C",
    "The Great Wall is visible from space",
    "Humans use only 10% of their brain"
]

# æ‰¹é‡æ ¸æŸ¥
results = [
    fact_checker.invoke({"messages": [("user", claim)]})
    for claim in claims
]

# ç”ŸæˆæŠ¥å‘Š
for result in results:
    fact_result = result['structured_response']
    print(f"{fact_result.verdict}: {fact_result.claim} ({fact_result.confidence:.0%})")
```

---

### æœ¬ç« å°ç»“

**Agent æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- âœ… Agent = è‡ªä¸»å†³ç­– + å·¥å…·ä½¿ç”¨ + è¿­ä»£æ‰§è¡Œ
- âœ… Agent ä¸ Chain çš„æœ¬è´¨åŒºåˆ«ï¼šåŠ¨æ€ vs å›ºå®š
- âœ… é€‚ç”¨åœºæ™¯ï¼šä¿¡æ¯æ£€ç´¢ã€å¤šæ­¥æ¨ç†ã€åŠ¨æ€å†³ç­–

**create_agent æ ¸å¿ƒå‚æ•°**ï¼š
- âœ… modelï¼šé€‰æ‹©åˆé€‚çš„ LLM
- âœ… toolsï¼šå®šä¹‰å¯ç”¨å·¥å…·åˆ—è¡¨
- âœ… system_promptï¼šæŒ‡å¯¼ Agent è¡Œä¸º
- âœ… recursion_limitï¼šé˜²æ­¢æ— é™å¾ªç¯ï¼ˆé€šè¿‡ config ä¼ å…¥ï¼‰
- âœ… response_formatï¼šç»“æ„åŒ–è¾“å‡º

**å®æˆ˜è¦ç‚¹**ï¼š
- âœ… å·¥å…·è®¾è®¡ï¼šæ¸…æ™°çš„æè¿°å’Œå‚æ•°å®šä¹‰
- âœ… æç¤ºè¯å·¥ç¨‹ï¼šæ˜ç¡®è§’è‰²ã€èƒ½åŠ›ã€æŒ‡å¯¼åŸåˆ™
- âœ… ç»“æ„åŒ–è¾“å‡ºï¼šPydantic Schema ä¿è¯ç±»å‹å®‰å…¨

**è®¾è®¡å“²å­¦**ï¼š
> ç®€å•çš„ API + å¼ºå¤§çš„ LLM + å®ç”¨çš„å·¥å…· = æ™ºèƒ½çš„ Agent

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼šåŸºç¡€ Agent**
   åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤©æ°”æŸ¥è¯¢ Agentï¼Œèƒ½å¤ŸæŸ¥è¯¢åŸå¸‚å¤©æ°”å¹¶è¿›è¡Œæ¸©åº¦å•ä½è½¬æ¢ã€‚

2. **ç»ƒä¹  2ï¼šå·¥å…·ç»„åˆ**
   å®ç°ä¸€ä¸ªæ•°å­¦æ•™å¸ˆ Agentï¼Œèƒ½å¤Ÿï¼š
   - è§£ç­”æ•°å­¦é—®é¢˜
   - éªŒè¯ç­”æ¡ˆ
   - ç»™å‡ºè¯¦ç»†æ­¥éª¤

3. **ç»ƒä¹  3ï¼šç»“æ„åŒ–è¾“å‡º**
   åˆ›å»ºä¸€ä¸ªæ–°é—»æ‘˜è¦ Agentï¼Œè¾“å‡ºåŒ…å«ï¼šæ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯ã€æƒ…æ„Ÿåˆ†æã€‚

4. **ç»ƒä¹  4ï¼šé”™è¯¯å¤„ç†**
   å®ç°ä¸€ä¸ªå¥å£®çš„ Agentï¼Œèƒ½å¤Ÿå¤„ç†ï¼š
   - å·¥å…·è°ƒç”¨å¤±è´¥
   - è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
   - æ— æ•ˆè¾“å…¥

5. **æ€è€ƒé¢˜ï¼š**
   - å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªä»»åŠ¡åº”è¯¥ä½¿ç”¨ Agent è¿˜æ˜¯ Chainï¼Ÿ
   - recursion_limit è®¾ç½®è¿‡å°æˆ–è¿‡å¤§ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
   - å¦‚ä½•è®¾è®¡ä¸€ä¸ªå¥½çš„ system_promptï¼Ÿ

---

## ç¬¬3ç« ï¼šå®æˆ˜æ¡ˆä¾‹ï¼šRAG Agent

### 3.1 RAG åŸºç¡€æ¦‚å¿µ

#### 3.1.1 æ–‡æ¡£å¤„ç†æµç¨‹

**ä»€ä¹ˆæ˜¯ RAGï¼Ÿ**

RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆ**æ£€ç´¢**å’Œ**ç”Ÿæˆ**çš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼º LLM çš„å›ç­”èƒ½åŠ›ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… è§£å†³ LLM çŸ¥è¯†æˆªæ­¢æ—¥æœŸé—®é¢˜
- âœ… æä¾›å¯éªŒè¯çš„ä¿¡æ¯æ¥æº
- âœ… é™ä½å¹»è§‰ï¼ˆHallucinationï¼‰
- âœ… æ”¯æŒç§æœ‰çŸ¥è¯†åº“

```mermaid
graph LR
    A[ç”¨æˆ·é—®é¢˜] --> B[æ£€ç´¢ç›¸å…³æ–‡æ¡£]
    B --> C[å‘é‡æ•°æ®åº“]
    C --> D[è¿”å› Top-K æ–‡æ¡£]
    D --> E[æ„å»º Context]
    E --> F[LLM ç”Ÿæˆç­”æ¡ˆ]
    F --> G[è¿”å›ç­”æ¡ˆ + æ¥æº]

    style B fill:#E3F2FD
    style C fill:#FFF9C4
    style F fill:#C8E6C9
```

**RAG å®Œæ•´æµç¨‹**

```mermaid
flowchart TD
    A[ç¦»çº¿ï¼šæ–‡æ¡£å¤„ç†] --> B[æ–‡æ¡£åŠ è½½]
    B --> C[æ–‡æ¡£åˆ†å— Chunking]
    C --> D[å‘é‡åŒ– Embedding]
    D --> E[å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“]

    F[åœ¨çº¿ï¼šæŸ¥è¯¢å¤„ç†] --> G[ç”¨æˆ·é—®é¢˜]
    G --> H[é—®é¢˜å‘é‡åŒ–]
    H --> I[ç›¸ä¼¼åº¦æ£€ç´¢]
    I --> E
    E --> J[è¿”å› Top-K æ–‡æ¡£]
    J --> K[LLM ç”Ÿæˆç­”æ¡ˆ]

    style A fill:#FFE0B2
    style F fill:#C5E1A5
```

**æ–‡æ¡£åˆ†å—ç­–ç•¥**

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºåˆ†å—å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å—å¤§å°
    chunk_overlap=200,      # å—ä¹‹é—´çš„é‡å 
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # åˆ†å‰²ä¼˜å…ˆçº§
)

# åˆ†å—æ–‡æ¡£
chunks = text_splitter.split_text(long_document)
```

**åˆ†å—ç­–ç•¥å¯¹æ¯”**ï¼š

| ç­–ç•¥ | chunk_size | chunk_overlap | é€‚ç”¨åœºæ™¯ |
|------|-----------|---------------|---------|
| å°å— | 200-500 | 50-100 | ç²¾ç¡®æ£€ç´¢ |
| ä¸­å— | 500-1000 | 100-200 | å¹³è¡¡æ€§èƒ½ |
| å¤§å— | 1000-2000 | 200-400 | ä¿æŒä¸Šä¸‹æ–‡ |

---

#### 3.1.2 å‘é‡åŒ–ä¸æ£€ç´¢ç­–ç•¥

**å‘é‡åŒ–ï¼ˆEmbeddingï¼‰**

å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼š

```python
from langchain_openai import OpenAIEmbeddings

# åˆ›å»º Embedding æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# å‘é‡åŒ–æ–‡æœ¬
vector = embeddings.embed_query("What is LangChain?")
print(len(vector))  # 1536 ç»´åº¦

# æ‰¹é‡å‘é‡åŒ–
vectors = embeddings.embed_documents([
    "Document 1",
    "Document 2",
    "Document 3"
])
```

**å‘é‡æ•°æ®åº“**

```python
from langchain_chroma import Chroma  # âœ… æ–°ç‰ˆï¼šä½¿ç”¨ langchain_chroma åŒ…

# åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆè‡ªåŠ¨æŒä¹…åŒ–ï¼‰
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒ‡å®šç›®å½•å³è‡ªåŠ¨æŒä¹…åŒ–
)

# âš ï¸ æ³¨æ„ï¼šChroma 0.4.x+ å·²åºŸå¼ƒ .persist() æ–¹æ³•
# ç°åœ¨åªéœ€æŒ‡å®š persist_directory å‚æ•°å³å¯è‡ªåŠ¨æŒä¹…åŒ–
```

**æ£€ç´¢ç­–ç•¥**

**1. ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆSimilarity Searchï¼‰**

```python
# åŸºç¡€ç›¸ä¼¼åº¦æ£€ç´¢
results = vectorstore.similarity_search(
    query="What is LangChain?",
    k=4  # è¿”å› Top-4
)

for doc in results:
    print(doc.page_content)
    print(doc.metadata)
```

**2. MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰**

å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼š

```python
# MMR æ£€ç´¢
results = vectorstore.max_marginal_relevance_search(
    query="What is LangChain?",
    k=4,
    fetch_k=20,  # å…ˆæ£€ç´¢20ä¸ª
    lambda_mult=0.5  # 0=å¤šæ ·æ€§, 1=ç›¸å…³æ€§
)
```

**3. å¸¦åˆ†æ•°çš„æ£€ç´¢**

```python
# è·å–ç›¸ä¼¼åº¦åˆ†æ•°
results = vectorstore.similarity_search_with_score(
    query="What is LangChain?",
    k=4
)

for doc, score in results:
    print(f"Score: {score:.4f}")
    print(doc.page_content)
```

---

### 3.2 RAG Agent å®ç°

#### 3.2.1 æ„å»ºæ£€ç´¢å·¥å…·

**å®Œæ•´ RAG å·¥å…·å®ç°**

```python
from langchain_core.tools import tool
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from typing import List
from langchain_core.documents import Document

# æ­¥éª¤1ï¼šåŠ è½½æ–‡æ¡£
# TextLoader ç”¨äºåŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶
loader = TextLoader("knowledge_base.txt")
documents: List[Document] = loader.load()

# æ­¥éª¤2ï¼šæ–‡æ¡£åˆ†å—
# RecursiveCharacterTextSplitter é€’å½’åœ°æŒ‰å­—ç¬¦åˆ†å‰²æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªå—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    chunk_overlap=200  # å—ä¹‹é—´çš„é‡å ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼‰
)
chunks: List[Document] = text_splitter.split_documents(documents)

# æ­¥éª¤3ï¼šåˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆè‡ªåŠ¨æŒä¹…åŒ–ï¼‰
# Embeddings æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma æ˜¯å‘é‡æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./db"  # æŒ‡å®šç›®å½•å³è‡ªåŠ¨æŒä¹…åŒ–
)

# æ­¥éª¤4ï¼šåˆ›å»ºæ£€ç´¢å·¥å…·
@tool
def search_knowledge_base(query: str) -> str:
    """Search the knowledge base for relevant information.

    Args:
        query: The search query string

    Returns:
        str: Formatted search results with document numbers
    """
    # æ£€ç´¢ Top-3 ç›¸å…³æ–‡æ¡£ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    docs: List[Document] = vectorstore.similarity_search(query, k=3)

    # æ ¼å¼åŒ–ç»“æœï¼Œæ·»åŠ æ–‡æ¡£ç¼–å·
    results: List[str] = []
    for i, doc in enumerate(docs, 1):
        results.append(f"[Document {i}]\n{doc.page_content}\n")

    return "\n".join(results)
```

**é«˜çº§æ£€ç´¢å·¥å…·**

```python
from typing import Optional, Dict, Any, List
from langchain_core.documents import Document

@tool
def advanced_search(
    query: str,
    filter_metadata: Optional[str] = None,
    max_results: int = 3
) -> str:
    """Advanced search with metadata filters.

    Args:
        query: The search query string
        filter_metadata: Optional filter in format 'key:value' (e.g., 'source:api')
        max_results: Maximum number of results to return

    Returns:
        str: JSON string of search results with metadata
    """
    # 1. æ„å»ºè¿‡æ»¤å™¨å­—å…¸
    filter_dict: Optional[Dict[str, Any]] = None
    if filter_metadata:
        # è§£æ 'key:value' æ ¼å¼çš„è¿‡æ»¤å™¨
        key, value = filter_metadata.split(":")
        filter_dict = {key: value}

    # 2. æ‰§è¡Œæ£€ç´¢ï¼ˆå¸¦è¿‡æ»¤å™¨ï¼‰
    docs: List[Document] = vectorstore.similarity_search(
        query,
        k=max_results,
        filter=filter_dict  # Chroma æ”¯æŒçš„å…ƒæ•°æ®è¿‡æ»¤
    )

    # 3. æ ¼å¼åŒ–è¾“å‡ºï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
    results: List[Dict[str, Any]] = []
    for doc in docs:
        results.append({
            "content": doc.page_content,
            "source": doc.metadata.get("source", "unknown"),
            "page": doc.metadata.get("page", "N/A")
        })

    return str(results)
```

---

#### 3.2.2 ç³»ç»Ÿæç¤ºè¯è®¾è®¡

**RAG Agent æç¤ºè¯æ¨¡æ¿**

```python
system_prompt = """You are a helpful AI assistant with access to a knowledge base.

Your capabilities:
- Search the knowledge base for accurate information
- Provide detailed answers based on retrieved documents
- Cite sources for all factual claims

Guidelines:
1. ALWAYS search the knowledge base before answering factual questions
2. Quote relevant passages from the documents
3. If information is not in the knowledge base, clearly state that
4. Cite document numbers when referencing information
5. Be concise but thorough

Response format:
- Answer the question directly
- Include relevant quotes in "quotation marks"
- List sources at the end: [Document 1], [Document 2], etc.

Example:
Question: What is LangChain?
Answer: LangChain is "a framework for developing applications powered by
language models" [Document 1]. It provides tools for "building applications
that combine LLMs with other sources of knowledge" [Document 2].

Sources: [Document 1], [Document 2]
"""
```

---

#### 3.2.3 è¿è¡Œä¸ä¼˜åŒ–ï¼ˆæŸ¥è¯¢é‡å†™ã€ç›¸å…³æ€§è¯„åˆ†ï¼‰

**åˆ›å»º RAG Agent**

```python
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent

# é…ç½®æ¨¡å‹
model = ChatOpenAI(model="gpt-4", temperature=0)

# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [search_knowledge_base]

# åˆ›å»º RAG Agent
rag_agent = create_agent(
    model=model,
    tools=tools,
    system_prompt=system_prompt,
)
```

**åŸºç¡€ä½¿ç”¨**

```python
# ç®€å•é—®ç­”
result = rag_agent.invoke({
    "messages": [("user", "What is LangChain?")]
})
print(result['messages'][-1].content)

# è¾“å‡ºç¤ºä¾‹ï¼š
# LangChain is "a framework for developing applications powered by language
# models" [Document 1]. It enables developers to "combine LLMs with external
# data sources and APIs" [Document 2].
#
# Sources: [Document 1], [Document 2]
```

**æŸ¥è¯¢é‡å†™ï¼ˆQuery Rewritingï¼‰**

æå‡æ£€ç´¢è´¨é‡ï¼š

```python
@tool
def rewrite_and_search(query: str) -> str:
    """Rewrite query for better retrieval, then search.

    Args:
        query: Original user query
    """
    # ä½¿ç”¨ LLM é‡å†™æŸ¥è¯¢
    rewrite_prompt = f"""Rewrite this query to be more specific and
    search-friendly. Only output the rewritten query.

    Original: {query}
    Rewritten:"""

    rewritten = model.invoke(rewrite_prompt).content

    # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢æ£€ç´¢
    docs = vectorstore.similarity_search(rewritten, k=3)

    return format_docs(docs)

# æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨
tools = [search_knowledge_base, rewrite_and_search]
```

**ç›¸å…³æ€§è¯„åˆ†ä¸è¿‡æ»¤**

```python
@tool
def scored_search(query: str, min_score: float = 0.7) -> str:
    """Search with relevance score filtering.

    Args:
        query: Search query
        min_score: Minimum relevance score (0-1)
    """
    # è·å–å¸¦åˆ†æ•°çš„ç»“æœ
    results = vectorstore.similarity_search_with_score(query, k=10)

    # è¿‡æ»¤ä½åˆ†ç»“æœ
    filtered_results = [
        (doc, score) for doc, score in results
        if score >= min_score
    ]

    # æ ¼å¼åŒ–è¾“å‡º
    output = []
    for doc, score in filtered_results[:3]:
        output.append(f"[Score: {score:.2f}]\n{doc.page_content}\n")

    return "\n".join(output) if output else "No relevant results found"
```

**å¤šæŸ¥è¯¢æ£€ç´¢**

å¢åŠ å¬å›ç‡ï¼š

```python
@tool
def multi_query_search(query: str) -> str:
    """Generate multiple queries and aggregate results.

    Args:
        query: Original query
    """
    # ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
    multi_query_prompt = f"""Generate 3 different versions of this query:

    Original: {query}

    Output format (one per line):
    1. ...
    2. ...
    3. ..."""

    variants = model.invoke(multi_query_prompt).content.split("\n")

    # å¯¹æ¯ä¸ªå˜ä½“æ£€ç´¢
    all_docs = []
    for variant in variants:
        docs = vectorstore.similarity_search(variant.strip(), k=2)
        all_docs.extend(docs)

    # å»é‡å¹¶è¿”å›
    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())

    return format_docs(unique_docs[:5])
```

**å®Œæ•´ RAG Agent ç¤ºä¾‹**

```python
from pydantic import BaseModel, Field
from typing import List

# å®šä¹‰ç»“æ„åŒ–è¾“å‡º
class RAGResponse(BaseModel):
    """RAG å“åº”æ ¼å¼"""
    answer: str = Field(description="å›ç­”å†…å®¹")
    quotes: List[str] = Field(description="å¼•ç”¨ç‰‡æ®µ")
    sources: List[str] = Field(description="æ¥æºåˆ—è¡¨")
    confidence: float = Field(ge=0, le=1, description="ç½®ä¿¡åº¦")

# åˆ›å»ºç»“æ„åŒ– RAG Agent
rag_agent = create_agent(
    model=model,
    tools=[
        search_knowledge_base,
        rewrite_and_search,
        scored_search,
        multi_query_search
    ],
    response_format=RAGResponse,
    system_prompt=system_prompt,
)

# ä½¿ç”¨
result = rag_agent.invoke({
    "messages": [("user", "Explain LangChain's architecture")]
})

# ç»“æ„åŒ–è®¿é—®
rag_result = result['structured_response']
print(f"Answer: {rag_result.answer}")
print(f"Quotes: {rag_result.quotes}")
print(f"Sources: {rag_result.sources}")
print(f"Confidence: {rag_result.confidence:.2%}")
```

---

### æœ¬ç« å°ç»“

**RAG æ ¸å¿ƒæµç¨‹**ï¼š
- âœ… æ–‡æ¡£å¤„ç†ï¼šåŠ è½½ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨
- âœ… æ£€ç´¢ç­–ç•¥ï¼šç›¸ä¼¼åº¦æ£€ç´¢ã€MMRã€å¸¦åˆ†æ•°æ£€ç´¢
- âœ… å·¥å…·æ„å»ºï¼šåŸºç¡€æ£€ç´¢ã€é«˜çº§è¿‡æ»¤ã€å¤šæŸ¥è¯¢

**ä¼˜åŒ–æŠ€å·§**ï¼š
- âœ… æŸ¥è¯¢é‡å†™ï¼šæå‡æ£€ç´¢è´¨é‡
- âœ… ç›¸å…³æ€§è¯„åˆ†ï¼šè¿‡æ»¤ä½è´¨é‡ç»“æœ
- âœ… å¤šæŸ¥è¯¢æ£€ç´¢ï¼šå¢åŠ å¬å›ç‡
- âœ… ç»“æ„åŒ–è¾“å‡ºï¼šä¿è¯ç­”æ¡ˆè´¨é‡

**æœ€ä½³å®è·µ**ï¼š
- âœ… åˆé€‚çš„åˆ†å—å¤§å°ï¼ˆ500-1000ï¼‰
- âœ… é€‚å½“çš„é‡å ï¼ˆ100-200ï¼‰
- âœ… æ˜ç¡®çš„ç³»ç»Ÿæç¤ºè¯
- âœ… å¼•ç”¨æ¥æºçš„éªŒè¯

**è®¾è®¡å“²å­¦**ï¼š
> æ£€ç´¢æ˜¯ä¸ºäº†å¢å¼ºï¼Œè€Œéæ›¿ä»£ LLM çš„æ¨ç†èƒ½åŠ›

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼šåŸºç¡€ RAG**
   å®ç°ä¸€ä¸ªç®€å•çš„ RAG Agentï¼Œèƒ½å¤Ÿä»æœ¬åœ°æ–‡æ¡£ä¸­æ£€ç´¢å¹¶å›ç­”é—®é¢˜ã€‚

2. **ç»ƒä¹  2ï¼šæŸ¥è¯¢ä¼˜åŒ–**
   å®ç°æŸ¥è¯¢é‡å†™åŠŸèƒ½ï¼Œå¯¹æ¯”é‡å†™å‰åçš„æ£€ç´¢è´¨é‡ã€‚

3. **ç»ƒä¹  3ï¼šå¤šæ•°æ®æº**
   æ„å»ºä¸€ä¸ªæ”¯æŒå¤šä¸ªçŸ¥è¯†åº“çš„ RAG Agentï¼ˆå¦‚ï¼šæŠ€æœ¯æ–‡æ¡£ + API æ–‡æ¡£ï¼‰ã€‚

4. **ç»ƒä¹  4ï¼šå¼•ç”¨éªŒè¯**
   å®ç°ä¸€ä¸ªéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿ Agent çš„å¼•ç”¨ç¡®å®æ¥è‡ªæ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚

5. **æ€è€ƒé¢˜ï¼š**
   - chunk_size å¤§å°å¦‚ä½•å½±å“æ£€ç´¢è´¨é‡ï¼Ÿ
   - MMR å’Œç›¸ä¼¼åº¦æ£€ç´¢æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
   - å¦‚ä½•è¯„ä¼° RAG ç³»ç»Ÿçš„è´¨é‡ï¼Ÿ

---

## ç¬¬4ç« ï¼šå®æˆ˜æ¡ˆä¾‹ï¼šSQL Agent

### 4.1 SQL Agent æ¶æ„

#### 4.1.1 æ•°æ®åº“è¿æ¥ä¸è¡¨ç»“æ„è·å–

**æ•°æ®åº“è¿æ¥**

```python
from sqlalchemy import create_engine, inspect
import pandas as pd

# è¿æ¥æ•°æ®åº“ï¼ˆSQLite ç¤ºä¾‹ï¼‰
database_uri = "sqlite:///example.db"
engine = create_engine(database_uri)

# æµ‹è¯•è¿æ¥
with engine.connect() as conn:
    result = conn.execute("SELECT 1")
    print("Database connected!")
```

**æ”¯æŒçš„æ•°æ®åº“ç±»å‹**ï¼š
- SQLiteï¼š`sqlite:///database.db`
- PostgreSQLï¼š`postgresql://user:pass@localhost/db`
- MySQLï¼š`mysql://user:pass@localhost/db`
- SQL Serverï¼š`mssql://user:pass@localhost/db`

**è¡¨ç»“æ„è·å–å·¥å…·**

```python
from langchain_core.tools import tool

@tool
def get_table_schema(table_name: str = None) -> str:
    """Get database table schema information.

    Args:
        table_name: Specific table name, or None for all tables
    """
    inspector = inspect(engine)

    if table_name:
        # è·å–ç‰¹å®šè¡¨çš„ä¿¡æ¯
        columns = inspector.get_columns(table_name)
        schema = f"Table: {table_name}\nColumns:\n"
        for col in columns:
            schema += f"  - {col['name']} ({col['type']})\n"
        return schema
    else:
        # è·å–æ‰€æœ‰è¡¨
        table_names = inspector.get_table_names()
        return f"Available tables: {', '.join(table_names)}"

@tool
def get_sample_data(table_name: str, limit: int = 3) -> str:
    """Get sample data from a table.

    Args:
        table_name: Table name
        limit: Number of rows to return
    """
    query = f"SELECT * FROM {table_name} LIMIT {limit}"
    df = pd.read_sql(query, engine)
    return df.to_string()
```

---

#### 4.1.2 SQL å·¥å…·å¼€å‘ï¼ˆæŸ¥è¯¢ã€å®‰å…¨é™åˆ¶ï¼‰

**SQL æŸ¥è¯¢å·¥å…·**

```python
@tool
def execute_sql_query(query: str) -> str:
    """Execute a SQL SELECT query and return results.

    Args:
        query: SQL SELECT query (READ ONLY)
    """
    # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ SELECT
    query_upper = query.upper().strip()
    if not query_upper.startswith("SELECT"):
        return "Error: Only SELECT queries are allowed"

    # å±é™©å…³é”®è¯æ£€æŸ¥
    dangerous_keywords = ["DROP", "DELETE", "UPDATE", "INSERT", "ALTER", "TRUNCATE"]
    for keyword in dangerous_keywords:
        if keyword in query_upper:
            return f"Error: {keyword} operations are not allowed"

    try:
        # æ‰§è¡ŒæŸ¥è¯¢
        df = pd.read_sql(query, engine)

        # é™åˆ¶è¿”å›è¡Œæ•°
        if len(df) > 100:
            return f"Warning: Result has {len(df)} rows. Showing first 100:\n{df.head(100).to_string()}"

        return df.to_string()

    except Exception as e:
        return f"Error executing query: {str(e)}"
```

**SQL æŸ¥è¯¢éªŒè¯å·¥å…·**

```python
@tool
def validate_sql_query(query: str) -> str:
    """Validate SQL query syntax without executing.

    Args:
        query: SQL query to validate
    """
    try:
        # ä½¿ç”¨ EXPLAIN éªŒè¯æŸ¥è¯¢
        with engine.connect() as conn:
            conn.execute(f"EXPLAIN {query}")
        return "Query syntax is valid"
    except Exception as e:
        return f"Query validation failed: {str(e)}"
```

**å®‰å…¨æ²™ç®±é…ç½®**

```python
from sqlalchemy import event
from sqlalchemy.pool import Pool

# è®¾ç½®æŸ¥è¯¢è¶…æ—¶ï¼ˆ5ç§’ï¼‰
@event.listens_for(Pool, "connect")
def set_timeout(dbapi_conn, connection_record):
    dbapi_conn.execute("PRAGMA busy_timeout = 5000")

# è®¾ç½®åªè¯»æ¨¡å¼
@event.listens_for(Pool, "connect")
def set_readonly(dbapi_conn, connection_record):
    dbapi_conn.execute("PRAGMA query_only = ON")
```

---

### 4.2 SQL Agent å®ç°

#### 4.2.1 ç³»ç»Ÿæç¤ºè¯ä¸ Agent åˆ›å»º

**SQL Agent æç¤ºè¯**

```python
system_prompt = """You are a SQL expert assistant with access to a database.

Your capabilities:
- Get table schemas and sample data
- Write and execute SQL SELECT queries
- Analyze query results and provide insights

Guidelines:
1. ALWAYS check table schema before writing queries
2. Use get_sample_data to understand data format
3. Write clear, optimized SQL queries
4. Only use SELECT statements (no modifications)
5. Explain your query logic
6. Validate queries before execution

Workflow:
1. User asks a question about data
2. Check relevant table schemas
3. (Optional) Get sample data to understand format
4. Write SQL query
5. Execute and analyze results
6. Provide clear answer with data

Example:
User: "How many users registered last month?"

Steps:
1. get_table_schema("users") â†’ check columns
2. execute_sql_query("SELECT COUNT(*) FROM users WHERE created_at >= date('now', '-1 month')")
3. Return: "There were 150 new users registered last month."
"""
```

**åˆ›å»º SQL Agent**

```python
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent

# æ¨¡å‹é…ç½®
model = ChatOpenAI(model="gpt-4", temperature=0)

# å·¥å…·åˆ—è¡¨
sql_tools = [
    get_table_schema,
    get_sample_data,
    execute_sql_query,
    validate_sql_query
]

# åˆ›å»º SQL Agent
sql_agent = create_agent(
    model=model,
    tools=sql_tools,
    system_prompt=system_prompt,
)
```

**åŸºç¡€ä½¿ç”¨**

```python
# ç®€å•æŸ¥è¯¢
result = sql_agent.invoke({
    "messages": [("user", "How many products are in stock?")]
})
print(result['messages'][-1].content)

# å¤æ‚åˆ†æ
result = sql_agent.invoke({
    "messages": [("user", "What are the top 5 customers by total purchase amount?")]
})
print(result['messages'][-1].content)

# æ—¶é—´åºåˆ—åˆ†æ
result = sql_agent.invoke({
    "messages": [("user", "Show monthly sales trend for the last 6 months")]
})
print(result['messages'][-1].content)
```

---

#### 4.2.2 Human-in-the-Loop å®¡æ‰¹æœºåˆ¶

**å®¡æ‰¹å·¥å…·å®ç°**

```python
from typing import Optional

# å…¨å±€å˜é‡å­˜å‚¨å¾…å®¡æ‰¹æŸ¥è¯¢
pending_approval = {}

@tool
def request_query_approval(query: str, reason: str) -> str:
    """Request human approval for a SQL query.

    Args:
        query: SQL query to approve
        reason: Reason for the query
    """
    approval_id = f"approval_{len(pending_approval) + 1}"
    pending_approval[approval_id] = {
        "query": query,
        "reason": reason,
        "status": "pending"
    }

    return f"""Query requires approval: {approval_id}

Query: {query}
Reason: {reason}

Status: Waiting for approval...
(Use approve_query('{approval_id}') or reject_query('{approval_id}'))
"""

def approve_query(approval_id: str) -> str:
    """Approve a pending query (called by human)."""
    if approval_id not in pending_approval:
        return "Approval ID not found"

    query = pending_approval[approval_id]["query"]
    pending_approval[approval_id]["status"] = "approved"

    # æ‰§è¡ŒæŸ¥è¯¢
    result = execute_sql_query.invoke(query)
    return f"Query approved and executed:\n{result}"

def reject_query(approval_id: str, reason: str = "") -> str:
    """Reject a pending query (called by human)."""
    if approval_id not in pending_approval:
        return "Approval ID not found"

    pending_approval[approval_id]["status"] = "rejected"
    return f"Query rejected. Reason: {reason}"
```

**æ•æ„Ÿæ“ä½œæ£€æµ‹**

```python
@tool
def smart_sql_query(query: str) -> str:
    """Execute SQL query with automatic approval for sensitive operations.

    Args:
        query: SQL query to execute
    """
    query_upper = query.upper()

    # å®šä¹‰éœ€è¦å®¡æ‰¹çš„æ“ä½œ
    sensitive_patterns = [
        ("DELETE", "Deleting data"),
        ("UPDATE", "Modifying data"),
        ("DROP", "Dropping tables"),
        ("ALTER", "Altering table structure"),
        ("TRUNCATE", "Truncating table")
    ]

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•æ„Ÿæ“ä½œ
    for pattern, reason in sensitive_patterns:
        if pattern in query_upper:
            return request_query_approval.invoke({
                "query": query,
                "reason": reason
            })

    # éæ•æ„Ÿæ“ä½œç›´æ¥æ‰§è¡Œ
    return execute_sql_query.invoke(query)
```

**å¸¦å®¡æ‰¹çš„ SQL Agent**

```python
# æ›´æ–°ç³»ç»Ÿæç¤ºè¯
approval_system_prompt = system_prompt + """

IMPORTANT - Approval Policy:
- For SELECT queries on single tables: Execute directly
- For JOIN queries on multiple tables: Request approval
- For queries affecting >1000 rows: Request approval
- For DELETE/UPDATE/DROP: ALWAYS request approval

Use smart_sql_query() which handles approval automatically.
"""

# åˆ›å»ºå¸¦å®¡æ‰¹çš„ Agent
sql_agent_with_approval = create_agent(
    model=model,
    tools=[
        get_table_schema,
        get_sample_data,
        smart_sql_query,
        request_query_approval
    ],
    system_prompt=approval_system_prompt,
)
```

**ä½¿ç”¨ç¤ºä¾‹**

```python
# ç®€å•æŸ¥è¯¢ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰
result = sql_agent_with_approval.invoke({
    "messages": [("user", "Count total orders")]
})
# ç›´æ¥è¿”å›ç»“æœ

# æ•æ„ŸæŸ¥è¯¢ï¼ˆéœ€è¦å®¡æ‰¹ï¼‰
result = sql_agent_with_approval.invoke({
    "messages": [("user", "Delete all orders from last year")]
})
# è¿”å›: Query requires approval: approval_1
#       Query: DELETE FROM orders WHERE year = 2024
#       Status: Waiting for approval...

# äººå·¥å®¡æ‰¹
approval_result = approve_query("approval_1")
# æˆ–
rejection_result = reject_query("approval_1", "Not authorized")
```

**ç»“æ„åŒ– SQL å“åº”**

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class SQLQueryResult(BaseModel):
    """SQL æŸ¥è¯¢ç»“æœ"""
    query: str = Field(description="æ‰§è¡Œçš„ SQL æŸ¥è¯¢")
    result_summary: str = Field(description="ç»“æœæ‘˜è¦")
    data: Optional[str] = Field(description="æ•°æ®è¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰")
    row_count: int = Field(description="è¿”å›è¡Œæ•°")
    insights: List[str] = Field(description="æ•°æ®æ´å¯Ÿ")

# åˆ›å»ºç»“æ„åŒ– SQL Agent
sql_agent_structured = create_agent(
    model=model,
    tools=sql_tools,
    response_format=SQLQueryResult,
    system_prompt=system_prompt
)

# ä½¿ç”¨
result = sql_agent_structured.invoke({
    "messages": [("user", "Top 5 products by sales")]
})

sql_result = result['structured_response']
print(f"Query: {sql_result.query}")
print(f"Summary: {sql_result.result_summary}")
print(f"Rows: {sql_result.row_count}")
print(f"Insights: {sql_result.insights}")
```

---

### æœ¬ç« å°ç»“

**SQL Agent æ ¸å¿ƒç»„ä»¶**ï¼š
- âœ… æ•°æ®åº“è¿æ¥ï¼šSQLAlchemy ç»Ÿä¸€æ¥å£
- âœ… è¡¨ç»“æ„å·¥å…·ï¼šè‡ªåŠ¨è·å– Schema å’Œæ ·æœ¬æ•°æ®
- âœ… æŸ¥è¯¢å·¥å…·ï¼šå®‰å…¨çš„ SQL æ‰§è¡Œï¼ˆåªè¯»ï¼‰
- âœ… å®¡æ‰¹æœºåˆ¶ï¼šHuman-in-the-Loop ä¿æŠ¤æ•æ„Ÿæ“ä½œ

**å®‰å…¨æœ€ä½³å®è·µ**ï¼š
- âœ… åªå…è®¸ SELECT æŸ¥è¯¢
- âœ… å…³é”®è¯é»‘åå•ï¼ˆDROPã€DELETE ç­‰ï¼‰
- âœ… æŸ¥è¯¢è¶…æ—¶æ§åˆ¶ï¼ˆ5ç§’ï¼‰
- âœ… ç»“æœè¡Œæ•°é™åˆ¶ï¼ˆ100è¡Œï¼‰
- âœ… æ•æ„Ÿæ“ä½œå®¡æ‰¹

**ç³»ç»Ÿæç¤ºè¯è¦ç‚¹**ï¼š
- âœ… æ˜ç¡®å·¥ä½œæµç¨‹
- âœ… å¼ºè°ƒå®‰å…¨è§„åˆ™
- âœ… æä¾›æŸ¥è¯¢ç¤ºä¾‹
- âœ… è¯´æ˜å®¡æ‰¹ç­–ç•¥

**è®¾è®¡å“²å­¦**ï¼š
> ä¾¿åˆ©æ€§ä¸å®‰å…¨æ€§çš„å¹³è¡¡ï¼šç®€å•æŸ¥è¯¢å¿«é€Ÿæ‰§è¡Œï¼Œæ•æ„Ÿæ“ä½œäººå·¥å®¡æ‰¹

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼šåŸºç¡€ SQL Agent**
   åˆ›å»ºä¸€ä¸ªç®€å•çš„ SQL Agentï¼Œèƒ½å¤ŸæŸ¥è¯¢ SQLite æ•°æ®åº“å¹¶å›ç­”é—®é¢˜ã€‚

2. **ç»ƒä¹  2ï¼šæŸ¥è¯¢ä¼˜åŒ–**
   å®ç°ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–å·¥å…·ï¼Œèƒ½å¤Ÿï¼š
   - æ£€æµ‹æ…¢æŸ¥è¯¢
   - å»ºè®®ç´¢å¼•
   - ä¼˜åŒ– JOIN è¯­å¥

3. **ç»ƒä¹  3ï¼šå¤šè¡¨åˆ†æ**
   æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«è¡¨å…³ç³»å¹¶æ‰§è¡Œ JOIN æŸ¥è¯¢çš„ Agentã€‚

4. **ç»ƒä¹  4ï¼šå®¡æ‰¹ç³»ç»Ÿ**
   å®ç°ä¸€ä¸ªå®Œæ•´çš„å®¡æ‰¹å·¥ä½œæµï¼š
   - è¯·æ±‚å®¡æ‰¹
   - é‚®ä»¶é€šçŸ¥
   - å®¡æ‰¹è®°å½•
   - è¶…æ—¶å¤„ç†

5. **æ€è€ƒé¢˜ï¼š**
   - å¦‚ä½•é˜²æ­¢ SQL æ³¨å…¥æ”»å‡»ï¼Ÿ
   - ä»€ä¹ˆæ ·çš„æŸ¥è¯¢åº”è¯¥éœ€è¦å®¡æ‰¹ï¼Ÿ
   - å¦‚ä½•å¤„ç†å¤§ç»“æœé›†ï¼ˆ>10000è¡Œï¼‰ï¼Ÿ
   - SQL Agent ä¸ RAG Agent æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ

---

---

## ç¬¬5ç« ï¼šMemory ä¸ä¸Šä¸‹æ–‡ç®¡ç†

> **ç›®æ ‡**: è®© Agent æ‹¥æœ‰è®°å¿†èƒ½åŠ›ï¼Œæ„å»ºæœ‰è¿ç»­æ€§çš„å¯¹è¯ç³»ç»Ÿ

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ„å»ºçš„ Agent éƒ½æ˜¯**æ— è®°å¿†çš„**â€”â€”æ¯æ¬¡å¯¹è¯éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒAgent æ— æ³•è®°ä½ä¹‹å‰çš„äº¤äº’ã€‚æœ¬ç« å°†æ•™ä½ å¦‚ä½•è®© Agent æ‹¥æœ‰è®°å¿†ã€‚

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦ Memory

#### 5.1.1 æ— è®°å¿† Agent çš„é—®é¢˜

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from typing import Dict, Any, List
from langchain_core.messages import BaseMessage

# 1. åˆ›å»ºæ— è®°å¿† Agentï¼ˆæ²¡æœ‰ checkpointerï¼‰
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[]  # ç®€å•ç¤ºä¾‹ï¼Œä¸ä½¿ç”¨å·¥å…·
)

# 2. ç¬¬ä¸€è½®å¯¹è¯
result1: Dict[str, Any] = agent.invoke({"messages": [("user", "æˆ‘å«å¼ ä¸‰")]})
print(result1["messages"][-1].content)
# è¾“å‡ºï¼šä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼

# 3. ç¬¬äºŒè½®å¯¹è¯ï¼ˆæ–°çš„ invoke è°ƒç”¨ï¼ŒçŠ¶æ€è¢«é‡ç½®ï¼‰
result2: Dict[str, Any] = agent.invoke({"messages": [("user", "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]})
print(result2["messages"][-1].content)
# è¾“å‡ºï¼šæŠ±æ­‰ï¼Œæˆ‘ä¸çŸ¥é“ä½ çš„åå­—
# âŒ Agent å¿˜è®°äº†ç¬¬ä¸€è½®å¯¹è¯ï¼å› ä¸ºæ¯æ¬¡ invoke éƒ½æ˜¯å…¨æ–°çš„çŠ¶æ€
```

### 5.2 ä½¿ç”¨ LangGraph Checkpointer å®ç°è®°å¿†

LangChain 1.0 æ¨èä½¿ç”¨ **LangGraph çš„ Checkpointer æœºåˆ¶**æ¥å®ç°å¯¹è¯è®°å¿†ã€‚

#### 5.2.1 åŸºç¡€è®°å¿†å®ç°

```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver
from langchain_openai import ChatOpenAI
from typing import Dict, Any
from langchain_core.runnables import RunnableConfig

# 1. åˆ›å»º Checkpointerï¼ˆå†…å­˜å­˜å‚¨ï¼‰
# InMemorySaver ä¼šåœ¨å†…å­˜ä¸­ä¿å­˜å¯¹è¯å†å²
checkpointer = InMemorySaver()

# 2. åˆ›å»ºå¸¦è®°å¿†çš„ Agent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[],  # ç®€å•ç¤ºä¾‹
    checkpointer=checkpointer  # å…³é”®ï¼ä¼ å…¥ checkpointer å¯ç”¨è®°å¿†
)

# 3. ä½¿ç”¨ thread_id å®ç°ä¼šè¯éš”ç¦»
# thread_id ç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯ä¼šè¯
config: RunnableConfig = {"configurable": {"thread_id": "user-123"}}

# 4. ç¬¬ä¸€è½®å¯¹è¯
result1: Dict[str, Any] = agent.invoke(
    {"messages": [("user", "æˆ‘å«å¼ ä¸‰")]},
    config=config  # ä¼ å…¥ configï¼ŒæŒ‡å®šä¼šè¯ ID
)
print(result1["messages"][-1].content)
# è¾“å‡ºï¼šä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼

# 5. ç¬¬äºŒè½®å¯¹è¯ï¼ˆä½¿ç”¨åŒä¸€ä¸ª thread_idï¼‰
result2: Dict[str, Any] = agent.invoke(
    {"messages": [("user", "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]},
    config=config  # ä½¿ç”¨ç›¸åŒçš„ thread_id
)
print(result2["messages"][-1].content)
# âœ… è¾“å‡ºï¼šä½ å«å¼ ä¸‰
# æˆåŠŸè®°ä½äº†ä¹‹å‰çš„å¯¹è¯ï¼Checkpointer ä¿å­˜äº†å†å²æ¶ˆæ¯
```

#### 5.2.2 æŒä¹…åŒ–è®°å¿†ï¼ˆSQLiteï¼‰

```python
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from typing import Dict, Any
from langchain_core.runnables import RunnableConfig

# 1. ä½¿ç”¨ SQLite æŒä¹…åŒ–è®°å¿†
# è®°å¿†ä¼šä¿å­˜åˆ°ç£ç›˜æ–‡ä»¶ï¼Œé‡å¯ç¨‹åºåä»ç„¶å­˜åœ¨
checkpointer = SqliteSaver.from_conn_string("./checkpoints.db")

# 2. åˆ›å»ºå¸¦æŒä¹…åŒ–è®°å¿†çš„ Agent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[],
    checkpointer=checkpointer  # ä½¿ç”¨ SQLite checkpointer
)

# 3. ä½¿ç”¨ä¼šè¯ ID
config: RunnableConfig = {"configurable": {"thread_id": "user-123"}}

# 4. æŸ¥è¯¢ä¹‹å‰çš„å¯¹è¯
# å³ä½¿é‡å¯ç¨‹åºï¼Œä¹Ÿèƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹
result: Dict[str, Any] = agent.invoke(
    {"messages": [("user", "æˆ‘ä¹‹å‰å‘Šè¯‰è¿‡ä½ ä»€ä¹ˆï¼Ÿ")]},
    config=config
)
print(result["messages"][-1].content)
# å¦‚æœä¹‹å‰æœ‰å¯¹è¯ï¼ŒAgent ä¼šè®°å¾—
```

#### 5.2.3 ç”Ÿäº§çº§è®°å¿†ï¼ˆPostgreSQLï¼‰

```python
from langgraph.checkpoint.postgres import PostgresSaver
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

# 1. ä½¿ç”¨ PostgreSQLï¼ˆé€‚åˆç”Ÿäº§ç¯å¢ƒï¼‰
# PostgreSQL æä¾›æ›´å¥½çš„æ€§èƒ½ã€å¹¶å‘æ”¯æŒå’Œæ•°æ®æŒä¹…æ€§
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost/chatdb"
)

# 2. åˆ›å»ºç”Ÿäº§çº§ Agent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[],
    checkpointer=checkpointer
)

# 3. ç”Ÿäº§ç¯å¢ƒä¼˜åŠ¿ï¼š
# - æ”¯æŒé«˜å¹¶å‘è®¿é—®
# - æ•°æ®å¤‡ä»½å’Œæ¢å¤
# - è·¨æœåŠ¡å™¨å…±äº«è®°å¿†
# - æ›´å¥½çš„æŸ¥è¯¢æ€§èƒ½
```

### 5.3 è®°å¿†ç®¡ç†æœ€ä½³å®è·µ

#### 5.3.1 ä¼šè¯éš”ç¦»

```python
# ä¸åŒç”¨æˆ·ä½¿ç”¨ä¸åŒçš„ thread_id
user1_config = {"configurable": {"thread_id": "user-001"}}
user2_config = {"configurable": {"thread_id": "user-002"}}

# ç”¨æˆ·1çš„å¯¹è¯
agent.invoke({"messages": [("user", "æˆ‘æ˜¯ç”¨æˆ·1")]}, config=user1_config)

# ç”¨æˆ·2çš„å¯¹è¯ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
agent.invoke({"messages": [("user", "æˆ‘æ˜¯ç”¨æˆ·2")]}, config=user2_config)
```

#### 5.3.2 æŸ¥çœ‹å¯¹è¯å†å²

```python
# è·å–æŸä¸ªä¼šè¯çš„å®Œæ•´å†å²
from langgraph.checkpoint.base import Checkpoint

config = {"configurable": {"thread_id": "user-123"}}
state = agent.get_state(config)

# æŸ¥çœ‹æ‰€æœ‰æ¶ˆæ¯
for msg in state.values["messages"]:
    print(f"{msg.type}: {msg.content}")
```

#### 5.3.3 æ¸…é™¤è®°å¿†

```python
# æ–¹æ³•1ï¼šä½¿ç”¨æ–°çš„ thread_id å¼€å§‹æ–°å¯¹è¯
new_config = {"configurable": {"thread_id": "user-123-new-session"}}

# æ–¹æ³•2ï¼šæ‰‹åŠ¨æ¸…é™¤ checkpointer ä¸­çš„æ•°æ®
# å…·ä½“æ–¹æ³•å–å†³äºä½¿ç”¨çš„ checkpointer ç±»å‹
```

### 5.4 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

é•¿å¯¹è¯ä¼šå¯¼è‡´ token æ¶ˆè€—å¿«é€Ÿå¢é•¿ã€‚ä»¥ä¸‹æ˜¯ä¼˜åŒ–ç­–ç•¥ï¼š

#### 5.4.1 é™åˆ¶å†å²é•¿åº¦

```python
from langchain_core.messages import trim_messages

# åªä¿ç•™æœ€è¿‘çš„ 10 æ¡æ¶ˆæ¯
def limit_history(state):
    messages = state["messages"]
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘10æ¡
    trimmed = trim_messages(
        messages,
        max_tokens=2000,
        strategy="last",
        token_counter=len,
    )
    return {"messages": trimmed}
```

#### 5.4.2 ä½¿ç”¨æ€»ç»“ç­–ç•¥

```python
from langchain_openai import ChatOpenAI

summarizer = ChatOpenAI(model="gpt-4o-mini")  # ä½¿ç”¨ä¾¿å®œæ¨¡å‹

def summarize_history(messages):
    """å¯¹å†å²å¯¹è¯è¿›è¡Œæ€»ç»“"""
    if len(messages) < 10:
        return messages

    # æ€»ç»“å‰é¢çš„æ¶ˆæ¯
    old_messages = messages[:-5]  # ä¿ç•™æœ€è¿‘5æ¡

    summary_prompt = f"""è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š
    {old_messages}

    æ€»ç»“ï¼š"""

    summary = summarizer.invoke(summary_prompt).content

    # è¿”å›æ€»ç»“ + æœ€è¿‘æ¶ˆæ¯
    return [
        ("system", f"ä¹‹å‰çš„å¯¹è¯æ€»ç»“ï¼š{summary}"),
        *messages[-5:]
    ]
```

### 5.5 ç»¼åˆå®æˆ˜ï¼šå®¢æœæœºå™¨äºº

```python
from langchain.agents import create_agent
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# 1. å®šä¹‰å·¥å…·
@tool
def query_order(order_id: str) -> str:
    """æŸ¥è¯¢è®¢å•çŠ¶æ€"""
    # æ¨¡æ‹ŸæŸ¥è¯¢
    return f"è®¢å• {order_id} å·²å‘è´§ï¼Œé¢„è®¡æ˜å¤©é€è¾¾"

# 2. åˆ›å»ºæŒä¹…åŒ– checkpointer
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://localhost/customer_service"
)

# 3. åˆ›å»º Agent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=[query_order],
    checkpointer=checkpointer
)

# 4. å®¢æœå¯¹è¯
def chat_with_customer(user_id: str, message: str):
    config = {"configurable": {"thread_id": f"customer-{user_id}"}}

    result = agent.invoke(
        {"messages": [("user", message)]},
        config=config
    )

    return result["messages"][-1].content

# ä½¿ç”¨ç¤ºä¾‹
chat_with_customer("user-456", "ä½ å¥½ï¼Œæˆ‘å«æå››")
# è¾“å‡ºï¼šä½ å¥½æå››ï¼

chat_with_customer("user-456", "æˆ‘çš„è®¢å•å·æ˜¯ 12345")
# è¾“å‡ºï¼šå¥½çš„ï¼Œè®©æˆ‘æŸ¥ä¸€ä¸‹... è®¢å• 12345 å·²å‘è´§ï¼Œé¢„è®¡æ˜å¤©é€è¾¾

# ç¬¬äºŒå¤©å®¢æˆ·å†æ¬¡å’¨è¯¢
chat_with_customer("user-456", "æˆ‘æ˜¨å¤©é—®çš„è®¢å•åˆ°äº†å—ï¼Ÿ")
# è¾“å‡ºï¼šæ‚¨æ˜¨å¤©å’¨è¯¢çš„è®¢å• 12345 åº”è¯¥ä»Šå¤©é€è¾¾
# âœ… Agent è®°ä½äº†ä¹‹å‰çš„è®¢å•å·ï¼
```

### 5.6 æœ¬ç« å°ç»“

**LangChain 1.0 è®°å¿†æœºåˆ¶**ï¼š
- âœ… ä½¿ç”¨ **Checkpointer** è€Œéæ—§çš„ Memory ç±»
- âœ… ä¸‰ç§ Checkpointerï¼šInMemorySaverã€SqliteSaverã€PostgresSaver
- âœ… é€šè¿‡ **thread_id** å®ç°ä¼šè¯éš”ç¦»
- âœ… æ”¯æŒæŒä¹…åŒ–ï¼Œé‡å¯ä¸ä¸¢å¤±

**æœ€ä½³å®è·µ**ï¼š
- âœ… å¼€å‘æµ‹è¯•ï¼šInMemorySaver
- âœ… å•æœºéƒ¨ç½²ï¼šSqliteSaver
- âœ… ç”Ÿäº§ç¯å¢ƒï¼šPostgresSaver
- âœ… æˆæœ¬æ§åˆ¶ï¼šé™åˆ¶å†å²é•¿åº¦æˆ–æ€»ç»“ç­–ç•¥

**ä¸‹ä¸€æ­¥**ï¼šç¬¬ä¸‰ç¯‡å°†æ·±å…¥ LangGraphï¼Œæ•™ä½ å®Œå…¨è‡ªå®šä¹‰ Agent çš„æ‰§è¡Œæµç¨‹å’ŒçŠ¶æ€ç®¡ç†ã€‚

---

**ç¬¬äºŒç¯‡å®Œç»“**

æ­å–œï¼ä½ å·²ç»å®Œæˆäº†ã€Šå¿«é€Ÿä¸Šæ‰‹å®æˆ˜ã€‹ç¯‡çš„å­¦ä¹ ï¼š
- âœ… ç¬¬3ç« ï¼šMessage ä¸ Tools åŸºç¡€
- âœ… ç¬¬4ç« ï¼šcreate_agent å¿«é€Ÿæ„å»º
- âœ… ç¬¬5ç« ï¼šå®æˆ˜æ¡ˆä¾‹ï¼šRAG Agent
- âœ… ç¬¬6ç« ï¼šå®æˆ˜æ¡ˆä¾‹ï¼šSQL Agent
- âœ… ç¬¬7ç« ï¼šMemory ä¸ä¸Šä¸‹æ–‡ç®¡ç†

**ä¸‹ä¸€æ­¥**ï¼šç¬¬ä¸‰ç¯‡ã€ŠLangGraph æ·±å…¥ã€‹å°†å¸¦ä½ ç†è§£ create_agent èƒŒåçš„æœºåˆ¶ï¼ŒæŒæ¡å®Œå…¨è‡ªå®šä¹‰èƒ½åŠ›ã€‚