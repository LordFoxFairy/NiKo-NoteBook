# ç¬¬ä¸€ç¯‡ï¼šåŸºç¡€è®¤çŸ¥

> **ç‰ˆæœ¬è¦æ±‚**: æœ¬æ•™ç¨‹åŸºäº LangChain 1.0+ã€LangGraph 1.0+ã€Python 3.10+

---

## ğŸ“‹ å‰ç½®å‡†å¤‡

### ç¯å¢ƒé…ç½®

åœ¨å¼€å§‹å­¦ä¹ ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®Œæˆä»¥ä¸‹ç¯å¢ƒé…ç½®ï¼š

#### 1. Python ç‰ˆæœ¬
```bash
python --version  # éœ€è¦ Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
```

#### 2. å®‰è£…ä¾èµ–
```bash
# ä½¿ç”¨ pip å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install langchain langchain-openai langgraph langchain-community

# æˆ–ä½¿ç”¨ uv (æ¨è)
uv pip install langchain langchain-openai langgraph langchain-community

# å¦‚éœ€æŒ‡å®šç‰ˆæœ¬ï¼ˆæ¨èä½¿ç”¨1.0.7æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰
pip install langchain>=1.0.7 langchain-openai>=1.0.3 langgraph>=1.0.3
```

#### 3. ç¯å¢ƒå˜é‡é…ç½®
```python
# åˆ›å»º .env æ–‡ä»¶
OPENAI_API_KEY=sk-your-api-key-here
LANGSMITH_API_KEY=your-langsmith-key  # å¯é€‰,ç”¨äºç›‘æ§
LANGSMITH_TRACING=true  # å¯é€‰

# åœ¨ä»£ç ä¸­åŠ è½½
from dotenv import load_dotenv
import os

load_dotenv()

# éªŒè¯ç¯å¢ƒå˜é‡
required_vars = ["OPENAI_API_KEY"]
for var in required_vars:
    if not os.getenv(var):
        raise EnvironmentError(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {var}")
```

#### 4. ä¾èµ–ç‰ˆæœ¬æ¸…å•
```toml
# pyproject.toml æ¨èé…ç½®
[tool.poetry.dependencies]
python = "^3.10"
langchain = "^1.0.7"
langchain-openai = "^1.0.3"
langgraph = "^1.0.3"
langchain-community = "^0.3.0"
langchain-core = "^1.0.7"
langsmith = "^0.4.43"
python-dotenv = "^1.0.0"

# requirements.txt æ ¼å¼
# langchain>=1.0.7
# langchain-openai>=1.0.3
# langgraph>=1.0.3
# langchain-community>=0.3.0
# langchain-core>=1.0.7
# langsmith>=0.4.43
# python-dotenv>=1.0.0
```

### å‰ç½®çŸ¥è¯†

å»ºè®®å…·å¤‡ä»¥ä¸‹åŸºç¡€çŸ¥è¯†ï¼š
- âœ… Python åŸºç¡€ (async/awaitã€ç±»å‹æ³¨è§£ã€è£…é¥°å™¨)
- âœ… LLM åŸºæœ¬æ¦‚å¿µ (Promptã€Tokenã€Temperatureç­‰)
- âœ… API è°ƒç”¨åŸºç¡€
- âœ… JSON æ•°æ®æ ¼å¼

---

## ç¬¬1ç« ï¼šLangChain ç”Ÿæ€å…¨æ™¯

---

### 1.1 æ¶æ„å±‚æ¬¡å…³ç³»

```mermaid
graph TD
    A[åº”ç”¨å±‚<br>Deep Agents / LangGraph Projects] --> B[ç¼–æ’å±‚<br>LangGraph]
    B --> C[é“¾è·¯å±‚<br>LangChain / LCEL]
    C --> D[ç›‘æ§å±‚<br>LangSmith]
    D --> E[å¤–éƒ¨èµ„æº<br>Models / APIs / Tools]
    style A fill:#C7E8CA,stroke:#6CBF84,stroke-width:1px
    style B fill:#E4F3FF,stroke:#74B3E1,stroke-width:1px
    style C fill:#FFF5D7,stroke:#F0C94E,stroke-width:1px
    style D fill:#FFE4E1,stroke:#E87461,stroke-width:1px
```

LangChain ç”Ÿæ€ç³»ç»Ÿç›®å‰å·²å½¢æˆâ€œå¤šå±‚ååŒâ€çš„æ¶æ„ä½“ç³»ï¼Œæ—¢å¯æ”¯æŒå¿«é€ŸåŸå‹å¼€å‘ï¼Œä¹Ÿå¯æ”¯æ’‘ç”Ÿäº§çº§ LLM åº”ç”¨ã€‚æ•´ä½“ç»“æ„å¦‚ä¸‹ï¼š

| å±‚çº§   | æ ¸å¿ƒç»„ä»¶                             | èŒè´£å®šä½                                | å…¸å‹åœºæ™¯                    |
| :----- | :----------------------------------- | :-------------------------------------- | :-------------------------- |
| åº”ç”¨å±‚ | **Deep Agents / LangGraph Projects** | å¤æ‚è‡ªæ²» Agentã€é•¿æœŸè¿è¡Œã€å¤š Agent åä½œ | æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ä»»åŠ¡ç³»ç»Ÿ    |
| ç¼–æ’å±‚ | **LangGraph**                        | çŠ¶æ€åŒ–æµç¨‹æ§åˆ¶ã€èŠ‚ç‚¹æ‰§è¡Œã€åˆ†æ”¯å¾ªç¯      | å¤š Agent ç¼–æ’ã€å¯è§†åŒ–çŠ¶æ€æµ |
| é“¾è·¯å±‚ | **LangChain / LCEL**                 | æ¨¡å‹è°ƒç”¨ã€æç¤ºç®¡ç†ã€å·¥å…·é›†æˆ            | RAGã€é—®ç­”ã€å¯¹è¯             |
| ç›‘æ§å±‚ | **LangSmith**                        | è°ƒè¯•ã€è§‚æµ‹ã€è¯„ä¼°ã€æˆæœ¬è¿½è¸ª              | DevOpsã€Evalsã€è´¨é‡ç›‘æ§     |

#### 1.1.1 LangChain ä¸ LangGraph çš„å…³ç³»

LangChain ä¸“æ³¨äº **é“¾å¼é€»è¾‘ä¸ Agent å°è£…**ï¼›LangGraph ä¸“æ³¨äº **æµç¨‹ç¼–æ’ä¸çŠ¶æ€ç®¡ç†**ã€‚

- **LangChainï¼š** ç”¨äºæ„å»ºå•æ¡æˆ–çº¿æ€§ chainï¼ˆPromptâ†’Modelâ†’Toolâ†’Outputï¼‰ã€‚  
- **LangGraphï¼š** ç”¨äºç®¡ç†å«åˆ†æ”¯ã€å¾ªç¯ã€å¹¶å‘çš„å¤æ‚æµç¨‹ï¼ˆå¯è§†åŒ–ã€æŒä¹…åŒ–çŠ¶æ€ï¼‰ã€‚  
- äºŒè€…å¯å¹¶ç”¨ï¼šLangGraph ä¸­çš„èŠ‚ç‚¹å¯è¿è¡Œ LangChain æˆ– LCEL æ„é€ çš„ chainã€‚

> **å›¾ 1-2 LangChain ä¸ LangGraph åä½œå…³ç³»å›¾**  

```mermaid
graph LR
    A["LangChain Chain\n(Promptâ†’Modelâ†’Toolâ†’Output)"]:::chain --> B["LangGraph Node"]:::node
    B --> C["LangGraph Flow\n(å¤šèŠ‚ç‚¹ç¼–æ’ / çŠ¶æ€æŒä¹…åŒ–)"]:::flow
    C --> D["LangGraph Studio\n(å¯è§†åŒ–ä¸ç›‘æ§)"]:::studio

    classDef chain fill:#FFF5D7,stroke:#F0C94E,stroke-width:1px;
    classDef node fill:#E4F3FF,stroke:#74B3E1,stroke-width:1px;
    classDef flow fill:#C7E8CA,stroke:#6CBF84,stroke-width:1px;
    classDef studio fill:#FFE4E1,stroke:#E87461,stroke-width:1px;
```

#### 1.1.2 å¦‚ä½•æ„å»º Agent

LangChain 1.0+ æä¾›ç»Ÿä¸€çš„ Agent æ„å»ºæ¥å£ï¼š**`create_agent`**

**å¿«é€Ÿå¼€å§‹ï¼šåˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ª Agent**

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# æ­¥éª¤1: å®šä¹‰å·¥å…·
@tool
def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”"""
    return f"{city}ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦25Â°C"

@tool
def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"

# æ­¥éª¤2: åˆ›å»º Agent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[get_weather, calculate],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œå¯ä»¥æŸ¥è¯¢å¤©æ°”å’Œè¿›è¡Œè®¡ç®—ã€‚"
)

# æ­¥éª¤3: è¿è¡Œ Agent
result = agent.invoke({
    "messages": [("user", "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿå¦å¤–å¸®æˆ‘ç®—ä¸€ä¸‹ 25 * 4")]
})

# æŸ¥çœ‹ç»“æœ
print(result["messages"][-1].content)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
åŒ—äº¬ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦25Â°Cã€‚
25 * 4 çš„è®¡ç®—ç»“æœæ˜¯ 100ã€‚
```

**æ ¸å¿ƒæ¦‚å¿µ**

**`create_agent` çš„å·¥ä½œåŸç†**ï¼š

```mermaid
graph LR
    A[ç”¨æˆ·è¾“å…¥] --> B[Agent æ¥æ”¶]
    B --> C[LLM åˆ†æ]
    C --> D{éœ€è¦å·¥å…·?}
    D -- æ˜¯ --> E[è°ƒç”¨å·¥å…·]
    E --> F[è·å–ç»“æœ]
    F --> C
    D -- å¦ --> G[ç”Ÿæˆå›å¤]
    G --> H[è¿”å›ç”¨æˆ·]
```

**å…³é”®å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `model` | ChatModel \| str | âœ… | ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ |
| `tools` | List[Tool] | âœ… | å¯ç”¨çš„å·¥å…·åˆ—è¡¨ |
| `system_prompt` | str | âŒ | ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰ Agent è¡Œä¸º |
| `checkpointer` | Checkpointer | âŒ | çŠ¶æ€æŒä¹…åŒ–ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰|
| `interrupt_before` | List[str] | âŒ | åœ¨æŒ‡å®šèŠ‚ç‚¹å‰æš‚åœï¼ˆéœ€è¦äººå·¥ç¡®è®¤ï¼‰|
| `interrupt_after` | List[str] | âŒ | åœ¨æŒ‡å®šèŠ‚ç‚¹åæš‚åœ |

**å®Œæ•´å·¥ä½œæµç¨‹**

1. **æ¨¡å‹ç»‘å®š**ï¼šæŒ‡å®šä½¿ç”¨çš„ LLMï¼ˆå¦‚ GPT-4ã€Claude ç­‰ï¼‰
2. **å·¥å…·æ³¨å†Œ**ï¼šæä¾› Agent å¯è°ƒç”¨çš„å·¥å…·é›†åˆ
3. **æç¤ºé…ç½®**ï¼šé€šè¿‡ `system_prompt` å®šä¹‰ Agent çš„è§’è‰²å’Œè¡Œä¸º
4. **å†³ç­–æ‰§è¡Œ**ï¼šLLM åŸºäº ReAct æ¨¡å¼è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
5. **ç»“æœè¿”å›**ï¼šè‡ªåŠ¨ç»„åˆå·¥å…·è¾“å‡ºå’Œ LLM å›å¤
6. **ç›‘æ§è¿½è¸ª**ï¼šé›†æˆ LangSmith å®ç°å…¨é“¾è·¯è¿½è¸ª

**å…³é”®ç‰¹æ€§**

- âœ… **å®˜æ–¹æ¨è**ï¼šLangChain 1.0+ æ ‡å‡† API
- âœ… **ç®€æ´æ˜“ç”¨**ï¼šç»Ÿä¸€çš„æ¥å£ï¼Œ3æ­¥å³å¯åˆ›å»º Agent
- âœ… **å®Œæ•´åŠŸèƒ½**ï¼šæ”¯æŒ middlewareã€cacheã€checkpointer
- âœ… **è‡ªåŠ¨å·¥å…·è°ƒç”¨**ï¼šLLM è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶ä½¿ç”¨å“ªä¸ªå·¥å…·
- âœ… **å¤šè½®å¯¹è¯**ï¼šæ”¯æŒçŠ¶æ€æŒä¹…åŒ–ï¼Œå®ç°ä¸Šä¸‹æ–‡è®°å¿†
- âœ… **é•¿æœŸæ”¯æŒ**ï¼šå®˜æ–¹ç»´æŠ¤ï¼ŒæŒç»­æ›´æ–°

```mermaid
graph LR
    A[Prompt Template] --> B[LLM / ChatModel]
    B --> C[Tool Selection]
    C --> D[Tool Execution]
    D --> E[Parser / Output Formatter]
    E --> F[è¿”å›ç»“æœ]
    F --> G[LangSmith Callback / Tracing]
```

---

#### 1.1.3 LCEL çš„å®šä½ä¸ä½œç”¨

LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ LangChain çš„â€œå£°æ˜å¼ç»„åˆè¯­æ³•â€ï¼Œç”¨äº **æ„å»ºå¯å¹¶è¡Œã€å¯æµå¼ã€å¯è¿½è¸ªçš„ Runnable é“¾**ã€‚

- **æ ¸å¿ƒæ¦‚å¿µï¼š**  
  - `RunnableSequence` é¡ºåºæ‰§è¡Œï¼›  
  - `RunnableParallel` å¹¶è¡Œæ‰§è¡Œï¼›  
  - æ”¯æŒ async / stream / batch ç»Ÿä¸€è°ƒç”¨ï¼›  
  - å¯ç›´æ¥åµŒå…¥ LangGraph èŠ‚ç‚¹ã€‚
- **ä»·å€¼ï¼š** åœ¨ä»£ç å±‚é¢æ„å»ºâ€œæ•°æ®æµç®¡çº¿â€ï¼Œå¦‚åŒ Node-RED æˆ– Airflow çš„è½»é‡åŒ–å®ç°ã€‚

```mermaid
graph TD
    A["è¾“å…¥æ•°æ®"]:::input --> B["RunnableSequence(é¡ºåºæ‰§è¡Œ)"]:::seq
    B --> C["RunnableParallel(å¹¶è¡Œæ‰§è¡Œ)"]:::par
    C --> D["æ¨¡å‹æ¨ç† / å·¥å…·è°ƒç”¨"]:::model
    D --> E["æµå¼è¾“å‡º / ç»“æ„åŒ–è§£æ"]:::output

    classDef input fill:#E3F2FD,stroke:#64B5F6,stroke-width:1px;
    classDef seq fill:#FFF9C4,stroke:#FDD835,stroke-width:1px;
    classDef par fill:#DCEDC8,stroke:#81C784,stroke-width:1px;
    classDef model fill:#F0F4C3,stroke:#C0CA33,stroke-width:1px;
    classDef output fill:#FFE0B2,stroke:#FB8C00,stroke-width:1px;
```

#### 1.1.4 LangSmith çš„ç›‘æ§èŒè´£

LangSmith æ˜¯ LangChain å®˜æ–¹æ¨å‡ºçš„å¯è§‚æµ‹æ€§ä¸è´¨é‡è¯„ä¼°å¹³å°ã€‚

**ä¸»è¦èŒè´£ï¼š**
- ğŸ” **Tracing** ï¼šè¿½è¸ª Chain/Graph/Agent æ¯ä¸ªè°ƒç”¨èŠ‚ç‚¹ã€‚  
- ğŸ“ˆ **Metrics** ï¼šç›‘æ§å»¶è¿Ÿã€Token ç”¨é‡ã€é”™è¯¯ç‡ã€æˆæœ¬ã€‚  
- ğŸ§ª **Evaluation** ï¼šå¯¹æ¨¡å‹æˆ– Agent è¾“å‡ºè¿›è¡Œæ‰“åˆ†ä¸å¯¹æ¯”ã€‚  
- âš™ï¸ **Integration** ï¼šä¸ LangChain ã€LangGraph ã€Deep Agents åŸç”Ÿé›†æˆã€‚

```mermaid
graph LR
    subgraph LangSmith
    A[Tracing<br>é“¾è·¯è¿½è¸ª] --> B[Metrics<br>æ€§èƒ½&æˆæœ¬]
    B --> C[Evals<br>æ¨¡å‹è¯„ä¼°]
    C --> D[Dashboard / Report]
    end
    D --> E[å¼€å‘è€… / å›¢é˜Ÿåä½œ]
```

### 1.2 æ ¸å¿ƒè®¾è®¡ç†å¿µ

```mermaid
mindmap
  root((LangChain Design))
    Provider-Agnostic
      å¤šæ¨¡å‹å…¼å®¹
      å¿«é€Ÿåˆ‡æ¢
    Runnable Protocol
      ç»Ÿä¸€æ¥å£
      å¯ç»„åˆæ‰§è¡Œ
    Middleware Driven
      Hook/Callback
      Metrics/Retry
    Production First
      ç¨³å®šæ€§
      å¯è§‚æµ‹æ€§
      æˆæœ¬æ§åˆ¶
```



#### 1.2.1 Provider-Agnostic è®¾è®¡

LangChain é€šè¿‡ç»Ÿä¸€æ¥å£å±è”½ LLM æä¾›å•†å·®å¼‚ï¼ˆOpenAIã€Anthropicã€Cohereã€Azure ç­‰ï¼‰ï¼Œ  
ä»¥ â€œProvider æ— å…³â€ çš„æ–¹å¼æ„å»ºåº”ç”¨ã€‚

- æ¨¡å‹åˆ‡æ¢æ— éœ€ä¿®æ”¹ä¸Šå±‚é€»è¾‘ã€‚  
- æ”¯æŒè·¨å¹³å°æˆæœ¬è¿½è¸ªä¸æ€§èƒ½æ¯”è¾ƒã€‚

#### 1.2.2 Runnable Protocol ç»Ÿä¸€æŠ½è±¡

Runnable æ˜¯ LangChain çš„æ ¸å¿ƒæ‰§è¡Œåè®®ï¼š  
> ä¸€åˆ‡çš† Runnableã€‚

åŒ…æ‹¬ Chainã€Agentã€Toolã€Prompt å‡å®ç°è¯¥æ¥å£ã€‚  
- ç»Ÿä¸€æ‰§è¡Œå…¥å£ï¼š`invoke()`ã€`ainvoke()`ã€`stream()`ã€‚  
- æ”¯æŒå¼‚æ­¥ã€æ‰¹é‡ã€æµå¼ã€å¯è¿½è¸ªè°ƒç”¨ã€‚  
- æ‰€æœ‰ Runnable å¯åµŒå¥—ã€ç»„åˆã€è£…é¥°ã€‚

```mermaid
graph TD
    A[Runnable] --> B[Chain]
    A --> C[Agent]
    A --> D[Tool]
    A --> E[Prompt]
    A --> F[LCEL ç»„åˆç»“æ„]
```

#### 1.2.3 Middleware-Driven æ¶æ„

LangChain æ”¯æŒ Callback / Hook / Tracing æœºåˆ¶ï¼Œå¯åœ¨æ‰§è¡Œæµä¸­æ’å…¥ä¸­é—´ä»¶ã€‚

å¸¸è§ä¸­é—´ä»¶ç”¨é€”ï¼š
- Token è®¡æ•°ä¸æˆæœ¬ç›‘æ§  
- æ—¥å¿—ä¸é”™è¯¯è¿½è¸ª  
- å®‰å…¨å®¡æŸ¥ä¸è®¿é—®æ§åˆ¶  
- é‡è¯•ä¸è¶…æ—¶æ§åˆ¶  

```mermaid
sequenceDiagram
    participant U as User
    participant C as Chain/Agent
    participant M as Middleware
    participant L as LangSmith

    U->>C: è°ƒç”¨æ‰§è¡Œ
    C->>M: è¿›å…¥ä¸­é—´ä»¶ (tokenè®¡æ•°/æ—¥å¿—)
    M->>L: ä¸ŠæŠ¥ç›‘æ§æ•°æ®
    L-->>M: è¿”å›ç›‘æ§ç»“æœ
    M-->>C: æ‰§è¡Œä¸»æµç¨‹
    C-->>U: è¿”å›è¾“å‡ºç»“æœ
```

#### 1.2.4 Production-First ç†å¿µ

LangChain 1.0 åŠ LangGraph 1.0 å‘å¸ƒåï¼Œç”Ÿæ€å…¨é¢è½¬å‘ **ç”Ÿäº§çº§ç¨³å®šæ€§ä¸å¯è§‚æµ‹æ€§**ã€‚  
æ ¸å¿ƒç›®æ ‡åŒ…æ‹¬ï¼š

- é•¿æœŸå…¼å®¹ï¼ˆå‘ 2.0 å¹³æ»‘è¿‡æ¸¡ï¼‰  
- æˆæœ¬å¯æ§ï¼ˆLangSmith ç›‘æ§ + è‡ªåŠ¨è®¡è´¹ï¼‰  
- æ¨¡å‹çƒ­æ›¿æ¢ï¼ˆProvider-agnosticï¼‰  
- å®Œæ•´ CI/CD ä¸ Evals é›†æˆ  

```mermaid
flowchart LR
    A[å¼€å‘é˜¶æ®µ<br>LangChain Prototype] --> B[æµ‹è¯•é˜¶æ®µ<br>LangSmith è°ƒè¯•]
    B --> C[éƒ¨ç½²é˜¶æ®µ<br>LangGraph ç¼–æ’]
    C --> D[ç›‘æ§é˜¶æ®µ<br>Metrics / Evals]
    D --> E[æŒç»­ä¼˜åŒ–<br>æ¨¡å‹&æç¤ºè°ƒæ•´]
```

### 1.3 æŠ€æœ¯é€‰å‹å†³ç­–æ ‘

```mermaid
graph TD
    A[åº”ç”¨éœ€æ±‚è¯„ä¼°] --> B{æµç¨‹æ˜¯å¦å¤æ‚?}
    B -- å¦ --> C[ä½¿ç”¨ create_agent<br>LangChain å¿«é€ŸåŸå‹]
    B -- æ˜¯ --> D{æ˜¯å¦éœ€è¦çŠ¶æ€ç®¡ç†?}
    D -- å¦ --> E[ä½¿ç”¨ LCEL æ„å»º chain]
    D -- æ˜¯ --> F{æ˜¯å¦ä¸ºé•¿æœŸè¿è¡Œ/è‡ªæ²»?}
    F -- å¦ --> G[ä½¿ç”¨ LangGraph ç¼–æ’]
    F -- æ˜¯ --> H[ä½¿ç”¨ Deep Agents<br>ç»“åˆ LangSmith ç›‘æ§]
```

### 1.3.1 ä½•æ—¶ä½¿ç”¨ create_agent
é€‚ç”¨åœºæ™¯ï¼š
- å• Agent æ‰§è¡Œï¼Œæµç¨‹çº¿æ€§ï¼›  
- éœ€è¦å¿«é€Ÿå®ç° Tool è°ƒç”¨ï¼›  
- ç”¨äº RAG ã€é—®ç­”ã€åŠ©æ‰‹ç±»åœºæ™¯ã€‚  

### 1.3.2 ä½•æ—¶æ·±å…¥ LangGraph
é€‚ç”¨åœºæ™¯ï¼š
- å¤š Agent åä½œï¼›  
- å­˜åœ¨åˆ†æ”¯ / å¾ªç¯ / çŠ¶æ€ç®¡ç†ï¼›  
- éœ€å¯è§†åŒ–ã€å¯è°ƒè¯•ã€æŒä¹…åŒ–è¿è¡Œã€‚  

### 1.3.3 ä½•æ—¶ä½¿ç”¨ Deep Agents
é€‚ç”¨åœºæ™¯ï¼š
- é•¿æœŸè¿è¡Œã€è‡ªä¸»å†³ç­– Agentï¼›  
- å¤æ‚ä»»åŠ¡æ‹†è§£ã€å­ Agent ç®¡ç†ï¼›  
- æŒç»­ä»»åŠ¡æ‰§è¡Œä¸å‘¨æœŸæ€§è§¦å‘ã€‚  

### 1.3.4 ä½•æ—¶éœ€è¦ Middleware
é€‚ç”¨åœºæ™¯ï¼š
- ç”Ÿäº§ç¯å¢ƒè¿è¡Œï¼›  
- éœ€è¦æ—¥å¿—ã€æŒ‡æ ‡ã€å®‰å…¨æ§åˆ¶ã€å›è°ƒã€‚  
æ¨èï¼šæ‰€æœ‰ Chain/Agent å‡å¯ç”¨ LangSmith Tracing + è‡ªå®šä¹‰ Callbackã€‚  

### 1.3.5 å…¸å‹åº”ç”¨åœºæ™¯åˆ†æ

| åœºæ™¯              | æ¨èæŠ€æœ¯                            | ç†ç”±                       |
| :---------------- | :---------------------------------- | :------------------------- |
| A. ä¼ä¸šæ–‡æ¡£é—®ç­”   | create_agent + LCEL                 | å¿«é€Ÿæ„å»º RAG é—®ç­”          |
| B. æ™ºèƒ½å®¢æœç³»ç»Ÿ   | LangChain Agent + Middleware        | éœ€å¤šè½®å¯¹è¯ä¸ç›‘æ§           |
| C. è‡ªåŠ¨åŒ–ä»»åŠ¡ç®¡ç† | LangGraph + Deep Agents + LangSmith | å¤æ‚ workflow + è‡ªæ²» agent |
| D. å†…å®¹æ‘˜è¦æˆ–è½¬æ¢ | LCEL                                | è½»é‡ã€é«˜å¹¶è¡Œã€å¯æµå¼       |

---

### æœ¬ç« å°ç»“

LangChain ç”Ÿæ€ä½“ç³»å¯æ¦‚æ‹¬ä¸ºï¼š

> **é“¾å¼é€»è¾‘ï¼ˆLangChainï¼‰ â†’ å›¾å¼ç¼–æ’ï¼ˆLangGraphï¼‰ â†’ ç›‘æ§è¯„ä¼°ï¼ˆLangSmithï¼‰ â†’ è‡ªæ²»è¿›åŒ–ï¼ˆDeep Agentsï¼‰**

æ ¸å¿ƒç†å¿µï¼š
- Provider-Agnostic  
- Runnable ç»Ÿä¸€æŠ½è±¡  
- Middleware å¯æ’æ¶æ„  
- Production-First éƒ¨ç½²æ€ç»´  

è®¾è®¡å“²å­¦ä¸Šï¼Œä»â€œç©å…·åŸå‹â€èµ°å‘â€œç”Ÿäº§å¯è§‚æµ‹â€çš„å·¥ç¨‹ç³»ç»Ÿã€‚

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼š**
   é€‰æ‹©ä¸€ä¸ªä¸šåŠ¡åœºæ™¯ï¼Œç”»å‡ºå…¶ LangChain æŠ€æœ¯é€‰å‹å†³ç­–è·¯å¾„ï¼ˆå‚è€ƒå›¾ 1-10ï¼‰ã€‚

2. **ç»ƒä¹  2ï¼š**
   ç¼–å†™ä¸€ä¸ª LCEL ä¾‹ç¨‹ï¼ˆPrompt â†’ Model â†’ Parser â†’ Toolï¼‰ï¼Œå¹¶æ ‡æ³¨ä½ ä¼šæ’å…¥å“ªäº› Middlewareã€‚

3. **ç»ƒä¹  3ï¼š**
   è®¾è®¡ä¸€ä¸ªé•¿æœŸè¿è¡Œ Agent ï¼ˆå¦‚ å¸‚åœºç›‘æ§æˆ–è‡ªåŠ¨æŠ¥å‘Šï¼‰ï¼Œè¯´æ˜å¦‚ä½•ç”¨ LangGraph + LangSmith å®ç°ç›‘æ§ä¸ Evalsã€‚

4. **æ€è€ƒé¢˜ï¼š**
   LCEL åœ¨ LangGraph èŠ‚ç‚¹ä¸­åµŒå¥—ä½¿ç”¨ä¼šå¸¦æ¥å“ªäº›ä¼˜åŠ¿ä¸ä»£ä»·ï¼Ÿ

---

## ç¬¬2ç« ï¼šæ ¸å¿ƒæŠ½è±¡ï¼šRunnable ä¸ LCEL

### 2.1 Runnable Protocol

#### 2.1.1 ä¸ºä»€ä¹ˆéœ€è¦ç»Ÿä¸€æŠ½è±¡

åœ¨ LangChain 1.0 ä¹‹å‰ï¼Œä¸åŒç»„ä»¶ï¼ˆPromptã€Modelã€Toolã€Chainï¼‰çš„è°ƒç”¨æ–¹å¼å„ä¸ç›¸åŒï¼Œå¯¼è‡´ï¼š
- **æ¥å£ä¸ä¸€è‡´**ï¼šå­¦ä¹ æˆæœ¬é«˜ï¼Œéš¾ä»¥ç»„åˆ
- **ç¼ºä¹æ ‡å‡†åŒ–**ï¼šæ— æ³•ç»Ÿä¸€è¿½è¸ªã€ç›‘æ§
- **ç»„åˆå›°éš¾**ï¼šä¸åŒç»„ä»¶éš¾ä»¥åµŒå¥—ä½¿ç”¨

**Runnable Protocol è§£å†³æ–¹æ¡ˆ**ï¼š

LangChain 1.0 å¼•å…¥ Runnable ä½œä¸º**ç»Ÿä¸€æ‰§è¡Œåè®®**ï¼Œæ‰€æœ‰ç»„ä»¶å‡å®ç°è¯¥æ¥å£ï¼š

```python
from langchain_core.runnables import Runnable

# æ‰€æœ‰ç»„ä»¶å‡å®ç° Runnable æ¥å£
class Runnable:
    def invoke(self, input, config=None): ...       # åŒæ­¥è°ƒç”¨
    def ainvoke(self, input, config=None): ...      # å¼‚æ­¥è°ƒç”¨
    def stream(self, input, config=None): ...       # æµå¼è¾“å‡º
    def astream(self, input, config=None): ...      # å¼‚æ­¥æµå¼
    def batch(self, inputs, config=None): ...       # æ‰¹é‡å¤„ç†
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

```mermaid
graph LR
    A[Runnable ç»Ÿä¸€æŠ½è±¡] --> B[ä¸€è‡´çš„è°ƒç”¨æ–¹å¼]
    A --> C[å¯ç»„åˆæ€§]
    A --> D[å¯è¿½è¸ªæ€§]
    A --> E[è‡ªåŠ¨ä¼˜åŒ–]

    B --> F[é™ä½å­¦ä¹ æˆæœ¬]
    C --> G[LCEL ç®¡é“]
    D --> H[LangSmith é›†æˆ]
    E --> I[æ‰¹å¤„ç†/å¹¶è¡Œ]
```

**å®é™…åº”ç”¨ç¤ºä¾‹**ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯ Runnable
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatOpenAI()
parser = StrOutputParser()

# ç»Ÿä¸€çš„è°ƒç”¨æ–¹å¼
result = prompt.invoke({"topic": "AI"})
result = model.invoke("Tell me a joke")
result = parser.invoke("some text")
```

---

#### 2.1.2 æ ¸å¿ƒæ–¹æ³•ï¼šinvokeã€streamã€batch

**invoke() - åŒæ­¥è°ƒç”¨**

æœ€åŸºç¡€çš„è°ƒç”¨æ–¹å¼ï¼Œé€‚ç”¨äºå•æ¬¡è¯·æ±‚ï¼š

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")

# åŒæ­¥è°ƒç”¨
response = model.invoke("What is LangChain?")
print(response.content)
```

**æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant User
    participant Runnable
    participant LLM
    participant LangSmith

    User->>Runnable: invoke(input)
    Runnable->>LangSmith: å¼€å§‹è¿½è¸ª
    Runnable->>LLM: å‘é€è¯·æ±‚
    LLM-->>Runnable: è¿”å›å®Œæ•´ç»“æœ
    Runnable->>LangSmith: è®°å½•ç»“æœ
    Runnable-->>User: è¿”å›è¾“å‡º
```

**stream() - æµå¼è¾“å‡º**

é€‚ç”¨äºéœ€è¦å®æ—¶åé¦ˆçš„åœºæ™¯ï¼ˆå¦‚èŠå¤©ç•Œé¢ï¼‰ï¼š

```python
# æµå¼è¾“å‡º
for chunk in model.stream("Tell me a long story"):
    print(chunk.content, end="", flush=True)
```

**æµå¼è¾“å‡ºçš„ä¼˜åŠ¿**ï¼š

- âœ… é™ä½é¦–å­—å»¶è¿Ÿï¼ˆTTFT - Time To First Tokenï¼‰
- âœ… æå‡ç”¨æˆ·ä½“éªŒï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰
- âœ… å‡å°‘è¶…æ—¶é£é™©

```mermaid
graph LR
    A[User Request] --> B[Stream Token 1]
    B --> C[Stream Token 2]
    C --> D[Stream Token 3]
    D --> E[...]
    E --> F[Stream Complete]

    style B fill:#E8F5E9
    style C fill:#E8F5E9
    style D fill:#E8F5E9
```

**batch() - æ‰¹é‡å¤„ç†**

é€‚ç”¨äºæ‰¹é‡è¯·æ±‚åœºæ™¯ï¼Œè‡ªåŠ¨ä¼˜åŒ–å¹¶å‘ï¼š

```python
# æ‰¹é‡å¤„ç†ï¼ˆè‡ªåŠ¨å¹¶å‘ä¼˜åŒ–ï¼‰
inputs = [
    "What is AI?",
    "What is ML?",
    "What is LLM?"
]

results = model.batch(inputs)
for result in results:
    print(result.content)
```

**æ‰¹é‡å¤„ç†çš„ä¼˜åŠ¿**ï¼š
- âœ… è‡ªåŠ¨å¹¶å‘æ§åˆ¶
- âœ… æˆæœ¬è¿½è¸ªèšåˆ
- âœ… é”™è¯¯å¤„ç†ä¼˜åŒ–

```mermaid
graph TD
    A[Batch Inputs] --> B[å¹¶å‘æ§åˆ¶å™¨]
    B --> C[Request 1]
    B --> D[Request 2]
    B --> E[Request 3]
    C --> F[ç»“æœèšåˆ]
    D --> F
    E --> F
    F --> G[Batch Results]
```

**abatch() - å¼‚æ­¥æ‰¹é‡å¤„ç†**

åœ¨éœ€è¦é«˜å¹¶å‘å¤„ç†å¤§é‡è¯·æ±‚æ—¶ï¼Œ`abatch()` æ¯”åŒæ­¥ `batch()` æ€§èƒ½æ›´å¥½ï¼š

```python
import asyncio
from langchain_openai import ChatOpenAI

model = ChatOpenAI()

async def async_batch_example():
    inputs = [
        "What is AI?",
        "What is ML?",
        "What is LLM?",
        "What is NLP?",
        "What is DL?"
    ]

    # å¼‚æ­¥æ‰¹é‡å¤„ç†
    results = await model.abatch(inputs)

    for i, result in enumerate(results):
        print(f"Result {i+1}: {result.content}")

# è¿è¡Œå¼‚æ­¥ä»»åŠ¡
asyncio.run(async_batch_example())
```

**abatch ä¸ batch çš„å¯¹æ¯”**ï¼š

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ |
|------|----------|------|
| `batch()` | ä¸­å°æ‰¹é‡ï¼ˆ<50ï¼‰ | å®ç°ç®€å•ï¼Œæ— éœ€async/await |
| `abatch()` | å¤§æ‰¹é‡ï¼ˆ50+ï¼‰ã€I/Oå¯†é›† | æ›´é«˜å¹¶å‘æ€§èƒ½ï¼Œèµ„æºåˆ©ç”¨ç‡é«˜ |

---

#### 2.1.3 å¼‚æ­¥æ–¹æ³•ï¼šainvokeã€astream

åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œå¼‚æ­¥æ–¹æ³•å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼š

**ainvoke() - å¼‚æ­¥è°ƒç”¨**

```python
import asyncio
from langchain_openai import ChatOpenAI

model = ChatOpenAI()

async def main():
    # å¼‚æ­¥è°ƒç”¨
    response = await model.ainvoke("What is async programming?")
    print(response.content)

asyncio.run(main())
```

**astream() - å¼‚æ­¥æµå¼**

```python
async def stream_example():
    async for chunk in model.astream("Tell me a story"):
        print(chunk.content, end="", flush=True)

asyncio.run(stream_example())
```

**å¹¶å‘æ€§èƒ½å¯¹æ¯”**

```python
# âŒ åŒæ­¥æ–¹å¼ï¼ˆä¸²è¡Œæ‰§è¡Œï¼Œæ…¢ï¼‰
def sync_batch():
    results = []
    for query in queries:
        results.append(model.invoke(query))
    return results

# âœ… å¼‚æ­¥æ–¹å¼ï¼ˆå¹¶å‘æ‰§è¡Œï¼Œå¿«ï¼‰
async def async_batch():
    tasks = [model.ainvoke(query) for query in queries]
    return await asyncio.gather(*tasks)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| è¯·æ±‚æ•° | åŒæ­¥è€—æ—¶ | å¼‚æ­¥è€—æ—¶ | æ€§èƒ½æå‡ |
|--------|---------|---------|---------|
| 10     | 30s     | 5s      | 6x      |
| 50     | 150s    | 15s     | 10x     |
| 100    | 300s    | 25s     | 12x     |

---

#### 2.1.4 Runnable ç±»å‹ï¼šLambdaã€Parallelã€Branchã€Fallbacks

**RunnableLambda - è‡ªå®šä¹‰å‡½æ•°åŒ…è£…**

å°†æ™®é€š Python å‡½æ•°åŒ…è£…ä¸º Runnableï¼š

```python
from langchain_core.runnables import RunnableLambda

def uppercase(text: str) -> str:
    return text.upper()

# åŒ…è£…ä¸º Runnable
runnable_upper = RunnableLambda(uppercase)

# ç»Ÿä¸€è°ƒç”¨æ–¹å¼
result = runnable_upper.invoke("hello")  # "HELLO"
```

**RunnableParallel - å¹¶è¡Œæ‰§è¡Œ**

åŒæ—¶æ‰§è¡Œå¤šä¸ª Runnableï¼Œç»“æœä»¥å­—å…¸å½¢å¼è¿”å›ï¼š

```python
from langchain_core.runnables import RunnableParallel

parallel = RunnableParallel(
    joke=ChatPromptTemplate.from_template("Tell a joke about {topic}") | model,
    poem=ChatPromptTemplate.from_template("Write a poem about {topic}") | model
)

# å¹¶è¡Œæ‰§è¡Œ
result = parallel.invoke({"topic": "AI"})
print(result["joke"])
print(result["poem"])
```

```mermaid
graph TD
    A[Input: topic='AI'] --> B[RunnableParallel]
    B --> C[Joke Generator]
    B --> D[Poem Generator]
    C --> E[Result Dict]
    D --> E
    E --> F[Output]
```

**RunnableBranch - æ¡ä»¶åˆ†æ”¯**

æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„æ‰§è¡Œè·¯å¾„ï¼š

```python
from langchain_core.runnables import RunnableBranch

branch = RunnableBranch(
    (lambda x: len(x) > 100, long_text_handler),
    (lambda x: len(x) > 10, medium_text_handler),
    short_text_handler  # é»˜è®¤åˆ†æ”¯
)

result = branch.invoke("some text")
```

```mermaid
graph TD
    A[Input] --> B{len > 100?}
    B -- Yes --> C[Long Text Handler]
    B -- No --> D{len > 10?}
    D -- Yes --> E[Medium Text Handler]
    D -- No --> F[Short Text Handler]
```

**with_fallbacks() - é™çº§å¤„ç†**

ä¸» Runnable å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆï¼š

```python
from langchain_openai import ChatOpenAI

primary_model = ChatOpenAI(model="gpt-4")
fallback_model = ChatOpenAI(model="gpt-3.5-turbo")

# ç›´æ¥ä½¿ç”¨ with_fallbacks æ–¹æ³•ï¼Œæ— éœ€å¯¼å…¥é¢å¤–ç±»
model_with_fallback = primary_model.with_fallbacks([fallback_model])

# å¦‚æœ GPT-4 å¤±è´¥ï¼Œè‡ªåŠ¨ä½¿ç”¨ GPT-3.5
result = model_with_fallback.invoke("Hello")
```

**å‚æ•°è¯´æ˜** (åŸºäºå®˜æ–¹APIæ–‡æ¡£éªŒè¯)ï¼š

**å¿…éœ€å‚æ•°**ï¼š

- `fallbacks`: `Sequence[Runnable]` - å¤‡ç”¨ Runnable åºåˆ—ï¼ŒæŒ‰é¡ºåºå°è¯•

**å¯é€‰å‚æ•°** (ä»…å…³é”®å­—å‚æ•°)ï¼š
- `exceptions_to_handle`: `Tuple[Type[BaseException], ...]` - éœ€è¦å¤„ç†çš„å¼‚å¸¸ç±»å‹å…ƒç»„ï¼Œé»˜è®¤ä¸º `(Exception,)`
- `exception_key`: `Optional[str]` - å¯é€‰çš„é”®åï¼Œç”¨äºå°†å¼‚å¸¸ä¿¡æ¯ä¼ é€’ç»™å¤‡ç”¨æ–¹æ¡ˆã€‚å¦‚ä¸º `None` (é»˜è®¤)ï¼Œå¼‚å¸¸ä¸ä¼ é€’ç»™å¤‡ç”¨æ–¹æ¡ˆ

**å®Œæ•´å‚æ•°ç¤ºä¾‹**ï¼š

```python
# âœ… ç¤ºä¾‹1: åªå¯¹ç‰¹å®šå¼‚å¸¸ç±»å‹æ‰§è¡Œé™çº§
model_with_fallback = primary_model.with_fallbacks(
    fallbacks=[fallback_model],  # âœ… å®˜æ–¹æ ‡å‡†å‚æ•°ï¼šfallbacks (å¤æ•°ï¼Œåˆ—è¡¨)
    exceptions_to_handle=(TimeoutError, ConnectionError),  # âœ… å®˜æ–¹æ ‡å‡†å‚æ•°
)

# âœ… ç¤ºä¾‹2: å°†å¼‚å¸¸ä¿¡æ¯ä¼ é€’ç»™å¤‡ç”¨æ–¹æ¡ˆ
from langchain_core.runnables import RunnableLambda

def handle_with_error_context(inputs):
    """å¤‡ç”¨æ–¹æ¡ˆå¯ä»¥è®¿é—®å¼‚å¸¸ä¿¡æ¯"""
    if "error" in inputs:
        print(f"Original error: {inputs['error']}")
    return fallback_model.invoke(inputs["input"])

model_with_error_context = primary_model.with_fallbacks(
    fallbacks=[RunnableLambda(handle_with_error_context)],  # âœ… ä½¿ç”¨ fallbacks å‚æ•°å
    exception_key="error"  # âœ… å®˜æ–¹æ ‡å‡†å‚æ•°ï¼šå¼‚å¸¸ä¼šä»¥ "error" é”®ä¼ é€’
)

# âš ï¸ é‡è¦ï¼šä½¿ç”¨ exception_key æ—¶ï¼Œä¸» Runnable å’Œæ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆéƒ½å¿…é¡»æ¥å—å­—å…¸è¾“å…¥
result = model_with_error_context.invoke({"input": "Hello"})
```

**API è§„èŒƒæ€»ç»“**ï¼š
```python
def with_fallbacks(
    self,
    fallbacks: Sequence[Runnable[Input, Output]],  # å¿…éœ€
    *,
    exceptions_to_handle: Tuple[Type[BaseException], ...] = (Exception,),  # å¯é€‰
    exception_key: Optional[str] = None  # å¯é€‰
) -> RunnableWithFallbacksT[Input, Output]:
    ...
```

```mermaid
graph LR
    A[Request] --> B[Primary: GPT-4]
    B -- Success --> C[Return Result]
    B -- Failure --> D[Fallback: GPT-3.5]
    D --> C
```

---

### 2.2 LCEL è¡¨è¾¾å¼è¯­è¨€

#### 2.2.1 å£°æ˜å¼ç»„åˆç†å¿µ

LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§**å£°æ˜å¼**è¯­æ³•ï¼Œç”¨äºç»„åˆ Runnable å¯¹è±¡ã€‚

**å‘½ä»¤å¼ vs å£°æ˜å¼**ï¼š

```python
# âŒ å‘½ä»¤å¼ï¼ˆæ‰‹åŠ¨æ§åˆ¶æµç¨‹ï¼‰
def imperative_chain(input):
    step1_result = prompt.invoke(input)
    step2_result = model.invoke(step1_result)
    step3_result = parser.invoke(step2_result)
    return step3_result

# âœ… å£°æ˜å¼ï¼ˆLCEL ç®¡é“ï¼‰
chain = prompt | model | parser
result = chain.invoke(input)
```

**LCEL çš„æ ¸å¿ƒä¼˜åŠ¿**ï¼š

```mermaid
mindmap
  root((LCEL))
    å£°æ˜å¼
      ä»£ç ç®€æ´
      æ„å›¾æ¸…æ™°
    å¯ç»„åˆ
      ç®¡é“è¿æ¥
      åµŒå¥—ç»„åˆ
    è‡ªåŠ¨ä¼˜åŒ–
      å¹¶è¡Œæ‰§è¡Œ
      æµå¼ä¼ è¾“
    å¯è¿½è¸ª
      LangSmith é›†æˆ
      Debug å‹å¥½
```

---

#### 2.2.2 ç®¡é“æ“ä½œç¬¦ `|` ä¸å¹¶è¡Œ `{}`

**ç®¡é“æ“ä½œç¬¦ `|` - é¡ºåºæ‰§è¡Œ**

å°†å¤šä¸ª Runnable ä¸²è”æˆç®¡é“ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# ç®¡é“ç»„åˆ
chain = (
    ChatPromptTemplate.from_template("Tell me about {topic}")
    | ChatOpenAI()
    | StrOutputParser()
)

# è‡ªåŠ¨æŒ‰é¡ºåºæ‰§è¡Œ
result = chain.invoke({"topic": "LangChain"})
```

**æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
graph LR
    A[Input] --> B[Prompt Template]
    B --> C[ChatOpenAI]
    C --> D[StrOutputParser]
    D --> E[Output]

    style B fill:#FFF9C4
    style C fill:#E3F2FD
    style D fill:#DCEDC8
```

**å¹¶è¡Œå­—å…¸ `{}` - å¹¶è¡Œæ‰§è¡Œ**

ä½¿ç”¨å­—å…¸è¯­æ³•å®ç°å¹¶è¡Œæ‰§è¡Œï¼š

```python
from langchain_core.runnables import RunnablePassthrough

chain = {
    "context": retriever | format_docs,
    "question": RunnablePassthrough()
} | prompt | model

# context å’Œ question å¹¶è¡Œå¤„ç†
result = chain.invoke("What is LangChain?")
```

**æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
graph TD
    A[Input] --> B[RunnableParallel]
    B --> C[context: retriever]
    B --> D[question: passthrough]
    C --> E[Merge Results]
    D --> E
    E --> F[Prompt]
    F --> G[Model]
```

**assign() - çŠ¶æ€æ›´æ–°å¿«æ·æ–¹å¼**

`RunnablePassthrough.assign()` æ˜¯ LCEL ä¸­æœ€å¸¸ç”¨çš„æ“ä½œä¹‹ä¸€ï¼Œç”¨äºåœ¨é“¾ä¸­æ·»åŠ æˆ–æ›´æ–°å­—æ®µï¼š

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºå‘é‡æ£€ç´¢å™¨
vectorstore = Chroma.from_texts(
    ["LangChainæ˜¯ä¸€ä¸ªAIåº”ç”¨æ¡†æ¶", "å®ƒæ”¯æŒRAGå’ŒAgent"],
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

# ä½¿ç”¨ assign() æ·»åŠ æ£€ç´¢ä¸Šä¸‹æ–‡
chain = (
    RunnablePassthrough.assign(
        context=retriever  # æ·»åŠ  context å­—æ®µ
    )
    | ChatPromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n{context}\n\né—®é¢˜: {question}"
    )
    | ChatOpenAI()
    | StrOutputParser()
)

# è¾“å…¥åªéœ€è¦ questionï¼Œcontext ä¼šè‡ªåŠ¨æ·»åŠ 
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯LangChain?"})
# å†…éƒ¨æµç¨‹: {"question": "..."} -> {"question": "...", "context": [...]}
```

**assign() çš„ä¼˜åŠ¿**ï¼š

1. **ä¿ç•™åŸå§‹è¾“å…¥**ï¼šä¸è¦†ç›–å·²æœ‰å­—æ®µ
2. **ç®€åŒ–ä»£ç **ï¼šé¿å…æ‰‹åŠ¨æ„é€ å­—å…¸
3. **é“¾å¼ç»„åˆ**ï¼šå¯ä»¥å¤šæ¬¡è°ƒç”¨

```python
# å¤šæ¬¡ assign å åŠ å­—æ®µ
chain = (
    RunnablePassthrough.assign(
        context=retriever  # æ·»åŠ æ£€ç´¢ç»“æœ
    )
    .assign(
        context_count=lambda x: len(x["context"])  # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    )
    .assign(
        timestamp=lambda x: "2025-11-17"  # æ·»åŠ æ—¶é—´æˆ³
    )
    | prompt
    | model
)

# è¾“å…¥: {"question": "..."}
# ç¬¬ä¸€æ­¥å: {"question": "...", "context": [...]}
# ç¬¬äºŒæ­¥å: {"question": "...", "context": [...], "context_count": 3}
# ç¬¬ä¸‰æ­¥å: {"question": "...", "context": [...], "context_count": 3, "timestamp": "..."}
```

**å¸¸è§ä½¿ç”¨åœºæ™¯**ï¼š

```python
# åœºæ™¯1: RAG æ·»åŠ æ£€ç´¢ä¸Šä¸‹æ–‡
rag_chain = (
    RunnablePassthrough.assign(context=retriever)
    | rag_prompt
    | model
)

# åœºæ™¯2: æ·»åŠ å¤šä¸ªæ•°æ®æº
multi_source_chain = (
    RunnablePassthrough.assign(
        docs=doc_retriever,
        history=history_retriever,
        metadata=metadata_fetcher
    )
    | prompt
    | model
)

# åœºæ™¯3: æ•°æ®è½¬æ¢
transform_chain = (
    RunnablePassthrough.assign(
        upper_text=lambda x: x["text"].upper(),
        word_count=lambda x: len(x["text"].split())
    )
    | processor
)
```

---

#### 2.2.3 ç»„åˆæ¨¡å¼ï¼šé¡ºåºã€å¹¶è¡Œã€æ¡ä»¶ã€å¾ªç¯

**é¡ºåºé“¾æ¥**

```python
# ç®€å•é¡ºåº
chain = step1 | step2 | step3

# å¤æ‚é¡ºåº
chain = (
    {"input": RunnablePassthrough()}
    | prompt
    | model
    | {"output": parser, "raw": RunnablePassthrough()}
)
```

**å¹¶è¡Œæ‰§è¡Œ**

```python
# å¹¶è¡Œè·å–å¤šä¸ªä¿¡æ¯
chain = RunnableParallel(
    summary=summarize_chain,
    keywords=extract_keywords_chain,
    sentiment=sentiment_chain
)
```

**æ¡ä»¶åˆ†æ”¯**

```python
from langchain_core.runnables import RunnableBranch

# æ ¹æ®è¾“å…¥é•¿åº¦é€‰æ‹©ä¸åŒå¤„ç†
chain = RunnableBranch(
    (lambda x: len(x["text"]) > 1000, long_text_chain),
    (lambda x: len(x["text"]) > 100, medium_text_chain),
    short_text_chain
)
```

**å¾ªç¯è¿­ä»£**

```python
# ä½¿ç”¨ RunnableLambda å®ç°å¾ªç¯
def iterative_refine(input):
    result = input
    for _ in range(3):
        result = refine_chain.invoke(result)
    return result

chain = RunnableLambda(iterative_refine)
```

---

### 2.3 é«˜çº§ç‰¹æ€§

#### 2.3.1 Fallback é™çº§ä¸ Retry é‡è¯•

**Fallback - è‡ªåŠ¨é™çº§**

```python
# å¤šçº§é™çº§
chain = (
    primary_model
    .with_fallbacks(fallbacks=[backup_model_1, backup_model_2])  # âœ… ä½¿ç”¨ fallbacks å‚æ•°å
)

# åªå¯¹ç‰¹å®šå¼‚å¸¸æ‰§è¡Œé™çº§
chain = (
    primary_model
    .with_fallbacks(
        fallbacks=[backup_model_1, backup_model_2],  # âœ… ä½¿ç”¨ fallbacks å‚æ•°å
        exceptions_to_handle=(TimeoutError, ConnectionError)  # âœ… å®˜æ–¹æ ‡å‡†å‚æ•°
    )
)
```

**é™çº§æµç¨‹**ï¼š

```mermaid
graph TD
    A[Request] --> B[Primary Model]
    B -- Success --> Z[Return]
    B -- Failure --> C[Backup Model 1]
    C -- Success --> Z
    C -- Failure --> D[Backup Model 2]
    D --> Z
```

**Retry - è‡ªåŠ¨é‡è¯•**

```python
# ç›´æ¥ä½¿ç”¨ with_retry æ–¹æ³•ï¼Œæ— éœ€å•ç‹¬å¯¼å…¥
chain = (
    prompt | model | parser
).with_retry(
    stop_after_attempt=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    wait_exponential_jitter=True,  # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
    retry_if_exception_type=(Exception,)  # æŒ‡å®šéœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
)
```

**å‚æ•°è¯´æ˜**ï¼ˆåŸºäºå®˜æ–¹APIæ–‡æ¡£éªŒè¯ï¼‰ï¼š
- `stop_after_attempt`ï¼šæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 3
- `wait_exponential_jitter`ï¼šæ˜¯å¦ä½¿ç”¨æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼Œé»˜è®¤ä¸º True
- `retry_if_exception_type`ï¼šéœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹å…ƒç»„ï¼Œé»˜è®¤ä¸º `(Exception,)`

**é‡è¯•ç­–ç•¥ç¤ºä¾‹**ï¼š
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# åªå¯¹ç‰¹å®šå¼‚å¸¸é‡è¯•
chain = (prompt | model | parser).with_retry(
    retry_if_exception_type=(TimeoutError, ConnectionError),
    stop_after_attempt=5,
    wait_exponential_jitter=True
)

# ç¦ç”¨æŒ‡æ•°é€€é¿ï¼ˆç«‹å³é‡è¯•ï¼‰
chain = (prompt | model | parser).with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=False  # ç¦ç”¨æŒ‡æ•°é€€é¿ï¼Œç«‹å³é‡è¯•
)
```

**é‡è¯•è¡Œä¸º**ï¼š
- æŒ‡æ•°é€€é¿ï¼š1s â†’ 2s â†’ 4s â†’ 8s
- æœ€å¤§é‡è¯•æ¬¡æ•°ï¼šå¯è‡ªå®šä¹‰ï¼ˆé»˜è®¤3æ¬¡ï¼‰
- é‡è¯•æ¡ä»¶ï¼šå¯æŒ‡å®šå¼‚å¸¸ç±»å‹ï¼ˆé»˜è®¤æ‰€æœ‰ Exceptionï¼‰

---

#### 2.3.2 Timeout è¶…æ—¶æ§åˆ¶

**é‡è¦**: `RunnableConfig` ä¸æ”¯æŒ `timeout` å‚æ•°ã€‚è¶…æ—¶æ§åˆ¶åº”åœ¨æ¨¡å‹å±‚é¢é…ç½®ã€‚

```python
from langchain_openai import ChatOpenAI

# âœ… æ­£ç¡®ï¼šåœ¨æ¨¡å‹æ„é€ æ—¶è®¾ç½®timeout
model = ChatOpenAI(
    model="gpt-4",
    timeout=30,  # 30ç§’è¶…æ—¶
    max_retries=2
)

chain = prompt | model | parser
result = chain.invoke(input)
```

**è¶…æ—¶ + é™çº§ç»„åˆç­–ç•¥**ï¼š

```python
from langchain_openai import ChatOpenAI

# ä¸»æ¨¡å‹ï¼šä¸¥æ ¼è¶…æ—¶
slow_model = ChatOpenAI(model="gpt-4", timeout=10)

# é™çº§æ¨¡å‹ï¼šå¿«é€Ÿå“åº”
fast_model = ChatOpenAI(model="gpt-3.5-turbo", timeout=5)

# ç»„åˆè¶…æ—¶ + é™çº§
chain = (prompt | slow_model | parser).with_fallbacks([
    prompt | fast_model | parser
])
```

**ä½¿ç”¨ RunnableConfig é…ç½®å…¶ä»–å‚æ•°**ï¼š

```python
from langchain_core.runnables import RunnableConfig

# RunnableConfigæ”¯æŒçš„å‚æ•°
result = chain.invoke(
    input,
    config=RunnableConfig(
        max_concurrency=5,      # æœ€å¤§å¹¶å‘æ•°
        tags=["production"],     # æ ‡ç­¾ï¼ˆç”¨äºç›‘æ§ï¼‰
        metadata={"user": "alice"}  # å…ƒæ•°æ®
    )
)
```

---

#### 2.3.3 ç¼“å­˜ä¸æ€§èƒ½ä¼˜åŒ–

> ğŸ’¡ **æç¤º**: æœ¬èŠ‚ä»‹ç» Runnable Protocol çš„åŸºç¡€æ€§èƒ½APIã€‚ç”Ÿäº§ç¯å¢ƒçš„æ·±åº¦æ€§èƒ½è°ƒä¼˜ã€æˆæœ¬æ§åˆ¶ã€ç¼“å­˜æ¶æ„ç­‰å†…å®¹ï¼Œè¯¦è§ **ç¬¬å…«ç¯‡ã€Šç”Ÿäº§å®è·µã€‹ç¬¬21ç« **ã€‚

**LLM ç¼“å­˜**

```python
from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache

# å¯ç”¨ç¼“å­˜
set_llm_cache(InMemoryCache())

# ç›¸åŒè¯·æ±‚ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
model.invoke("What is AI?")  # è°ƒç”¨ LLM
model.invoke("What is AI?")  # è¿”å›ç¼“å­˜ï¼ˆä¸è°ƒç”¨ LLMï¼‰
```

**æ‰¹å¤„ç†ä¼˜åŒ–**

```python
# æ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆä½¿ç”¨max_concurrencyæ§åˆ¶å¹¶å‘ï¼‰
chain = prompt | model.with_config({"max_concurrency": 10})

# å†…éƒ¨è‡ªåŠ¨åˆå¹¶è¯·æ±‚
results = chain.batch(inputs)
```

**æµå¼ä¼˜åŒ–**

```python
# æµå¼ä¼ è¾“å‡å°‘å»¶è¿Ÿ
for chunk in chain.stream(input):
    print(chunk, end="")
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | æ™®é€šè°ƒç”¨ | ä¼˜åŒ–å | æ€§èƒ½æå‡ |
|------|---------|--------|---------|
| ç¼“å­˜ | 2s | 50ms | 40x |
| æ‰¹å¤„ç† | 10s | 2s | 5x |
| æµå¼ | TTFT 2s | TTFT 200ms | 10x |

---

### æœ¬ç« å°ç»“

**Runnable Protocol æ ¸å¿ƒè¦ç‚¹**ï¼š
- âœ… ç»Ÿä¸€æ¥å£ï¼šinvokeã€streamã€batchã€ainvokeã€astream
- âœ… å¯ç»„åˆæ€§ï¼šLambdaã€Parallelã€Branchã€Fallbacks
- âœ… å¯è¿½è¸ªæ€§ï¼šè‡ªåŠ¨é›†æˆ LangSmith
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼šå¼‚æ­¥ã€æ‰¹å¤„ç†ã€ç¼“å­˜

**LCEL æ ¸å¿ƒè¦ç‚¹**ï¼š
- âœ… å£°æ˜å¼ç»„åˆï¼š`|` ç®¡é“ã€`{}` å¹¶è¡Œ
- âœ… è‡ªåŠ¨ä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œã€æµå¼ä¼ è¾“
- âœ… é«˜çº§ç‰¹æ€§ï¼šFallbackã€Retryã€Timeoutã€Cache

**è®¾è®¡å“²å­¦**ï¼š
> ä¸€åˆ‡çš† Runnableï¼Œæ‰€æœ‰ç»„ä»¶ç»Ÿä¸€æ¥å£ï¼Œå£°æ˜å¼ç»„åˆï¼Œè‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œã€‚

---

### æ€è€ƒä¸ç»ƒä¹ 

1. **ç»ƒä¹  1ï¼šåŸºç¡€ç®¡é“**
   æ„å»ºä¸€ä¸ª LCEL ç®¡é“ï¼šPrompt â†’ Model â†’ Parserï¼Œå®ç°ä¸€ä¸ªç®€å•çš„é—®ç­”ç³»ç»Ÿã€‚

2. **ç»ƒä¹  2ï¼šå¹¶è¡Œå¤„ç†**
   ä½¿ç”¨ RunnableParallel åŒæ—¶ç”Ÿæˆä¸€ä¸ªç¬‘è¯ã€ä¸€é¦–è¯—å’Œä¸€ä¸ªæ•…äº‹ï¼Œè¾“å…¥ä¸»é¢˜ä¸º"AI"ã€‚

3. **ç»ƒä¹  3ï¼šé”™è¯¯å¤„ç†**
   å®ç°ä¸€ä¸ªå¸¦æœ‰ Fallback å’Œ Retry çš„ chainï¼Œä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ã€‚

4. **ç»ƒä¹  4ï¼šæ€§èƒ½ä¼˜åŒ–**
   å¯¹æ¯”åŒæ­¥æ‰¹å¤„ç†å’Œå¼‚æ­¥æ‰¹å¤„ç†çš„æ€§èƒ½å·®å¼‚ï¼ˆ10ä¸ªè¯·æ±‚ï¼‰ã€‚

5. **æ€è€ƒé¢˜ï¼š**
   - ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥ä½¿ç”¨ stream è€Œä¸æ˜¯ invokeï¼Ÿ
   - RunnableBranch å’Œç®€å•çš„ if-else æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
   - å¦‚ä½•åœ¨ LCEL ä¸­å®ç°å¾ªç¯é€»è¾‘ï¼Ÿ
