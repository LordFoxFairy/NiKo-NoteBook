# ç¬¬å…­ç¯‡ï¼šæ–‡æ¡£å¤„ç†å·¥ç¨‹

## å‰è¨€

åœ¨å‰ä¸¤ç¯‡ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†RAGçš„åŸºç¡€çŸ¥è¯†ï¼ˆç¬¬å››ç¯‡ï¼‰å’Œé«˜çº§ä¼˜åŒ–æŠ€æœ¯ï¼ˆç¬¬äº”ç¯‡ï¼‰ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ**æ–‡æ¡£å¤„ç†**å¾€å¾€æ˜¯RAGç³»ç»Ÿæœ€å¤§çš„ç—›ç‚¹ï¼š

**å¸¸è§PDFå¤„ç†é—®é¢˜**ï¼š
1. **å­¦æœ¯è®ºæ–‡**ï¼šå¤æ‚çš„æ•°å­¦å…¬å¼ã€å¤šæ å¸ƒå±€ã€å›¾è¡¨
2. **æ‰«ææ–‡æ¡£**ï¼šéœ€è¦OCRè¯†åˆ«ï¼Œå¯èƒ½æœ‰å™ªç‚¹ã€å€¾æ–œ
3. **å¤šè¯­è¨€æ–‡æ¡£**ï¼šä¸­è‹±æ–‡æ··åˆã€ç‰¹æ®Šå­—ç¬¦
4. **å¤æ‚è¡¨æ ¼**ï¼šè·¨é¡µè¡¨æ ¼ã€åµŒå¥—è¡¨æ ¼
5. **å›¾ç‰‡ä¸å›¾è¡¨**ï¼šéœ€è¦æå–å¹¶å…³è”ä¸Šä¸‹æ–‡

**ä¼ ç»Ÿå·¥å…·çš„å±€é™**ï¼š
```python
# PyPDF2/pypdf - åŸºç¡€PDFè§£æ
âŒ æ— æ³•å¤„ç†æ‰«æPDF
âŒ å…¬å¼è¯†åˆ«å·®
âŒ å¤šæ å¸ƒå±€æ··ä¹±

# pdfplumber - ç¨å¥½çš„è§£æ
âš ï¸ æ‰«ææ–‡æ¡£æ— æ³•å¤„ç†
âš ï¸ å¤æ‚å…¬å¼ä¸¢å¤±
âš ï¸ å›¾è¡¨æå–æœ‰é™
```

æœ¬ç¯‡å°†æ·±å…¥æ¢è®¨**LangChainç”Ÿæ€ä¸‹çš„æ–‡æ¡£å¤„ç†æ–¹æ¡ˆ**ï¼Œè®©ä½ çš„RAGç³»ç»Ÿèƒ½å¤Ÿå¤„ç†99%çš„çœŸå®æ–‡æ¡£ã€‚

---

## å­¦ä¹ è·¯å¾„

```mermaid
graph LR
    A[Document Loaders<br/>PyPDF/PDFPlumber] --> B[OCRé›†æˆ<br/>Tesseract/PaddleOCR]
    B --> C[Unstructured.io<br/>ç»Ÿä¸€å¤„ç†æ¡†æ¶]
    C --> D[Text Splitters<br/>æ™ºèƒ½åˆ†å—]
    D --> E[é›†æˆåˆ°RAG]

    style A fill:#e1f5e1
    style C fill:#fff4e1
    style E fill:#ffe1e1
```

**æœ¬ç¯‡è¦†ç›–å†…å®¹**ï¼š
- **ç¬¬1ç« **ï¼šLangChain Document Loaders - PDFå¤„ç†å·¥å…·å¯¹æ¯”
- **ç¬¬2ç« **ï¼šOCRæŠ€æœ¯é›†æˆ - Tesseract, PaddleOCR
- **ç¬¬3ç« **ï¼šUnstructured.io - ç»Ÿä¸€æ–‡æ¡£å¤„ç†æ¡†æ¶
- **ç¬¬4ç« **ï¼šText Splitters - æ™ºèƒ½åˆ†å—ç­–ç•¥
- **ç¬¬5ç« **ï¼šç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline

---

## ç¬¬1ç« ï¼šLangChain Document Loaders

### 1.1 Document Loadersæ¦‚è¿°

#### 1.1.1 æ ¸å¿ƒæ¦‚å¿µ

**Document Loaders** æ˜¯LangChainä¸­ç”¨äºåŠ è½½å„ç§æ ¼å¼æ–‡æ¡£çš„ç»Ÿä¸€æ¥å£ï¼š

```python
from langchain_core.document_loaders import BaseLoader
from langchain_core.documents import Document
```

**æ‰€æœ‰Loadersçš„ç»Ÿä¸€API**ï¼š
```python
from langchain_community.document_loaders import PyPDFLoader

# 1. å®ä¾‹åŒ–Loader
loader = PyPDFLoader("document.pdf")

# 2. åŠ è½½æ–‡æ¡£ï¼ˆè¿”å›Documentå¯¹è±¡åˆ—è¡¨ï¼‰
documents = loader.load()

# 3. æ‡’åŠ è½½ï¼ˆé€‚åˆå¤§æ–‡ä»¶ï¼‰
for doc in loader.lazy_load():
    print(doc.page_content[:100])
```

---

#### 1.1.2 PDFç±»å‹åˆ†ç±»

**Type 1: åŸç”ŸPDFï¼ˆText-based PDFï¼‰**
```python
# ç‰¹å¾ï¼šæ–‡æœ¬å¯ç›´æ¥å¤åˆ¶
# ç”Ÿæˆæ–¹å¼ï¼šWordã€LaTeXã€ä»£ç ç”Ÿæˆ
# å¤„ç†éš¾åº¦ï¼šâ­ ç®€å•
# æ¨èå·¥å…·ï¼šPyPDFLoader, PyMuPDFLoader

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("./document.pdf")
pages = loader.load()
print(f"æå–åˆ°{len(pages)}é¡µæ–‡æœ¬")
```

**Type 2: æ‰«æPDFï¼ˆImage-based PDFï¼‰**
```python
# ç‰¹å¾ï¼šæ— æ³•å¤åˆ¶æ–‡æœ¬ï¼ˆå›¾ç‰‡ï¼‰
# ç”Ÿæˆæ–¹å¼ï¼šæ‰«æä»ªã€æ‹ç…§
# å¤„ç†éš¾åº¦ï¼šâ­â­â­ å›°éš¾
# æ¨èå·¥å…·ï¼šUnstructuredPDFLoader + OCR
```

**Type 3: æ··åˆPDFï¼ˆMixed PDFï¼‰**
```python
# ç‰¹å¾ï¼šéƒ¨åˆ†æ–‡æœ¬å¯å¤åˆ¶ï¼Œéƒ¨åˆ†æ˜¯å›¾ç‰‡
# åœºæ™¯ï¼šå­¦æœ¯è®ºæ–‡ï¼ˆæ–‡å­—+å…¬å¼å›¾ç‰‡ï¼‰
# å¤„ç†éš¾åº¦ï¼šâ­â­â­â­ å¾ˆå›°éš¾
# æ¨èå·¥å…·ï¼šUnstructured.io
```

---

### 1.2 PDF Loaderså¯¹æ¯”

#### 1.2.1 åŸºç¡€å·¥å…·å¯¹æ¯”

```python
from langchain_community.document_loaders import (
    PyPDFLoader,              # åŸºäºPyPDF2ï¼Œæœ€åŸºç¡€
    PDFPlumberLoader,         # æ›´å¥½çš„è¡¨æ ¼æ”¯æŒ
    PyMuPDFLoader,           # åŸºäºPyMuPDFï¼Œé€Ÿåº¦å¿«
    UnstructuredPDFLoader,   # æœ€å¼ºå¤§ï¼Œæ”¯æŒOCR
    PyPDFium2Loader,         # åŸºäºPDFium
    PDFMinerLoader           # åŸºäºPDFMiner
)

# å¿«é€Ÿå¯¹æ¯”æµ‹è¯•
import time

pdf_path = "./test.pdf"

# Test 1: PyPDFLoaderï¼ˆæœ€å¸¸ç”¨ï¼‰
start = time.time()
loader1 = PyPDFLoader(pdf_path)
docs1 = loader1.load()
time1 = time.time() - start
print(f"PyPDFLoader: {len(docs1)}é¡µ, {time1:.2f}s")

# Test 2: PDFPlumberLoaderï¼ˆè¡¨æ ¼æ”¯æŒå¥½ï¼‰
start = time.time()
loader2 = PDFPlumberLoader(pdf_path)
docs2 = loader2.load()
time2 = time.time() - start
print(f"PDFPlumberLoader: {len(docs2)}é¡µ, {time2:.2f}s")

# Test 3: PyMuPDFLoaderï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰
start = time.time()
loader3 = PyMuPDFLoader(pdf_path)
docs3 = loader3.load()
time3 = time.time() - start
print(f"PyMuPDFLoader: {len(docs3)}é¡µ, {time3:.2f}s")
```

**æ€§èƒ½å¯¹æ¯”**ï¼ˆåŸºäº100é¡µPDFï¼‰ï¼š

| å·¥å…· | é€Ÿåº¦ | æ–‡æœ¬è´¨é‡ | è¡¨æ ¼æ”¯æŒ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|---------|---------|
| **PyPDFLoader** | â­â­â­â­â­ å¿« | â­â­â­ ä¸­ç­‰ | âŒ å·® | ç®€å•æ–‡æ¡£ |
| **PDFPlumberLoader** | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¥½ | âœ… ä¼˜ç§€ | åŒ…å«è¡¨æ ¼ |
| **PyMuPDFLoader** | â­â­â­â­â­ æœ€å¿« | â­â­â­â­ å¥½ | â­â­â­ ä¸­ç­‰ | å¤§æ‰¹é‡å¤„ç† |
| **UnstructuredPDFLoader** | â­â­ æ…¢ | â­â­â­â­â­ æœ€å¥½ | âœ… ä¼˜ç§€ | å¤æ‚æ–‡æ¡£ |

---

#### 1.2.2 å·¥å…·é€‰æ‹©å†³ç­–æ ‘

```
PDFæ–‡æ¡£ç±»å‹
â”œâ”€â”€ ç®€å•æ–‡æœ¬PDF
â”‚   â””â”€â”€ PyPDFLoaderï¼ˆæœ€å¿«ï¼‰
â”œâ”€â”€ åŒ…å«è¡¨æ ¼
â”‚   â””â”€â”€ PDFPlumberLoaderï¼ˆè¡¨æ ¼è¯†åˆ«å¥½ï¼‰
â”œâ”€â”€ æ‰«æPDF
â”‚   â””â”€â”€ UnstructuredPDFLoader + OCR
â”œâ”€â”€ å­¦æœ¯è®ºæ–‡ï¼ˆå…¬å¼+å›¾è¡¨ï¼‰
â”‚   â””â”€â”€ UnstructuredPDFLoader (hi_res)
â””â”€â”€ å¤æ‚å¤šè¯­è¨€
    â””â”€â”€ Unstructured.io + OCR
```

---

### 1.3 å®æˆ˜ï¼šè¡¨æ ¼æå–

#### 1.3.1 PDFPlumberè¡¨æ ¼æå–

```python
import pdfplumber
from typing import List, Dict
from langchain_core.documents import Document

def extract_tables_pdfplumber(pdf_path: str) -> List[Dict]:
    """ä½¿ç”¨pdfplumberæå–è¡¨æ ¼"""
    tables_data = []

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            # æå–è¡¨æ ¼
            tables = page.extract_tables()

            for table_num, table in enumerate(tables, 1):
                # è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
                if table and len(table) > 0:
                    headers = table[0]  # ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
                    rows = table[1:]

                    table_dict = {
                        'page': page_num,
                        'table_num': table_num,
                        'headers': headers,
                        'rows': rows,
                        'text': format_table_as_text(headers, rows)
                    }
                    tables_data.append(table_dict)

    return tables_data

def format_table_as_text(headers: List, rows: List[List]) -> str:
    """å°†è¡¨æ ¼æ ¼å¼åŒ–ä¸ºMarkdown"""
    lines = []

    # è¡¨å¤´
    lines.append("| " + " | ".join(str(h) for h in headers) + " |")
    lines.append("|" + "|".join(["---"] * len(headers)) + "|")

    # æ•°æ®è¡Œ
    for row in rows:
        lines.append("| " + " | ".join(str(cell) for cell in row) + " |")

    return "\n".join(lines)

# ä½¿ç”¨ç¤ºä¾‹
tables = extract_tables_pdfplumber("./financial_report.pdf")
print(f"æå–åˆ°{len(tables)}ä¸ªè¡¨æ ¼")

for table in tables[:2]:
    print(f"\né¡µ{table['page']}ï¼Œè¡¨æ ¼{table['table_num']}ï¼š")
    print(table['text'])
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æå–åˆ°3ä¸ªè¡¨æ ¼

é¡µ2ï¼Œè¡¨æ ¼1ï¼š
| å­£åº¦ | æ”¶å…¥ | æ”¯å‡º | åˆ©æ¶¦ |
|---|---|---|---|
| Q1 | 1000ä¸‡ | 800ä¸‡ | 200ä¸‡ |
| Q2 | 1200ä¸‡ | 900ä¸‡ | 300ä¸‡ |
```

---

#### 1.3.2 é›†æˆåˆ°RAGç³»ç»Ÿ

```python
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.agents import create_agent
from langchain_core.tools import tool
from langchain_core.documents import Document

# æ­¥éª¤1: åŠ è½½PDFï¼ˆåŒ…å«è¡¨æ ¼ï¼‰
loader = PDFPlumberLoader("./reports/financial_Q1.pdf")
documents = loader.load()

# æ­¥éª¤2: æå–å¹¶æ ¼å¼åŒ–è¡¨æ ¼
tables = extract_tables_pdfplumber("./reports/financial_Q1.pdf")
table_docs = [
    Document(
        page_content=f"è¡¨æ ¼ï¼ˆé¡µ{t['page']}ï¼‰ï¼š\n{t['text']}",
        metadata={'page': t['page'], 'type': 'table'}
    )
    for t in tables
]

# åˆå¹¶æ–‡æœ¬å’Œè¡¨æ ¼
all_docs = documents + table_docs

# æ­¥éª¤3: åˆ†å—å¹¶å­˜å‚¨
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = splitter.split_documents(all_docs)

vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())

# æ­¥éª¤4: åˆ›å»ºæ£€ç´¢å·¥å…·
@tool
def search_financial_report(query: str) -> str:
    """æœç´¢è´¢æŠ¥æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    results = retriever.invoke(query)

    formatted = []
    for doc in results:
        doc_type = doc.metadata.get('type', 'text')
        formatted.append(
            f"[{doc_type.upper()}] é¡µ{doc.metadata.get('page', '?')}\n"
            f"{doc.page_content}"
        )
    return "\n\n".join(formatted)

# æ­¥éª¤5: åˆ›å»ºAgent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search_financial_report],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªè´¢æŠ¥åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥æŸ¥è¯¢è´¢æŠ¥æ–‡æ¡£ä¸­çš„æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ã€‚

æ³¨æ„ï¼š
- è¡¨æ ¼æ•°æ®ä»¥Markdownæ ¼å¼å±•ç¤º
- å¼•ç”¨æ—¶è¯·æ³¨æ˜é¡µç 
- å¯¹äºæ•°å€¼å¯¹æ¯”ï¼Œè¯·æä¾›å…·ä½“æ•°æ®
"""
)

# æµ‹è¯•æŸ¥è¯¢
result = agent.invoke({
    "messages": [("user", "Q1å’ŒQ2çš„æ”¶å…¥å¯¹æ¯”å¦‚ä½•ï¼Ÿ")]
})
print(result["messages"][-1].content)
```

---

### 1.4 PDFå¤„ç†æœ€ä½³å®è·µ

#### 1.4.1 é¢„å¤„ç†æ£€æŸ¥

```python
import fitz  # PyMuPDF

def analyze_pdf(pdf_path: str) -> dict:
    """åˆ†æPDFæ–‡æ¡£ç±»å‹å’Œç‰¹å¾"""
    doc = fitz.open(pdf_path)

    analysis = {
        'total_pages': len(doc),
        'has_text': False,
        'has_images': False,
        'text_pages': 0,
        'image_pages': 0,
        'estimated_type': None
    }

    for page in doc:
        # æ£€æŸ¥æ–‡æœ¬
        text = page.get_text()
        if text.strip():
            analysis['has_text'] = True
            analysis['text_pages'] += 1

        # æ£€æŸ¥å›¾ç‰‡
        images = page.get_images()
        if images:
            analysis['has_images'] = True
            analysis['image_pages'] += 1

    # åˆ¤æ–­PDFç±»å‹
    if analysis['text_pages'] == analysis['total_pages']:
        analysis['estimated_type'] = 'åŸç”ŸPDFï¼ˆæ–‡æœ¬ï¼‰'
    elif analysis['image_pages'] == analysis['total_pages']:
        analysis['estimated_type'] = 'æ‰«æPDFï¼ˆå›¾ç‰‡ï¼‰'
    else:
        analysis['estimated_type'] = 'æ··åˆPDF'

    doc.close()
    return analysis

# ä½¿ç”¨
info = analyze_pdf("./document.pdf")
print(f"PDFç±»å‹ï¼š{info['estimated_type']}")
print(f"æ€»é¡µæ•°ï¼š{info['total_pages']}")
print(f"æ–‡æœ¬é¡µï¼š{info['text_pages']}")
print(f"å›¾ç‰‡é¡µï¼š{info['image_pages']}")

# æ ¹æ®ç±»å‹é€‰æ‹©å·¥å…·
if info['estimated_type'] == 'åŸç”ŸPDFï¼ˆæ–‡æœ¬ï¼‰':
    print("æ¨èï¼šPyPDFLoader æˆ– PDFPlumberLoader")
elif info['estimated_type'] == 'æ‰«æPDFï¼ˆå›¾ç‰‡ï¼‰':
    print("æ¨èï¼šUnstructuredPDFLoader + OCR")
else:
    print("æ¨èï¼šUnstructured.io ç»Ÿä¸€å¤„ç†")
```

---

#### 1.4.2 é”™è¯¯å¤„ç†ä¸é™çº§ç­–ç•¥

```python
from typing import Optional, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_pdf_with_fallback(pdf_path: str) -> Optional[List[Document]]:
    """ä½¿ç”¨å¤šç§å·¥å…·å°è¯•åŠ è½½PDFï¼Œå¸¦é™çº§ç­–ç•¥"""

    # ç­–ç•¥1: å°è¯•PyMuPDFLoaderï¼ˆæœ€å¿«ï¼‰
    try:
        logger.info("å°è¯•PyMuPDFLoader...")
        from langchain_community.document_loaders import PyMuPDFLoader
        loader = PyMuPDFLoader(pdf_path)
        docs = loader.load()

        # éªŒè¯æå–è´¨é‡
        total_text = "".join([doc.page_content for doc in docs])
        if len(total_text) > 100:  # è‡³å°‘100å­—ç¬¦
            logger.info("âœ… PyMuPDFLoaderæˆåŠŸ")
            return docs
    except Exception as e:
        logger.warning(f"PyMuPDFLoaderå¤±è´¥: {e}")

    # ç­–ç•¥2: å°è¯•PDFPlumberLoaderï¼ˆè¡¨æ ¼æ”¯æŒå¥½ï¼‰
    try:
        logger.info("å°è¯•PDFPlumberLoader...")
        from langchain_community.document_loaders import PDFPlumberLoader
        loader = PDFPlumberLoader(pdf_path)
        docs = loader.load()

        if len(docs) > 0:
            logger.info("âœ… PDFPlumberLoaderæˆåŠŸ")
            return docs
    except Exception as e:
        logger.warning(f"PDFPlumberLoaderå¤±è´¥: {e}")

    # ç­–ç•¥3: å°è¯•UnstructuredPDFLoaderï¼ˆæœ€å¼ºå¤§ä½†æ…¢ï¼‰
    try:
        logger.info("å°è¯•UnstructuredPDFLoader...")
        from langchain_community.document_loaders import UnstructuredPDFLoader
        loader = UnstructuredPDFLoader(pdf_path)
        docs = loader.load()

        logger.info("âœ… UnstructuredPDFLoaderæˆåŠŸ")
        return docs
    except Exception as e:
        logger.error(f"UnstructuredPDFLoaderå¤±è´¥: {e}")

    # æ‰€æœ‰ç­–ç•¥å¤±è´¥
    logger.error("âŒ æ‰€æœ‰PDFåŠ è½½ç­–ç•¥å¤±è´¥")
    return None

# ä½¿ç”¨
docs = load_pdf_with_fallback("./difficult.pdf")
if docs:
    print(f"æˆåŠŸåŠ è½½{len(docs)}é¡µæ–‡æ¡£")
else:
    print("PDFåŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶")
```

---

### å°ç»“

**ç¬¬1ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Document Loadersç»Ÿä¸€API**ï¼š
   - æ‰€æœ‰loadersä» `langchain_community.document_loaders` å¯¼å…¥
   - åŸºç¡€æ¥å£ä» `langchain_core.document_loaders` å¯¼å…¥
   - ç»Ÿä¸€çš„ `load()` å’Œ `lazy_load()` æ–¹æ³•

2. **PDFå·¥å…·é€‰æ‹©**ï¼š
   - ç®€å•æ–‡æ¡£ â†’ PyPDFLoaderï¼ˆå¿«é€Ÿï¼‰
   - åŒ…å«è¡¨æ ¼ â†’ PDFPlumberLoaderï¼ˆè¡¨æ ¼è¯†åˆ«å¥½ï¼‰
   - å¤æ‚æ–‡æ¡£ â†’ UnstructuredPDFLoaderï¼ˆåŠŸèƒ½å¼ºå¤§ï¼‰

3. **æœ€ä½³å®è·µ**ï¼š
   - âœ… é¢„å…ˆåˆ†æPDFç±»å‹
   - âœ… ä½¿ç”¨é™çº§ç­–ç•¥ï¼ˆå¤šå·¥å…·å°è¯•ï¼‰
   - âœ… éªŒè¯æå–è´¨é‡
   - âœ… è¡¨æ ¼å•ç‹¬å¤„ç†å¹¶æ ¼å¼åŒ–

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬2ç« å°†æ·±å…¥æ¢è®¨**OCRæŠ€æœ¯é›†æˆ**ï¼Œè§£å†³æ‰«æPDFå’Œå›¾ç‰‡æ–‡æ¡£çš„è¯†åˆ«é—®é¢˜ã€‚

---

## ç¬¬2ç« ï¼šOCRæŠ€æœ¯é›†æˆ

### 2.1 OCRæŠ€æœ¯æ¦‚è¿°

#### 2.1.1 ä»€ä¹ˆæ˜¯OCR

**OCRï¼ˆOptical Character Recognitionï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰**ï¼š
```
æ‰«æPDF/å›¾ç‰‡ â†’ OCRå¼•æ“ â†’ å¯æœç´¢æ–‡æœ¬
```

**åº”ç”¨åœºæ™¯**ï¼š
- âœ… æ‰«ææ–‡æ¡£è¯†åˆ«
- âœ… å›¾ç‰‡ä¸­çš„æ–‡å­—æå–
- âœ… æ‰‹å†™ä½“è¯†åˆ«
- âœ… å¤šè¯­è¨€æ–‡æ¡£å¤„ç†

---

#### 2.1.2 OCRå·¥å…·å¯¹æ¯”

| å·¥å…· | å‡†ç¡®ç‡ | é€Ÿåº¦ | å¤šè¯­è¨€ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|--------|------|---------|
| **Tesseract** | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¿« | âœ… æ”¯æŒ100+è¯­è¨€ | å…è´¹ | é€šç”¨åœºæ™¯ |
| **PaddleOCR** | â­â­â­â­ å¥½ | â­â­â­â­ å¿« | âœ… ä¸­æ–‡ä¼˜ç§€ | å…è´¹ | ä¸­æ–‡æ–‡æ¡£ |
| **EasyOCR** | â­â­â­â­ å¥½ | â­â­â­ ä¸­ç­‰ | âœ… 80+è¯­è¨€ | å…è´¹ | å¤šè¯­è¨€ |
| **Google Vision API** | â­â­â­â­â­ æœ€å¥½ | â­â­â­â­ å¿« | âœ… å…¨é¢ | $$$ ä»˜è´¹ | å•†ä¸šåº”ç”¨ |
| **AWS Textract** | â­â­â­â­â­ æœ€å¥½ | â­â­â­â­â­ æœ€å¿« | âœ… å…¨é¢ | $$$ ä»˜è´¹ | è¡¨æ ¼+è¡¨å• |

---

### 2.2 Tesseract OCRé›†æˆ

#### 2.2.1 å®‰è£…ä¸é…ç½®

```bash
# å®‰è£…Tesseract
# macOS
brew install tesseract

# Ubuntu
sudo apt-get install tesseract-ocr

# å®‰è£…ä¸­æ–‡è¯­è¨€åŒ…
brew install tesseract-lang  # macOS
sudo apt-get install tesseract-ocr-chi-sim  # Ubuntu

# å®‰è£…Pythonåº“
pip install pytesseract pillow pdf2image
```

#### 2.2.2 åŸºç¡€OCRç¤ºä¾‹

```python
import pytesseract
from PIL import Image
from pdf2image import convert_from_path

def ocr_image(image_path: str, lang: str = 'eng') -> str:
    """å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«"""
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image, lang=lang)
    return text

def ocr_pdf(pdf_path: str, lang: str = 'eng') -> str:
    """å¯¹PDFè¿›è¡ŒOCRè¯†åˆ«"""
    # è½¬æ¢PDFä¸ºå›¾ç‰‡
    images = convert_from_path(pdf_path)

    # å¯¹æ¯ä¸€é¡µè¿›è¡ŒOCR
    all_text = []
    for page_num, image in enumerate(images, 1):
        print(f"å¤„ç†ç¬¬{page_num}é¡µ...")
        text = pytesseract.image_to_string(image, lang=lang)
        all_text.append(f"--- ç¬¬{page_num}é¡µ ---\n{text}")

    return "\n\n".join(all_text)

# ä½¿ç”¨ç¤ºä¾‹
# è‹±æ–‡æ–‡æ¡£
text_eng = ocr_pdf("./scanned_doc.pdf", lang='eng')
print(text_eng[:200])

# ä¸­æ–‡æ–‡æ¡£
text_chi = ocr_pdf("./chinese_doc.pdf", lang='chi_sim')
print(text_chi[:200])

# ä¸­è‹±æ–‡æ··åˆ
text_mixed = ocr_pdf("./mixed_doc.pdf", lang='chi_sim+eng')
print(text_mixed[:200])
```

---

#### 2.2.3 é›†æˆåˆ°LangChain

```python
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
from typing import List
import pytesseract
from pdf2image import convert_from_path

class OCRPDFLoader:
    """æ”¯æŒOCRçš„PDFåŠ è½½å™¨ï¼ˆå…¼å®¹LangChainï¼‰"""

    def __init__(self, pdf_path: str, lang: str = 'eng'):
        self.pdf_path = pdf_path
        self.lang = lang

    def load(self) -> List[Document]:
        """åŠ è½½PDFï¼Œè‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦OCR"""
        # å…ˆå°è¯•ç›´æ¥æå–æ–‡æœ¬
        try:
            loader = PyPDFLoader(self.pdf_path)
            docs = loader.load()

            # æ£€æŸ¥æå–è´¨é‡
            total_text = "".join([doc.page_content for doc in docs])

            if len(total_text.strip()) > 100:
                # æ–‡æœ¬å……è¶³ï¼Œç›´æ¥è¿”å›
                return docs
        except:
            pass

        # æ–‡æœ¬ä¸è¶³æˆ–å¤±è´¥ï¼Œä½¿ç”¨OCR
        return self._load_with_ocr()

    def _load_with_ocr(self) -> List[Document]:
        """ä½¿ç”¨OCRåŠ è½½PDF"""
        images = convert_from_path(self.pdf_path)

        documents = []
        for page_num, image in enumerate(images, 1):
            text = pytesseract.image_to_string(image, lang=self.lang)

            doc = Document(
                page_content=text,
                metadata={
                    'source': self.pdf_path,
                    'page': page_num,
                    'ocr': True
                }
            )
            documents.append(doc)

        return documents

# ä½¿ç”¨
loader = OCRPDFLoader("./scanned_document.pdf", lang='chi_sim+eng')
docs = loader.load()

print(f"åŠ è½½äº†{len(docs)}é¡µæ–‡æ¡£")
print(f"æ˜¯å¦ä½¿ç”¨OCRï¼š{docs[0].metadata.get('ocr', False)}")
```

---

### 2.3 PaddleOCRé›†æˆï¼ˆä¸­æ–‡ä¼˜ç§€ï¼‰

#### 2.3.1 å®‰è£…ä¸é…ç½®

```bash
# å®‰è£…PaddleOCR
pip install paddleocr paddlepaddle

# GPUç‰ˆæœ¬ï¼ˆå¯é€‰ï¼Œæ›´å¿«ï¼‰
pip install paddlepaddle-gpu
```

#### 2.3.2 åŸºç¡€ä½¿ç”¨

```python
from paddleocr import PaddleOCR
from PIL import Image
from pdf2image import convert_from_path
from langchain_core.documents import Document
from typing import List

# åˆå§‹åŒ–OCR
ocr = PaddleOCR(
    use_angle_cls=True,  # ä½¿ç”¨æ–¹å‘åˆ†ç±»å™¨
    lang='ch',           # ä¸­æ–‡
    use_gpu=False        # ä½¿ç”¨CPUï¼ˆå¦‚æœGPUä¸å¯ç”¨ï¼‰
)

def paddle_ocr_image(image_path: str) -> str:
    """ä½¿ç”¨PaddleOCRè¯†åˆ«å›¾ç‰‡"""
    result = ocr.ocr(image_path, cls=True)

    # æå–æ–‡æœ¬
    texts = []
    for line in result[0]:
        text = line[1][0]  # æ–‡æœ¬å†…å®¹
        confidence = line[1][1]  # ç½®ä¿¡åº¦
        if confidence > 0.5:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦
            texts.append(text)

    return "\n".join(texts)

def paddle_ocr_pdf(pdf_path: str) -> List[Document]:
    """ä½¿ç”¨PaddleOCRå¤„ç†PDF"""
    images = convert_from_path(pdf_path)
    documents = []

    for page_num, image in enumerate(images, 1):
        # ä¿å­˜ä¸ºä¸´æ—¶å›¾ç‰‡
        temp_path = f"/tmp/page_{page_num}.png"
        image.save(temp_path)

        # OCRè¯†åˆ«
        result = ocr.ocr(temp_path, cls=True)

        # æå–æ–‡æœ¬ï¼ˆä¿æŒå¸ƒå±€ï¼‰
        texts = []
        for line in result[0]:
            text = line[1][0]
            confidence = line[1][1]

            if confidence > 0.6:
                texts.append(text)

        doc = Document(
            page_content="\n".join(texts),
            metadata={
                'source': pdf_path,
                'page': page_num,
                'ocr': 'PaddleOCR'
            }
        )
        documents.append(doc)

    return documents

# ä½¿ç”¨
docs = paddle_ocr_pdf("./chinese_scanned.pdf")
print(f"æå–{len(docs)}é¡µæ–‡æ¡£")
for doc in docs[:2]:
    print(f"\né¡µ{doc.metadata['page']}ï¼š")
    print(doc.page_content[:200])
```

---

#### 2.3.3 è¡¨æ ¼è¯†åˆ«

```python
from paddleocr import PPStructure
from typing import Dict

# åˆå§‹åŒ–è¡¨æ ¼è¯†åˆ«
table_engine = PPStructure(
    table=True,
    ocr=True,
    show_log=False
)

def extract_tables_paddle(pdf_path: str) -> List[Dict]:
    """ä½¿ç”¨PaddleOCRæå–è¡¨æ ¼"""
    images = convert_from_path(pdf_path)
    all_tables = []

    for page_num, image in enumerate(images, 1):
        temp_path = f"/tmp/page_{page_num}.png"
        image.save(temp_path)

        # ç»“æ„åŒ–åˆ†æ
        result = table_engine(temp_path)

        for item in result:
            if item['type'] == 'table':
                # æå–è¡¨æ ¼HTML
                table_html = item['res']['html']

                # è½¬æ¢ä¸ºMarkdownï¼ˆç®€åŒ–ï¼‰
                table_md = html_table_to_markdown(table_html)

                all_tables.append({
                    'page': page_num,
                    'html': table_html,
                    'markdown': table_md
                })

    return all_tables

def html_table_to_markdown(html: str) -> str:
    """å°†HTMLè¡¨æ ¼è½¬æ¢ä¸ºMarkdown"""
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html, 'html.parser')
    table = soup.find('table')

    if not table:
        return ""

    rows = table.find_all('tr')
    md_lines = []

    for i, row in enumerate(rows):
        cells = row.find_all(['td', 'th'])
        md_line = "| " + " | ".join([cell.get_text().strip() for cell in cells]) + " |"
        md_lines.append(md_line)

        # æ·»åŠ åˆ†éš”çº¿ï¼ˆåœ¨è¡¨å¤´åï¼‰
        if i == 0:
            md_lines.append("|" + "|".join(["---"] * len(cells)) + "|")

    return "\n".join(md_lines)

# ä½¿ç”¨
tables = extract_tables_paddle("./report_with_tables.pdf")
print(f"æå–åˆ°{len(tables)}ä¸ªè¡¨æ ¼")

for table in tables[:2]:
    print(f"\né¡µ{table['page']}çš„è¡¨æ ¼ï¼š")
    print(table['markdown'])
```

---

### 2.4 äº‘OCRæœåŠ¡é›†æˆ

#### 2.4.1 Google Cloud Vision API

```python
from google.cloud import vision
import io

def google_ocr(image_path: str) -> str:
    """ä½¿ç”¨Google Cloud Vision APIè¿›è¡ŒOCR"""
    client = vision.ImageAnnotatorClient()

    with io.open(image_path, 'rb') as image_file:
        content = image_file.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)
    texts = response.text_annotations

    if texts:
        return texts[0].description
    return ""

# æ³¨æ„ï¼šéœ€è¦é…ç½®Google Cloudå‡­æ®
# export GOOGLE_APPLICATION_CREDENTIALS="path/to/credentials.json"
```

---

#### 2.4.2 AWS Textractï¼ˆè¡¨æ ¼è¯†åˆ«å¼ºï¼‰

```python
import boto3

def aws_textract(pdf_path: str) -> dict:
    """ä½¿ç”¨AWS Textractæå–PDFï¼ˆåŒ…æ‹¬è¡¨æ ¼ï¼‰"""
    textract = boto3.client('textract')

    with open(pdf_path, 'rb') as document:
        response = textract.analyze_document(
            Document={'Bytes': document.read()},
            FeatureTypes=['TABLES', 'FORMS']
        )

    # æå–æ–‡æœ¬
    text = ""
    tables = []

    for block in response['Blocks']:
        if block['BlockType'] == 'LINE':
            text += block['Text'] + "\n"
        elif block['BlockType'] == 'TABLE':
            # æå–è¡¨æ ¼
            table = extract_table_from_block(block, response['Blocks'])
            tables.append(table)

    return {
        'text': text,
        'tables': tables
    }

# æ³¨æ„ï¼šéœ€è¦AWSå‡­æ®é…ç½®
```

---

#### 2.4.3 æˆæœ¬å¯¹æ¯”

| æœåŠ¡ | å®šä»· | å…è´¹é¢åº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|---------|
| **Tesseract** | å…è´¹ | æ— é™ | å¼€å‘æµ‹è¯•ã€ä½æˆæœ¬ |
| **PaddleOCR** | å…è´¹ | æ— é™ | ä¸­æ–‡æ–‡æ¡£ |
| **Google Vision** | $1.5/1000é¡µ | 1000é¡µ/æœˆ | é«˜å‡†ç¡®ç‡éœ€æ±‚ |
| **AWS Textract** | $1.5/1000é¡µï¼ˆæ–‡æ¡£ï¼‰<br/>$15/1000é¡µï¼ˆè¡¨æ ¼ï¼‰ | 1000é¡µ/æœˆ | è¡¨æ ¼è¯†åˆ« |

**æˆæœ¬ä¼˜åŒ–å»ºè®®**ï¼š
1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨Tesseract/PaddleOCR
2. **ç”Ÿäº§é˜¶æ®µï¼ˆä½é‡ï¼‰**ï¼šäº‘æœåŠ¡å…è´¹é¢åº¦
3. **ç”Ÿäº§é˜¶æ®µï¼ˆé«˜é‡ï¼‰**ï¼š
   - ä¸­æ–‡ä¸ºä¸» â†’ PaddleOCRè‡ªå»º
   - è¡¨æ ¼ä¸ºä¸» â†’ AWS Textract
   - å¤šè¯­è¨€ â†’ Google Vision

---

---

### 2.4 MinerU - å­¦æœ¯æ–‡æ¡£ä¸“ç”¨è§£æå™¨

> **ç‰ˆæœ¬ä¿¡æ¯**: MinerU 2.6.4+ (2025-11-04æ›´æ–°)
> **é¡¹ç›®åœ°å€**: https://github.com/opendatalab/MinerU

#### 2.4.1 MinerUç®€ä»‹

**MinerU** æ˜¯ç”±OpenDataLabå¼€å‘çš„ä¸“ä¸šæ–‡æ¡£è§£æå·¥å…·ï¼Œç‰¹åˆ«é’ˆå¯¹**å­¦æœ¯è®ºæ–‡PDF**è¿›è¡Œä¼˜åŒ–ã€‚å®ƒåœ¨InternLMå¤§æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼€å‘ï¼Œä¸“ä¸ºå°†å¤æ‚PDFè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼è€Œè®¾è®¡ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… **å…¬å¼è¯†åˆ«LaTeXè¾“å‡º**ï¼šæ•°å­¦å…¬å¼è½¬æ¢ä¸ºLaTeXæ ¼å¼
- âœ… **å¤æ‚è¡¨æ ¼æå–**ï¼šè¡¨æ ¼è½¬HTMLï¼Œæ”¯æŒè·¨é¡µè¡¨æ ¼åˆå¹¶
- âœ… **å¤šæ å¸ƒå±€å¤„ç†**ï¼šè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€å’Œé˜…è¯»é¡ºåº
- âœ… **å¤šè¯­è¨€OCR**ï¼šæ”¯æŒ109ç§è¯­è¨€çš„OCRè¯†åˆ«
- âœ… **å¤šæ ¼å¼è¾“å‡º**ï¼šMarkdownã€JSONæ ¼å¼è¾“å‡º
- âœ… **å¤šå¹³å°æ”¯æŒ**ï¼šCPUã€GPU (CUDA)ã€NPU (CANN)ã€MPS (Apple Silicon)

**ä¸ä¼ ç»Ÿå·¥å…·å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | PyPDF | Unstructured | MinerU |
|------|-------|--------------|--------|
| **LaTeXå…¬å¼** | âŒ ä¸æ”¯æŒ | âš ï¸ åŸºç¡€è¯†åˆ« | âœ… ä¸“ä¸šçº§ |
| **å¤æ‚è¡¨æ ¼** | âŒ å·® | â­â­â­ å¥½ | â­â­â­â­â­ ä¼˜ç§€ |
| **å¤šæ å¸ƒå±€** | âŒ æ··ä¹± | â­â­â­ å¯ç”¨ | â­â­â­â­â­ ä¼˜ç§€ |
| **å­¦æœ¯è®ºæ–‡** | âŒ ä¸é€‚ç”¨ | â­â­â­ å¯ç”¨ | â­â­â­â­â­ ä¸“ç”¨ |
| **å¤„ç†é€Ÿåº¦** | â­â­â­â­â­ æœ€å¿« | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¿« |

---

#### 2.4.2 å®‰è£…æ–¹æ³•

```bash
# åŸºç¡€å®‰è£…ï¼ˆCPUç‰ˆæœ¬ï¼‰
pip install mineru

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
mineru download-models

# GPUåŠ é€Ÿï¼ˆå¯é€‰ï¼Œéœ€CUDAæ”¯æŒï¼‰
# è‡ªåŠ¨æ£€æµ‹CUDAï¼Œæ— éœ€é¢å¤–é…ç½®
```

**ç³»ç»Ÿè¦æ±‚**ï¼š
- Python >= 3.8
- (å¯é€‰) CUDA 11.8+ ç”¨äºGPUåŠ é€Ÿ
- (å¯é€‰) Apple Siliconç”¨æˆ·å¯ä½¿ç”¨MPSåŠ é€Ÿ

---

#### 2.4.3 åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
import subprocess
import json
from pathlib import Path
from typing import Dict, List

def parse_pdf_with_mineru(
    pdf_path: str,
    output_dir: str = "./output",
    backend: str = "pipeline"  # "pipeline" æˆ– "vlm"
) -> Dict:
    """
    ä½¿ç”¨MinerUè§£æPDFæ–‡æ¡£

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        backend: å¤„ç†å¼•æ“
            - "pipeline": ä¼ ç»ŸCV/OCRæ–¹æ³•ï¼ˆé«˜å‡†ç¡®ç‡ï¼‰
            - "vlm": MinerU2.5å¤šæ¨¡æ€æ¨¡å‹ï¼ˆç«¯åˆ°ç«¯æ¨ç†ï¼‰

    Returns:
        è§£æç»“æœå­—å…¸ï¼ŒåŒ…å«markdownå’ŒJSONæ ¼å¼
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è§£æ
    cmd = [
        "mineru",
        "parse",
        pdf_path,
        "--output-dir", output_dir,
        "--backend", backend
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )

        # è¯»å–ç”Ÿæˆçš„Markdownæ–‡ä»¶
        pdf_name = Path(pdf_path).stem
        md_file = output_path / f"{pdf_name}.md"
        json_file = output_path / f"{pdf_name}.json"

        markdown_content = ""
        json_content = {}

        if md_file.exists():
            with open(md_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

        if json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                json_content = json.load(f)

        return {
            'success': True,
            'markdown': markdown_content,
            'json': json_content,
            'output_dir': str(output_path)
        }

    except subprocess.CalledProcessError as e:
        return {
            'success': False,
            'error': str(e),
            'stderr': e.stderr
        }

# ä½¿ç”¨ç¤ºä¾‹
result = parse_pdf_with_mineru(
    "./academic_paper.pdf",
    output_dir="./parsed_output",
    backend="pipeline"
)

if result['success']:
    print("è§£ææˆåŠŸï¼")
    print(f"Markdowné•¿åº¦: {len(result['markdown'])} å­—ç¬¦")
    print(f"è¾“å‡ºç›®å½•: {result['output_dir']}")

    # é¢„è§ˆMarkdownå‰500å­—ç¬¦
    print("\nMarkdowné¢„è§ˆ:")
    print(result['markdown'][:500])
else:
    print(f"è§£æå¤±è´¥: {result['error']}")
```

---

#### 2.4.4 ä¸LangChainé›†æˆ

```python
from langchain_core.documents import Document
from langchain_core.document_loaders import BaseLoader
from typing import List
import subprocess
import json
from pathlib import Path

class MinerULoader(BaseLoader):
    """MinerUæ–‡æ¡£åŠ è½½å™¨ï¼ˆå…¼å®¹LangChainï¼‰"""

    def __init__(
        self,
        file_path: str,
        backend: str = "pipeline",
        output_dir: str = "./mineru_cache"
    ):
        """
        åˆå§‹åŒ–MinerUåŠ è½½å™¨

        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            backend: å¤„ç†å¼•æ“ ("pipeline" æˆ– "vlm")
            output_dir: ç¼“å­˜ç›®å½•
        """
        self.file_path = file_path
        self.backend = backend
        self.output_dir = output_dir

    def load(self) -> List[Document]:
        """åŠ è½½å¹¶è§£æPDFæ–‡æ¡£"""
        # æ£€æŸ¥ç¼“å­˜
        cache_path = Path(self.output_dir)
        pdf_name = Path(self.file_path).stem
        md_file = cache_path / f"{pdf_name}.md"
        json_file = cache_path / f"{pdf_name}.json"

        # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥è¯»å–
        if md_file.exists() and json_file.exists():
            print(f"âœ… ä½¿ç”¨ç¼“å­˜: {md_file}")
            return self._load_from_cache(md_file, json_file)

        # è¿è¡ŒMinerUè§£æ
        print(f"ğŸ”„ æ­£åœ¨è§£æPDF: {self.file_path}")
        cmd = [
            "mineru",
            "parse",
            self.file_path,
            "--output-dir", self.output_dir,
            "--backend", self.backend
        ]

        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return self._load_from_cache(md_file, json_file)

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"MinerUè§£æå¤±è´¥: {e.stderr.decode()}")

    def _load_from_cache(self, md_file: Path, json_file: Path) -> List[Document]:
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        # è¯»å–Markdown
        with open(md_file, 'r', encoding='utf-8') as f:
            markdown = f.read()

        # è¯»å–JSONï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
        with open(json_file, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)

        # æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æ¡£ï¼ˆåŸºäºMarkdownæ ‡é¢˜ï¼‰
        documents = self._split_by_headers(markdown, meta_data)

        return documents

    def _split_by_headers(self, markdown: str, metadata: dict) -> List[Document]:
        """æŒ‰Markdownæ ‡é¢˜åˆ†å‰²æ–‡æ¡£"""
        lines = markdown.split('\n')
        current_section = []
        current_header = "Introduction"
        documents = []

        for line in lines:
            # æ£€æµ‹æ ‡é¢˜ï¼ˆ# æˆ– ##ï¼‰
            if line.startswith('#'):
                # ä¿å­˜å½“å‰ç« èŠ‚
                if current_section:
                    content = '\n'.join(current_section)
                    if content.strip():
                        documents.append(Document(
                            page_content=content,
                            metadata={
                                'source': self.file_path,
                                'section': current_header,
                                'parser': 'MinerU',
                                'backend': self.backend
                            }
                        ))

                # å¼€å§‹æ–°ç« èŠ‚
                current_header = line.lstrip('#').strip()
                current_section = [line]
            else:
                current_section.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            content = '\n'.join(current_section)
            if content.strip():
                documents.append(Document(
                    page_content=content,
                    metadata={
                        'source': self.file_path,
                        'section': current_header,
                        'parser': 'MinerU',
                        'backend': self.backend
                    }
                ))

        return documents

# ä½¿ç”¨ç¤ºä¾‹
loader = MinerULoader(
    "./research_paper.pdf",
    backend="pipeline"
)

documents = loader.load()
print(f"åŠ è½½äº†{len(documents)}ä¸ªæ–‡æ¡£å—")

for doc in documents[:3]:
    print(f"\nç« èŠ‚: {doc.metadata['section']}")
    print(f"å†…å®¹é¢„è§ˆ: {doc.page_content[:200]}...")
```

---

#### 2.4.5 é«˜çº§ç‰¹æ€§ï¼šå…¬å¼ä¸è¡¨æ ¼æå–

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# æ­¥éª¤1: ä½¿ç”¨MinerUåŠ è½½å­¦æœ¯è®ºæ–‡
loader = MinerULoader("./complex_paper.pdf", backend="pipeline")
documents = loader.load()

# æ­¥éª¤2: æå–ç‰¹æ®Šå…ƒç´ ï¼ˆå…¬å¼ã€è¡¨æ ¼ï¼‰
formulas = []
tables = []
text_sections = []

for doc in documents:
    content = doc.page_content

    # è¯†åˆ«LaTeXå…¬å¼ï¼ˆMinerUä¼šå°†å…¬å¼åŒ…è£¹åœ¨$$æˆ–$ä¸­ï¼‰
    if '$$' in content or '$' in content:
        formulas.append(doc)

    # è¯†åˆ«è¡¨æ ¼ï¼ˆMarkdownè¡¨æ ¼æ ¼å¼ï¼‰
    if '|' in content and '---' in content:
        tables.append(doc)

    # æ™®é€šæ–‡æœ¬
    if not ('$$' in content or '|' in content):
        text_sections.append(doc)

print(f"æå–åˆ° {len(formulas)} ä¸ªå…¬å¼ç« èŠ‚")
print(f"æå–åˆ° {len(tables)} ä¸ªè¡¨æ ¼ç« èŠ‚")
print(f"æå–åˆ° {len(text_sections)} ä¸ªæ–‡æœ¬ç« èŠ‚")

# æ­¥éª¤3: äºŒæ¬¡åˆ†å—
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
all_splits = splitter.split_documents(documents)

# æ­¥éª¤4: æ„å»ºå‘é‡åº“
vectorstore = Chroma.from_documents(
    documents=all_splits,
    embedding=OpenAIEmbeddings()
)

print(f"å‘é‡åº“æ„å»ºå®Œæˆï¼Œå…± {len(all_splits)} ä¸ªchunk")
```

---

#### 2.4.6 é€‚ç”¨åœºæ™¯å¯¹æ¯”

**ä½•æ—¶ä½¿ç”¨MinerU**ï¼š

| åœºæ™¯ | æ¨èå·¥å…· | åŸå›  |
|------|---------|------|
| å­¦æœ¯è®ºæ–‡ï¼ˆå«å…¬å¼ï¼‰ | âœ… **MinerU** | ä¸“ä¸šLaTeXå…¬å¼è¯†åˆ« |
| ç ”ç©¶æŠ¥å‘Šï¼ˆå¤šæ å¸ƒå±€ï¼‰ | âœ… **MinerU** | ä¼˜ç§€çš„å¸ƒå±€åˆ†æ |
| æŠ€æœ¯æ–‡æ¡£ï¼ˆå¤æ‚è¡¨æ ¼ï¼‰ | âœ… **MinerU** | è·¨é¡µè¡¨æ ¼åˆå¹¶ |
| ç®€å•PDFæ–‡æ¡£ | PyPDFLoader | MinerUè¿‡é‡ |
| å¤šæ ¼å¼æ–‡æ¡£ï¼ˆWord/HTMLï¼‰ | Unstructured | MinerUä»…æ”¯æŒPDF |
| æ‰«æPDFï¼ˆä½è´¨é‡ï¼‰ | Unstructured + OCR | MinerUä¾èµ–æ–‡æœ¬å±‚ |

---

#### 2.4.7 æˆæœ¬ä¸æ€§èƒ½åˆ†æ

**æˆæœ¬å¯¹æ¯”**ï¼š

| æ–¹æ¡ˆ | ç›´æ¥æˆæœ¬ | è®¡ç®—èµ„æº | é€‚ç”¨è§„æ¨¡ |
|------|---------|---------|---------|
| **MinerU (CPU)** | å…è´¹ | ä¸­ç­‰ | å°æ‰¹é‡ï¼ˆ<100æ–‡æ¡£ï¼‰ |
| **MinerU (GPU)** | å…è´¹ | éœ€GPUæœåŠ¡å™¨ | å¤§æ‰¹é‡ï¼ˆ100+æ–‡æ¡£ï¼‰ |
| **Unstructured** | å…è´¹ | ä½ | é€šç”¨åœºæ™¯ |
| **AWS Textract** | $1.5/1000é¡µ | æ— éœ€è‡ªå»º | å•†ä¸šåº”ç”¨ |

**æ€§èƒ½æµ‹è¯•**ï¼ˆåŸºäº100é¡µå­¦æœ¯è®ºæ–‡ï¼‰ï¼š

```python
import time

# æµ‹è¯•1: MinerU (pipelineåç«¯)
start = time.time()
loader1 = MinerULoader("./paper.pdf", backend="pipeline")
docs1 = loader1.load()
time1 = time.time() - start
print(f"MinerU (pipeline): {len(docs1)}ä¸ªæ–‡æ¡£å—, {time1:.2f}ç§’")

# æµ‹è¯•2: MinerU (vlmåç«¯ - Apple Siliconä¼˜åŒ–)
start = time.time()
loader2 = MinerULoader("./paper.pdf", backend="vlm")
docs2 = loader2.load()
time2 = time.time() - start
print(f"MinerU (vlm): {len(docs2)}ä¸ªæ–‡æ¡£å—, {time2:.2f}ç§’")

# æµ‹è¯•3: Unstructured (å¯¹æ¯”)
from langchain_community.document_loaders import UnstructuredFileLoader
start = time.time()
loader3 = UnstructuredFileLoader("./paper.pdf", strategy="hi_res")
docs3 = loader3.load()
time3 = time.time() - start
print(f"Unstructured: {len(docs3)}ä¸ªæ–‡æ¡£å—, {time3:.2f}ç§’")
```

**é¢„æœŸç»“æœ**ï¼š
- MinerU (pipeline): ~30-50ç§’ï¼ˆCPUï¼‰
- MinerU (vlm): ~15-25ç§’ï¼ˆApple Silicon/GPUåŠ é€Ÿï¼‰
- Unstructured (hi_res): ~60-90ç§’

---

### 2.5 DeepSeek Janus - å¤šæ¨¡æ€ç†è§£ï¼ˆå®éªŒæ€§ï¼‰

> **ç‰ˆæœ¬ä¿¡æ¯**: Janus-Pro 1B/7B (2025æœ€æ–°)
> **é¡¹ç›®åœ°å€**: https://github.com/deepseek-ai/Janus

#### 2.5.1 DeepSeek Janusç®€ä»‹

**Janus** æ˜¯DeepSeekæ¨å‡ºçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒ**è§†è§‰ç†è§£**å’Œ**å›¾åƒç”Ÿæˆ**ã€‚è™½ç„¶ä¸æ˜¯ä¸“é—¨çš„OCRå·¥å…·ï¼Œä½†å…¶å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›å¯ç”¨äºæ–‡æ¡£å›¾åƒçš„æ–‡å­—è¯†åˆ«å’Œç†è§£ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… **å¤šæ¨¡æ€ç†è§£**ï¼šåŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬
- âœ… **å¤æ‚åœºæ™¯è¯†åˆ«**ï¼šæ‰‹å†™ä½“ã€å€¾æ–œæ–‡æœ¬ã€å¤æ‚èƒŒæ™¯
- âœ… **ä¸Šä¸‹æ–‡ç†è§£**ï¼šä¸ä»…è¯†åˆ«æ–‡å­—ï¼Œè¿˜ç†è§£è¯­ä¹‰
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒä¸­è‹±æ–‡ç­‰å¤šè¯­è¨€

**ä¸ä¼ ç»ŸOCRå¯¹æ¯”**ï¼š

| ç‰¹æ€§ | PaddleOCR | Tesseract | Janus |
|------|-----------|-----------|-------|
| **å‡†ç¡®ç‡** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **è¯­ä¹‰ç†è§£** | âŒ | âŒ | âœ… å¼ºå¤§ |
| **æ‰‹å†™è¯†åˆ«** | â­â­â­ | â­â­ | â­â­â­â­â­ |
| **å¤„ç†é€Ÿåº¦** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **èµ„æºéœ€æ±‚** | ä½ | ä½ | é«˜ï¼ˆéœ€GPUï¼‰ |

---

#### 2.5.2 å®‰è£…ä¸é…ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/deepseek-ai/Janus.git
cd Janus

# å®‰è£…ä¾èµ–
pip install -e .

# éœ€è¦PyTorch >= 2.0
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

**ç³»ç»Ÿè¦æ±‚**ï¼š
- Python >= 3.8
- CUDA 11.8+ (æ¨èä½¿ç”¨GPU)
- è‡³å°‘16GBå†…å­˜ï¼ˆ1Bæ¨¡å‹ï¼‰æˆ–32GBå†…å­˜ï¼ˆ7Bæ¨¡å‹ï¼‰

---

#### 2.5.3 åŸºç¡€OCRç¤ºä¾‹

```python
import torch
from transformers import AutoModelForCausalLM
from janus.models import MultiModalityCausalLM, VLChatProcessor
from janus.utils.io import load_pil_images
from PIL import Image

def ocr_with_janus(image_path: str, model_path: str = "deepseek-ai/Janus-Pro-1B") -> str:
    """
    ä½¿ç”¨Janusè¿›è¡ŒOCRè¯†åˆ«

    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆJanus-Pro-1B æˆ– Janus-Pro-7Bï¼‰

    Returns:
        è¯†åˆ«çš„æ–‡æœ¬å†…å®¹
    """
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    vl_chat_processor = VLChatProcessor.from_pretrained(model_path)
    vl_gpt = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

    # åŠ è½½å›¾ç‰‡
    image = Image.open(image_path)

    # æ„å»ºå¯¹è¯ï¼ˆOCRæç¤ºï¼‰
    conversation = [
        {
            "role": "<|User|>",
            "content": "<image_placeholder>\nè¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œå¹¶æŒ‰åŸå§‹é¡ºåºè¾“å‡ºã€‚",
            "images": [image]
        },
        {
            "role": "<|Assistant|>",
            "content": ""
        }
    ]

    # å‡†å¤‡è¾“å…¥
    pil_images = load_pil_images(conversation)
    prepare_inputs = vl_chat_processor(
        conversations=conversation,
        images=pil_images,
        force_batchify=True
    )

    # ç”Ÿæˆè¾“å‡º
    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

    with torch.no_grad():
        outputs = vl_gpt.language_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=prepare_inputs.attention_mask,
            pad_token_id=vl_chat_processor.tokenizer.eos_token_id,
            bos_token_id=vl_chat_processor.tokenizer.bos_token_id,
            eos_token_id=vl_chat_processor.tokenizer.eos_token_id,
            max_new_tokens=512,
            do_sample=False
        )

    # è§£ç è¾“å‡º
    answer = vl_chat_processor.tokenizer.decode(
        outputs[0].cpu().tolist(),
        skip_special_tokens=True
    )

    return answer

# ä½¿ç”¨ç¤ºä¾‹
text = ocr_with_janus("./scanned_page.jpg", model_path="deepseek-ai/Janus-Pro-1B")
print("è¯†åˆ«ç»“æœ:")
print(text)
```

---

#### 2.5.4 è¡¨æ ¼ç†è§£ç¤ºä¾‹

```python
def extract_table_with_janus(image_path: str) -> str:
    """
    ä½¿ç”¨Janusæå–å¹¶ç†è§£è¡¨æ ¼

    ä¼˜åŠ¿ï¼šä¸ä»…è¯†åˆ«æ–‡å­—ï¼Œè¿˜èƒ½ç†è§£è¡¨æ ¼ç»“æ„
    """
    # åŠ è½½æ¨¡å‹ï¼ˆä»£ç å¤ç”¨ä¸Šé¢çš„ç¤ºä¾‹ï¼‰
    vl_chat_processor = VLChatProcessor.from_pretrained("deepseek-ai/Janus-Pro-1B")
    vl_gpt = AutoModelForCausalLM.from_pretrained(
        "deepseek-ai/Janus-Pro-1B",
        trust_remote_code=True
    ).to(torch.bfloat16).cuda().eval()

    image = Image.open(image_path)

    # ç‰¹æ®Šæç¤ºï¼šè¦æ±‚Markdownæ ¼å¼è¾“å‡º
    conversation = [
        {
            "role": "<|User|>",
            "content": "<image_placeholder>\nè¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„è¡¨æ ¼ï¼Œå¹¶ä»¥Markdownæ ¼å¼è¾“å‡ºã€‚ä¿ç•™è¡¨å¤´å’Œæ‰€æœ‰æ•°æ®è¡Œã€‚",
            "images": [image]
        },
        {
            "role": "<|Assistant|>",
            "content": ""
        }
    ]

    pil_images = load_pil_images(conversation)
    prepare_inputs = vl_chat_processor(
        conversations=conversation,
        images=pil_images,
        force_batchify=True
    )

    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

    with torch.no_grad():
        outputs = vl_gpt.language_model.generate(
            inputs_embeds=inputs_embeds,
            max_new_tokens=1024,  # è¡¨æ ¼å¯èƒ½è¾ƒé•¿
            do_sample=False
        )

    table_md = vl_chat_processor.tokenizer.decode(
        outputs[0].cpu().tolist(),
        skip_special_tokens=True
    )

    return table_md

# ä½¿ç”¨
table = extract_table_with_janus("./table_image.png")
print("è¡¨æ ¼Markdown:")
print(table)
```

---

#### 2.5.5 ä¸LangChainé›†æˆ

```python
from langchain_core.documents import Document
from langchain_core.document_loaders import BaseLoader
from typing import List
from pdf2image import convert_from_path

class JanusOCRLoader(BaseLoader):
    """åŸºäºJanusçš„OCRåŠ è½½å™¨ï¼ˆé€‚ç”¨äºå›¾ç‰‡PDFï¼‰"""

    def __init__(
        self,
        file_path: str,
        model_path: str = "deepseek-ai/Janus-Pro-1B"
    ):
        self.file_path = file_path
        self.model_path = model_path

        # åŠ è½½æ¨¡å‹ï¼ˆåˆå§‹åŒ–æ—¶åŠ è½½ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        self.vl_chat_processor = VLChatProcessor.from_pretrained(model_path)
        self.vl_gpt = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True
        ).to(torch.bfloat16).cuda().eval()

    def load(self) -> List[Document]:
        """åŠ è½½PDFå¹¶è¿›è¡ŒOCRè¯†åˆ«"""
        # è½¬æ¢PDFä¸ºå›¾ç‰‡
        images = convert_from_path(self.file_path)
        documents = []

        for page_num, image in enumerate(images, 1):
            print(f"å¤„ç†ç¬¬{page_num}é¡µ...")

            # ä½¿ç”¨Janusè¿›è¡ŒOCR
            text = self._ocr_image(image)

            doc = Document(
                page_content=text,
                metadata={
                    'source': self.file_path,
                    'page': page_num,
                    'ocr': 'Janus',
                    'model': self.model_path
                }
            )
            documents.append(doc)

        return documents

    def _ocr_image(self, image: Image.Image) -> str:
        """å¯¹å•ä¸ªå›¾ç‰‡è¿›è¡ŒOCR"""
        conversation = [
            {
                "role": "<|User|>",
                "content": "<image_placeholder>\nè¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œä¿æŒåŸå§‹æ ¼å¼å’Œé¡ºåºã€‚",
                "images": [image]
            },
            {
                "role": "<|Assistant|>",
                "content": ""
            }
        ]

        pil_images = load_pil_images(conversation)
        prepare_inputs = self.vl_chat_processor(
            conversations=conversation,
            images=pil_images,
            force_batchify=True
        )

        inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs)

        with torch.no_grad():
            outputs = self.vl_gpt.language_model.generate(
                inputs_embeds=inputs_embeds,
                max_new_tokens=512,
                do_sample=False
            )

        text = self.vl_chat_processor.tokenizer.decode(
            outputs[0].cpu().tolist(),
            skip_special_tokens=True
        )

        return text

# ä½¿ç”¨ç¤ºä¾‹
loader = JanusOCRLoader(
    "./scanned_document.pdf",
    model_path="deepseek-ai/Janus-Pro-1B"
)

documents = loader.load()
print(f"åŠ è½½äº†{len(documents)}é¡µæ–‡æ¡£")

# é›†æˆåˆ°RAG
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = splitter.split_documents(documents)

vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())
print("å‘é‡åº“æ„å»ºå®Œæˆ")
```

---

#### 2.5.6 ä¸PaddleOCRæ€§èƒ½å¯¹æ¯”

```python
import time

# æµ‹è¯•æ–‡æ¡£ï¼šåŒ…å«æ‰‹å†™ä½“å’Œå¤æ‚èƒŒæ™¯çš„æ‰«æPDF
test_pdf = "./complex_scanned.pdf"

# æµ‹è¯•1: PaddleOCR
print("æµ‹è¯•PaddleOCR...")
start = time.time()
from paddleocr import PaddleOCR
ocr = PaddleOCR(use_angle_cls=True, lang='ch')
# ... (PaddleOCRå¤„ç†ä»£ç )
time_paddle = time.time() - start

# æµ‹è¯•2: Janus
print("æµ‹è¯•Janus...")
start = time.time()
loader = JanusOCRLoader(test_pdf, model_path="deepseek-ai/Janus-Pro-1B")
docs_janus = loader.load()
time_janus = time.time() - start

print(f"\næ€§èƒ½å¯¹æ¯”ï¼š")
print(f"PaddleOCR: {time_paddle:.2f}ç§’")
print(f"Janus:     {time_janus:.2f}ç§’")
print(f"\nå‡†ç¡®ç‡å¯¹æ¯”ï¼š")
print(f"PaddleOCR: ~85-90% (æ ‡å‡†åœºæ™¯)")
print(f"Janus:     ~90-95% (å¤æ‚åœºæ™¯ï¼Œå°¤å…¶æ‰‹å†™ä½“)")
```

**é€‚ç”¨åœºæ™¯å¯¹æ¯”**ï¼š

| åœºæ™¯ | PaddleOCR | Janus |
|------|-----------|-------|
| **æ ‡å‡†å°åˆ·ä½“** | âœ… æ¨èï¼ˆå¿«é€Ÿï¼‰ | âš ï¸ è¿‡é‡ |
| **æ‰‹å†™ä½“** | â­â­â­ | â­â­â­â­â­ æ¨è |
| **å¤æ‚èƒŒæ™¯** | â­â­â­ | â­â­â­â­â­ æ¨è |
| **éœ€è¯­ä¹‰ç†è§£** | âŒ | âœ… æ¨è |
| **æ‰¹é‡å¤„ç†** | âœ… æ¨èï¼ˆå¿«ï¼‰ | âš ï¸ æ…¢ |
| **èµ„æºå—é™** | âœ… æ¨è | âŒ éœ€GPU |

---

### å°ç»“

**ç¬¬2ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **OCRå·¥å…·é€‰æ‹©**ï¼š
   - ä¸­æ–‡æ–‡æ¡£ â†’ PaddleOCRï¼ˆå…è´¹+é«˜å‡†ç¡®ç‡ï¼‰
   - å­¦æœ¯è®ºæ–‡ â†’ **MinerU**ï¼ˆå…¬å¼+è¡¨æ ¼ä¸“ç”¨ï¼‰
   - æ‰‹å†™ä½“/å¤æ‚åœºæ™¯ â†’ **Janus**ï¼ˆå¤šæ¨¡æ€ç†è§£ï¼‰
   - å¤šè¯­è¨€ â†’ EasyOCR / Google Vision
   - è¡¨æ ¼+è¡¨å• â†’ AWS Textract

2. **LangChainé›†æˆ**ï¼š
   - åˆ›å»ºè‡ªå®šä¹‰OCRPDFLoader
   - MinerULoaderï¼ˆå­¦æœ¯æ–‡æ¡£ä¸“ç”¨ï¼‰
   - JanusOCRLoaderï¼ˆå¤æ‚åœºæ™¯ï¼‰
   - è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦OCR
   - ä¿æŒDocumentå¯¹è±¡å…¼å®¹æ€§

3. **æ–°å¢å·¥å…·ä¼˜åŠ¿**ï¼š
   - **MinerU**: ä¸“ä¸ºå­¦æœ¯è®ºæ–‡è®¾è®¡ï¼ŒLaTeXå…¬å¼è¯†åˆ«ã€å¤šæ å¸ƒå±€å¤„ç†
   - **Janus**: å¤§æ¨¡å‹é©±åŠ¨OCRï¼Œè¯­ä¹‰ç†è§£èƒ½åŠ›å¼ºï¼Œæ‰‹å†™ä½“è¯†åˆ«ä¼˜ç§€

4. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å›¾ç‰‡é¢„å¤„ç†ï¼ˆå»å™ªã€çŸ«æ­£ï¼‰æå‡å‡†ç¡®ç‡
   - æ‰¹é‡å¤„ç†ï¼ˆå¹¶è¡ŒOCRï¼‰
   - ç¼“å­˜OCRç»“æœé¿å…é‡å¤å¤„ç†
   - GPUåŠ é€Ÿï¼ˆMinerU/Janusï¼‰

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬3ç« å°†ä»‹ç»**Unstructured.io**ç»Ÿä¸€æ–‡æ¡£å¤„ç†æ¡†æ¶ï¼Œä¸€ç«™å¼è§£å†³å¤šæ ¼å¼æ–‡æ¡£å¤„ç†ã€‚

---

## ç¬¬3ç« ï¼šUnstructured.ioç»Ÿä¸€å¤„ç†æ¡†æ¶

### 3.1 Unstructured.ioç®€ä»‹

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… æ”¯æŒ30+æ–‡ä»¶æ ¼å¼ï¼ˆPDF, DOCX, HTML, MD, CSV...ï¼‰
- âœ… è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£ç±»å‹å’Œç»“æ„
- âœ… æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼ˆæ”¯æŒVLMå¢å¼ºï¼‰
- âœ… è¡¨æ ¼ã€å›¾ç‰‡è‡ªåŠ¨æå–
- âœ… ä¸LangChainæ— ç¼é›†æˆ

#### 3.1.1 å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install unstructured

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«OCRã€å›¾ç‰‡å¤„ç†ï¼‰
pip install "unstructured[all-docs]"

# ä»…PDFæ”¯æŒ
pip install "unstructured[pdf]"
```

---

### 3.2 åŸºç¡€ä½¿ç”¨

#### 3.2.1 è‡ªåŠ¨æ£€æµ‹ä¸å¤„ç†

```python
from langchain_community.document_loaders import UnstructuredFileLoader

# è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶å¤„ç†
loader = UnstructuredFileLoader("./document.pdf")  # æˆ– .docx, .htmlç­‰
documents = loader.load()

print(f"æå–åˆ°{len(documents)}ä¸ªæ–‡æ¡£å—")
for doc in documents[:2]:
    print(f"\nç±»å‹ï¼š{doc.metadata.get('category', 'unknown')}")
    print(f"å†…å®¹ï¼š{doc.page_content[:150]}...")
```

#### 3.2.2 åˆ†å—ç­–ç•¥

```python
from langchain_community.document_loaders import UnstructuredFileLoader

# æŒ‰å…ƒç´ åˆ†å—
loader = UnstructuredFileLoader(
    "./document.pdf",
    mode="elements",  # ä¿ç•™å…ƒç´ ç»“æ„
    strategy="fast"    # å¿«é€Ÿæ¨¡å¼ï¼ˆæˆ–"hi_res"é«˜ç²¾åº¦ï¼‰
)
documents = loader.load()

# æŸ¥çœ‹å…ƒç´ ç±»å‹
for doc in documents[:5]:
    category = doc.metadata.get('category', 'unknown')
    print(f"{category}: {doc.page_content[:80]}...")
```

**åˆ†å—æ¨¡å¼å¯¹æ¯”**ï¼š

| modeå‚æ•° | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|
| `"single"` | æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªå— | å°æ–‡æ¡£ |
| `"elements"` | æŒ‰å…ƒç´ åˆ†å—ï¼ˆæ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼ï¼‰ | ç»“æ„åŒ–æ–‡æ¡£ |
| `"paged"` | æŒ‰é¡µåˆ†å— | éœ€è¦ä¿ç•™é¡µç ä¿¡æ¯ |

**strategyå‚æ•°å¯¹æ¯”**ï¼š

| strategyå‚æ•° | é€Ÿåº¦ | å‡†ç¡®ç‡ | é€‚ç”¨åœºæ™¯ |
|-------------|------|--------|---------|
| `"fast"` | â­â­â­â­â­ æœ€å¿« | â­â­â­ ä¸­ç­‰ | åŸç”ŸPDFï¼Œå¿«é€Ÿå¤„ç† |
| `"hi_res"` | â­â­ æ…¢ | â­â­â­â­â­ æœ€å¥½ | æ‰«æPDFï¼Œå¤æ‚å¸ƒå±€ |
| `"ocr_only"` | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¥½ | çº¯å›¾ç‰‡PDF |

---

### 3.3 é«˜çº§ç‰¹æ€§

#### 3.3.1 è¡¨æ ¼æå–

```python
from unstructured.partition.pdf import partition_pdf

# é«˜ç²¾åº¦æ¨¡å¼ï¼ˆåŒ…æ‹¬è¡¨æ ¼ï¼‰
elements = partition_pdf(
    "./financial_report.pdf",
    strategy="hi_res",           # é«˜ç²¾åº¦OCR
    infer_table_structure=True,  # æ¨æ–­è¡¨æ ¼ç»“æ„
    extract_images_in_pdf=True   # æå–å›¾ç‰‡
)

# åˆ†ç±»å…ƒç´ 
tables = []
texts = []

for element in elements:
    if element.category == "Table":
        tables.append({
            'html': element.metadata.text_as_html,
            'text': element.text
        })
    else:
        texts.append(element.text)

print(f"æå–åˆ°{len(tables)}ä¸ªè¡¨æ ¼")
print(f"æå–åˆ°{len(texts)}ä¸ªæ–‡æœ¬å—")
```

---

#### 3.3.2 é›†æˆåˆ°RAGç³»ç»Ÿ

```python
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.agents import create_agent
from langchain_core.tools import tool

# æ­¥éª¤1: ä½¿ç”¨Unstructuredå¤„ç†æ–‡æ¡£
loader = UnstructuredFileLoader(
    "./complex_document.pdf",
    mode="elements",
    strategy="hi_res"
)
documents = loader.load()

# æ­¥éª¤2: äºŒæ¬¡åˆ†å—ï¼ˆå¯é€‰ï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
splits = splitter.split_documents(documents)

# æ­¥éª¤3: æ„å»ºå‘é‡åº“
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()
)

# æ­¥éª¤4: åˆ›å»ºRAGå·¥å…·
@tool
def search_complex_document(query: str) -> str:
    """æœç´¢å¤æ‚æ–‡æ¡£ï¼ˆåŒ…æ‹¬è¡¨æ ¼ã€å›¾è¡¨è¯´æ˜ï¼‰"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    results = retriever.invoke(query)

    formatted = []
    for doc in results:
        category = doc.metadata.get('category', 'text')
        formatted.append(f"[{category}]\n{doc.page_content}")

    return "\n\n".join(formatted)

# æ­¥éª¤5: åˆ›å»ºAgent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search_complex_document],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥æŸ¥è¯¢å¤æ‚æ–‡æ¡£ã€‚

æ–‡æ¡£å·²ç»è¿‡æ™ºèƒ½è§£æï¼ŒåŒ…å«ï¼š
- æ–‡æœ¬æ®µè½
- è¡¨æ ¼æ•°æ®
- æ ‡é¢˜ç»“æ„

è¯·æ ¹æ®æŸ¥è¯¢è¿”å›æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
"""
)

# ä½¿ç”¨
result = agent.invoke({
    "messages": [("user", "æ–‡æ¡£ä¸­çš„ä¸»è¦æ•°æ®æ˜¯ä»€ä¹ˆï¼Ÿ")]
})
print(result["messages"][-1].content)
```

---

### 3.4 æ€§èƒ½ä¼˜åŒ–

#### 3.4.1 æ‰¹é‡å¤„ç†

```python
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List
from langchain_core.documents import Document

def process_single_file(file_path: str) -> List[Document]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        loader = UnstructuredFileLoader(file_path, strategy="fast")
        return loader.load()
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥ {file_path}: {e}")
        return []

def batch_process_directory(directory: str, max_workers: int = 4) -> List[Document]:
    """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£"""
    all_docs = []
    files = list(Path(directory).rglob("*.pdf"))

    print(f"æ‰¾åˆ°{len(files)}ä¸ªPDFæ–‡ä»¶")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_file, str(f)): f
            for f in files
        }

        for future in as_completed(futures):
            file = futures[future]
            try:
                docs = future.result()
                all_docs.extend(docs)
                print(f"âœ… {file.name}: {len(docs)}ä¸ªæ–‡æ¡£å—")
            except Exception as e:
                print(f"âŒ {file.name}: {e}")

    return all_docs

# ä½¿ç”¨
all_documents = batch_process_directory("./documents", max_workers=4)
print(f"\næ€»è®¡å¤„ç†ï¼š{len(all_documents)}ä¸ªæ–‡æ¡£å—")
```

---

### å°ç»“

**ç¬¬3ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Unstructured.ioä¼˜åŠ¿**ï¼š
   - âœ… å¤šæ ¼å¼æ”¯æŒï¼ˆ30+æ ¼å¼ï¼‰
   - âœ… è‡ªåŠ¨ç»“æ„æ£€æµ‹
   - âœ… è¡¨æ ¼æ™ºèƒ½æå–
   - âœ… ä¸LangChainæ— ç¼é›†æˆ
   - âœ… æ”¯æŒVLMå¢å¼ºï¼ˆå›¾åƒæè¿°ã€OCRä¼˜åŒ–ï¼‰

2. **ä½¿ç”¨å»ºè®®**ï¼š
   - ç®€å•æ–‡æ¡£ â†’ `strategy="fast"`
   - æ‰«æPDF â†’ `strategy="hi_res"`
   - æ‰¹é‡å¤„ç† â†’ å¹¶è¡Œå¤„ç†ï¼ˆThreadPoolExecutorï¼‰

3. **æœ€ä½³å®è·µ**ï¼š
   - å…ˆç”¨"fast"æ¨¡å¼æµ‹è¯•
   - è¡¨æ ¼é‡è¦æ—¶ç”¨`infer_table_structure=True`
   - å¤§æ‰¹é‡å¤„ç†æ—¶æ§åˆ¶å¹¶å‘æ•°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬4ç« å°†ä»‹ç»**Text Splitters**ï¼Œæ·±å…¥æ¢è®¨LangChainçš„æ™ºèƒ½åˆ†å—ç­–ç•¥ã€‚

---

## ç¬¬4ç« ï¼šText Splitters - æ™ºèƒ½åˆ†å—ç­–ç•¥

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦Text Splitters

**é—®é¢˜åœºæ™¯**ï¼š
```python
# åŠ è½½çš„æ–‡æ¡£å¯èƒ½å¾ˆé•¿
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("long_book.pdf")
docs = loader.load()

print(f"é¡µæ•°ï¼š{len(docs)}")  # è¾“å‡ºï¼š500
print(f"ç¬¬ä¸€é¡µå­—ç¬¦æ•°ï¼š{len(docs[0].page_content)}")  # è¾“å‡ºï¼š5000+
```

**æŒ‘æˆ˜**ï¼š
1. **å‘é‡åŒ–é™åˆ¶**ï¼šEmbeddingæ¨¡å‹é€šå¸¸æœ‰tokené™åˆ¶ï¼ˆå¦‚8191 tokensï¼‰
2. **æ£€ç´¢è´¨é‡**ï¼šå¤ªå¤§çš„å—ä¼šé™ä½æ£€ç´¢ç²¾åº¦
3. **ä¸Šä¸‹æ–‡çª—å£**ï¼šLLMçš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨Text Splitterså°†é•¿æ–‡æ¡£åˆ†å‰²ä¸ºé€‚åˆçš„chunk

---

### 4.2 RecursiveCharacterTextSplitter

#### 4.2.1 åŸºç¡€ä½¿ç”¨

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# åˆ›å»ºSplitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯ä¸ªchunkçš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=100,      # chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    length_function=len,    # è®¡ç®—é•¿åº¦çš„å‡½æ•°
    is_separator_regex=False
)

# åˆ†å‰²æ–‡æ¡£
splits = splitter.split_documents(documents)

print(f"åŸå§‹æ–‡æ¡£ï¼š{len(documents)}ä¸ª")
print(f"åˆ†å‰²åï¼š{len(splits)}ä¸ªchunk")
print(f"ç¬¬ä¸€ä¸ªchunkï¼š\n{splits[0].page_content[:200]}")
```

---

#### 4.2.2 å·¥ä½œåŸç†

**é€’å½’åˆ†å‰²ç­–ç•¥**ï¼š
```python
# RecursiveCharacterTextSplitterçš„é»˜è®¤åˆ†éš”ç¬¦åˆ—è¡¨
separators = [
    "\n\n",  # æ®µè½åˆ†éš”
    "\n",    # è¡Œåˆ†éš”
    " ",     # ç©ºæ ¼
    ""       # å­—ç¬¦çº§åˆ«
]
```

**åˆ†å‰²æµç¨‹**ï¼š
1. å°è¯•ç”¨`\n\n`åˆ†å‰²
2. å¦‚æœchunkä»ç„¶è¿‡å¤§ï¼Œç”¨`\n`åˆ†å‰²
3. å¦‚æœä»è¿‡å¤§ï¼Œç”¨ç©ºæ ¼åˆ†å‰²
4. æœ€ååœ¨å­—ç¬¦çº§åˆ«åˆ†å‰²

**ä¼˜åŠ¿**ï¼šå°½å¯èƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§

---

#### 4.2.3 å‚æ•°è°ƒä¼˜

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# é…ç½®1: çŸ­chunkï¼ˆé€‚åˆç²¾ç¡®æ£€ç´¢ï¼‰
short_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# é…ç½®2: é•¿chunkï¼ˆé€‚åˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
long_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=200
)

# é…ç½®3: è‡ªå®šä¹‰åˆ†éš”ç¬¦ï¼ˆä»£ç æ–‡æ¡£ï¼‰
code_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)

# å¯¹æ¯”æ•ˆæœ
docs = loader.load()
short_splits = short_splitter.split_documents(docs)
long_splits = long_splitter.split_documents(docs)

print(f"çŸ­chunk: {len(short_splits)}ä¸ª")
print(f"é•¿chunk: {len(long_splits)}ä¸ª")
```

**å‚æ•°é€‰æ‹©å»ºè®®**ï¼š

| åœºæ™¯ | chunk_size | chunk_overlap | è¯´æ˜ |
|------|-----------|---------------|------|
| **ç²¾ç¡®æ£€ç´¢** | 500-800 | 50-100 | å°chunkæé«˜æ£€ç´¢ç²¾åº¦ |
| **ä¿ç•™ä¸Šä¸‹æ–‡** | 1500-2000 | 150-200 | å¤§chunkä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ |
| **é€šç”¨åœºæ™¯** | 1000 | 100 | å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡ |

---

### 4.3 å…¶ä»–Text Splitters

#### 4.3.1 CharacterTextSplitter

```python
from langchain_text_splitters import CharacterTextSplitter

# ç®€å•çš„å­—ç¬¦åˆ†å‰²å™¨
splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=100
)

splits = splitter.split_documents(documents)
```

**å¯¹æ¯”RecursiveCharacterTextSplitter**ï¼š
- âŒ åªä½¿ç”¨å•ä¸€åˆ†éš”ç¬¦ï¼ˆä¸é€’å½’ï¼‰
- âœ… æ›´å¿«ï¼ˆé€‚åˆç®€å•åœºæ™¯ï¼‰

---

#### 4.3.2 TokenTextSplitter

```python
from langchain_text_splitters import TokenTextSplitter

# åŸºäºtokenæ•°é‡åˆ†å‰²ï¼ˆè€Œéå­—ç¬¦æ•°ï¼‰
splitter = TokenTextSplitter(
    chunk_size=500,      # tokenæ•°é‡
    chunk_overlap=50
)

splits = splitter.split_documents(documents)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦ç²¾ç¡®æ§åˆ¶tokenæ•°é‡
- âœ… é¿å…è¶…å‡ºEmbeddingæ¨¡å‹é™åˆ¶

---

#### 4.3.3 MarkdownHeaderTextSplitter

```python
from langchain_text_splitters import MarkdownHeaderTextSplitter

# æŒ‰Markdownæ ‡é¢˜åˆ†å‰²
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

# é€‚ç”¨äºMarkdownæ–‡æ¡£
md_text = """
# ç¬¬ä¸€ç« 
## ç¬¬ä¸€èŠ‚
å†…å®¹1
### å°èŠ‚1
å†…å®¹2
## ç¬¬äºŒèŠ‚
å†…å®¹3
"""

splits = splitter.split_text(md_text)

for split in splits:
    print(f"å…ƒæ•°æ®: {split.metadata}")
    print(f"å†…å®¹: {split.page_content}\n")
```

---

#### 4.3.4 HTMLHeaderTextSplitter

```python
from langchain_text_splitters import HTMLHeaderTextSplitter

# æŒ‰HTMLæ ‡é¢˜åˆ†å‰²
headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
]

splitter = HTMLHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

# é€‚ç”¨äºHTMLæ–‡æ¡£
html_text = """
<h1>ç¬¬ä¸€ç« </h1>
<p>ç« èŠ‚å†…å®¹</p>
<h2>ç¬¬ä¸€èŠ‚</h2>
<p>å°èŠ‚å†…å®¹</p>
"""

splits = splitter.split_text(html_text)
```

---

### 4.4 å®æˆ˜ï¼šå¤šçº§åˆ†å—ç­–ç•¥

#### 4.4.1 ç»„åˆä½¿ç”¨Splitters

```python
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æ­¥éª¤1: ä½¿ç”¨UnstructuredæŒ‰å…ƒç´ åˆ†å—
loader = UnstructuredFileLoader(
    "./document.pdf",
    mode="elements"
)
documents = loader.load()

# æ­¥éª¤2: äºŒæ¬¡åˆ†å—ï¼ˆé’ˆå¯¹è¿‡é•¿çš„å…ƒç´ ï¼‰
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# åªå¯¹é•¿æ–‡æ¡£è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
final_splits = []
for doc in documents:
    if len(doc.page_content) > 1000:
        # éœ€è¦åˆ†å‰²
        splits = splitter.split_documents([doc])
        final_splits.extend(splits)
    else:
        # ä¿æŒåŸæ ·
        final_splits.append(doc)

print(f"åŸå§‹å…ƒç´ ï¼š{len(documents)}ä¸ª")
print(f"æœ€ç»ˆchunkï¼š{len(final_splits)}ä¸ª")
```

---

#### 4.4.2 è¯­ä¹‰åˆ†å—ï¼ˆSemantic Chunkingï¼‰

```python
from langchain_text_splitters import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—
semantic_splitter = SemanticChunker(
    embeddings=OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",  # æˆ–"standard_deviation", "interquartile"
    breakpoint_threshold_amount=90
)

splits = semantic_splitter.split_documents(documents)

print(f"è¯­ä¹‰åˆ†å—ï¼š{len(splits)}ä¸ª")
```

**å·¥ä½œåŸç†**ï¼š
1. å¯¹æ¯ä¸ªå¥å­ç”Ÿæˆembedding
2. è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
3. åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ†å‰²ï¼ˆè¯­ä¹‰è¾¹ç•Œï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… ä¿ç•™è¯­ä¹‰å®Œæ•´æ€§
- âœ… è‡ªåŠ¨è¯†åˆ«ä¸»é¢˜è¾¹ç•Œ

**åŠ£åŠ¿**ï¼š
- âŒ éœ€è¦è°ƒç”¨Embedding APIï¼ˆæˆæœ¬ï¼‰
- âŒ é€Ÿåº¦æ…¢

---

### 4.5 åˆ†å—è´¨é‡è¯„ä¼°

```python
def evaluate_chunking(chunks: list) -> dict:
    """è¯„ä¼°åˆ†å—è´¨é‡"""
    lengths = [len(chunk.page_content) for chunk in chunks]

    stats = {
        'total_chunks': len(chunks),
        'avg_length': sum(lengths) / len(lengths) if lengths else 0,
        'min_length': min(lengths) if lengths else 0,
        'max_length': max(lengths) if lengths else 0,
        'std_dev': None  # å¯ä»¥è®¡ç®—æ ‡å‡†å·®
    }

    # æ£€æŸ¥åˆ†å¸ƒ
    too_small = sum(1 for l in lengths if l < 100)
    too_large = sum(1 for l in lengths if l > 2000)

    stats['too_small'] = too_small
    stats['too_large'] = too_large
    stats['quality'] = 'good' if (too_small + too_large) < len(chunks) * 0.1 else 'poor'

    return stats

# ä½¿ç”¨
stats = evaluate_chunking(splits)
print(f"æ€»chunkæ•°ï¼š{stats['total_chunks']}")
print(f"å¹³å‡é•¿åº¦ï¼š{stats['avg_length']:.0f}")
print(f"è´¨é‡è¯„ä¼°ï¼š{stats['quality']}")
```

---

### å°ç»“

**ç¬¬4ç« æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Text Splittersé‡è¦æ€§**ï¼š
   - é€‚é…å‘é‡åŒ–æ¨¡å‹çš„tokené™åˆ¶
   - æé«˜æ£€ç´¢ç²¾åº¦
   - æ§åˆ¶ä¸Šä¸‹æ–‡çª—å£å¤§å°

2. **Splitteré€‰æ‹©**ï¼š
   - é€šç”¨åœºæ™¯ â†’ RecursiveCharacterTextSplitter
   - ç²¾ç¡®tokenæ§åˆ¶ â†’ TokenTextSplitter
   - Markdownæ–‡æ¡£ â†’ MarkdownHeaderTextSplitter
   - è¯­ä¹‰å®Œæ•´æ€§ â†’ SemanticChunker

3. **å‚æ•°è°ƒä¼˜**ï¼š
   - chunk_size: 500-2000ï¼ˆæ ¹æ®åœºæ™¯ï¼‰
   - chunk_overlap: 10-20%çš„chunk_size
   - è¯„ä¼°åˆ†å—è´¨é‡å¹¶è¿­ä»£ä¼˜åŒ–

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
ç¬¬5ç« å°†æ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»º**ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline**ã€‚

---

## ç¬¬5ç« ï¼šç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline

### 5.1 Pipelineè®¾è®¡

#### 5.1.1 å®Œæ•´æµç¨‹

```
æ–‡æ¡£è¾“å…¥
  â†“
ç±»å‹æ£€æµ‹ï¼ˆPDF, DOCX, HTML...ï¼‰
  â†“
è´¨é‡æ£€æµ‹ï¼ˆåŸç”Ÿ vs æ‰«æï¼‰
  â†“
é€‰æ‹©å¤„ç†ç­–ç•¥
  â”œâ”€â”€ åŸç”ŸPDF â†’ PyPDFLoader
  â”œâ”€â”€ æ‰«æPDF â†’ UnstructuredPDFLoader + OCR
  â”œâ”€â”€ åŒ…å«è¡¨æ ¼ â†’ PDFPlumberLoader
  â””â”€â”€ å¤æ‚æ–‡æ¡£ â†’ Unstructured (hi_res)
  â†“
åå¤„ç†ï¼ˆæ¸…æ´—ã€æ ¼å¼åŒ–ï¼‰
  â†“
æ™ºèƒ½åˆ†å—ï¼ˆRecursiveCharacterTextSplitterï¼‰
  â†“
å‘é‡åŒ– + å­˜å‚¨
  â†“
RAGç³»ç»Ÿ
```

---

#### 5.1.2 å®Œæ•´å®ç°

```python
from typing import List, Dict, Optional
from pathlib import Path
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    UnstructuredFileLoader,
    PyPDFLoader,
    PDFPlumberLoader,
    Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, use_ocr: bool = True, use_tables: bool = True):
        self.use_ocr = use_ocr
        self.use_tables = use_tables
        self.embeddings = OpenAIEmbeddings()

    def process_document(self, file_path: str) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆè‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰"""
        # æ­¥éª¤1: æ£€æµ‹æ–‡ä»¶ç±»å‹
        file_type = self._detect_file_type(file_path)
        logger.info(f"æ–‡ä»¶ç±»å‹ï¼š{file_type}")

        # æ­¥éª¤2: é€‰æ‹©å¤„ç†ç­–ç•¥
        if file_type == 'PDF':
            return self._process_pdf(file_path)
        elif file_type in ['DOCX', 'DOC']:
            return self._process_docx(file_path)
        elif file_type in ['HTML', 'MD']:
            return self._process_web(file_path)
        else:
            # é€šç”¨å¤„ç†
            return self._process_generic(file_path)

    def _detect_file_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        suffix = Path(file_path).suffix.lower()
        type_map = {
            '.pdf': 'PDF',
            '.docx': 'DOCX',
            '.doc': 'DOC',
            '.html': 'HTML',
            '.md': 'MD',
            '.txt': 'TXT'
        }
        return type_map.get(suffix, 'UNKNOWN')

    def _process_pdf(self, pdf_path: str) -> List[Document]:
        """å¤„ç†PDFï¼ˆå¸¦æ™ºèƒ½é™çº§ï¼‰"""
        # å°è¯•1: å¿«é€ŸåŠ è½½
        try:
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()

            total_text = "".join([doc.page_content for doc in docs])
            if len(total_text) > 100:
                logger.info("âœ… åŸç”ŸPDFï¼Œä½¿ç”¨PyPDFLoader")
                return docs
        except Exception as e:
            logger.warning(f"PyPDFLoaderå¤±è´¥: {e}")

        # å°è¯•2: è¡¨æ ¼æ”¯æŒ
        if self.use_tables:
            try:
                loader = PDFPlumberLoader(pdf_path)
                docs = loader.load()
                logger.info("âœ… ä½¿ç”¨PDFPlumberLoaderï¼ˆè¡¨æ ¼æ”¯æŒï¼‰")
                return docs
            except Exception as e:
                logger.warning(f"PDFPlumberLoaderå¤±è´¥: {e}")

        # å°è¯•3: Unstructuredï¼ˆæœ€å¼ºå¤§ï¼‰
        try:
            strategy = "hi_res" if self.use_ocr else "fast"
            loader = UnstructuredFileLoader(
                pdf_path,
                strategy=strategy,
                mode="elements"
            )
            docs = loader.load()
            logger.info(f"âœ… ä½¿ç”¨Unstructuredï¼ˆ{strategy}ï¼‰")
            return docs
        except Exception as e:
            logger.error(f"Unstructuredå¤±è´¥: {e}")
            return []

    def _process_docx(self, file_path: str) -> List[Document]:
        """å¤„ç†DOCX"""
        loader = Docx2txtLoader(file_path)
        return loader.load()

    def _process_web(self, file_path: str) -> List[Document]:
        """å¤„ç†HTML/Markdown"""
        loader = UnstructuredFileLoader(file_path)
        return loader.load()

    def _process_generic(self, file_path: str) -> List[Document]:
        """é€šç”¨å¤„ç†"""
        loader = UnstructuredFileLoader(file_path)
        return loader.load()

    def build_vectorstore(
        self,
        documents: List[Document],
        chunk_size: int = 1000,
        chunk_overlap: int = 100
    ) -> Chroma:
        """æ„å»ºå‘é‡åº“"""
        # åˆ†å—
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        splits = splitter.split_documents(documents)

        # å‘é‡åŒ–
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings
        )

        return vectorstore

# ä½¿ç”¨ç¤ºä¾‹
processor = DocumentProcessor(use_ocr=True, use_tables=True)

# å¤„ç†å•ä¸ªæ–‡æ¡£
docs = processor.process_document("./complex_document.pdf")
print(f"æå–{len(docs)}ä¸ªæ–‡æ¡£å—")

# æ„å»ºå‘é‡åº“
vectorstore = processor.build_vectorstore(docs)
print("å‘é‡åº“æ„å»ºå®Œæˆ")
```

---

### 5.2 æ™ºèƒ½æ–‡æ¡£è·¯ç”±ç­–ç•¥

#### 5.2.1 DocumentRouterå®ç°

åŸºäºæ–‡æ¡£ç±»å‹å’Œç‰¹å¾ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤„ç†å·¥å…·ï¼š

```python
from typing import List, Dict, Optional, Literal
from pathlib import Path
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
import fitz  # PyMuPDF

class DocumentRouter:
    """æ™ºèƒ½æ–‡æ¡£è·¯ç”±å™¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤„ç†å·¥å…·"""

    def __init__(
        self,
        enable_mineru: bool = True,
        enable_janus: bool = False,  # éœ€GPUï¼Œé»˜è®¤å…³é—­
        enable_ocr: bool = True
    ):
        """
        åˆå§‹åŒ–è·¯ç”±å™¨

        Args:
            enable_mineru: æ˜¯å¦å¯ç”¨MinerUï¼ˆå­¦æœ¯æ–‡æ¡£ï¼‰
            enable_janus: æ˜¯å¦å¯ç”¨Janusï¼ˆéœ€GPUï¼‰
            enable_ocr: æ˜¯å¦å¯ç”¨OCRï¼ˆPaddleOCR/Tesseractï¼‰
        """
        self.enable_mineru = enable_mineru
        self.enable_janus = enable_janus
        self.enable_ocr = enable_ocr

    def route(self, file_path: str) -> List[Document]:
        """
        æ™ºèƒ½è·¯ç”±å¹¶å¤„ç†æ–‡æ¡£

        Args:
            file_path: æ–‡æ¡£è·¯å¾„

        Returns:
            Documentå¯¹è±¡åˆ—è¡¨
        """
        # æ­¥éª¤1: åˆ†ææ–‡æ¡£ç‰¹å¾
        analysis = self._analyze_document(file_path)

        print(f"\n{'='*60}")
        print(f"æ–‡æ¡£è·¯ç”±åˆ†æï¼š{Path(file_path).name}")
        print(f"{'='*60}")
        print(f"æ–‡æ¡£ç±»å‹: {analysis['doc_type']}")
        print(f"æ¨èå·¥å…·: {analysis['recommended_tool']}")
        print(f"åŸå› : {analysis['reason']}")
        print(f"{'='*60}\n")

        # æ­¥éª¤2: æ ¹æ®æ¨èå·¥å…·å¤„ç†
        tool = analysis['recommended_tool']

        if tool == 'MinerU':
            return self._process_with_mineru(file_path, analysis)
        elif tool == 'Janus':
            return self._process_with_janus(file_path, analysis)
        elif tool == 'UnstructuredOCR':
            return self._process_with_unstructured_ocr(file_path, analysis)
        elif tool == 'PDFPlumber':
            return self._process_with_pdfplumber(file_path, analysis)
        elif tool == 'PyPDF':
            return self._process_with_pypdf(file_path, analysis)
        elif tool == 'Unstructured':
            return self._process_with_unstructured(file_path, analysis)
        else:
            raise ValueError(f"æœªçŸ¥å·¥å…·: {tool}")

    def _analyze_document(self, file_path: str) -> Dict:
        """åˆ†ææ–‡æ¡£ç‰¹å¾å¹¶æ¨èå·¥å…·"""
        file_ext = Path(file_path).suffix.lower()

        # éPDFæ–‡æ¡£
        if file_ext != '.pdf':
            return {
                'doc_type': f'{file_ext.upper()} Document',
                'recommended_tool': 'Unstructured',
                'reason': 'Unstructuredæ”¯æŒå¤šç§æ ¼å¼ï¼ˆDOCX/HTML/MDç­‰ï¼‰',
                'features': {}
            }

        # PDFæ–‡æ¡£ç‰¹å¾åˆ†æ
        features = self._analyze_pdf_features(file_path)

        # å†³ç­–æ ‘è·¯ç”±
        if features['is_scanned']:
            # æ‰«æPDF
            if features.get('has_handwriting', False) and self.enable_janus:
                return {
                    'doc_type': 'Scanned PDF (Handwriting)',
                    'recommended_tool': 'Janus',
                    'reason': 'Januså¯¹æ‰‹å†™ä½“è¯†åˆ«å‡†ç¡®ç‡æœ€é«˜ï¼ˆ90-95%ï¼‰',
                    'features': features
                }
            elif self.enable_ocr:
                return {
                    'doc_type': 'Scanned PDF',
                    'recommended_tool': 'UnstructuredOCR',
                    'reason': 'Unstructured + OCRé€‚åˆæ‰«ææ–‡æ¡£',
                    'features': features
                }

        # å­¦æœ¯è®ºæ–‡
        if features['is_academic'] and self.enable_mineru:
            return {
                'doc_type': 'Academic PDF',
                'recommended_tool': 'MinerU',
                'reason': 'MinerUä¸“ä¸ºå­¦æœ¯è®ºæ–‡ä¼˜åŒ–ï¼ˆLaTeXå…¬å¼+å¤šæ å¸ƒå±€ï¼‰',
                'features': features
            }

        # åŒ…å«å¤æ‚è¡¨æ ¼
        if features['has_tables'] and features['table_count'] > 3:
            if self.enable_mineru:
                return {
                    'doc_type': 'PDF with Complex Tables',
                    'recommended_tool': 'MinerU',
                    'reason': 'MinerUæ”¯æŒè·¨é¡µè¡¨æ ¼åˆå¹¶å’Œå¤æ‚è¡¨æ ¼è¯†åˆ«',
                    'features': features
                }
            else:
                return {
                    'doc_type': 'PDF with Tables',
                    'recommended_tool': 'PDFPlumber',
                    'reason': 'PDFPlumberè¡¨æ ¼æå–èƒ½åŠ›ä¼˜ç§€',
                    'features': features
                }

        # ç®€å•åŸç”ŸPDF
        if features['text_ratio'] > 0.8:
            return {
                'doc_type': 'Native PDF (Simple)',
                'recommended_tool': 'PyPDF',
                'reason': 'PyPDFLoaderé€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆç®€å•æ–‡æœ¬PDF',
                'features': features
            }

        # é»˜è®¤ï¼šå¤æ‚PDF
        return {
            'doc_type': 'Complex PDF',
            'recommended_tool': 'Unstructured',
            'reason': 'Unstructuredç»Ÿä¸€å¤„ç†å¤æ‚å¸ƒå±€',
            'features': features
        }

    def _analyze_pdf_features(self, pdf_path: str) -> Dict:
        """åˆ†æPDFç‰¹å¾"""
        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)

            features = {
                'total_pages': total_pages,
                'has_text': False,
                'has_images': False,
                'has_tables': False,
                'is_scanned': False,
                'is_academic': False,
                'text_ratio': 0.0,
                'table_count': 0
            }

            text_pages = 0
            image_pages = 0
            total_text_length = 0

            for page in doc:
                # æ–‡æœ¬åˆ†æ
                text = page.get_text()
                if text.strip():
                    features['has_text'] = True
                    text_pages += 1
                    total_text_length += len(text)

                    # æ£€æµ‹å­¦æœ¯ç‰¹å¾
                    if any(keyword in text.lower() for keyword in ['abstract', 'introduction', 'references', 'doi:', 'arxiv']):
                        features['is_academic'] = True

                    # ç®€å•è¡¨æ ¼æ£€æµ‹ï¼ˆåŸºäºå…³é”®å­—ï¼‰
                    if '|' in text or 'table' in text.lower():
                        features['has_tables'] = True
                        features['table_count'] += 1

                # å›¾ç‰‡åˆ†æ
                images = page.get_images()
                if images:
                    features['has_images'] = True
                    image_pages += 1

            # è®¡ç®—æ–‡æœ¬æ¯”ä¾‹
            if total_pages > 0:
                features['text_ratio'] = text_pages / total_pages

            # åˆ¤æ–­æ˜¯å¦æ‰«æPDF
            if features['text_ratio'] < 0.3:
                features['is_scanned'] = True

            doc.close()
            return features

        except Exception as e:
            print(f"PDFåˆ†æå¤±è´¥: {e}")
            return {
                'total_pages': 0,
                'has_text': False,
                'is_scanned': True,  # ä¿å®ˆç­–ç•¥
                'is_academic': False,
                'text_ratio': 0.0
            }

    def _process_with_mineru(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨MinerUå¤„ç†"""
        from langchain_core.document_loaders import BaseLoader
        # ä½¿ç”¨å‰é¢å®šä¹‰çš„MinerULoader
        loader = MinerULoader(file_path, backend="pipeline")
        return loader.load()

    def _process_with_janus(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨Januså¤„ç†"""
        # ä½¿ç”¨å‰é¢å®šä¹‰çš„JanusOCRLoader
        loader = JanusOCRLoader(file_path, model_path="deepseek-ai/Janus-Pro-1B")
        return loader.load()

    def _process_with_unstructured_ocr(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨Unstructured + OCRå¤„ç†"""
        from langchain_community.document_loaders import UnstructuredFileLoader
        loader = UnstructuredFileLoader(
            file_path,
            strategy="hi_res",
            mode="elements"
        )
        return loader.load()

    def _process_with_pdfplumber(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨PDFPlumberå¤„ç†"""
        from langchain_community.document_loaders import PDFPlumberLoader
        loader = PDFPlumberLoader(file_path)
        return loader.load()

    def _process_with_pypdf(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨PyPDFå¤„ç†"""
        from langchain_community.document_loaders import PyPDFLoader
        loader = PyPDFLoader(file_path)
        return loader.load()

    def _process_with_unstructured(self, file_path: str, analysis: Dict) -> List[Document]:
        """ä½¿ç”¨Unstructuredå¤„ç†"""
        from langchain_community.document_loaders import UnstructuredFileLoader
        loader = UnstructuredFileLoader(
            file_path,
            strategy="fast",
            mode="elements"
        )
        return loader.load()

# ä½¿ç”¨ç¤ºä¾‹
router = DocumentRouter(
    enable_mineru=True,
    enable_janus=False,  # GPUèµ„æºå……è¶³æ—¶å¯å¼€å¯
    enable_ocr=True
)

# æµ‹è¯•ä¸åŒç±»å‹æ–‡æ¡£
test_files = [
    "./simple_report.pdf",        # ç®€å•æ–‡æœ¬PDF
    "./academic_paper.pdf",        # å­¦æœ¯è®ºæ–‡
    "./financial_table.pdf",       # åŒ…å«è¡¨æ ¼
    "./scanned_document.pdf",      # æ‰«æPDF
]

for file_path in test_files:
    if Path(file_path).exists():
        docs = router.route(file_path)
        print(f"å¤„ç†å®Œæˆ: {len(docs)}ä¸ªæ–‡æ¡£å—\n")
```

---

#### 5.2.2 è·¯ç”±ç­–ç•¥å¯è§†åŒ–

```python
def visualize_routing_decision():
    """å¯è§†åŒ–è·¯ç”±å†³ç­–æ ‘"""
    print("""
æ–‡æ¡£è·¯ç”±å†³ç­–æ ‘
================

PDFæ–‡æ¡£
â”œâ”€â”€ æ˜¯å¦æ‰«æPDFï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ åŒ…å«æ‰‹å†™ä½“ï¼Ÿ
â”‚   â”‚   â”œâ”€â”€ æ˜¯ â†’ Janus (GPUå¯ç”¨æ—¶)
â”‚   â”‚   â””â”€â”€ å¦ â†’ Unstructured + OCR
â”‚   â””â”€â”€ å¦ â†’ ç»§ç»­åˆ†æ
â”‚
â”œâ”€â”€ æ˜¯å¦å­¦æœ¯è®ºæ–‡ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ MinerU (å…¬å¼+å¤šæ å¸ƒå±€ä¸“ç”¨)
â”‚   â””â”€â”€ å¦ â†’ ç»§ç»­åˆ†æ
â”‚
â”œâ”€â”€ æ˜¯å¦åŒ…å«å¤æ‚è¡¨æ ¼ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ MinerU > PDFPlumber
â”‚   â””â”€â”€ å¦ â†’ ç»§ç»­åˆ†æ
â”‚
â”œâ”€â”€ æ–‡æœ¬æ¯”ä¾‹ > 80%ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ PyPDFLoader (æœ€å¿«)
â”‚   â””â”€â”€ å¦ â†’ Unstructured (ç»Ÿä¸€å¤„ç†)
â”‚
â””â”€â”€ éPDFæ ¼å¼ â†’ Unstructured (æ”¯æŒDOCX/HTML/MDç­‰)

å·¥å…·ä¼˜å…ˆçº§
----------
1. MinerU      â­â­â­â­â­  å­¦æœ¯è®ºæ–‡ã€å¤æ‚è¡¨æ ¼
2. Janus       â­â­â­â­â­  æ‰‹å†™ä½“ã€å¤æ‚åœºæ™¯ï¼ˆéœ€GPUï¼‰
3. Unstructured â­â­â­â­   å¤æ‚æ–‡æ¡£ã€å¤šæ ¼å¼
4. PDFPlumber  â­â­â­â­   è¡¨æ ¼æå–
5. PyPDFLoader â­â­â­    ç®€å•PDFï¼ˆæœ€å¿«ï¼‰
    """)

visualize_routing_decision()
```

---

#### 5.2.3 æ‰¹é‡è·¯ç”±å¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple

class BatchDocumentRouter:
    """æ‰¹é‡æ–‡æ¡£è·¯ç”±å¤„ç†å™¨"""

    def __init__(self, router: DocumentRouter, max_workers: int = 4):
        self.router = router
        self.max_workers = max_workers

    def process_directory(
        self,
        directory: str,
        recursive: bool = True
    ) -> Dict[str, List[Document]]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ–‡æ¡£

        Args:
            directory: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å­ç›®å½•

        Returns:
            æ–‡ä»¶è·¯å¾„ -> Documentåˆ—è¡¨çš„å­—å…¸
        """
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
        pattern = "**/*" if recursive else "*"
        all_files = []

        for ext in ['.pdf', '.docx', '.html', '.md', '.txt']:
            files = Path(directory).glob(f"{pattern}{ext}")
            all_files.extend(files)

        print(f"æ‰¾åˆ°{len(all_files)}ä¸ªæ–‡æ¡£æ–‡ä»¶")

        # å¹¶è¡Œå¤„ç†
        results = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._process_single, str(f)): f
                for f in all_files
            }

            for future in as_completed(futures):
                file_path = futures[future]
                try:
                    docs = future.result()
                    results[str(file_path)] = docs
                    print(f"âœ… {file_path.name}: {len(docs)}ä¸ªæ–‡æ¡£å—")
                except Exception as e:
                    print(f"âŒ {file_path.name}: {e}")
                    results[str(file_path)] = []

        return results

    def _process_single(self, file_path: str) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        return self.router.route(file_path)

    def generate_report(self, results: Dict[str, List[Document]]) -> str:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        total_files = len(results)
        successful = sum(1 for docs in results.values() if docs)
        total_chunks = sum(len(docs) for docs in results.values())

        report = f"""
æ–‡æ¡£å¤„ç†æŠ¥å‘Š
============

æ€»æ–‡ä»¶æ•°: {total_files}
æˆåŠŸå¤„ç†: {successful}
å¤±è´¥æ•°é‡: {total_files - successful}
æ€»æ–‡æ¡£å—: {total_chunks}

æ–‡ä»¶è¯¦æƒ…:
--------
"""
        for file_path, docs in results.items():
            status = "âœ…" if docs else "âŒ"
            report += f"{status} {Path(file_path).name}: {len(docs)}ä¸ªchunk\n"

        return report

# ä½¿ç”¨ç¤ºä¾‹
router = DocumentRouter(enable_mineru=True, enable_ocr=True)
batch_router = BatchDocumentRouter(router, max_workers=4)

# æ‰¹é‡å¤„ç†
results = batch_router.process_directory("./knowledge_base", recursive=True)

# ç”ŸæˆæŠ¥å‘Š
report = batch_router.generate_report(results)
print(report)

# æ„å»ºç»Ÿä¸€å‘é‡åº“
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

all_docs = []
for docs in results.values():
    all_docs.extend(docs)

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = splitter.split_documents(all_docs)

vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings())
print(f"\nå‘é‡åº“æ„å»ºå®Œæˆï¼Œå…±{len(splits)}ä¸ªchunk")
```

---

#### 5.2.4 æ€§èƒ½å¯¹æ¯”å®éªŒ

```python
import time
from typing import Dict

def benchmark_routers(test_files: List[str]) -> Dict:
    """å¯¹æ¯”ä¸åŒè·¯ç”±ç­–ç•¥çš„æ€§èƒ½"""

    results = {
        'smart_routing': {},
        'pypdf_only': {},
        'unstructured_only': {}
    }

    # ç­–ç•¥1: æ™ºèƒ½è·¯ç”±
    print("\n=== æ™ºèƒ½è·¯ç”± ===")
    router = DocumentRouter(enable_mineru=True, enable_ocr=True)
    for file_path in test_files:
        start = time.time()
        docs = router.route(file_path)
        elapsed = time.time() - start
        results['smart_routing'][file_path] = {
            'time': elapsed,
            'chunks': len(docs)
        }
        print(f"{Path(file_path).name}: {elapsed:.2f}s, {len(docs)} chunks")

    # ç­–ç•¥2: ä»…ä½¿ç”¨PyPDF
    print("\n=== PyPDF Only ===")
    from langchain_community.document_loaders import PyPDFLoader
    for file_path in test_files:
        start = time.time()
        try:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            elapsed = time.time() - start
            results['pypdf_only'][file_path] = {
                'time': elapsed,
                'chunks': len(docs)
            }
            print(f"{Path(file_path).name}: {elapsed:.2f}s, {len(docs)} chunks")
        except Exception as e:
            print(f"{Path(file_path).name}: å¤±è´¥ - {e}")

    # ç­–ç•¥3: ä»…ä½¿ç”¨Unstructured
    print("\n=== Unstructured Only ===")
    from langchain_community.document_loaders import UnstructuredFileLoader
    for file_path in test_files:
        start = time.time()
        try:
            loader = UnstructuredFileLoader(file_path, strategy="hi_res")
            docs = loader.load()
            elapsed = time.time() - start
            results['unstructured_only'][file_path] = {
                'time': elapsed,
                'chunks': len(docs)
            }
            print(f"{Path(file_path).name}: {elapsed:.2f}s, {len(docs)} chunks")
        except Exception as e:
            print(f"{Path(file_path).name}: å¤±è´¥ - {e}")

    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
test_files = [
    "./simple.pdf",
    "./academic.pdf",
    "./scanned.pdf",
    "./tables.pdf"
]

benchmark_results = benchmark_routers(test_files)

# åˆ†æç»“æœ
print("\n=== æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===")
for strategy, files in benchmark_results.items():
    total_time = sum(f['time'] for f in files.values())
    print(f"{strategy}: æ€»è€—æ—¶ {total_time:.2f}s")
```

---

### 5.3 æ–‡æ¡£è´¨é‡æ£€æµ‹

#### 5.3.1 è´¨é‡è¯„åˆ†

```python
def assess_document_quality(documents: List[Document]) -> Dict:
    """è¯„ä¼°æ–‡æ¡£æå–è´¨é‡"""
    total_text = "".join([doc.page_content for doc in documents])

    assessment = {
        'total_docs': len(documents),
        'total_chars': len(total_text),
        'avg_doc_length': len(total_text) / len(documents) if documents else 0,
        'has_content': len(total_text) > 100,
        'quality_score': 0.0
    }

    # è®¡ç®—è´¨é‡åˆ†æ•°
    score = 0

    # æœ‰è¶³å¤Ÿå†…å®¹ (+40åˆ†)
    if assessment['total_chars'] > 1000:
        score += 40
    elif assessment['total_chars'] > 100:
        score += 20

    # å¹³å‡æ–‡æ¡£é•¿åº¦åˆç† (+30åˆ†)
    avg_len = assessment['avg_doc_length']
    if 200 < avg_len < 2000:
        score += 30
    elif 100 < avg_len < 5000:
        score += 15

    # æ–‡æ¡£æ•°é‡åˆç† (+30åˆ†)
    if 5 < len(documents) < 100:
        score += 30
    elif len(documents) > 0:
        score += 15

    assessment['quality_score'] = score

    # è¯„çº§
    if score >= 80:
        assessment['rating'] = 'ä¼˜ç§€'
    elif score >= 60:
        assessment['rating'] = 'è‰¯å¥½'
    elif score >= 40:
        assessment['rating'] = 'ä¸€èˆ¬'
    else:
        assessment['rating'] = 'å·®'

    return assessment

# ä½¿ç”¨
docs = processor.process_document("./document.pdf")
quality = assess_document_quality(docs)

print(f"è´¨é‡è¯„åˆ†ï¼š{quality['quality_score']}/100 ({quality['rating']})")
print(f"æ–‡æ¡£å—æ•°ï¼š{quality['total_docs']}")
print(f"æ€»å­—ç¬¦æ•°ï¼š{quality['total_chars']}")
```

---

### 5.3 å®Œæ•´RAGç³»ç»Ÿç¤ºä¾‹

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# æ­¥éª¤1: å¤„ç†æ–‡æ¡£ç›®å½•
processor = DocumentProcessor(use_ocr=True, use_tables=True)

all_docs = []
for pdf_file in Path("./knowledge_base").glob("*.pdf"):
    print(f"\nå¤„ç†ï¼š{pdf_file.name}")
    docs = processor.process_document(str(pdf_file))

    # è´¨é‡æ£€æµ‹
    quality = assess_document_quality(docs)
    print(f"  è´¨é‡ï¼š{quality['rating']} ({quality['quality_score']}/100)")

    if quality['quality_score'] >= 40:
        all_docs.extend(docs)
    else:
        print(f"  âš ï¸ è´¨é‡è¿‡ä½ï¼Œè·³è¿‡")

# æ­¥éª¤2: æ„å»ºå‘é‡åº“
vectorstore = processor.build_vectorstore(all_docs)
print(f"\nå‘é‡åº“æ„å»ºå®Œæˆï¼Œå…±{len(all_docs)}ä¸ªæ–‡æ¡£å—")

# æ­¥éª¤3: åˆ›å»ºRAGå·¥å…·
@tool
def search_knowledge_base(query: str) -> str:
    """æœç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒPDF, DOCX, HTMLç­‰å¤šç§æ ¼å¼ï¼‰"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    results = retriever.invoke(query)

    formatted = []
    for doc in results:
        source = doc.metadata.get('source', 'unknown')
        page = doc.metadata.get('page', '?')
        formatted.append(
            f"æ¥æºï¼š{Path(source).name} (é¡µ{page})\n"
            f"{doc.page_content}"
        )
    return "\n\n---\n\n".join(formatted)

# æ­¥éª¤4: åˆ›å»ºAgent
agent = create_agent(
    model=ChatOpenAI(model="gpt-4"),
    tools=[search_knowledge_base],
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¼ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ã€‚

çŸ¥è¯†åº“å·²åŒ…å«ï¼š
- PDFæ–‡æ¡£ï¼ˆåŸç”Ÿ+æ‰«æï¼‰
- Wordæ–‡æ¡£
- HTMLæ–‡æ¡£
- Markdownæ–‡æ¡£

æŸ¥è¯¢æ—¶ä¼šè‡ªåŠ¨åŒ¹é…æœ€ç›¸å…³çš„å†…å®¹å¹¶æä¾›æ¥æºã€‚
"""
)

# æµ‹è¯•
result = agent.invoke({
    "messages": [("user", "äº§å“çš„æŠ€æœ¯è§„æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ")]
})
print(result["messages"][-1].content)
```

---

### å…¨ç¯‡æ€»ç»“

**æœ¬ç¯‡ï¼ˆæ–‡æ¡£å¤„ç† LangChainç¯‡ï¼‰æ¶µç›–æŠ€æœ¯**ï¼š

| ç« èŠ‚ | æ ¸å¿ƒæŠ€æœ¯ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| ç¬¬1ç«  | Document Loadersï¼ˆPyPDF, PDFPlumber, Unstructuredï¼‰ | PDFæ–‡æ¡£å¤„ç† |
| ç¬¬2ç«  | OCRé›†æˆï¼ˆTesseract, PaddleOCRï¼‰ | æ‰«ææ–‡æ¡£ã€å›¾ç‰‡è¯†åˆ« |
| ç¬¬3ç«  | Unstructured.ioç»Ÿä¸€æ¡†æ¶ | å¤šæ ¼å¼ã€å¤æ‚å¸ƒå±€ |
| ç¬¬4ç«  | Text Splittersï¼ˆRecursive, Semanticï¼‰ | æ™ºèƒ½åˆ†å— |
| ç¬¬5ç«  | ç”Ÿäº§çº§Pipeline | ä¼ä¸šçŸ¥è¯†åº“ |

---

## æ€è€ƒä¸ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šPDFå¤„ç†å¯¹æ¯”å®éªŒ

é€‰æ‹©3ç§ä¸åŒç±»å‹çš„PDFï¼ˆåŸç”Ÿã€æ‰«æã€æ··åˆï¼‰ï¼Œå¯¹æ¯”å·¥å…·æ€§èƒ½ï¼š
1. PyPDFLoader
2. PDFPlumberLoader
3. UnstructuredFileLoader (fast vs hi_res)

æµ‹è¯•æŒ‡æ ‡ï¼šæå–å‡†ç¡®ç‡ã€å¤„ç†æ—¶é—´

### ç»ƒä¹ 2ï¼šæ„å»ºä¼ä¸šæ–‡æ¡£åº“

å®ç°å®Œæ•´çš„æ–‡æ¡£å¤„ç†Pipelineï¼š
1. æ‰¹é‡å¤„ç†å¤šç§æ ¼å¼ï¼ˆPDF, DOCX, HTMLï¼‰
2. è´¨é‡æ£€æµ‹ä¸è¿‡æ»¤
3. æ™ºèƒ½åˆ†å—ç­–ç•¥
4. æ„å»ºRAGç³»ç»Ÿ

### ç»ƒä¹ 3ï¼šText Splitterså¯¹æ¯”

å¯¹æ¯”ä¸åŒSplitterçš„æ•ˆæœï¼š
1. RecursiveCharacterTextSplitter
2. TokenTextSplitter
3. SemanticChunker

---

## å‚è€ƒèµ„æº

**å®˜æ–¹æ–‡æ¡£**ï¼š
- [LangChain Document Loaders](https://docs.langchain.com/oss/python/integrations/document_loaders/)
- [LangChain Text Splitters](https://docs.langchain.com/oss/python/langchain/overview)
- [Unstructured.io](https://docs.unstructured.io/)
- [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)
- [pdfplumber](https://github.com/jsvine/pdfplumber)

**äº‘æœåŠ¡**ï¼š
- [Google Cloud Vision](https://cloud.google.com/vision)
- [AWS Textract](https://aws.amazon.com/textract/)

---

**ç¬¬åä¸€ç¯‡ï¼ˆLangChainç¯‡ï¼‰å®Œæˆ**ï¼

ä½ å·²ç»æŒæ¡äº†LangChainç”Ÿæ€ä¸‹çš„æ–‡æ¡£å¤„ç†å®Œæ•´æŠ€æœ¯æ ˆï¼š
- âœ… Document Loadersï¼ˆPDFã€DOCXã€HTMLç­‰ï¼‰
- âœ… OCRæŠ€æœ¯é›†æˆï¼ˆTesseractã€PaddleOCRï¼‰
- âœ… Unstructured.ioç»Ÿä¸€æ¡†æ¶
- âœ… Text Splittersæ™ºèƒ½åˆ†å—
- âœ… ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†Pipeline

**ä¸‹ä¸€æ­¥å­¦ä¹ **ï¼š
åç»­ç« èŠ‚å°†èšç„¦**Deep Agentsã€Middleware å·¥ç¨‹åŒ–ã€å¤šAgentåä½œ**ç­‰é«˜çº§ä¸»é¢˜ã€‚
