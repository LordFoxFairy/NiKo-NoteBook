# 第8章 性能优化与部署

## 8.1 GPU推理优化

### 8.1.1 TensorRT加速

```python
import tensorrt as trt
from torch2trt import torch2trt

class TensorRTOptimizer:
    def convert_model(self, model, dummy_input):
        model_trt = torch2trt(
            model,
            [dummy_input],
            fp16_mode=True,
            max_batch_size=32,
            workspace_size=1 << 30  # 1GB
        )
        
        # 保存
        torch.save(model_trt.state_dict(), 'model_trt.pth')
        
        return model_trt

# 实测加速比
# LivePortrait: 12ms → 5ms (2.4x)
# Wav2Lip: 18ms → 8ms (2.25x)
```

### 8.1.2 模型量化

```python
import torch.quantization as quantization

class ModelQuantizer:
    def quantize_dynamic(self, model):
        quantized_model = quantization.quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.Conv2d},
            dtype=torch.qint8
        )
        
        return quantized_model

# INT8量化效果
# 模型大小: 420MB → 105MB (4x压缩)
# 速度: 12ms → 7ms (1.7x加速)
# 精度损失: <2%
```

## 8.2 批处理优化

```python
class BatchProcessor:
    def __init__(self, batch_size=16):
        self.batch_size = batch_size
        self.queue = asyncio.Queue()
    
    async def process_batch(self):
        batch = []
        
        while len(batch) < self.batch_size:
            try:
                item = await asyncio.wait_for(self.queue.get(), timeout=0.1)
                batch.append(item)
            except asyncio.TimeoutError:
                break
        
        if batch:
            # 批量推理
            results = await self.model.batch_inference(batch)
            return results
```

## 8.3 Kubernetes部署

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: digital-human
spec:
  replicas: 3
  selector:
    matchLabels:
      app: digital-human
  template:
    metadata:
      labels:
        app: digital-human
    spec:
      containers:
      - name: digital-human
        image: digital-human:latest
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
        ports:
        - containerPort: 8000
```

## 8.4 监控与日志

```python
from prometheus_client import Counter, Histogram
import logging

# Prometheus指标
request_count = Counter('requests_total', 'Total requests')
latency_histogram = Histogram('latency_seconds', 'Request latency')

class Monitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @latency_histogram.time()
    async def track_request(self, func):
        request_count.inc()
        
        try:
            result = await func()
            self.logger.info(f"Request succeeded")
            return result
        except Exception as e:
            self.logger.error(f"Request failed: {e}")
            raise
```

## 8.5 性能基准

| 配置 | 并发数 | 延迟(P95) | 吞吐量(QPS) | 成本/小时 |
|------|--------|-----------|-------------|-----------|
| RTX 3060 | 5 | 800ms | 6 | $0.5 |
| RTX 4090 | 15 | 350ms | 42 | $1.5 |
| A100 | 50 | 250ms | 200 | $3.5 |

## 8.6 本章小结
- TensorRT可实现2-3x推理加速
- INT8量化压缩75%模型大小
- K8s实现弹性伸缩
- Prometheus监控系统健康
