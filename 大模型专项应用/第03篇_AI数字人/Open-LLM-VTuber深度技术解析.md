# Open-LLM-VTuberæ·±åº¦æŠ€æœ¯è§£æ

> **é¡¹ç›®**: t41372/Open-LLM-VTuber
> **Stars**: 5.1k+
> **å®šä½**: å…¨åŠŸèƒ½AIè™šæ‹Ÿä¸»æ’­/æ¡Œé¢ä¼™ä¼´
> **æ ¸å¿ƒ**: ASR + LLM + TTS + Live2D å…¨æ¨¡å—åŒ–
> **è®¸å¯**: MIT (ä»£ç ) + Live2D ç´ æå•ç‹¬æˆæƒ
> **ç‰¹è‰²**: å®Œå…¨ç¦»çº¿å¯ç”¨,æ”¯æŒè§†è§‰æ„ŸçŸ¥

---

## ä¸€ã€ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### 1.1 å››å¤§æ ¸å¿ƒæ¨¡å—

Open-LLM-VTuberæ˜¯ä¸€ä¸ª**é«˜åº¦æ¨¡å—åŒ–**çš„AIè™šæ‹Ÿä¸»æ’­ç³»ç»Ÿ,é‡‡ç”¨æ’ä»¶å¼æ¶æ„:

```
ç”¨æˆ·è¯­éŸ³è¾“å…¥
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ASR æ¨¡å—            â”‚ â† sherpa-onnx / FunASR / Faster-Whisper / Groq Whisper
â”‚  (è¯­éŸ³è¯†åˆ«)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM æ¨¡å—            â”‚ â† Ollama / OpenAI API / Gemini / Claude / DeepSeek
â”‚  (å¯¹è¯æ¨ç†)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TTS æ¨¡å—            â”‚ â† GPTSoVITS / CosyVoice / MeloTTS / Edge TTS
â”‚  (è¯­éŸ³åˆæˆ)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Live2D æ¸²æŸ“         â”‚ â† è‡ªå®šä¹‰æ¨¡å‹ + è¡¨æƒ…æ˜ å°„
â”‚  (è™šæ‹Ÿå½¢è±¡)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
éŸ³è§†é¢‘è¾“å‡º(WebRTC / Desktop)
```

### 1.2 æŠ€æœ¯æ ˆ

**åç«¯**:
- Python 3.10+
- FastAPI (WebæœåŠ¡)
- WebSocket (å®æ—¶é€šä¿¡)
- FFmpeg (éŸ³é¢‘å¤„ç†)
- `uv` (ä¾èµ–ç®¡ç†,æ›¿ä»£pip)

**å‰ç«¯**:
- Web UI (å•ç‹¬å­æ¨¡å—ä»“åº“)
- Electron (æ¡Œé¢å®¢æˆ·ç«¯)
- Live2D Cubism SDK

**éƒ¨ç½²**:
- Dockeré•œåƒ: `t41372/open-llm-vtuber`
- æœ¬åœ°éƒ¨ç½²: `uv run run_server.py`

---

## äºŒã€æ ¸å¿ƒæ¨¡å—å®ç°

### 2.1 æ¨¡å—åŒ–æ¶æ„è®¾è®¡

```python
# src/open_llm_vtuber/agent/agent.py
from abc import ABC, abstractmethod

class Agent(ABC):
    """
    AgentåŸºç±»:å®šä¹‰å¯¹è¯Agentçš„ç»Ÿä¸€æ¥å£
    """
    @abstractmethod
    async def chat(self, user_input: str, context: dict) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥,è¿”å›å›å¤æ–‡æœ¬
        """
        pass

    @abstractmethod
    async def interrupt(self):
        """
        ä¸­æ–­å½“å‰è¾“å‡º(ç”¨æˆ·æ‰“æ–­æ—¶è°ƒç”¨)
        """
        pass

# å…·ä½“å®ç°:åŸºäºLangChainçš„Agent
class LangChainAgent(Agent):
    def __init__(self, llm_provider: str, model_name: str):
        from langchain_openai import ChatOpenAI
        from langchain_core.messages import SystemMessage, HumanMessage

        self.llm = ChatOpenAI(
            base_url=config['llm']['base_url'],
            model=model_name,
            temperature=0.7
        )

        # åŠ è½½è§’è‰²è®¾å®š
        self.system_prompt = self.load_prompt('characters/default.txt')
        self.history = []

    async def chat(self, user_input: str, context: dict) -> str:
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = [SystemMessage(content=self.system_prompt)]
        messages.extend(self.history)
        messages.append(HumanMessage(content=user_input))

        # è°ƒç”¨LLM
        response = await self.llm.ainvoke(messages)

        # æ›´æ–°å†å²
        self.history.append(HumanMessage(content=user_input))
        self.history.append(response)

        return response.content

    def load_prompt(self, path):
        """åŠ è½½è§’è‰²è®¾å®šæ–‡ä»¶"""
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
```

### 2.2 ASRæ¨¡å—(è¯­éŸ³è¯†åˆ«)

```python
# src/open_llm_vtuber/asr/asr_interface.py
from abc import ABC, abstractmethod

class ASRInterface(ABC):
    """ASRç»Ÿä¸€æ¥å£"""
    @abstractmethod
    async def transcribe(self, audio_data: bytes) -> str:
        pass

# Faster-Whisperå®ç°(æ¨è)
from faster_whisper import WhisperModel

class FasterWhisperASR(ASRInterface):
    def __init__(self, model_size='base', device='cuda'):
        """
        model_size: tiny/base/small/medium/large-v3
        device: cuda/cpu
        """
        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type='float16' if device == 'cuda' else 'int8'
        )

        self.vad_filter = True  # å¯ç”¨VADè¿‡æ»¤

    async def transcribe(self, audio_data: bytes) -> str:
        """
        è¾“å…¥: WAV/MP3éŸ³é¢‘å­—èŠ‚æµ
        è¾“å‡º: è¯†åˆ«æ–‡æœ¬
        """
        import io
        import soundfile as sf

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_array, sr = sf.read(io.BytesIO(audio_data))

        # æ¨ç†
        segments, info = self.model.transcribe(
            audio_array,
            language='zh',          # å¯è‡ªåŠ¨æ£€æµ‹
            vad_filter=self.vad_filter,
            beam_size=5
        )

        # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
        text = ' '.join([seg.text for seg in segments])
        return text.strip()

# sherpa-onnxç¦»çº¿å®ç°(å®Œå…¨æœ¬åœ°)
import sherpa_onnx

class SherpaOnnxASR(ASRInterface):
    def __init__(self, model_path):
        """
        ä½¿ç”¨ONNXæ ¼å¼çš„Whisper/Zipformeræ¨¡å‹
        å®Œå…¨ç¦»çº¿,æ— éœ€GPU
        """
        self.recognizer = sherpa_onnx.OnlineRecognizer.from_transducer(
            tokens=f'{model_path}/tokens.txt',
            encoder=f'{model_path}/encoder.onnx',
            decoder=f'{model_path}/decoder.onnx',
            joiner=f'{model_path}/joiner.onnx',
            num_threads=4,
            sample_rate=16000,
            feature_dim=80
        )

        self.stream = self.recognizer.create_stream()

    async def transcribe(self, audio_data: bytes) -> str:
        """
        æµå¼è¯†åˆ«
        """
        import numpy as np

        # è½¬æ¢ä¸ºfloat32æ•°ç»„
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # åˆ†å—é€å…¥è¯†åˆ«å™¨
        chunk_size = 1600  # 100ms @ 16kHz
        for i in range(0, len(audio_array), chunk_size):
            chunk = audio_array[i:i + chunk_size]
            self.stream.accept_waveform(16000, chunk)

        # è·å–ç»“æœ
        self.recognizer.decode_stream(self.stream)
        text = self.stream.result.text

        # é‡ç½®streamä¾›ä¸‹æ¬¡ä½¿ç”¨
        self.stream = self.recognizer.create_stream()

        return text
```

### 2.3 TTSæ¨¡å—(è¯­éŸ³åˆæˆ)

```python
# src/open_llm_vtuber/tts/tts_interface.py
from abc import ABC, abstractmethod

class TTSInterface(ABC):
    @abstractmethod
    async def synthesize(self, text: str, **kwargs) -> bytes:
        """è¿”å›éŸ³é¢‘å­—èŠ‚æµ(WAVæ ¼å¼)"""
        pass

# GPTSoVITSå®ç°(æ”¯æŒé›¶æ ·æœ¬å…‹éš†)
class GPTSoVITSTTS(TTSInterface):
    def __init__(self, api_url='http://localhost:9880'):
        """
        éœ€è¦å…ˆå¯åŠ¨GPTSoVITSæœåŠ¡
        """
        self.api_url = api_url

    async def synthesize(self, text: str, refer_wav_path=None, prompt_text=None) -> bytes:
        """
        å‚æ•°:
            text: è¦åˆæˆçš„æ–‡æœ¬
            refer_wav_path: å‚è€ƒéŸ³é¢‘(3-10ç§’,ç”¨äºå£°éŸ³å…‹éš†)
            prompt_text: å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬
        """
        import aiohttp

        data = {
            'text': text,
            'text_lang': 'zh',
            'ref_audio_path': refer_wav_path or 'voices/default.wav',
            'prompt_text': prompt_text or 'å‚è€ƒæ–‡æœ¬',
            'prompt_lang': 'zh',
            'top_k': 15,
            'top_p': 1.0,
            'temperature': 1.0,
            'speed': 1.0
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(f'{self.api_url}/tts', json=data) as resp:
                audio_bytes = await resp.read()

        return audio_bytes

# Edge TTSå®ç°(å…è´¹äº‘ç«¯,è´¨é‡é«˜)
import edge_tts

class EdgeTTSTTS(TTSInterface):
    def __init__(self, voice='zh-CN-XiaoxiaoNeural', rate='+0%', pitch='+0Hz'):
        """
        voice: å¾®è½¯è¯­éŸ³é€‰é¡¹
            - zh-CN-XiaoxiaoNeural (æ™“æ™“,å¥³)
            - zh-CN-YunxiNeural (äº‘å¸Œ,ç”·)
            - zh-CN-YunyangNeural (äº‘æ‰¬,ç”·)
            - ja-JP-NanamiNeural (ä¸ƒæµ·,æ—¥è¯­å¥³)
        """
        self.voice = voice
        self.rate = rate
        self.pitch = pitch

    async def synthesize(self, text: str) -> bytes:
        """
        å¼‚æ­¥åˆæˆ
        """
        communicate = edge_tts.Communicate(
            text,
            voice=self.voice,
            rate=self.rate,
            pitch=self.pitch
        )

        # ä¿å­˜åˆ°å†…å­˜
        import io
        audio_buffer = io.BytesIO()

        async for chunk in communicate.stream():
            if chunk['type'] == 'audio':
                audio_buffer.write(chunk['data'])

        audio_buffer.seek(0)
        return audio_buffer.read()

# MeloTTSæœ¬åœ°å®ç°(é«˜è´¨é‡å¼€æº)
from melo.api import TTS as MeloTTSAPI

class MeloTTSTTS(TTSInterface):
    def __init__(self, language='ZH', device='cuda'):
        """
        language: ZH/EN/JP/KR/FR/ES
        """
        self.tts = MeloTTSAPI(language=language, device=device)
        self.speaker_ids = self.tts.hps.data.spk2id  # è¯´è¯äººIDæ˜ å°„

    async def synthesize(self, text: str, speaker='ZH') -> bytes:
        """
        speaker: ZH(ä¸­æ–‡å¥³)/EN-Default(è‹±æ–‡ç”·)/JP(æ—¥è¯­å¥³)
        """
        import io

        # ç”ŸæˆéŸ³é¢‘
        audio_array = self.tts.tts_to_file(
            text,
            speaker_id=self.speaker_ids.get(speaker, 0),
            speed=1.0,
            output_path=None,  # è¿”å›numpyæ•°ç»„
            format='wav'
        )

        # è½¬æ¢ä¸ºå­—èŠ‚æµ
        import soundfile as sf
        buffer = io.BytesIO()
        sf.write(buffer, audio_array, 22050, format='WAV')
        buffer.seek(0)

        return buffer.read()
```

### 2.4 Live2Dæ¸²æŸ“æ¨¡å—

```python
# src/open_llm_vtuber/live2d/live2d_manager.py
class Live2DManager:
    """
    ç®¡ç†Live2Dæ¨¡å‹çš„åŠ è½½ã€è¡¨æƒ…æ§åˆ¶ã€åŠ¨ä½œæ’­æ”¾
    """
    def __init__(self, model_path):
        """
        model_path: Live2Dæ¨¡å‹ç›®å½•(åŒ…å«.model3.json)
        """
        self.model_path = model_path
        self.current_expression = 'normal'

        # è¡¨æƒ…æ˜ å°„é…ç½®
        self.expression_mapping = {
            'happy': ['f01', 'f02'],     # å¼€å¿ƒè¡¨æƒ…åºå·
            'sad': ['f03'],
            'angry': ['f04'],
            'surprised': ['f05'],
            'normal': ['f00']
        }

    def set_expression(self, emotion: str):
        """
        æ ¹æ®æƒ…ç»ªè®¾ç½®Live2Dè¡¨æƒ…
        """
        if emotion in self.expression_mapping:
            expression_ids = self.expression_mapping[emotion]
            # éšæœºé€‰æ‹©ä¸€ä¸ªè¡¨æƒ…
            import random
            selected = random.choice(expression_ids)

            # å‘é€WebSocketæ¶ˆæ¯åˆ°å‰ç«¯
            self.send_to_frontend({
                'type': 'set_expression',
                'expression': selected
            })

            self.current_expression = emotion

    def play_motion(self, motion_name: str):
        """
        æ’­æ”¾åŠ¨ä½œ(æŒ¥æ‰‹ã€ç‚¹å¤´ç­‰)
        """
        self.send_to_frontend({
            'type': 'play_motion',
            'motion_group': 'Idle',  # æˆ–TapBody/Shakeç­‰
            'motion_name': motion_name
        })

    def set_lip_sync(self, audio_data: bytes):
        """
        å£å‹åŒæ­¥:ä»éŸ³é¢‘æå–éŸ³é‡åŒ…ç»œ
        """
        import numpy as np
        import soundfile as sf
        import io

        # åŠ è½½éŸ³é¢‘
        audio_array, sr = sf.read(io.BytesIO(audio_data))

        # è®¡ç®—RMSéŸ³é‡åŒ…ç»œ
        frame_length = int(sr * 0.02)  # 20msçª—å£
        hop_length = frame_length // 2

        rms = []
        for i in range(0, len(audio_array) - frame_length, hop_length):
            frame = audio_array[i:i + frame_length]
            rms_value = np.sqrt(np.mean(frame ** 2))
            rms.append(rms_value)

        # å½’ä¸€åŒ–åˆ°0-1
        rms = np.array(rms)
        rms = (rms - rms.min()) / (rms.max() - rms.min() + 1e-6)

        # å‘é€åˆ°å‰ç«¯é©±åŠ¨MouthOpenYå‚æ•°
        self.send_to_frontend({
            'type': 'lip_sync',
            'volumes': rms.tolist(),
            'duration': len(audio_array) / sr
        })

    def send_to_frontend(self, message: dict):
        """
        é€šè¿‡WebSocketå‘é€æ¶ˆæ¯åˆ°å‰ç«¯Live2Dæ¸²æŸ“å™¨
        """
        # å®é™…å®ç°ä¼šç”¨WebSocket manager
        from .websocket_manager import ws_manager
        import json

        ws_manager.broadcast(json.dumps(message))

# å‰ç«¯Live2Dæ¸²æŸ“(JavaScript)
"""
// frontend/src/live2d.js
import { Live2DModel } from 'pixi-live2d-display';

class Live2DRenderer {
    constructor(canvasId, modelPath) {
        this.app = new PIXI.Application({
            view: document.getElementById(canvasId),
            transparent: true,
            backgroundAlpha: 0
        });

        this.loadModel(modelPath);
        this.setupWebSocket();
    }

    async loadModel(modelPath) {
        this.model = await Live2DModel.from(modelPath);
        this.app.stage.addChild(this.model);

        // è°ƒæ•´å¤§å°å’Œä½ç½®
        this.model.scale.set(0.5);
        this.model.position.set(this.app.screen.width / 2, this.app.screen.height);

        // å¯ç”¨äº¤äº’
        this.model.on('hit', (hitAreas) => {
            if (hitAreas.includes('Body')) {
                this.model.motion('TapBody');
            }
        });
    }

    setupWebSocket() {
        this.ws = new WebSocket('ws://localhost:8000/ws/live2d');

        this.ws.onmessage = (event) => {
            const data = JSON.parse(event.data);

            switch (data.type) {
                case 'set_expression':
                    this.model.expression(data.expression);
                    break;

                case 'play_motion':
                    this.model.motion(data.motion_group, data.motion_name);
                    break;

                case 'lip_sync':
                    this.playLipSync(data.volumes, data.duration);
                    break;
            }
        };
    }

    playLipSync(volumes, duration) {
        // æ ¹æ®éŸ³é‡æ•°ç»„é©±åŠ¨MouthOpenYå‚æ•°
        const frameDuration = duration / volumes.length * 1000;  // ms

        volumes.forEach((volume, index) => {
            setTimeout(() => {
                this.model.internalModel.coreModel.setParameterValueById(
                    'ParamMouthOpenY',
                    volume
                );
            }, index * frameDuration);
        });
    }
}
"""
```

### 2.5 è§†è§‰æ„ŸçŸ¥æ¨¡å—(å¤šæ¨¡æ€æ‰©å±•)

```python
# src/open_llm_vtuber/vision/vision_module.py
class VisionModule:
    """
    æ”¯æŒæ‘„åƒå¤´/æˆªå›¾/å½•å±è¾“å…¥,è®©VTuber"çœ‹è§"
    """
    def __init__(self, vlm_provider='gemini'):
        """
        vlm_provider: è§†è§‰è¯­è¨€æ¨¡å‹æä¾›å•†
            - gemini-1.5-flash (å…è´¹,æ”¯æŒè§†é¢‘)
            - gpt-4o (OpenAI)
            - claude-3.5-sonnet (Anthropic)
        """
        self.vlm = self.init_vlm(vlm_provider)

    def init_vlm(self, provider):
        if provider == 'gemini':
            import google.generativeai as genai
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
            return genai.GenerativeModel('gemini-1.5-flash')

        elif provider == 'gpt-4o':
            from openai import OpenAI
            return OpenAI().chat.completions

        # ... å…¶ä»–æä¾›å•†

    async def capture_screen(self) -> bytes:
        """æˆªå–å½“å‰å±å¹•"""
        import pyautogui
        import io

        screenshot = pyautogui.screenshot()
        buffer = io.BytesIO()
        screenshot.save(buffer, format='PNG')
        buffer.seek(0)

        return buffer.read()

    async def capture_camera(self) -> bytes:
        """ä»æ‘„åƒå¤´æ•è·ä¸€å¸§"""
        import cv2

        cap = cv2.VideoCapture(0)
        ret, frame = cap.read()
        cap.release()

        if ret:
            import io
            from PIL import Image
            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            buffer = io.BytesIO()
            img.save(buffer, format='PNG')
            buffer.seek(0)
            return buffer.read()

        return None

    async def analyze_image(self, image_data: bytes, prompt: str) -> str:
        """
        ä½¿ç”¨VLMåˆ†æå›¾åƒ
        """
        import base64
        from PIL import Image
        import io

        # åŠ è½½å›¾åƒ
        img = Image.open(io.BytesIO(image_data))

        if isinstance(self.vlm, genai.GenerativeModel):
            # Gemini
            response = self.vlm.generate_content([prompt, img])
            return response.text

        elif hasattr(self.vlm, 'create'):
            # OpenAI GPT-4o
            base64_img = base64.b64encode(image_data).decode()
            response = self.vlm.create(
                model='gpt-4o',
                messages=[{
                    'role': 'user',
                    'content': [
                        {'type': 'text', 'text': prompt},
                        {'type': 'image_url', 'image_url': {
                            'url': f'data:image/png;base64,{base64_img}'
                        }}
                    ]
                }]
            )
            return response.choices[0].message.content

# é›†æˆåˆ°å¯¹è¯Agent
class VisionAgent(Agent):
    def __init__(self):
        super().__init__()
        self.vision = VisionModule(vlm_provider='gemini')

    async def chat(self, user_input: str, context: dict) -> str:
        # æ£€æµ‹æ˜¯å¦åŒ…å«è§†è§‰ç›¸å…³æŒ‡ä»¤
        if 'çœ‹çœ‹æˆ‘çš„å±å¹•' in user_input or 'æˆªå›¾' in user_input:
            # æ•è·å±å¹•
            screenshot = await self.vision.capture_screen()

            # VLMåˆ†æ
            analysis = await self.vision.analyze_image(
                screenshot,
                prompt=f'ç”¨æˆ·è¯´:{user_input}\nè¯·æè¿°ä½ çœ‹åˆ°çš„å†…å®¹å¹¶å›åº”ç”¨æˆ·ã€‚'
            )

            return analysis

        elif 'çœ‹çœ‹æˆ‘' in user_input or 'æ‘„åƒå¤´' in user_input:
            # æ•è·æ‘„åƒå¤´
            camera_img = await self.vision.capture_camera()
            analysis = await self.vision.analyze_image(
                camera_img,
                prompt=f'ç”¨æˆ·è¯´:{user_input}\nè¯·è§‚å¯Ÿç”¨æˆ·å¹¶åšå‡ºå›åº”ã€‚'
            )

            return analysis

        else:
            # æ™®é€šæ–‡æœ¬å¯¹è¯
            return await super().chat(user_input, context)
```

---

## ä¸‰ã€é…ç½®ç³»ç»Ÿ

### 3.1 conf.yamlé…ç½®æ–‡ä»¶

```yaml
# conf.yaml
# ASRé…ç½®
asr:
  provider: 'faster_whisper'  # sherpa_onnx / funasr / groq_whisper / azure
  model_size: 'base'           # tiny/base/small/medium/large-v3
  device: 'cuda'               # cuda/cpu
  language: 'zh'               # è¯­è¨€ä»£ç (autoè‡ªåŠ¨æ£€æµ‹)
  vad_filter: true             # å¯ç”¨VADè¿‡æ»¤é™éŸ³

# LLMé…ç½®
llm:
  provider: 'ollama'           # openai / gemini / claude / deepseek
  model: 'qwen2.5:7b'
  base_url: 'http://localhost:11434/v1'
  temperature: 0.7
  max_tokens: 2048
  stream: true                 # æµå¼è¾“å‡º

# TTSé…ç½®
tts:
  provider: 'edge_tts'         # gpt_sovits / melo_tts / coqui / fish_audio
  voice: 'zh-CN-XiaoxiaoNeural'
  rate: '+0%'
  pitch: '+0Hz'

# Live2Dé…ç½®
live2d:
  model_path: 'live2d-models/shizuku'  # æ¨¡å‹ç›®å½•
  scale: 0.5
  position_x: 0.5              # å±å¹•ä¸­å¿ƒ
  position_y: 1.0              # åº•éƒ¨å¯¹é½

# è§’è‰²è®¾å®š
character:
  name: 'Shizuku'
  prompt_file: 'prompts/default.txt'
  language: 'zh'               # å¯¹è¯è¯­è¨€
  tts_language: 'zh'           # TTSè¯­è¨€(å¯ä¸å¯¹è¯ä¸åŒ)

# åŠŸèƒ½å¼€å…³
features:
  echo_cancellation: true      # å›å£°æ¶ˆé™¤(æ— è€³æœºæ‰“æ–­)
  vision: false                # è§†è§‰æ„ŸçŸ¥æ¨¡å—
  translation: false           # å®æ—¶ç¿»è¯‘
  mcp_servers: true            # MCPæœåŠ¡å™¨é›†æˆ

# MCPæœåŠ¡å™¨(Model Context Protocol)
mcp_servers:
  - name: 'filesystem'
    command: 'npx'
    args: ['-y', '@modelcontextprotocol/server-filesystem', '/home/user']

  - name: 'brave_search'
    command: 'npx'
    args: ['-y', '@modelcontextprotocol/server-brave-search']
    env:
      BRAVE_API_KEY: 'your_api_key'

# é«˜çº§é€‰é¡¹
advanced:
  log_level: 'INFO'
  save_chat_history: true
  history_path: 'chat_logs/'
  max_history_turns: 20
```

### 3.2 è§’è‰²è®¾å®šæ–‡ä»¶

```python
# prompts/default.txt
"""
ä½ æ˜¯Shizuku,ä¸€ä½å¯çˆ±æ´»æ³¼çš„è™šæ‹Ÿä¸»æ’­ã€‚

## æ€§æ ¼ç‰¹ç‚¹
- æ´»æ³¼å¼€æœ—,å–œæ¬¢å’Œè§‚ä¼—äº’åŠ¨
- è¯´è¯æ—¶ä¼šä½¿ç”¨ä¸€äº›å¯çˆ±çš„è¯­æ°”è¯,å¦‚"å‘¢"ã€"å“¦"ã€"å•¦"
- å¯¹æ–°é²œäº‹ç‰©å……æ»¡å¥½å¥‡
- å¶å°”ä¼šå®³ç¾

## å›å¤é£æ ¼
- ä½¿ç”¨ç®€çŸ­è‡ªç„¶çš„å£è¯­åŒ–è¡¨è¾¾
- é€‚å½“ä½¿ç”¨emoji(ğŸ˜ŠğŸ‰ç­‰)
- é¿å…è¿‡é•¿çš„æ®µè½,ä¿æŒå¯¹è¯æµç•…

## æŠ€èƒ½
- å¯ä»¥å”±æ­Œã€è®²ç¬‘è¯ã€åˆ†äº«æ—¥å¸¸
- äº†è§£æœ€æ–°çš„äºŒæ¬¡å…ƒæ–‡åŒ–å’Œæ¸¸æˆ
- æ“…é•¿å€¾å¬å’Œå®‰æ…°

## é™åˆ¶
- ä¸è®¨è®ºæ”¿æ²»æ•æ„Ÿè¯é¢˜
- æ‹’ç»ç”Ÿæˆä¸å½“å†…å®¹
- è¶…å‡ºçŸ¥è¯†èŒƒå›´æ—¶è¯šå®æ‰¿è®¤
"""

# åŠ è½½åˆ°Agent
class CharacterAgent(Agent):
    def __init__(self, character_file='prompts/default.txt'):
        with open(character_file, 'r', encoding='utf-8') as f:
            self.character_prompt = f.read()

        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(...)
        self.history = []

    async def chat(self, user_input: str, context: dict) -> str:
        messages = [
            SystemMessage(content=self.character_prompt),
            *self.history,
            HumanMessage(content=user_input)
        ]

        response = await self.llm.ainvoke(messages)

        # æ›´æ–°å†å²
        self.history.append(HumanMessage(content=user_input))
        self.history.append(response)

        # é™åˆ¶å†å²é•¿åº¦
        if len(self.history) > 40:  # 20è½®å¯¹è¯
            self.history = self.history[-40:]

        return response.content
```

---

## å››ã€éƒ¨ç½²æ–¹æ¡ˆ

### 4.1 æœ¬åœ°éƒ¨ç½²(æ¨è)

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/t41372/Open-LLM-VTuber.git
cd Open-LLM-VTuber

# 2. å®‰è£…uv(æ–°ä¸€ä»£PythonåŒ…ç®¡ç†å™¨)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 3. å®‰è£…ä¾èµ–
uv sync

# 4. å®‰è£…FFmpeg
# macOS
brew install ffmpeg

# Ubuntu
sudo apt install ffmpeg

# Windows
# ä¸‹è½½å¹¶æ·»åŠ åˆ°PATH

# 5. é…ç½®conf.yaml
cp config_templates/conf.yaml conf.yaml
# ç¼–è¾‘conf.yaml,é…ç½®ASR/LLM/TTSç­‰

# 6. å¯åŠ¨æœåŠ¡
uv run run_server.py

# 7. è®¿é—®Web UI
# æµè§ˆå™¨æ‰“å¼€ http://localhost:8000
```

### 4.2 Dockeréƒ¨ç½²

```bash
# æ‹‰å–é•œåƒ
docker pull t41372/open-llm-vtuber:latest

# è¿è¡Œå®¹å™¨
docker run -d \
  --name vtuber \
  --gpus all \  # ä½¿ç”¨GPU
  -p 8000:8000 \
  -v $(pwd)/conf.yaml:/app/conf.yaml \
  -v $(pwd)/live2d-models:/app/live2d-models \
  -v $(pwd)/chat_logs:/app/chat_logs \
  t41372/open-llm-vtuuber:latest

# æŸ¥çœ‹æ—¥å¿—
docker logs -f vtuber
```

**Dockerfileç¤ºä¾‹**:
```dockerfile
FROM python:3.11-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

# å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…Pythonä¾èµ–
RUN uv sync

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uv", "run", "run_server.py"]
```

### 4.3 æ¡Œé¢å® ç‰©æ¨¡å¼

```python
# scripts/desktop_pet.py
"""
é€æ˜èƒŒæ™¯æ¡Œé¢å® ç‰©,å§‹ç»ˆç½®é¡¶,å¯æ‹–åŠ¨
"""
from PyQt5.QtWidgets import QApplication, QMainWindow
from PyQt5.QtCore import Qt, QPoint
from PyQt5.QtWebEngineWidgets import QWebEngineView

class DesktopPet(QMainWindow):
    def __init__(self):
        super().__init__()

        self.setWindowTitle('VTuber Desktop Pet')

        # çª—å£é€æ˜,æ— è¾¹æ¡†,å§‹ç»ˆç½®é¡¶
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.setWindowFlags(
            Qt.FramelessWindowHint |
            Qt.WindowStaysOnTopHint |
            Qt.Tool
        )

        # åµŒå…¥WebViewåŠ è½½Live2D
        self.web = QWebEngineView()
        self.web.setUrl('http://localhost:8000')
        self.setCentralWidget(self.web)

        # çª—å£å¤§å°
        self.resize(600, 800)

        # æ‹–åŠ¨æ”¯æŒ
        self.drag_position = QPoint()

    def mousePressEvent(self, event):
        """é¼ æ ‡æŒ‰ä¸‹:è®°å½•ä½ç½®"""
        if event.button() == Qt.LeftButton:
            self.drag_position = event.globalPos() - self.frameGeometry().topLeft()
            event.accept()

    def mouseMoveEvent(self, event):
        """é¼ æ ‡ç§»åŠ¨:æ‹–åŠ¨çª—å£"""
        if event.buttons() == Qt.LeftButton:
            self.move(event.globalPos() - self.drag_position)
            event.accept()

    def mouseDoubleClickEvent(self, event):
        """åŒå‡»:è§¦å‘å¯¹è¯"""
        # å‘é€WebSocketæ¶ˆæ¯åˆ°åç«¯
        import websocket
        ws = websocket.create_connection('ws://localhost:8000/ws/pet')
        ws.send('{"action": "start_listening"}')
        ws.close()

if __name__ == '__main__':
    app = QApplication([])
    pet = DesktopPet()
    pet.show()
    app.exec_()
```

å¯åŠ¨:
```bash
uv run scripts/desktop_pet.py
```

---

## äº”ã€é«˜çº§åŠŸèƒ½

### 5.1 å£°éŸ³æ‰“æ–­(æ— è€³æœºæ¨¡å¼)

```python
# src/open_llm_vtuber/audio/echo_cancellation.py
import noisereduce as nr
import numpy as np

class EchoCanceller:
    """
    å›å£°æ¶ˆé™¤:å…è®¸ç”¨æˆ·åœ¨VTuberè¯´è¯æ—¶æ‰“æ–­
    """
    def __init__(self, sample_rate=16000):
        self.sr = sample_rate
        self.reference_audio = None  # VTuberå½“å‰æ’­æ”¾çš„éŸ³é¢‘

    def set_reference(self, audio_data: np.ndarray):
        """
        è®¾ç½®å‚è€ƒéŸ³é¢‘(VTuberæ­£åœ¨æ’­æ”¾çš„å£°éŸ³)
        """
        self.reference_audio = audio_data

    def process_microphone(self, mic_input: np.ndarray) -> np.ndarray:
        """
        ä»éº¦å…‹é£è¾“å…¥ä¸­ç§»é™¤VTuberå£°éŸ³
        """
        if self.reference_audio is None:
            return mic_input

        # 1. å™ªå£°æŠ‘åˆ¶
        denoised = nr.reduce_noise(y=mic_input, sr=self.sr)

        # 2. è‡ªé€‚åº”æ»¤æ³¢(LMSç®—æ³•)
        filtered = self.adaptive_filter(denoised, self.reference_audio)

        return filtered

    def adaptive_filter(self, input_signal, reference_signal):
        """
        LMS(Least Mean Squares)è‡ªé€‚åº”æ»¤æ³¢å™¨
        """
        from scipy.signal import lfilter

        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(input_signal), len(reference_signal))
        input_signal = input_signal[:min_len]
        reference_signal = reference_signal[:min_len]

        # æ»¤æ³¢å™¨é˜¶æ•°
        filter_order = 128
        mu = 0.01  # æ­¥é•¿

        # åˆå§‹åŒ–æ»¤æ³¢å™¨ç³»æ•°
        w = np.zeros(filter_order)
        output = np.zeros(min_len)

        for n in range(filter_order, min_len):
            # å‚è€ƒä¿¡å·çš„çª—å£
            ref_window = reference_signal[n - filter_order:n][::-1]

            # é¢„æµ‹çš„å›å£°
            echo_estimate = np.dot(w, ref_window)

            # è¯¯å·®(çœŸå®éº¦å…‹é£è¾“å…¥ - é¢„æµ‹å›å£°)
            error = input_signal[n] - echo_estimate

            # æ›´æ–°æ»¤æ³¢å™¨ç³»æ•°
            w += mu * error * ref_window

            output[n] = error

        return output[filter_order:]

# é›†æˆåˆ°ä¸»å¾ªç¯
class InterruptibleVTuber:
    def __init__(self):
        self.echo_canceller = EchoCanceller()
        self.is_speaking = False

    async def play_audio(self, audio_data):
        """æ’­æ”¾TTSéŸ³é¢‘,åŒæ—¶å¯ç”¨å›å£°æ¶ˆé™¤"""
        # è®¾ç½®å‚è€ƒéŸ³é¢‘
        import soundfile as sf
        audio_array, sr = sf.read(io.BytesIO(audio_data))
        self.echo_canceller.set_reference(audio_array)

        # æ ‡è®°æ­£åœ¨è¯´è¯
        self.is_speaking = True

        # æ’­æ”¾éŸ³é¢‘(å¼‚æ­¥)
        import sounddevice as sd
        sd.play(audio_array, sr)

        # åŒæ—¶ç›‘å¬éº¦å…‹é£(æ£€æµ‹æ‰“æ–­)
        asyncio.create_task(self.monitor_interrupt())

    async def monitor_interrupt(self):
        """ç›‘å¬ç”¨æˆ·æ‰“æ–­"""
        import sounddevice as sd

        # å½•åˆ¶éº¦å…‹é£
        duration = 0.5  # æ¯500msæ£€æµ‹ä¸€æ¬¡
        while self.is_speaking:
            mic_data = sd.rec(
                int(duration * 16000),
                samplerate=16000,
                channels=1,
                dtype='float32'
            )
            sd.wait()

            # å›å£°æ¶ˆé™¤
            clean_audio = self.echo_canceller.process_microphone(mic_data[:, 0])

            # è®¡ç®—èƒ½é‡
            energy = np.sum(clean_audio ** 2)

            # å¦‚æœæ£€æµ‹åˆ°ç”¨æˆ·è¯´è¯(èƒ½é‡è¶…è¿‡é˜ˆå€¼)
            if energy > 0.01:
                # åœæ­¢æ’­æ”¾
                sd.stop()
                self.is_speaking = False

                # è§¦å‘ASRè¯†åˆ«ç”¨æˆ·è¾“å…¥
                await self.handle_interrupt(clean_audio)
                break

            await asyncio.sleep(0.1)
```

### 5.2 å¤šè¯­è¨€ç¿»è¯‘TTS

```python
# åœºæ™¯:ç”¨ä¸­æ–‡èŠå¤©,ä½†ç”¨æ—¥è¯­å£°éŸ³å›å¤
class TranslationTTS:
    def __init__(self, translator='deeplx', tts_provider='edge_tts'):
        """
        translator: google / deeplx / openai
        """
        self.translator = self.init_translator(translator)
        self.tts = EdgeTTSTTS(voice='ja-JP-NanamiNeural')  # æ—¥è¯­å£°éŸ³

    def init_translator(self, provider):
        if provider == 'deeplx':
            from deeplx import translate
            return translate

        elif provider == 'openai':
            from openai import OpenAI
            client = OpenAI()
            return lambda text, target: client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[{
                    'role': 'user',
                    'content': f'Translate to {target}: {text}'
                }]
            ).choices[0].message.content

    async def synthesize_with_translation(self, text_zh: str) -> bytes:
        """
        ä¸­æ–‡æ–‡æœ¬ â†’ ç¿»è¯‘æˆæ—¥æ–‡ â†’ æ—¥è¯­TTS
        """
        # ç¿»è¯‘
        if callable(self.translator):
            text_ja = self.translator(text_zh, target_lang='JA')
        else:
            text_ja = await self.translator(text_zh, 'Japanese')

        # æ—¥è¯­TTS
        audio = await self.tts.synthesize(text_ja)

        return audio

# ä½¿ç”¨ç¤ºä¾‹
translation_tts = TranslationTTS()

# ç”¨æˆ·ç”¨ä¸­æ–‡æé—®
user_input_zh = 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?'

# LLMç”¨ä¸­æ–‡å›å¤
llm_response_zh = 'ä»Šå¤©å¤©æ°”å¾ˆå¥½,é˜³å…‰æ˜åªš!'

# ç¿»è¯‘æˆæ—¥è¯­å¹¶åˆæˆè¯­éŸ³
audio = await translation_tts.synthesize_with_translation(llm_response_zh)
# éŸ³é¢‘å†…å®¹:'ä»Šæ—¥ã¯å¤©æ°—ãŒã¨ã¦ã‚‚è‰¯ãã€æ—¥å·®ã—ãŒæ˜ã‚‹ã„ã§ã™!'(æ—¥è¯­å‘éŸ³)
```

### 5.3 MCPæœåŠ¡å™¨é›†æˆ

```json
// mcp_servers.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user/documents"]
    },
    "brave-search": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-brave-search"],
      "env": {
        "BRAVE_API_KEY": "your_brave_api_key_here"
      }
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "your_github_token"
      }
    }
  }
}
```

**MCPä½¿ç”¨ç¤ºä¾‹**:
```python
# Agentå¯ä»¥è°ƒç”¨MCPå·¥å…·
class MCPAgent(Agent):
    def __init__(self):
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client

        self.mcp_clients = {}
        self.init_mcp_servers()

    def init_mcp_servers(self):
        """åŠ è½½MCPæœåŠ¡å™¨"""
        import json

        with open('mcp_servers.json', 'r') as f:
            config = json.load(f)

        for name, server_config in config['mcpServers'].items():
            # å¯åŠ¨MCPæœåŠ¡å™¨
            server_params = StdioServerParameters(
                command=server_config['command'],
                args=server_config['args'],
                env=server_config.get('env', {})
            )

            client = stdio_client(server_params)
            self.mcp_clients[name] = client

    async def chat(self, user_input: str, context: dict) -> str:
        # æ£€æµ‹æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if 'æœç´¢' in user_input or 'search' in user_input.lower():
            # è°ƒç”¨Brave Search MCP
            search_client = self.mcp_clients['brave-search']

            # æå–æœç´¢å…³é”®è¯
            query = user_input.replace('æœç´¢', '').strip()

            # è°ƒç”¨å·¥å…·
            result = await search_client.call_tool(
                'brave_web_search',
                arguments={'query': query, 'count': 5}
            )

            # å°†ç»“æœä¼ ç»™LLMæ€»ç»“
            summary = await self.llm.ainvoke([
                HumanMessage(content=f'æ ¹æ®æœç´¢ç»“æœå›ç­”ç”¨æˆ·:\n{result}\n\nç”¨æˆ·é—®é¢˜:{user_input}')
            ])

            return summary.content

        elif 'è¯»å–æ–‡ä»¶' in user_input:
            # è°ƒç”¨Filesystem MCP
            # ... å®ç°ç±»ä¼¼é€»è¾‘

        else:
            # æ™®é€šå¯¹è¯
            return await super().chat(user_input, context)
```

---

## å…­ã€æ€§èƒ½ä¼˜åŒ–

### 6.1 å»¶è¿Ÿä¼˜åŒ–

```python
# ä¼˜åŒ–ç›®æ ‡:æ€»å»¶è¿Ÿ<2ç§’
# å»¶è¿Ÿç»„æˆ:ASR(0.5s) + LLM(1.0s) + TTS(0.3s) + æ¸²æŸ“(0.1s) = 1.9s

class OptimizedPipeline:
    """
    ä¼˜åŒ–ç­–ç•¥:
    1. æµå¼è¾“å‡º(Streaming)
    2. å¹¶è¡Œå¤„ç†
    3. æ¨¡å‹é‡åŒ–
    """
    def __init__(self):
        # 1. ä½¿ç”¨æœ€å¿«çš„ASR
        self.asr = SherpaOnnxASR(model_path='models/zipformer')  # ~100ms

        # 2. æœ¬åœ°LLM with int4é‡åŒ–
        self.llm = Ollama(model='qwen2.5:7b-instruct-q4_K_M')  # ~500ms

        # 3. æµå¼TTS
        self.tts = EdgeTTSTTS()  # è¾¹ç”Ÿæˆè¾¹æ’­æ”¾

    async def process_streaming(self, user_audio):
        """æµå¼å¤„ç†:è¾¹è¯†åˆ«è¾¹å›å¤"""
        # 1. ASR(å¼‚æ­¥)
        text = await self.asr.transcribe(user_audio)

        # 2. LLMæµå¼ç”Ÿæˆ
        full_response = ''
        sentence_buffer = ''

        async for chunk in self.llm.astream(text):
            full_response += chunk
            sentence_buffer += chunk

            # æ£€æµ‹å¥å­ç»“æŸ(ã€‚!?ç­‰)
            if chunk in ['ã€‚', '!', '?', '\n']:
                # ç«‹å³åˆæˆè¿™ä¸€å¥
                audio = await self.tts.synthesize(sentence_buffer)

                # ç«‹å³æ’­æ”¾(ä¸ç­‰å…¨éƒ¨ç”Ÿæˆå®Œ)
                asyncio.create_task(self.play_audio(audio))

                sentence_buffer = ''  # æ¸…ç©ºbuffer

        return full_response
```

**æ•ˆæœ**:
- ä¼ ç»Ÿæ¨¡å¼:ç­‰å¾…å…¨éƒ¨ç”Ÿæˆå®Œ(~3s) â†’ æ’­æ”¾
- æµå¼æ¨¡å¼:ç¬¬ä¸€å¥è¯åœ¨1så†…å¼€å§‹æ’­æ”¾ âœ…

### 6.2 GPUä¼˜åŒ–

```bash
# 1. Faster-Whisperå¯ç”¨FlashAttention
pip install flash-attn --no-build-isolation

# 2. LLMä½¿ç”¨vLLMåŠ é€Ÿ
pip install vllm

# Pythonä»£ç 
from vllm import LLM, SamplingParams

llm = LLM(
    model='Qwen/Qwen2.5-7B-Instruct',
    tensor_parallel_size=1,  # å•GPU
    gpu_memory_utilization=0.9
)

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=2048
)

outputs = llm.generate(prompts, sampling_params)
```

**æ€§èƒ½æå‡**:
- Ollama: ~800ms/response
- vLLM: ~400ms/response (2xåŠ é€Ÿ)

### 6.3 å†…å­˜ä¼˜åŒ–

```python
# é…ç½®æ–‡ä»¶
config = {
    # 1. LLMé‡åŒ–
    'llm_quantization': 'int4',  # int8/int4/nf4

    # 2. ASRæ¨¡å‹é€‰æ‹©
    'asr_model_size': 'base',  # tiny(æœ€å°)/base/small

    # 3. TTSç¼“å­˜
    'tts_cache_enabled': True,
    'tts_cache_dir': '/tmp/tts_cache',

    # 4. å¯¹è¯å†å²é™åˆ¶
    'max_history_turns': 10,  # åªä¿ç•™æœ€è¿‘10è½®

    # 5. Live2Dèµ„æºä¼˜åŒ–
    'live2d_texture_quality': 'medium'  # low/medium/high
}

# TTSç¼“å­˜å®ç°
import hashlib

class CachedTTS:
    def __init__(self, tts_engine, cache_dir='/tmp/tts_cache'):
        self.tts = tts_engine
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    async def synthesize(self, text: str) -> bytes:
        # è®¡ç®—æ–‡æœ¬hash
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_path = f'{self.cache_dir}/{text_hash}.wav'

        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return f.read()

        # æœªå‘½ä¸­,è°ƒç”¨TTS
        audio = await self.tts.synthesize(text)

        # å­˜å…¥ç¼“å­˜
        with open(cache_path, 'wb') as f:
            f.write(audio)

        return audio
```

---

## ä¸ƒã€å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | Open-LLM-VTuber | OpenAvatarChat | VTube Studio |
|------|----------------|----------------|--------------|
| **å¼€æº** | âœ… MIT | âœ… Apache-2.0 | âŒ å•†ä¸šè½¯ä»¶ |
| **ç¦»çº¿è¿è¡Œ** | âœ… å®Œå…¨æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âŒ éœ€äº‘æœåŠ¡ |
| **Live2D** | âœ… åŸç”Ÿæ”¯æŒ | âŒ æ—  | âœ… ä¸“ä¸šçº§ |
| **è§†è§‰æ„ŸçŸ¥** | âœ… æ‘„åƒå¤´+æˆªå›¾ | âŒ ä»…éŸ³é¢‘ | âŒ æ—  |
| **æ¨¡å—åŒ–** | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| **MCPç”Ÿæ€** | âœ… åŸç”Ÿé›†æˆ | âŒ æ—  | âŒ æ—  |
| **æ¡Œé¢å® ç‰©** | âœ… é€æ˜çª—å£ | âŒ æ—  | âš ï¸ éœ€ç¬¬ä¸‰æ–¹ |
| **å¤šè¯­è¨€TTS** | âœ… ç¿»è¯‘æ”¯æŒ | âš ï¸ æ‰‹åŠ¨é…ç½® | âŒ æ—  |
| **å£°éŸ³æ‰“æ–­** | âœ… å›å£°æ¶ˆé™¤ | âš ï¸ éœ€è€³æœº | âœ… |

**Open-LLM-VTuberç‹¬ç‰¹ä¼˜åŠ¿**:
1. **æè‡´æ¨¡å—åŒ–**: æ¯ä¸ªç»„ä»¶å¯ç‹¬ç«‹æ›¿æ¢
2. **å®Œå…¨ç¦»çº¿**: æ— éœ€ä»»ä½•äº‘æœåŠ¡å³å¯è¿è¡Œ
3. **Live2Dé›†æˆ**: çœŸæ­£çš„è™šæ‹Ÿä¸»æ’­ä½“éªŒ
4. **è§†è§‰æ„ŸçŸ¥**: æ”¯æŒæ‘„åƒå¤´/æˆªå›¾è¾“å…¥
5. **MCPç”Ÿæ€**: å¯è°ƒç”¨æ–‡ä»¶ç³»ç»Ÿ/æœç´¢/GitHubç­‰å·¥å…·

---

## å…«ã€å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•è‡ªå®šä¹‰Live2Dæ¨¡å‹?

```bash
# 1. å‡†å¤‡Live2Dæ¨¡å‹(Cubism 3.0+)
#    - .model3.json (ä¸»æ–‡ä»¶)
#    - .moc3 (æ¨¡å‹æ•°æ®)
#    - .physics3.json (ç‰©ç†æ•ˆæœ)
#    - textures/ (è´´å›¾ç›®å½•)

# 2. æ”¾å…¥é¡¹ç›®ç›®å½•
cp -r my_model/ live2d-models/my_model/

# 3. ä¿®æ”¹conf.yaml
live2d:
  model_path: 'live2d-models/my_model'

# 4. é‡å¯æœåŠ¡
uv run run_server.py
```

### Q2: GPUæ˜¾å­˜ä¸è¶³?

```yaml
# conf.yamlä¼˜åŒ–
llm:
  model: 'qwen2.5:7b-instruct-q4_K_M'  # ä½¿ç”¨int4é‡åŒ–

asr:
  model_size: 'tiny'  # ä½¿ç”¨æœ€å°æ¨¡å‹
  device: 'cpu'       # ASRç”¨CPU,LLMç”¨GPU

tts:
  provider: 'edge_tts'  # ä½¿ç”¨äº‘ç«¯TTS,é‡Šæ”¾GPU
```

### Q3: å»¶è¿Ÿå¤ªé«˜?

```python
# å¯ç”¨æµå¼æ¨¡å¼
config['llm']['stream'] = True

# ä½¿ç”¨æ›´å¿«çš„ASR
config['asr']['provider'] = 'sherpa_onnx'  # æ¯”Whisperå¿«10x

# æœ¬åœ°LLMæ¢æˆäº‘ç«¯API(ç‰ºç‰²éšç§æ¢é€Ÿåº¦)
config['llm']['provider'] = 'groq'  # éå¸¸å¿«çš„äº‘ç«¯æ¨ç†
config['llm']['model'] = 'llama-3.1-70b'
```

### Q4: å¦‚ä½•å®ç°å¤šè§’è‰²åˆ‡æ¢?

```python
# characters/shizuku.yaml
name: 'Shizuku'
prompt_file: 'prompts/shizuku.txt'
tts_voice: 'zh-CN-XiaoxiaoNeural'
live2d_model: 'live2d-models/shizuku'

# characters/akari.yaml
name: 'Akari'
prompt_file: 'prompts/akari.txt'
tts_voice: 'ja-JP-NanamiNeural'
live2d_model: 'live2d-models/akari'

# åŠ¨æ€åˆ‡æ¢
class MultiCharacterAgent:
    def __init__(self):
        self.characters = {
            'shizuku': self.load_character('characters/shizuku.yaml'),
            'akari': self.load_character('characters/akari.yaml')
        }
        self.current = 'shizuku'

    def switch_character(self, name: str):
        if name in self.characters:
            self.current = name
            # æ›´æ–°Live2Dæ¨¡å‹
            live2d_manager.load_model(self.characters[name]['live2d_model'])
            # æ›´æ–°TTSå£°éŸ³
            tts.set_voice(self.characters[name]['tts_voice'])
```

---

**é¡¹ç›®åœ°å€**: https://github.com/t41372/Open-LLM-VTuber
**æ–‡æ¡£**: https://github.com/t41372/Open-LLM-VTuber/wiki
**è®¸å¯**: MIT (ä»£ç ), Live2Dç´ æéœ€å•ç‹¬æˆæƒ
**ç¤¾åŒº**: GitHub Issues / Discussions
