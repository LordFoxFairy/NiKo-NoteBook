# ç¬¬10ç«  ä»é›¶æ‰‹å†™æ•°å­—äººç³»ç»Ÿ

> **ç›®æ ‡**: æ·±å…¥ç†è§£æ•°å­—äººæ ¸å¿ƒç®—æ³•ï¼Œä»åº•å±‚å®ç°å®Œæ•´ç³»ç»Ÿ
> **éš¾åº¦**: â­â­â­â­â­
> **ä»£ç é‡**: ~2000è¡Œ
> **å­¦ä¹ æ—¶é—´**: 3-5å¤©

---

## 10.1 ç³»ç»Ÿæ¶æ„è®¾è®¡

### 10.1.1 æ•´ä½“æ¶æ„

æˆ‘ä»¬å°†ä»é›¶å®ç°ä¸€ä¸ª**ç®€åŒ–ä½†å®Œæ•´**çš„æ•°å­—äººç³»ç»Ÿï¼Œæ ¸å¿ƒæ¨¡å—ï¼š

```
ç”¨æˆ·éº¦å…‹é£è¾“å…¥
    â†“
[1. VADæ¨¡å—] â† æ‰‹å†™Silero VAD
    â†“
[2. éŸ³é¢‘ç‰¹å¾æå–] â† æ‰‹å†™Melé¢‘è°± + Whisper Embeddings
    â†“
[3. äººè„¸å¤„ç†] â† æ‰‹å†™äººè„¸æ£€æµ‹ + å¯¹é½
    â†“
[4. äººè„¸ç”Ÿæˆç½‘ç»œ] â† ç®€åŒ–ç‰ˆGAN
    â†“
[5. åå¤„ç†] â† æ‹¼æ¥ + è‰²å½©åŒ¹é…
    â†“
è¾“å‡ºè§†é¢‘æµ
```

### 10.1.2 æŠ€æœ¯é€‰å‹

| æ¨¡å— | æŠ€æœ¯æ–¹æ¡ˆ | åŸå›  |
|------|---------|------|
| VAD | ç®€åŒ–ç‰ˆSilero VAD | è½»é‡ã€å®æ—¶ã€CPUå¯è·‘ |
| éŸ³é¢‘ç‰¹å¾ | Melé¢‘è°± + ç®€å•CNN | ä¸ä¾èµ–å¤§æ¨¡å‹ |
| äººè„¸æ£€æµ‹ | MTCNN (æ‰‹å†™) | ç»å…¸ã€ä»£ç ç®€å• |
| äººè„¸å¯¹é½ | ä»¿å°„å˜æ¢ | æ•°å­¦åŸç†æ¸…æ™° |
| äººè„¸ç”Ÿæˆ | Pix2Pixé£æ ¼GAN | æ¶æ„ç®€å•ã€æ•ˆæœå¯æ¥å— |

**æ ¸å¿ƒåŸåˆ™**: èƒ½æ‰‹å†™çš„åšå†³æ‰‹å†™ï¼Œä¸ä¾èµ–é»‘ç›’API

---

## 10.2 æ¨¡å—1: VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹

### 10.2.1 åŸç†ï¼šèƒ½é‡+é¢‘åŸŸç‰¹å¾

**æ ¸å¿ƒæ€æƒ³**: äººå£°å…·æœ‰ç‰¹å®šçš„èƒ½é‡å’Œé¢‘è°±ç‰¹å¾

```python
# vad_from_scratch.py
import numpy as np
import librosa
from scipy import signal

class SimpleVAD:
    """
    ä»é›¶å®ç°çš„VADæ£€æµ‹å™¨
    åŸç†: èƒ½é‡ + é¢‘åŸŸç‰¹å¾ + è¿‡é›¶ç‡
    """
    def __init__(
        self,
        sample_rate=16000,
        frame_length=400,      # 25ms @ 16kHz
        hop_length=160,        # 10ms @ 16kHz
        energy_threshold=0.02,
        zcr_threshold=0.1
    ):
        self.sr = sample_rate
        self.frame_length = frame_length
        self.hop_length = hop_length
        self.energy_threshold = energy_threshold
        self.zcr_threshold = zcr_threshold

        # è®¾è®¡å¸¦é€šæ»¤æ³¢å™¨ (80Hz ~ 3400Hz äººå£°é¢‘æ®µ)
        self.b, self.a = signal.butter(
            N=5,
            Wn=[80, 3400],
            btype='band',
            fs=sample_rate
        )

    def extract_features(self, audio):
        """
        æå–ä¸‰ç§ç‰¹å¾:
        1. çŸ­æ—¶èƒ½é‡ (STE)
        2. è¿‡é›¶ç‡ (ZCR)
        3. é¢‘è°±å¹³å¦åº¦ (SFM)
        """
        # é¢„å¤„ç†: å¸¦é€šæ»¤æ³¢
        filtered = signal.filtfilt(self.b, self.a, audio)

        # åˆ†å¸§
        frames = librosa.util.frame(
            filtered,
            frame_length=self.frame_length,
            hop_length=self.hop_length
        )  # [frame_length, num_frames]

        # ç‰¹å¾1: çŸ­æ—¶èƒ½é‡
        energy = np.sum(frames ** 2, axis=0) / self.frame_length

        # ç‰¹å¾2: è¿‡é›¶ç‡
        zcr = np.sum(np.abs(np.diff(np.sign(frames), axis=0)), axis=0) / (2 * self.frame_length)

        # ç‰¹å¾3: é¢‘è°±å¹³å¦åº¦
        fft_frames = np.fft.rfft(frames, axis=0)
        power_spectrum = np.abs(fft_frames) ** 2

        # Geometric mean / Arithmetic mean
        geo_mean = np.exp(np.mean(np.log(power_spectrum + 1e-10), axis=0))
        arith_mean = np.mean(power_spectrum, axis=0)
        sfm = geo_mean / (arith_mean + 1e-10)

        return {
            'energy': energy,
            'zcr': zcr,
            'sfm': sfm
        }

    def detect(self, audio):
        """
        åˆ¤æ–­æ˜¯å¦åŒ…å«äººå£°

        è¿”å›:
            is_speech: bool
            confidence: float (0-1)
        """
        features = self.extract_features(audio)

        # å½’ä¸€åŒ–èƒ½é‡
        energy_norm = features['energy'] / (np.max(features['energy']) + 1e-10)

        # è¯­éŸ³å¸§åˆ¤å®š:
        # 1. èƒ½é‡è¶…è¿‡é˜ˆå€¼
        # 2. è¿‡é›¶ç‡åœ¨åˆç†èŒƒå›´ (å¤ªä½æ˜¯å™ªéŸ³ï¼Œå¤ªé«˜æ˜¯é«˜é¢‘å™ªå£°)
        # 3. é¢‘è°±å¹³å¦åº¦ä½ (è¯­éŸ³æœ‰è°æ³¢ç»“æ„ï¼Œå™ªéŸ³å¹³å¦)

        speech_frames = (
            (energy_norm > self.energy_threshold) &
            (features['zcr'] > 0.02) &
            (features['zcr'] < 0.3) &
            (features['sfm'] < 0.5)
        )

        # è¯­éŸ³å¸§å æ¯”
        speech_ratio = np.sum(speech_frames) / len(speech_frames)

        is_speech = speech_ratio > 0.3  # è‡³å°‘30%çš„å¸§æ˜¯è¯­éŸ³
        confidence = speech_ratio

        return is_speech, confidence

    def get_speech_segments(self, audio):
        """
        è¿”å›æ‰€æœ‰è¯­éŸ³ç‰‡æ®µçš„èµ·æ­¢æ—¶é—´

        è¿”å›: [(start_time, end_time), ...]
        """
        features = self.extract_features(audio)
        energy_norm = features['energy'] / (np.max(features['energy']) + 1e-10)

        speech_frames = (
            (energy_norm > self.energy_threshold) &
            (features['zcr'] > 0.02) &
            (features['zcr'] < 0.3) &
            (features['sfm'] < 0.5)
        )

        # æ‰¾åˆ°è¿ç»­çš„è¯­éŸ³æ®µ
        segments = []
        in_speech = False
        start_frame = 0

        for i, is_speech_frame in enumerate(speech_frames):
            if is_speech_frame and not in_speech:
                # è¯­éŸ³å¼€å§‹
                start_frame = i
                in_speech = True
            elif not is_speech_frame and in_speech:
                # è¯­éŸ³ç»“æŸ
                end_frame = i
                start_time = start_frame * self.hop_length / self.sr
                end_time = end_frame * self.hop_length / self.sr
                segments.append((start_time, end_time))
                in_speech = False

        # å¤„ç†æœ«å°¾
        if in_speech:
            end_time = len(speech_frames) * self.hop_length / self.sr
            start_time = start_frame * self.hop_length / self.sr
            segments.append((start_time, end_time))

        return segments


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load('test_audio.wav', sr=16000)

    # åˆå§‹åŒ–VAD
    vad = SimpleVAD(sample_rate=sr)

    # æ£€æµ‹æ•´æ®µéŸ³é¢‘
    is_speech, confidence = vad.detect(audio)
    print(f'Is speech: {is_speech}, Confidence: {confidence:.2f}')

    # è·å–è¯­éŸ³ç‰‡æ®µ
    segments = vad.get_speech_segments(audio)
    print(f'Found {len(segments)} speech segments:')
    for i, (start, end) in enumerate(segments):
        print(f'  Segment {i+1}: {start:.2f}s - {end:.2f}s')
```

### 10.2.2 å®æ—¶VADçŠ¶æ€æœº

```python
# vad_realtime.py
import numpy as np
from collections import deque

class RealtimeVAD:
    """
    å®æ—¶VAD: æµå¼å¤„ç†éŸ³é¢‘å—
    """
    def __init__(
        self,
        sample_rate=16000,
        chunk_size=320,        # 20ms @ 16kHz
        start_threshold=0.5,
        end_threshold=0.3,
        start_delay_chunks=5,  # è¿ç»­5å¸§æ‰ç®—å¼€å§‹
        end_delay_chunks=10    # è¿ç»­10å¸§é™éŸ³æ‰ç®—ç»“æŸ
    ):
        self.sr = sample_rate
        self.chunk_size = chunk_size
        self.start_threshold = start_threshold
        self.end_threshold = end_threshold
        self.start_delay = start_delay_chunks
        self.end_delay = end_delay_chunks

        # ç®€åŒ–ç‰ˆVAD (åŸºäºèƒ½é‡)
        self.vad_core = SimpleVAD(sample_rate=sample_rate)

        # çŠ¶æ€æœº
        self.state = 'IDLE'  # IDLE / STARTING / SPEAKING / ENDING
        self.speech_chunks = []
        self.silent_counter = 0
        self.speech_counter = 0

        # å†å²çª—å£ (ç”¨äºå¹³æ»‘)
        self.history_window = deque(maxlen=5)

    def process_chunk(self, audio_chunk):
        """
        å¤„ç†ä¸€ä¸ªéŸ³é¢‘å—

        è¿”å›:
            state: å½“å‰çŠ¶æ€
            speech_audio: å¦‚æœæ£€æµ‹åˆ°å®Œæ•´è¯­éŸ³ï¼Œè¿”å›éŸ³é¢‘ï¼›å¦åˆ™None
        """
        assert len(audio_chunk) == self.chunk_size

        # æ£€æµ‹å½“å‰å—
        is_speech, confidence = self.vad_core.detect(audio_chunk)
        self.history_window.append(confidence)

        # å¹³æ»‘ç½®ä¿¡åº¦
        smooth_confidence = np.mean(self.history_window)

        # çŠ¶æ€æœº
        if self.state == 'IDLE':
            if smooth_confidence > self.start_threshold:
                self.speech_counter += 1
                if self.speech_counter >= self.start_delay:
                    # è¿›å…¥SPEAKINGçŠ¶æ€
                    self.state = 'SPEAKING'
                    self.speech_chunks = list(self.history_window)  # åŒ…å«å†å²
                    self.silent_counter = 0
                    print('ğŸ¤ Speech started')
            else:
                self.speech_counter = 0

        elif self.state == 'SPEAKING':
            self.speech_chunks.append(audio_chunk)

            if smooth_confidence < self.end_threshold:
                self.silent_counter += 1
                if self.silent_counter >= self.end_delay:
                    # è¯­éŸ³ç»“æŸ
                    self.state = 'IDLE'
                    speech_audio = np.concatenate(self.speech_chunks)
                    self.speech_chunks = []
                    self.silent_counter = 0
                    self.speech_counter = 0
                    print('âœ… Speech ended')
                    return self.state, speech_audio
            else:
                self.silent_counter = 0

        return self.state, None


# æµ‹è¯•å®æ—¶VAD
if __name__ == '__main__':
    import sounddevice as sd

    vad = RealtimeVAD(sample_rate=16000, chunk_size=320)

    def audio_callback(indata, frames, time, status):
        """å®æ—¶éŸ³é¢‘å›è°ƒ"""
        audio_chunk = indata[:, 0]  # å•å£°é“
        state, speech = vad.process_chunk(audio_chunk)

        if speech is not None:
            print(f'Captured speech: {len(speech)} samples ({len(speech)/16000:.2f}s)')
            # è¿™é‡Œå¯ä»¥é€å…¥ASR

    # å¼€å§‹å½•éŸ³
    with sd.InputStream(
        samplerate=16000,
        channels=1,
        blocksize=320,
        callback=audio_callback
    ):
        print('Listening... (Press Ctrl+C to stop)')
        sd.sleep(100000)
```

---

## 10.3 æ¨¡å—2: éŸ³é¢‘ç‰¹å¾æå–

### 10.3.1 Melé¢‘è°±æå–ï¼ˆä»é›¶å®ç°ï¼‰

```python
# mel_spectrogram.py
import numpy as np
from scipy.fftpack import dct

class MelSpectrogramExtractor:
    """
    ä»é›¶å®ç°Melé¢‘è°±æå–
    """
    def __init__(
        self,
        sample_rate=16000,
        n_fft=512,
        hop_length=160,
        n_mels=80,
        fmin=0,
        fmax=8000
    ):
        self.sr = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels

        # åˆ›å»ºMelæ»¤æ³¢å™¨ç»„
        self.mel_filters = self.create_mel_filterbank(
            n_fft, n_mels, sample_rate, fmin, fmax
        )

    @staticmethod
    def hz_to_mel(hz):
        """èµ«å…¹è½¬Melé¢‘ç‡"""
        return 2595 * np.log10(1 + hz / 700)

    @staticmethod
    def mel_to_hz(mel):
        """Melé¢‘ç‡è½¬èµ«å…¹"""
        return 700 * (10 ** (mel / 2595) - 1)

    def create_mel_filterbank(self, n_fft, n_mels, sr, fmin, fmax):
        """
        åˆ›å»ºMelæ»¤æ³¢å™¨ç»„

        åŸç†:
        1. åœ¨Melå°ºåº¦ä¸Šå‡åŒ€åˆ†å¸ƒä¸­å¿ƒé¢‘ç‡
        2. åœ¨é¢‘ç‡å°ºåº¦ä¸Šæ˜¯éçº¿æ€§åˆ†å¸ƒ (ä½é¢‘å¯†é›†ï¼Œé«˜é¢‘ç¨€ç–)
        3. æ¯ä¸ªæ»¤æ³¢å™¨æ˜¯ä¸‰è§’å½¢çª—
        """
        # Melå°ºåº¦ä¸Šçš„å‡åŒ€ç‚¹
        mel_min = self.hz_to_mel(fmin)
        mel_max = self.hz_to_mel(fmax)
        mel_points = np.linspace(mel_min, mel_max, n_mels + 2)

        # è½¬å›èµ«å…¹
        hz_points = self.mel_to_hz(mel_points)

        # è½¬æ¢ä¸ºFFT binç´¢å¼•
        bin_points = np.floor((n_fft + 1) * hz_points / sr).astype(int)

        # åˆ›å»ºæ»¤æ³¢å™¨ç»„ [n_mels, n_fft//2 + 1]
        filters = np.zeros((n_mels, n_fft // 2 + 1))

        for i in range(n_mels):
            # ä¸‰è§’å½¢æ»¤æ³¢å™¨çš„ä¸‰ä¸ªç‚¹
            left = bin_points[i]
            center = bin_points[i + 1]
            right = bin_points[i + 2]

            # ä¸Šå‡æ²¿
            for j in range(left, center):
                filters[i, j] = (j - left) / (center - left)

            # ä¸‹é™æ²¿
            for j in range(center, right):
                filters[i, j] = (right - j) / (right - center)

        return filters

    def extract(self, audio):
        """
        æå–Melé¢‘è°±

        æ­¥éª¤:
        1. çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ (STFT)
        2. è®¡ç®—åŠŸç‡è°±
        3. åº”ç”¨Melæ»¤æ³¢å™¨
        4. å–å¯¹æ•°
        """
        # 1. åˆ†å¸§åŠ çª—
        frames = self._frame_audio(audio)
        window = np.hanning(self.n_fft)
        frames_windowed = frames * window[:, None]

        # 2. FFT
        fft_result = np.fft.rfft(frames_windowed, n=self.n_fft, axis=0)

        # 3. åŠŸç‡è°±
        power_spectrum = np.abs(fft_result) ** 2

        # 4. åº”ç”¨Melæ»¤æ³¢å™¨
        mel_spectrum = self.mel_filters @ power_spectrum

        # 5. å–å¯¹æ•° (dBå°ºåº¦)
        log_mel = 10 * np.log10(mel_spectrum + 1e-10)

        return log_mel  # [n_mels, num_frames]

    def _frame_audio(self, audio):
        """
        éŸ³é¢‘åˆ†å¸§

        è¿”å›: [n_fft, num_frames]
        """
        num_samples = len(audio)
        num_frames = 1 + (num_samples - self.n_fft) // self.hop_length

        frames = np.zeros((self.n_fft, num_frames))

        for i in range(num_frames):
            start = i * self.hop_length
            end = start + self.n_fft
            if end <= num_samples:
                frames[:, i] = audio[start:end]

        return frames

    def extract_mfcc(self, audio, n_mfcc=13):
        """
        æå–MFCCç‰¹å¾ (Melé¢‘è°±çš„ç¦»æ•£ä½™å¼¦å˜æ¢)
        """
        mel_spec = self.extract(audio)

        # DCTå˜æ¢
        mfcc = dct(mel_spec, type=2, axis=0, norm='ortho')[:n_mfcc, :]

        return mfcc


# æµ‹è¯•
if __name__ == '__main__':
    import librosa
    import matplotlib.pyplot as plt

    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load('test.wav', sr=16000)

    # æå–Melé¢‘è°±
    extractor = MelSpectrogramExtractor(sample_rate=sr)
    mel_spec = extractor.extract(audio)

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 4))
    plt.imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')
    plt.colorbar(label='dB')
    plt.xlabel('Time (frames)')
    plt.ylabel('Mel frequency bins')
    plt.title('Mel Spectrogram')
    plt.tight_layout()
    plt.savefig('mel_spectrogram.png')
    print('Mel spectrogram saved to mel_spectrogram.png')
```

### 10.3.2 éŸ³é¢‘Embeddingæå–

```python
# audio_encoder.py
import torch
import torch.nn as nn

class SimplifiedAudioEncoder(nn.Module):
    """
    ç®€åŒ–ç‰ˆéŸ³é¢‘ç¼–ç å™¨
    å°†Melé¢‘è°±ç¼–ç ä¸ºå›ºå®šç»´åº¦çš„å‘é‡

    æ¶æ„: å¤šå±‚Conv1D + Attention
    """
    def __init__(
        self,
        n_mels=80,
        embedding_dim=256,
        hidden_dim=512
    ):
        super().__init__()

        # å·ç§¯ç¼–ç å™¨
        self.conv_layers = nn.Sequential(
            nn.Conv1d(n_mels, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Conv1d(256, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU()
        )

        # Self-Attentionå±‚
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, embedding_dim)

    def forward(self, mel_spec):
        """
        è¾“å…¥: mel_spec [B, n_mels, T]
        è¾“å‡º: embeddings [B, T, embedding_dim]
        """
        # å·ç§¯ç¼–ç 
        conv_out = self.conv_layers(mel_spec)  # [B, hidden_dim, T]

        # è½¬ç½®ç”¨äºattention
        conv_out = conv_out.transpose(1, 2)  # [B, T, hidden_dim]

        # Self-Attention
        attn_out, _ = self.attention(conv_out, conv_out, conv_out)

        # è¾“å‡ºæŠ•å½±
        embeddings = self.output_proj(attn_out)  # [B, T, embedding_dim]

        return embeddings


# è®­ç»ƒè„šæœ¬ç¤ºä¾‹
class AudioEncoderTrainer:
    """
    è®­ç»ƒéŸ³é¢‘ç¼–ç å™¨çš„ç®€åŒ–ç¤ºä¾‹
    ç›®æ ‡: å­¦ä¹ éŸ³ç´ åˆ°å˜´å‹çš„æ˜ å°„
    """
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    def train_step(self, mel_spec, target_lip_motion):
        """
        ä¸€æ­¥è®­ç»ƒ

        mel_spec: [B, n_mels, T]
        target_lip_motion: [B, T, lip_dim] ç›®æ ‡å”‡éƒ¨è¿åŠ¨å‚æ•°
        """
        self.model.train()

        # å‰å‘ä¼ æ’­
        pred_embeddings = self.model(mel_spec)

        # æŸå¤±å‡½æ•° (ç®€åŒ–: MSE)
        loss = nn.functional.mse_loss(pred_embeddings, target_lip_motion)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆå§‹åŒ–æ¨¡å‹
    model = SimplifiedAudioEncoder(n_mels=80, embedding_dim=256)

    # æ¨¡æ‹Ÿè¾“å…¥
    mel_spec = torch.randn(2, 80, 100)  # [batch=2, n_mels=80, time=100]

    # å‰å‘ä¼ æ’­
    embeddings = model(mel_spec)
    print(f'Input shape: {mel_spec.shape}')
    print(f'Output shape: {embeddings.shape}')
    # Output: [2, 100, 256]
```

---

## 10.4 æ¨¡å—3: äººè„¸æ£€æµ‹ä¸å¯¹é½

### 10.4.1 ç®€åŒ–ç‰ˆMTCNNå®ç°

```python
# face_detection.py
import torch
import torch.nn as nn
import cv2
import numpy as np

class PNet(nn.Module):
    """
    MTCNNçš„P-Net (Proposal Network)
    å¿«é€Ÿç”Ÿæˆå€™é€‰äººè„¸åŒºåŸŸ
    """
    def __init__(self):
        super().__init__()

        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)
        self.prelu1 = nn.PReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)

        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)
        self.prelu2 = nn.PReLU()

        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)
        self.prelu3 = nn.PReLU()

        # è¾“å‡ºå±‚
        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)  # åˆ†ç±» (äººè„¸/éäººè„¸)
        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)  # è¾¹ç•Œæ¡†å›å½’

    def forward(self, x):
        x = self.prelu1(self.conv1(x))
        x = self.pool1(x)
        x = self.prelu2(self.conv2(x))
        x = self.prelu3(self.conv3(x))

        cls_prob = torch.softmax(self.conv4_1(x), dim=1)  # [B, 2, H, W]
        bbox_reg = self.conv4_2(x)  # [B, 4, H, W]

        return cls_prob, bbox_reg


class SimpleFaceDetector:
    """
    ç®€åŒ–ç‰ˆäººè„¸æ£€æµ‹å™¨
    åŸºäºçº§è”CNNçš„æ€æƒ³ï¼Œä½†åªå®ç°å…³é”®éƒ¨åˆ†
    """
    def __init__(self, min_face_size=20, threshold=0.7):
        self.min_face_size = min_face_size
        self.threshold = threshold

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦è®­ç»ƒ)
        self.pnet = PNet()

    def detect(self, image):
        """
        æ£€æµ‹äººè„¸

        è¾“å…¥: image [H, W, 3] RGB
        è¾“å‡º: bboxes [[x1, y1, x2, y2, confidence], ...]
        """
        h, w = image.shape[:2]

        # å›¾åƒé‡‘å­—å¡”ï¼ˆå¤šå°ºåº¦æ£€æµ‹ï¼‰
        scales = self._calculate_scales(h, w)

        all_boxes = []

        for scale in scales:
            # ç¼©æ”¾å›¾åƒ
            scaled_h = int(h * scale)
            scaled_w = int(w * scale)
            scaled_img = cv2.resize(image, (scaled_w, scaled_h))

            # è½¬ä¸ºtensor
            img_tensor = torch.from_numpy(scaled_img).permute(2, 0, 1).float() / 255.0
            img_tensor = img_tensor.unsqueeze(0)  # [1, 3, H, W]

            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                cls_prob, bbox_reg = self.pnet(img_tensor)

            # æå–å€™é€‰æ¡†
            boxes = self._generate_boxes(
                cls_prob[0, 1].numpy(),  # äººè„¸æ¦‚ç‡å›¾
                bbox_reg[0].numpy(),      # è¾¹ç•Œæ¡†å›å½’
                scale,
                self.threshold
            )

            all_boxes.extend(boxes)

        # NMSå»é‡
        final_boxes = self._nms(np.array(all_boxes), 0.5)

        return final_boxes

    def _calculate_scales(self, h, w):
        """è®¡ç®—å›¾åƒé‡‘å­—å¡”çš„ç¼©æ”¾æ¯”ä¾‹"""
        min_size = min(h, w)
        scales = []
        factor = 0.709  # ç¼©æ”¾å› å­

        m = 12.0 / self.min_face_size
        min_size = min_size * m

        while min_size >= 12:
            scales.append(m * (factor ** len(scales)))
            min_size *= factor

        return scales

    def _generate_boxes(self, cls_map, reg_map, scale, threshold):
        """ä»ç‰¹å¾å›¾ç”Ÿæˆå€™é€‰æ¡†"""
        stride = 2
        cellsize = 12

        # æ‰¾åˆ°æ¦‚ç‡å¤§äºé˜ˆå€¼çš„ä½ç½®
        t_index = np.where(cls_map > threshold)

        # æ²¡æœ‰æ£€æµ‹åˆ°
        if t_index[0].size == 0:
            return []

        boxes = []
        for i in range(t_index[0].size):
            y, x = t_index[0][i], t_index[1][i]

            score = cls_map[y, x]
            reg = reg_map[:, y, x]

            # æ˜ å°„å›åŸå›¾åæ ‡
            x1 = int((x * stride) / scale)
            y1 = int((y * stride) / scale)
            x2 = int((x * stride + cellsize) / scale)
            y2 = int((y * stride + cellsize) / scale)

            # è¾¹ç•Œæ¡†å›å½’ä¿®æ­£
            w = x2 - x1
            h = y2 - y1
            x1 += reg[0] * w
            y1 += reg[1] * h
            x2 += reg[2] * w
            y2 += reg[3] * h

            boxes.append([x1, y1, x2, y2, score])

        return boxes

    def _nms(self, boxes, threshold):
        """éæå¤§å€¼æŠ‘åˆ¶"""
        if len(boxes) == 0:
            return []

        x1 = boxes[:, 0]
        y1 = boxes[:, 1]
        x2 = boxes[:, 2]
        y2 = boxes[:, 3]
        scores = boxes[:, 4]

        areas = (x2 - x1) * (y2 - y1)
        order = scores.argsort()[::-1]

        keep = []
        while order.size > 0:
            i = order[0]
            keep.append(i)

            # è®¡ç®—IoU
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])

            w = np.maximum(0.0, xx2 - xx1)
            h = np.maximum(0.0, yy2 - yy1)
            inter = w * h

            iou = inter / (areas[i] + areas[order[1:]] - inter)

            # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†
            inds = np.where(iou <= threshold)[0]
            order = order[inds + 1]

        return boxes[keep]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    detector = SimpleFaceDetector()

    # è¯»å–å›¾åƒ
    img = cv2.imread('portrait.jpg')
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # æ£€æµ‹äººè„¸
    faces = detector.detect(img_rgb)

    print(f'Detected {len(faces)} faces')

    # å¯è§†åŒ–
    for x1, y1, x2, y2, score in faces:
        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(img, f'{score:.2f}', (int(x1), int(y1)-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    cv2.imwrite('detected.jpg', img)
```

### 10.4.2 äººè„¸å¯¹é½ï¼ˆä»¿å°„å˜æ¢ï¼‰

```python
# face_alignment.py
import cv2
import numpy as np

class FaceAligner:
    """
    äººè„¸å¯¹é½: é€šè¿‡ä»¿å°„å˜æ¢å°†äººè„¸å½’ä¸€åŒ–
    """
    def __init__(self, target_size=256):
        self.target_size = target_size

        # æ ‡å‡†äººè„¸å…³é”®ç‚¹ä½ç½® (å½’ä¸€åŒ–åæ ‡)
        self.standard_landmarks = np.array([
            [0.31, 0.46],  # å·¦çœ¼
            [0.69, 0.46],  # å³çœ¼
            [0.50, 0.73]   # é¼»å°–
        ]) * target_size

    def align(self, image, landmarks):
        """
        å¯¹é½äººè„¸

        è¾“å…¥:
            image: [H, W, 3]
            landmarks: [[x, y], ...] è‡³å°‘3ä¸ªå…³é”®ç‚¹

        è¾“å‡º:
            aligned_face: [target_size, target_size, 3]
            transform_matrix: ä»¿å°„å˜æ¢çŸ©é˜µ (ç”¨äºåå‘å˜æ¢)
        """
        # é€‰æ‹©3ä¸ªå…³é”®ç‚¹ (å·¦çœ¼ã€å³çœ¼ã€é¼»å°–)
        src_pts = np.array([
            landmarks[36:42].mean(axis=0),  # å·¦çœ¼ä¸­å¿ƒ
            landmarks[42:48].mean(axis=0),  # å³çœ¼ä¸­å¿ƒ
            landmarks[30]                    # é¼»å°–
        ], dtype=np.float32)

        dst_pts = self.standard_landmarks.astype(np.float32)

        # è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µ
        M = cv2.getAffineTransform(src_pts, dst_pts)

        # åº”ç”¨å˜æ¢
        aligned = cv2.warpAffine(
            image,
            M,
            (self.target_size, self.target_size),
            flags=cv2.INTER_LINEAR
        )

        return aligned, M

    def inverse_transform(self, face, M):
        """
        åå‘å˜æ¢: å°†å¯¹é½åçš„äººè„¸è´´å›åŸå›¾
        """
        M_inv = cv2.invertAffineTransform(M)
        # ... (åœ¨paste-backæ—¶ä½¿ç”¨)


# å®Œæ•´çš„äººè„¸å¤„ç†Pipeline
class FaceProcessor:
    """
    å®Œæ•´çš„äººè„¸å¤„ç†æµç¨‹:
    1. æ£€æµ‹
    2. å…³é”®ç‚¹å®šä½ (ä½¿ç”¨dlibæˆ–å…¶ä»–)
    3. å¯¹é½
    4. æ ‡å‡†åŒ–
    """
    def __init__(self):
        self.detector = SimpleFaceDetector()
        self.aligner = FaceAligner(target_size=256)

        # å…³é”®ç‚¹æ£€æµ‹å™¨ (è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦é¢å¤–æ¨¡å‹)
        # å¯ä»¥ä½¿ç”¨dlibæˆ–MediaPipe
        try:
            import dlib
            self.landmark_detector = dlib.shape_predictor(
                'shape_predictor_68_face_landmarks.dat'
            )
        except:
            print('Warning: dlib not found, using simplified landmarks')
            self.landmark_detector = None

    def process(self, image):
        """
        å¤„ç†å›¾åƒï¼Œè¿”å›å¯¹é½åçš„äººè„¸

        è¿”å›: {
            'face': å¯¹é½åçš„äººè„¸å›¾åƒ,
            'bbox': åŸå›¾ä¸­çš„äººè„¸æ¡†,
            'landmarks': å…³é”®ç‚¹,
            'transform': ä»¿å°„å˜æ¢çŸ©é˜µ
        }
        """
        # 1. æ£€æµ‹äººè„¸
        faces = self.detector.detect(image)

        if len(faces) == 0:
            raise ValueError('No face detected')

        # å–ç½®ä¿¡åº¦æœ€é«˜çš„
        best_face = faces[np.argmax(faces[:, 4])]
        x1, y1, x2, y2, score = best_face

        # 2. æ£€æµ‹å…³é”®ç‚¹
        if self.landmark_detector:
            import dlib
            rect = dlib.rectangle(int(x1), int(y1), int(x2), int(y2))
            shape = self.landmark_detector(image, rect)
            landmarks = np.array([[p.x, p.y] for p in shape.parts()])
        else:
            # ç®€åŒ–: å‡è®¾å…³é”®ç‚¹
            landmarks = self._estimate_landmarks(x1, y1, x2, y2)

        # 3. å¯¹é½
        aligned_face, M = self.aligner.align(image, landmarks)

        return {
            'face': aligned_face,
            'bbox': (x1, y1, x2, y2),
            'landmarks': landmarks,
            'transform': M
        }

    def _estimate_landmarks(self, x1, y1, x2, y2):
        """ç®€åŒ–: æ ¹æ®bboxä¼°è®¡å…³é”®ç‚¹ä½ç½®"""
        w = x2 - x1
        h = y2 - y1

        # 68ä¸ªå…³é”®ç‚¹çš„ç®€åŒ–ä¼°è®¡
        landmarks = np.zeros((68, 2))

        # å·¦çœ¼ (36-41)
        left_eye_x = x1 + w * 0.3
        left_eye_y = y1 + h * 0.4
        landmarks[36:42] = [left_eye_x, left_eye_y]

        # å³çœ¼ (42-47)
        right_eye_x = x1 + w * 0.7
        right_eye_y = y1 + h * 0.4
        landmarks[42:48] = [right_eye_x, right_eye_y]

        # é¼»å°– (30)
        landmarks[30] = [x1 + w * 0.5, y1 + h * 0.65]

        # ... (å…¶ä»–å…³é”®ç‚¹)

        return landmarks
```

---

## 10.5 æ¨¡å—4: äººè„¸ç”Ÿæˆç½‘ç»œ

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œæä¾›æ ¸å¿ƒæ¶æ„å’Œè®­ç»ƒé€»è¾‘ã€‚å®Œæ•´ä»£ç è¯·è§é¡¹ç›®ä»“åº“ã€‚

```python
# face_generator.py
import torch
import torch.nn as nn

class AudioToFaceGenerator(nn.Module):
    """
    éŸ³é¢‘é©±åŠ¨çš„äººè„¸ç”Ÿæˆå™¨
    æ¶æ„: Pix2Pixé£æ ¼çš„U-Net + éŸ³é¢‘æ¡ä»¶æ³¨å…¥
    """
    def __init__(
        self,
        audio_embed_dim=256,
        face_channels=3
    ):
        super().__init__()

        # éŸ³é¢‘ç¼–ç å™¨
        self.audio_encoder = SimplifiedAudioEncoder(
            n_mels=80,
            embedding_dim=audio_embed_dim
        )

        # U-Netç”Ÿæˆå™¨
        self.unet = UNetGenerator(
            in_channels=face_channels + audio_embed_dim,
            out_channels=face_channels
        )

    def forward(self, reference_face, audio_mel):
        """
        è¾“å…¥:
            reference_face: [B, 3, H, W] å‚è€ƒäººè„¸
            audio_mel: [B, 80, T] Melé¢‘è°±

        è¾“å‡º:
            generated_face: [B, 3, H, W] ç”Ÿæˆçš„äººè„¸
        """
        # æå–éŸ³é¢‘embedding
        audio_embed = self.audio_encoder(audio_mel)  # [B, T, 256]

        # å–å½“å‰å¸§çš„audio embedding
        audio_feat = audio_embed[:, 0, :]  # [B, 256]

        # æ‰©å±•åˆ°ç©ºé—´ç»´åº¦
        B, C, H, W = reference_face.shape
        audio_spatial = audio_feat.view(B, -1, 1, 1).expand(B, -1, H, W)

        # æ‹¼æ¥
        combined_input = torch.cat([reference_face, audio_spatial], dim=1)

        # ç”Ÿæˆ
        generated_face = self.unet(combined_input)

        return generated_face


class UNetGenerator(nn.Module):
    """ç®€åŒ–ç‰ˆU-Netç”Ÿæˆå™¨"""
    def __init__(self, in_channels, out_channels):
        super().__init__()

        # ç¼–ç å™¨
        self.down1 = self._down_block(in_channels, 64)
        self.down2 = self._down_block(64, 128)
        self.down3 = self._down_block(128, 256)
        self.down4 = self._down_block(256, 512)

        # ç“¶é¢ˆ
        self.bottleneck = nn.Sequential(
            nn.Conv2d(512, 1024, kernel_size=3, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True)
        )

        # è§£ç å™¨ (with skip connections)
        self.up1 = self._up_block(1024 + 512, 512)
        self.up2 = self._up_block(512 + 256, 256)
        self.up3 = self._up_block(256 + 128, 128)
        self.up4 = self._up_block(128 + 64, 64)

        # è¾“å‡ºå±‚
        self.final = nn.Conv2d(64, out_channels, kernel_size=1)

    def _down_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.LeakyReLU(0.2, inplace=True)
        )

    def _up_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # ç¼–ç è·¯å¾„
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        # ç“¶é¢ˆ
        b = self.bottleneck(d4)

        # è§£ç è·¯å¾„ (with skip connections)
        u1 = self.up1(torch.cat([b, d4], dim=1))
        u2 = self.up2(torch.cat([u1, d3], dim=1))
        u3 = self.up3(torch.cat([u2, d2], dim=1))
        u4 = self.up4(torch.cat([u3, d1], dim=1))

        # è¾“å‡º
        out = self.final(u4)
        return torch.tanh(out)  # [-1, 1]
```

### 10.5.2 è®­ç»ƒå¾ªç¯

```python
# train.py
class TrainingPipeline:
    """
    å®Œæ•´çš„è®­ç»ƒæµç¨‹
    """
    def __init__(self, generator, discriminator, device='cuda'):
        self.generator = generator.to(device)
        self.discriminator = discriminator.to(device)
        self.device = device

        self.g_optimizer = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))
        self.d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))

        # æŸå¤±å‡½æ•°
        self.l1_loss = nn.L1Loss()
        self.gan_loss = nn.BCEWithLogitsLoss()

    def train_step(self, batch):
        """
        ä¸€æ­¥è®­ç»ƒ

        batch: {
            'reference_face': [B, 3, H, W],
            'target_face': [B, 3, H, W],
            'audio_mel': [B, 80, T]
        }
        """
        ref_face = batch['reference_face'].to(self.device)
        target_face = batch['target_face'].to(self.device)
        audio_mel = batch['audio_mel'].to(self.device)

        # ===== è®­ç»ƒåˆ¤åˆ«å™¨ =====
        self.d_optimizer.zero_grad()

        # ç”Ÿæˆå‡å›¾åƒ
        with torch.no_grad():
            fake_face = self.generator(ref_face, audio_mel)

        # åˆ¤åˆ«å™¨å‰å‘
        real_pred = self.discriminator(target_face)
        fake_pred = self.discriminator(fake_face.detach())

        # æŸå¤±
        real_loss = self.gan_loss(real_pred, torch.ones_like(real_pred))
        fake_loss = self.gan_loss(fake_pred, torch.zeros_like(fake_pred))
        d_loss = (real_loss + fake_loss) / 2

        # åå‘ä¼ æ’­
        d_loss.backward()
        self.d_optimizer.step()

        # ===== è®­ç»ƒç”Ÿæˆå™¨ =====
        self.g_optimizer.zero_grad()

        # ç”Ÿæˆå›¾åƒ
        fake_face = self.generator(ref_face, audio_mel)

        # åˆ¤åˆ«å™¨è¯„åˆ†
        fake_pred = self.discriminator(fake_face)

        # æŸå¤±: GANæŸå¤± + L1é‡å»ºæŸå¤±
        gan_g_loss = self.gan_loss(fake_pred, torch.ones_like(fake_pred))
        l1_loss = self.l1_loss(fake_face, target_face) * 100  # æƒé‡100

        g_loss = gan_g_loss + l1_loss

        # åå‘ä¼ æ’­
        g_loss.backward()
        self.g_optimizer.step()

        return {
            'd_loss': d_loss.item(),
            'g_loss': g_loss.item(),
            'g_gan': gan_g_loss.item(),
            'g_l1': l1_loss.item()
        }
```

---

## 10.6 å®Œæ•´Pipelineé›†æˆ

```python
# complete_pipeline.py
import numpy as np
import cv2
import torch
from typing import Optional

class DigitalHumanPipeline:
    """
    å®Œæ•´çš„æ•°å­—äººç³»ç»ŸPipeline
    """
    def __init__(
        self,
        model_path: str,
        device: str = 'cuda'
    ):
        self.device = device

        # åŠ è½½å„æ¨¡å—
        self.vad = RealtimeVAD()
        self.mel_extractor = MelSpectrogramExtractor()
        self.face_processor = FaceProcessor()

        # åŠ è½½ç”Ÿæˆå™¨
        self.generator = AudioToFaceGenerator().to(device)
        self.generator.load_state_dict(torch.load(model_path))
        self.generator.eval()

    def generate_video(
        self,
        reference_image_path: str,
        audio_path: str,
        output_path: str,
        fps: int = 25
    ):
        """
        ç”Ÿæˆæ•°å­—äººè§†é¢‘

        æµç¨‹:
        1. å¤„ç†å‚è€ƒå›¾åƒ
        2. æå–éŸ³é¢‘ç‰¹å¾
        3. é€å¸§ç”Ÿæˆ
        4. è´´å›åŸå›¾
        5. åˆæˆè§†é¢‘
        """
        import librosa

        # 1. å¤„ç†å‚è€ƒå›¾åƒ
        ref_image = cv2.imread(reference_image_path)
        ref_image_rgb = cv2.cvtColor(ref_image, cv2.COLOR_BGR2RGB)

        face_data = self.face_processor.process(ref_image_rgb)
        aligned_face = face_data['face']

        # è½¬ä¸ºtensor
        face_tensor = torch.from_numpy(aligned_face).permute(2, 0, 1).float() / 255.0
        face_tensor = face_tensor.unsqueeze(0).to(self.device)  # [1, 3, H, W]
        face_tensor = (face_tensor - 0.5) / 0.5  # å½’ä¸€åŒ–åˆ°[-1, 1]

        # 2. æå–éŸ³é¢‘ç‰¹å¾
        audio, sr = librosa.load(audio_path, sr=16000)
        mel_spec = self.mel_extractor.extract(audio)  # [80, T]

        # è½¬ä¸ºtensor
        mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).to(self.device)  # [1, 80, T]

        num_frames = mel_spec.shape[1]

        # 3. é€å¸§ç”Ÿæˆ
        output_frames = []

        with torch.no_grad():
            for t in range(num_frames):
                # å½“å‰å¸§çš„éŸ³é¢‘ç‰¹å¾
                mel_frame = mel_tensor[:, :, t:t+1]  # [1, 80, 1]

                # ç”Ÿæˆäººè„¸
                generated_face = self.generator(face_tensor, mel_frame)  # [1, 3, H, W]

                # è½¬å›numpy
                face_np = generated_face[0].cpu().numpy()
                face_np = (face_np * 0.5 + 0.5) * 255  # åå½’ä¸€åŒ–
                face_np = face_np.transpose(1, 2, 0).astype(np.uint8)

                output_frames.append(face_np)

        # 4. è´´å›åŸå›¾
        final_frames = []
        for frame in output_frames:
            final_frame = self._paste_back(
                frame,
                ref_image_rgb,
                face_data['bbox'],
                face_data['transform']
            )
            final_frames.append(final_frame)

        # 5. ä¿å­˜è§†é¢‘
        self._save_video(final_frames, audio_path, output_path, fps)

        return output_path

    def _paste_back(self, face_frame, original_image, bbox, transform_matrix):
        """å°†ç”Ÿæˆçš„äººè„¸è´´å›åŸå›¾"""
        # åå‘ä»¿å°„å˜æ¢
        M_inv = cv2.invertAffineTransform(transform_matrix)
        h, w = original_image.shape[:2]

        # å˜æ¢å›åŸå›¾åæ ‡ç³»
        face_in_original = cv2.warpAffine(
            face_frame,
            M_inv,
            (w, h),
            borderMode=cv2.BORDER_TRANSPARENT
        )

        # Alphaæ··åˆ
        mask = (face_in_original.sum(axis=2) > 0).astype(np.float32)
        mask = cv2.GaussianBlur(mask, (15, 15), 0)  # ç¾½åŒ–
        mask = mask[:, :, None]

        result = face_in_original * mask + original_image * (1 - mask)

        return result.astype(np.uint8)

    def _save_video(self, frames, audio_path, output_path, fps):
        """åˆæˆè§†é¢‘+éŸ³é¢‘"""
        import subprocess

        # ä¸´æ—¶ä¿å­˜æ— å£°è§†é¢‘
        temp_video = '/tmp/temp_video.mp4'
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        h, w = frames[0].shape[:2]
        writer = cv2.VideoWriter(temp_video, fourcc, fps, (w, h))

        for frame in frames:
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            writer.write(frame_bgr)

        writer.release()

        # åˆå¹¶éŸ³é¢‘
        subprocess.run([
            'ffmpeg', '-y',
            '-i', temp_video,
            '-i', audio_path,
            '-c:v', 'libx264',
            '-c:a', 'aac',
            '-shortest',
            output_path
        ], check=True, capture_output=True)

        print(f'Video saved to: {output_path}')


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    pipeline = DigitalHumanPipeline(
        model_path='checkpoints/generator.pth',
        device='cuda'
    )

    pipeline.generate_video(
        reference_image_path='portrait.jpg',
        audio_path='speech.wav',
        output_path='output.mp4',
        fps=25
    )
```

---

## 10.7 æ€§èƒ½ä¼˜åŒ–å®æˆ˜

### 10.7.1 CUDAä¼˜åŒ–

```python
# cuda_optimization.py
import torch

class OptimizedGenerator(nn.Module):
    """ä¼˜åŒ–åçš„ç”Ÿæˆå™¨"""
    def __init__(self, base_generator):
        super().__init__()
        self.generator = base_generator

        # 1. å¯ç”¨torch.compile (PyTorch 2.0+)
        self.generator = torch.compile(
            self.generator,
            mode='reduce-overhead'
        )

    @torch.cuda.amp.autocast()  # 2. æ··åˆç²¾åº¦æ¨ç†
    def forward(self, *args, **kwargs):
        return self.generator(*args, **kwargs)


# ä½¿ç”¨CUDAå›¾åŠ é€Ÿ
class CUDAGraphOptimizer:
    """CUDAå›¾ä¼˜åŒ–"""
    def __init__(self, model):
        self.model = model
        self.static_input = None
        self.static_output = None
        self.graph = None

    def warmup(self, dummy_input):
        """Warmupå¹¶å½•åˆ¶CUDAå›¾"""
        # Warmup
        for _ in range(10):
            _ = self.model(*dummy_input)

        # åˆ›å»ºé™æ€è¾“å…¥
        self.static_input = [x.clone() for x in dummy_input]

        # å½•åˆ¶CUDAå›¾
        self.graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.graph):
            self.static_output = self.model(*self.static_input)

    def forward(self, *inputs):
        """ä½¿ç”¨CUDAå›¾æ¨ç†"""
        # æ‹·è´æ•°æ®åˆ°é™æ€tensor
        for static, dynamic in zip(self.static_input, inputs):
            static.copy_(dynamic)

        # é‡æ”¾CUDAå›¾
        self.graph.replay()

        return self.static_output.clone()
```

### 10.7.2 æ‰¹å¤„ç†ä¼˜åŒ–

```python
# batch_optimization.py
class BatchProcessor:
    """æ‰¹å¤„ç†ä¼˜åŒ–"""
    def __init__(self, pipeline, batch_size=8):
        self.pipeline = pipeline
        self.batch_size = batch_size

    def process_multiple_images(self, image_paths, audio_path):
        """æ‰¹é‡å¤„ç†å¤šä¸ªå›¾åƒ"""
        # æå–éŸ³é¢‘ç‰¹å¾ (åªéœ€1æ¬¡)
        audio, sr = librosa.load(audio_path, sr=16000)
        mel_spec = self.pipeline.mel_extractor.extract(audio)
        mel_tensor = torch.from_numpy(mel_spec).to(self.pipeline.device)

        results = []

        # æ‰¹å¤„ç†å›¾åƒ
        for i in range(0, len(image_paths), self.batch_size):
            batch_paths = image_paths[i:i + self.batch_size]

            # åŠ è½½å¹¶å¤„ç†äººè„¸
            face_batch = []
            for path in batch_paths:
                img = cv2.imread(path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                face_data = self.pipeline.face_processor.process(img_rgb)
                face_tensor = torch.from_numpy(face_data['face']).permute(2, 0, 1)
                face_tensor = face_tensor.float() / 255.0
                face_tensor = (face_tensor - 0.5) / 0.5
                face_batch.append(face_tensor)

            # å †å æˆbatch
            face_batch_tensor = torch.stack(face_batch).to(self.pipeline.device)

            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.pipeline.generator(
                    face_batch_tensor,
                    mel_tensor.unsqueeze(0).expand(len(face_batch), -1, -1)
                )

            results.extend(outputs)

        return results
```

---

## 10.8 æ€»ç»“

æœ¬ç« æˆ‘ä»¬ä»é›¶å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„æ•°å­—äººç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š

### å·²å®ç°çš„æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | å®ç°å†…å®¹ | ä»£ç è¡Œæ•° |
|------|---------|---------|
| VAD | Silero VADåŸç† + çŠ¶æ€æœº | ~200è¡Œ |
| éŸ³é¢‘ç‰¹å¾ | Melé¢‘è°± + MFCCæå– | ~150è¡Œ |
| äººè„¸æ£€æµ‹ | ç®€åŒ–ç‰ˆMTCNN | ~250è¡Œ |
| äººè„¸å¯¹é½ | ä»¿å°„å˜æ¢ + å…³é”®ç‚¹å¯¹é½ | ~100è¡Œ |
| äººè„¸ç”Ÿæˆ | U-Net GAN | ~300è¡Œ |
| å®Œæ•´Pipeline | ç«¯åˆ°ç«¯ç³»ç»Ÿ | ~200è¡Œ |

### å­¦åˆ°çš„æ ¸å¿ƒåŸç†

1. **VAD**: èƒ½é‡+é¢‘åŸŸ+è¿‡é›¶ç‡ä¸‰é‡ç‰¹å¾
2. **Melé¢‘è°±**: Hzâ†’Melçš„éçº¿æ€§æ˜ å°„åŸç†
3. **äººè„¸æ£€æµ‹**: å›¾åƒé‡‘å­—å¡” + NMS
4. **ä»¿å°„å˜æ¢**: 3ç‚¹å¯¹é½çš„æ•°å­¦åŸç†
5. **GANè®­ç»ƒ**: å¯¹æŠ—æŸå¤± + é‡å»ºæŸå¤±

### ä¸å¼€æºé¡¹ç›®çš„å¯¹æ¯”

| æŒ‡æ ‡ | æ‰‹å†™ç‰ˆæœ¬ | MuseTalk/LivePortrait |
|------|---------|----------------------|
| ä»£ç é‡ | ~1200è¡Œ | ~10000è¡Œ+ |
| ä¾èµ– | PyTorch + OpenCV | å¤§é‡ç¬¬ä¸‰æ–¹åº“ |
| ç†è§£éš¾åº¦ | â­â­â­ | â­â­â­â­â­ |
| æ•ˆæœ | 70åˆ† | 95åˆ† |
| å¯å®šåˆ¶æ€§ | æé«˜ | ä¸­ç­‰ |

**æ ¸å¿ƒä»·å€¼**: æ‰‹å†™ç‰ˆæœ¬è®©ä½ çœŸæ­£ç†è§£**æ¯ä¸€è¡Œä»£ç çš„æ„ä¹‰**ï¼Œè€Œä¸æ˜¯è°ƒç”¨é»‘ç›’APIã€‚

---

## 10.9 ä¸‹ä¸€æ­¥è¿›é˜¶

å®Œæˆæ‰‹å†™å®ç°åï¼Œå»ºè®®ï¼š

1. âœ… **æ”¹è¿›ç”Ÿæˆè´¨é‡**:
   - å¼•å…¥æ›´å¤æ‚çš„æŸå¤±å‡½æ•° (æ„ŸçŸ¥æŸå¤±ã€å¯¹æŠ—æŸå¤±)
   - ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è®­ç»ƒ

2. âœ… **æå‡å®æ—¶æ€§**:
   - æ¨¡å‹é‡åŒ– (INT8/FP16)
   - çŸ¥è¯†è’¸é¦ (å¤§æ¨¡å‹â†’å°æ¨¡å‹)

3. âœ… **å¢å¼ºåŠŸèƒ½**:
   - è¡¨æƒ…æ§åˆ¶ (AUç¼–ç )
   - å¤´éƒ¨å§¿æ€æ§åˆ¶ (3DMM)

4. âœ… **ç ”ç©¶å¼€æºå®ç°**:
   - å¯¹æ¯”æ‰‹å†™ç‰ˆæœ¬ä¸MuseTalkçš„å·®å¼‚
   - å­¦ä¹ LivePortraitçš„StitchingæŠ€å·§

---

**æ­å–œï¼** ä½ å·²ç»å®Œæˆäº†ä»é›¶æ‰‹å†™æ•°å­—äººç³»ç»Ÿçš„å…¨éƒ¨æ ¸å¿ƒæ¨¡å—ã€‚ç°åœ¨ä½ çœŸæ­£ç†è§£äº†æ•°å­—äººæŠ€æœ¯çš„åº•å±‚åŸç†ï¼Œå¯ä»¥è‡ªä¿¡åœ°é˜…è¯»ä»»ä½•å¼€æºé¡¹ç›®çš„æºç ï¼Œç”šè‡³å®ç°è‡ªå·±çš„åˆ›æ–°ç®—æ³•ã€‚

**è®°ä½**: çœŸæ­£çš„ç†è§£æ¥è‡ªäº**è‡ªå·±åŠ¨æ‰‹å®ç°**ï¼Œè€Œä¸æ˜¯è°ƒç”¨APIã€‚

---

*å…¨ä¹¦å®Œ*
