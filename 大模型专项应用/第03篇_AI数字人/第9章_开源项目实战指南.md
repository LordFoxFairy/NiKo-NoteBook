# 第9章 开源项目实战指南

> **目标**: 手把手带你运行和定制主流AI数字人开源项目
> **难度**: ⭐⭐⭐
> **预计时间**: 每个项目30-60分钟

---

## 9.1 MuseTalk实战：实时音频驱动（30+ FPS）

### 9.1.1 项目概述

**MuseTalk** 是腾讯音乐天琴实验室开发的实时数字人系统，核心特点：
- ✅ **实时性能**: 30+ FPS (Tesla V100)
- ✅ **非扩散模型**: 单步生成，延迟<100ms
- ✅ **多语言支持**: 中英日韩
- ✅ **开源商用**: MIT协议

**适用场景**: 实时直播、视频会议虚拟形象、多语言客服

---

### 9.1.2 环境搭建

#### 系统要求

```bash
# 硬件要求
GPU: NVIDIA RTX 3060+ (6GB+ VRAM)
CPU: Intel i5-10400 / AMD Ryzen 5 3600
RAM: 16GB+
存储: 15GB (代码2GB + 模型10GB + 数据3GB)

# 软件要求
操作系统: Ubuntu 20.04+ / Windows 10/11 / macOS 12+
Python: 3.10+
CUDA: 11.8 / 12.1 (如使用GPU)
FFmpeg: 4.4+
```

#### Step 1: 创建虚拟环境

```bash
# 使用conda (推荐)
conda create -n musetalk python=3.10
conda activate musetalk

# 或使用venv
python3.10 -m venv venv_musetalk
source venv_musetalk/bin/activate  # Linux/macOS
# venv_musetalk\Scripts\activate  # Windows
```

#### Step 2: 克隆仓库

```bash
git clone https://github.com/TMElyralab/MuseTalk.git
cd MuseTalk

# 检查版本（推荐使用稳定版本）
git checkout v1.5  # 或最新稳定版
```

#### Step 3: 安装依赖

```bash
# 安装PyTorch (根据CUDA版本选择)
# CUDA 11.8
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121

# CPU only (不推荐，速度慢)
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0

# 安装其他依赖
pip install -r requirements.txt

# 安装MMPose (人脸关键点检测)
pip install openmim
mim install mmengine
mim install "mmcv>=2.0.1"
mim install "mmdet>=3.1.0"
mim install "mmpose>=1.1.0"

# 安装FFmpeg (视频处理)
# Ubuntu/Debian
sudo apt-get update && sudo apt-get install -y ffmpeg

# macOS
brew install ffmpeg

# Windows: 下载并添加到PATH
# https://ffmpeg.org/download.html
```

#### Step 4: 下载预训练模型

```bash
# 自动下载脚本
python scripts/download_models.py

# 手动下载（如果自动失败）
# 1. 访问 https://huggingface.co/TMElyralab/MuseTalk
# 2. 下载模型文件到 ./models/musetalk/
# 必需文件:
#   - musetalk.json (配置文件)
#   - diffusion_pytorch_model.safetensors (UNet权重)
#   - vae/ (VAE权重目录)

# 验证模型文件
ls -lh models/musetalk/
# 应该看到:
# - musetalk.json (1KB)
# - diffusion_pytorch_model.safetensors (~1.8GB)
# - vae/ (目录)
```

#### Step 5: 验证安装

```bash
# 测试GPU可用性
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
python -c "import torch; print(f'GPU name: {torch.cuda.get_device_name(0)}')"

# 测试FFmpeg
ffmpeg -version

# 测试MMPose
python -c "from mmpose.apis import MMPoseInferencer; print('MMPose OK')"
```

---

### 9.1.3 快速开始：生成第一个数字人视频

#### 示例1: 使用官方素材

```bash
# 基础推理
python inference.py \
    --source assets/examples/portrait1.jpg \
    --audio assets/examples/speech1.wav \
    --output output_video1.mp4

# 等待1-2分钟...
# 输出: output_video1.mp4 (256x256 分辨率)
```

**查看结果**:
```bash
# Linux/macOS
ffplay output_video1.mp4

# 或使用任意视频播放器
vlc output_video1.mp4
```

#### 示例2: 使用自己的图片和音频

```bash
# 准备素材
# 1. 人像图片: my_photo.jpg (建议1024x1024, 正面清晰人脸)
# 2. 音频文件: my_audio.wav (16kHz, 单声道, WAV格式)

# 转换音频格式 (如果需要)
ffmpeg -i input.mp3 -ar 16000 -ac 1 my_audio.wav

# 运行推理
python inference.py \
    --source my_photo.jpg \
    --audio my_audio.wav \
    --output my_result.mp4 \
    --fps 25 \
    --batch_size 8
```

#### 示例3: 使用Gradio Web界面

```bash
# 启动Web UI
python app.py

# 浏览器访问: http://localhost:7860
# 界面功能:
# - 上传参考图像
# - 上传驱动音频
# - 调整参数 (batch_size, FPS等)
# - 在线预览生成结果
```

---

### 9.1.4 核心功能详解

#### 参数说明

```python
# inference.py 关键参数
python inference.py \
    --source <图像路径> \         # 必需: 参考人像图片
    --audio <音频路径> \          # 必需: 驱动音频
    --output <输出路径> \         # 必需: 输出视频路径
    --fps 25 \                    # 可选: 视频帧率 (默认25)
    --batch_size 8 \              # 可选: 批处理大小 (默认8, 越大越快但占显存)
    --face_det_batch_size 4 \     # 可选: 人脸检测批大小
    --bbox_shift 0 \              # 可选: 人脸框偏移 (-10~10, 调整裁剪范围)
    --device cuda \               # 可选: 设备 (cuda/cpu)
    --half \                      # 可选: 启用FP16推理 (减少显存50%)
    --save_mask                   # 可选: 保存人脸mask用于调试
```

#### Python API调用

```python
# api_usage.py
from musetalk import MuseTalkPipeline
import cv2

# 初始化Pipeline
pipeline = MuseTalkPipeline(
    model_path='./models/musetalk',
    device='cuda',
    precision='fp16'  # fp32/fp16
)

# 生成视频
output_path = pipeline.generate(
    reference_image='portrait.jpg',
    audio='speech.wav',
    output='result.mp4',
    fps=25,
    batch_size=8
)

print(f'Video saved to: {output_path}')

# 逐帧生成 (实时场景)
reference_image = cv2.imread('portrait.jpg')
face_data = pipeline.process_reference(reference_image)

audio_features = pipeline.extract_audio_features('speech.wav')

for t, audio_feat in enumerate(audio_features):
    frame = pipeline.generate_frame(
        face_data=face_data,
        audio_feature=audio_feat
    )

    # 处理帧 (例如通过WebRTC发送)
    cv2.imwrite(f'frame_{t:04d}.jpg', frame)
```

---

### 9.1.5 进阶技巧

#### 技巧1: 性能优化

```python
# optimize_inference.py
from musetalk import MuseTalkPipeline
import torch

# 1. 启用torch.compile加速 (PyTorch 2.0+)
pipeline = MuseTalkPipeline(model_path='./models/musetalk')
pipeline.model = torch.compile(pipeline.model, mode='reduce-overhead')

# 首次推理会编译 (慢), 后续加速20-30%
output = pipeline.generate('portrait.jpg', 'audio.wav', 'output.mp4')

# 2. CUDA图加速 (固定输入尺寸)
pipeline.enable_cuda_graph()  # 加速至~10ms/frame

# 3. 批处理优化
# 一次处理多个图像 + 同一音频
results = pipeline.batch_generate(
    reference_images=['img1.jpg', 'img2.jpg', 'img3.jpg'],
    audio='shared_audio.wav',
    batch_size=16  # 显存允许的最大值
)
```

#### 技巧2: 显存优化

```python
# low_memory_mode.py

# 方案A: 降低分辨率
config = {
    'face_size': 128,  # 默认256, 降到128可减少75%显存
    'batch_size': 4     # 减小批大小
}

# 方案B: VAE分块解码
pipeline.enable_vae_tiling(
    tile_size=512,
    overlap=64
)

# 方案C: 模型量化
from torch.quantization import quantize_dynamic
pipeline.model = quantize_dynamic(
    pipeline.model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# 方案D: Offload到CPU (牺牲速度)
pipeline.enable_sequential_cpu_offload()
```

#### 技巧3: 提升唇形同步精度

```python
# improve_lipsync.py

# 1. 确保音频质量
import librosa
audio, sr = librosa.load('input.wav', sr=16000)

# 去除静音段
from scipy.signal import savgol_filter
audio_smoothed = savgol_filter(audio, window_length=51, polyorder=3)

# 2. 调整bbox_shift参数
# bbox_shift: 正值向下移，负值向上移
# 如果嘴巴被切掉: bbox_shift=5
# 如果嘴巴太小: bbox_shift=-5

# 3. 使用更强的人脸检测器
pipeline.face_processor.detector = 'retinaface'  # 默认yolov5

# 4. 手动标注人脸区域
bbox = [x1, y1, x2, y2]  # 手动框选
output = pipeline.generate(
    reference_image='portrait.jpg',
    audio='speech.wav',
    bbox=bbox  # 跳过自动检测
)
```

#### 技巧4: 背景融合优化

```python
# paste_back_optimization.py

config = {
    # 增大羽化半径，平滑边界
    'feather_radius': 20,  # 默认10

    # 高斯模糊核大小
    'blur_kernel_size': 15,  # 默认7

    # Alpha混合权重
    'alpha': 0.95,  # 1.0=完全覆盖, 0.8=半透明

    # 颜色校正
    'color_correction': True  # 匹配原图色调
}

pipeline = MuseTalkPipeline(model_path='./models/musetalk', config=config)
```

---

### 9.1.6 常见问题排查

#### 问题1: `CUDA out of memory`

**原因**: 显存不足

**解决方案**:
```python
# 1. 降低batch_size
--batch_size 4  # 默认8

# 2. 启用FP16
--half

# 3. 降低分辨率
--face_size 128

# 4. 清理显存
import torch
torch.cuda.empty_cache()

# 5. 检查显存占用
nvidia-smi
```

#### 问题2: 人脸检测失败

**错误信息**: `No face detected in the image`

**解决方案**:
```python
# 1. 检查图片质量
# - 分辨率 >= 512x512
# - 正面清晰人脸
# - 光线充足

# 2. 调整检测阈值
pipeline.face_processor.detection_threshold = 0.5  # 默认0.7

# 3. 手动提供bbox
bbox = [100, 100, 400, 400]  # [x1, y1, x2, y2]
pipeline.generate(..., bbox=bbox)

# 4. 可视化检测结果
python debug_face_detection.py --image portrait.jpg
# 会输出标注了人脸框的调试图片
```

#### 问题3: 唇形不同步

**表现**: 嘴巴动作与声音对不上

**解决方案**:
```bash
# 1. 检查音频采样率
ffprobe audio.wav
# 必须是16kHz

# 2. 重采样音频
ffmpeg -i input.wav -ar 16000 -ac 1 output_16k.wav

# 3. 检查音频长度
# 音频时长应与视频匹配

# 4. 使用更高质量的音频
# - 无噪音
# - 清晰的人声
# - 避免多人对话
```

#### 问题4: 生成速度慢

**表现**: <5 FPS

**诊断**:
```python
import time
import torch

# 测试各环节耗时
start = time.time()
face_data = pipeline.process_reference(image)
print(f'Face processing: {time.time() - start:.2f}s')

start = time.time()
audio_feat = pipeline.extract_audio_features(audio)
print(f'Audio extraction: {time.time() - start:.2f}s')

start = time.time()
frame = pipeline.generate_frame(face_data, audio_feat[0])
print(f'Frame generation: {time.time() - start:.2f}s')
```

**优化**:
```python
# 1. 确认GPU被使用
assert torch.cuda.is_available()
assert next(pipeline.model.parameters()).is_cuda

# 2. 启用编译加速
pipeline.model = torch.compile(pipeline.model)

# 3. 增大batch_size
--batch_size 16

# 4. 检查是否有其他程序占用GPU
nvidia-smi
```

#### 问题5: 输出视频没有声音

**原因**: FFmpeg合成时音频轨丢失

**解决方案**:
```bash
# 手动合并音视频
ffmpeg -i silent_video.mp4 -i audio.wav \
    -c:v copy -c:a aac -shortest \
    final_output.mp4

# 或在代码中确保audio参数正确
pipeline.generate(
    reference_image='img.jpg',
    audio='audio.wav',  # 不能为None
    output='output.mp4'
)
```

---

### 9.1.7 实战案例：多语言虚拟主播

```python
# multilingual_vtuber.py
from musetalk import MuseTalkPipeline
from edge_tts import Communicate
import asyncio

class MultilingualVTuber:
    """
    支持中英日的虚拟主播系统
    """
    def __init__(self, avatar_image):
        self.pipeline = MuseTalkPipeline(model_path='./models/musetalk')
        self.avatar_image = avatar_image

        # TTS语音配置
        self.tts_voices = {
            'zh': 'zh-CN-XiaoxiaoNeural',
            'en': 'en-US-JennyNeural',
            'ja': 'ja-JP-NanamiNeural'
        }

    async def text_to_speech(self, text: str, language: str):
        """文本转语音"""
        voice = self.tts_voices[language]
        communicate = Communicate(text, voice)

        audio_data = b''
        async for chunk in communicate.stream():
            if chunk['type'] == 'audio':
                audio_data += chunk['data']

        # 保存音频
        audio_path = f'/tmp/tts_{language}.wav'
        with open(audio_path, 'wb') as f:
            f.write(audio_data)

        return audio_path

    async def speak(self, text: str, language: str):
        """让虚拟主播说话"""
        # 1. TTS合成语音
        audio_path = await self.text_to_speech(text, language)

        # 2. 生成面部动画
        video_path = self.pipeline.generate(
            reference_image=self.avatar_image,
            audio=audio_path,
            output=f'output_{language}.mp4'
        )

        return video_path

# 使用示例
async def main():
    vtuber = MultilingualVTuber(avatar_image='my_avatar.jpg')

    # 中文
    await vtuber.speak('大家好，我是AI虚拟主播小琴', 'zh')

    # 英文
    await vtuber.speak('Hello everyone, welcome to my channel', 'en')

    # 日语
    await vtuber.speak('皆さん、こんにちは', 'ja')

if __name__ == '__main__':
    asyncio.run(main())
```

**运行**:
```bash
pip install edge-tts
python multilingual_vtuber.py

# 输出:
# - output_zh.mp4 (中文视频)
# - output_en.mp4 (英文视频)
# - output_ja.mp4 (日语视频)
```

---

## 9.2 LivePortrait实战：高质量表情迁移

### 9.2.1 项目概述

**LivePortrait** 是快手视觉生成团队开发的肖像动画系统，特点：
- ✅ **高质量**: Stitching Network消除拼接痕迹
- ✅ **精确控制**: Retargeting局部编辑
- ✅ **运动复用**: 运动模板保护隐私
- ✅ **多模态**: 支持视频/图像/音频驱动

**适用场景**: 短视频创作、视频编辑、数字人驱动

---

### 9.2.2 快速开始

#### 安装

```bash
# 克隆仓库
git clone https://github.com/KwaiVGI/LivePortrait.git
cd LivePortrait

# 创建环境
conda create -n liveportrait python=3.10
conda activate liveportrait

# 安装依赖
pip install -r requirements.txt

# 首次运行会自动下载模型 (~5GB)
python inference.py \
    --source assets/examples/source/s6.jpg \
    --driving assets/examples/driving/d0.mp4 \
    --output output.mp4
```

#### 基础使用

```bash
# 图像驱动图像 (表情克隆)
python inference.py \
    -s portrait.jpg \
    -d expression_ref.jpg \
    --output result.jpg

# 视频驱动图像 (标准用法)
python inference.py \
    -s portrait.jpg \
    -d dance.mp4 \
    --output portrait_dance.mp4

# 音频驱动图像 (需要额外配置)
python inference.py \
    -s portrait.jpg \
    -d speech.wav \
    --flag_audio_driven \
    --output portrait_speech.mp4
```

---

### 9.2.3 核心功能

#### 功能1: 表情强度控制

```bash
# driving_multiplier: 控制表情强度
# <1.0: 减弱表情
# =1.0: 原始表情
# >1.0: 夸张表情

python inference.py \
    -s neutral_face.jpg \
    -d smile.mp4 \
    --driving_multiplier 0.5 \  # 温和的微笑
    --output gentle_smile.mp4

python inference.py \
    -s neutral_face.jpg \
    -d smile.mp4 \
    --driving_multiplier 2.0 \  # 夸张的笑容
    --output big_smile.mp4
```

#### 功能2: 局部编辑

```python
# region_control_demo.py
from liveportrait import LivePortraitPipeline
from liveportrait.utils.retargeting import RetargetingController

pipeline = LivePortraitPipeline()
controller = RetargetingController()

# 只控制嘴部，眼睛保持不动
region_control = {
    'mouth': 2.0,      # 嘴部动作放大2倍
    'eye_left': 0.0,   # 左眼静止
    'eye_right': 0.0   # 右眼静止
}

pipeline.execute(
    source='speaker.jpg',
    driving='speech.mp4',
    output='lipsync_only.mp4',
    region_control=region_control
)
```

#### 功能3: 运动模板复用

```bash
# 步骤1: 首次生成时保存运动模板
python inference.py \
    -s person1.jpg \
    -d dance.mp4 \
    --save_motion dance_motion.pkl \
    --output person1_dance.mp4

# 步骤2: 复用模板到其他人脸 (跳过motion extraction)
python inference.py \
    -s person2.jpg \
    -d dance_motion.pkl \
    --output person2_dance.mp4

python inference.py \
    -s person3.jpg \
    -d dance_motion.pkl \
    --output person3_dance.mp4

# 优势:
# - 批量处理快10倍+
# - 运动模板不含身份信息，保护隐私
```

---

### 9.2.4 进阶技巧

#### 技巧1: torch.compile加速

```python
# 启用编译加速 (PyTorch 2.0+)
from liveportrait import LivePortraitPipeline, InferenceConfig

config = InferenceConfig(
    flag_do_torch_compile=True  # 启用编译
)

pipeline = LivePortraitPipeline(inference_cfg=config)

# 首次推理会编译 (1-2分钟)
# 后续推理加速 20-30%
```

#### 技巧2: 批处理

```python
# batch_process.py
from liveportrait import LivePortraitPipeline

pipeline = LivePortraitPipeline()

# 提取驱动视频运动特征 (仅1次)
motion_dict = pipeline.extract_driving_motion('dance.mp4')

# 批量处理多个源图像
sources = ['person1.jpg', 'person2.jpg', 'person3.jpg', 'person4.jpg']

for source in sources:
    appearance_feat = pipeline.extract_appearance(source)
    output_frames = pipeline.generate_frames(appearance_feat, motion_dict)
    pipeline.save_video(output_frames, f'{source}_dance.mp4')
```

#### 技巧3: 视频编辑 (V2V)

```bash
# 编辑已有视频的表情
python inference.py \
    -s original_video.mp4 \      # 源视频
    -d expression_ref.mp4 \       # 表情参考
    --flag_video_editing_head_rotation \  # 保留原视频头部姿态
    --driving_multiplier 0.8 \    # 温和的表情迁移
    --output edited_video.mp4
```

---

### 9.2.5 常见问题

#### 问题1: 拼接痕迹明显

```bash
# 确保启用stitching
python inference.py \
    -s img.jpg \
    -d video.mp4 \
    --flag_stitching \      # 必须启用
    --flag_pasteback \      # 贴回原始背景
    --output result.mp4
```

#### 问题2: 表情不自然

```bash
# 调整driving_multiplier
--driving_multiplier 0.7  # 减弱
```

#### 问题3: 驱动视频人脸未对齐

```bash
# 启用自动裁剪
--flag_crop_driving_video
```

---

## 9.3 OpenAvatarChat实战：完整对话系统

### 9.3.1 项目概述

**OpenAvatarChat** 是完整的数字人对话系统，集成：
- VAD (语音活动检测)
- ASR (语音识别)
- LLM (对话生成)
- TTS (语音合成)
- Avatar (数字人渲染)

**特点**: 模块化、配置驱动、支持多种后端

---

### 9.3.2 快速开始

```bash
# 安装
git clone https://github.com/HumanAIGC-Engineering/OpenAvatarChat.git
cd OpenAvatarChat

# 使用uv包管理器 (比pip快10倍)
pip install uv
uv sync --all-packages

# 配置API密钥
export DASHSCOPE_API_KEY="your_key"  # 阿里云百炼
export OPENAI_API_KEY="your_key"     # OpenAI (可选)

# 选择配置文件
cp config/chat_with_openai_compatible_bailian_cosyvoice.yaml my_config.yaml

# 启动服务
uv run src/demo.py --config my_config.yaml

# 浏览器访问: http://localhost:7860
```

---

### 9.3.3 配置详解

```yaml
# my_config.yaml

# VAD (语音活动检测)
vad:
  handler: silero
  config:
    speaking_threshold: 0.5  # 语音概率阈值
    start_delay: 2048        # 开始说话延迟 (采样点)
    end_delay: 2048          # 结束说话延迟

# ASR (语音识别)
asr:
  handler: sensevoice  # sensevoice / faster_whisper / sherpa_onnx
  config:
    model_name: "iic/SenseVoiceSmall"
    device: "cuda"

# LLM (对话模型)
llm:
  handler: openai_compatible
  config:
    api_key: ${OPENAI_API_KEY}
    api_url: "https://api.openai.com/v1"
    model_name: "gpt-4o-mini"
    system_prompt: "你是一个友好的AI助手"

# TTS (语音合成)
tts:
  handler: cosyvoice_bailian  # 阿里云百炼TTS
  config:
    api_key: ${DASHSCOPE_API_KEY}
    voice_id: "longwan"  # 音色选择

# Avatar (数字人渲染)
avatar:
  handler: liteavatar
  config:
    avatar_name: "sample_data"
    fps: 25
```

---

### 9.3.4 自定义配置

#### 使用本地Ollama模型

```yaml
llm:
  handler: openai_compatible
  config:
    api_key: "ollama"
    api_url: "http://localhost:11434/v1"
    model_name: "qwen2.5:32b"
    system_prompt: "你是伊蕾娜,一个旅行中的魔女"
```

#### 使用免费Edge TTS

```yaml
tts:
  handler: edgetts
  config:
    voice: "zh-CN-XiaoxiaoNeural"  # 微软晓晓
    rate: "+0%"
    pitch: "+0Hz"
```

---

## 9.4 Open-LLM-VTuber实战：虚拟主播系统

### 9.4.1 项目概述

**Open-LLM-VTuber** 是功能最完整的虚拟主播系统：
- ✅ Live2D支持
- ✅ 视觉感知 (摄像头/截图)
- ✅ MCP工具集成
- ✅ 完全离线运行

---

### 9.4.2 快速开始

```bash
# 安装
git clone https://github.com/t41372/Open-LLM-VTuber.git
cd Open-LLM-VTuber

# 安装uv
pip install uv

# 安装依赖
uv sync

# 安装FFmpeg
sudo apt install ffmpeg  # Ubuntu
brew install ffmpeg      # macOS

# 配置
cp config_templates/conf.yaml conf.yaml
# 编辑conf.yaml配置ASR/LLM/TTS

# 启动
uv run run_server.py

# 访问: http://localhost:8000
```

---

### 9.4.3 配置示例

```yaml
# conf.yaml

# ASR
asr:
  provider: 'faster_whisper'
  model_size: 'base'
  device: 'cuda'
  language: 'zh'

# LLM
llm:
  provider: 'ollama'
  model: 'qwen2.5:7b'
  base_url: 'http://localhost:11434/v1'
  temperature: 0.7

# TTS
tts:
  provider: 'edge_tts'
  voice: 'zh-CN-XiaoxiaoNeural'

# Live2D
live2d:
  model_path: 'live2d-models/shizuku'
  scale: 0.5

# 功能开关
features:
  vision: true  # 视觉感知
  echo_cancellation: true  # 回声消除
```

---

### 9.4.4 进阶：多角色切换

```python
# 创建角色配置
# characters/character1.yaml
name: 'Shizuku'
prompt_file: 'prompts/shizuku.txt'
tts_voice: 'zh-CN-XiaoxiaoNeural'
live2d_model: 'live2d-models/shizuku'

# characters/character2.yaml
name: 'Akari'
prompt_file: 'prompts/akari.txt'
tts_voice: 'ja-JP-NanamiNeural'
live2d_model: 'live2d-models/akari'

# 动态切换
# 在Web UI中点击切换，或通过API:
curl -X POST http://localhost:8000/api/switch_character \
    -H "Content-Type: application/json" \
    -d '{"character": "akari"}'
```

---

## 9.5 性能对比总结

| 项目 | 延迟 | FPS | 显存 | 难度 | 适用场景 |
|------|------|-----|------|------|---------|
| **MuseTalk** | <100ms | 30+ | 4GB | ⭐⭐ | 实时直播、视频会议 |
| **LivePortrait** | ~300ms | 25-30 | 6GB | ⭐⭐⭐ | 视频创作、后期编辑 |
| **OpenAvatarChat** | 1-2s | 25 | 8GB | ⭐⭐⭐⭐ | 完整对话系统 |
| **Open-LLM-VTuber** | 1-2s | 30 | 10GB | ⭐⭐⭐⭐ | 虚拟主播、桌面助手 |

---

## 9.6 下一步

完成开源项目实战后，建议：
1. ✅ 选择1-2个项目深入研究
2. ✅ 阅读源码理解实现原理
3. ✅ 尝试修改和定制功能
4. ✅ 进入第10章学习从零手写实现

**核心要点**:
- 先**会用**开源项目
- 再**理解**实现原理
- 最后**自己写**完整系统

---

*下一章预告: 第10章将带你从零手写一个数字人系统，深入理解每个模块的实现细节。*