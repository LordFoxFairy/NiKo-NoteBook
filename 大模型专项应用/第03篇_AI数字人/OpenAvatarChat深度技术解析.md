# OpenAvatarChat æ·±åº¦æŠ€æœ¯è§£æ - ä»åŸç†åˆ°å®ç°

> **ç›®æ ‡**: æ·±å…¥ç†è§£OpenAvatarChatçš„å®ç°åŸç†,å¹¶èƒ½å¤Ÿè‡ªå·±åŠ¨æ‰‹æ„å»ºç±»ä¼¼ç³»ç»Ÿ
> **GitHub**: https://github.com/HumanAIGC-Engineering/OpenAvatarChat (2.8kâ­)

---

## ğŸ“‹ å­¦ä¹ ç›®æ ‡

å­¦å®Œæœ¬æ–‡æ¡£,ä½ å°†èƒ½å¤Ÿ:
- âœ… ç†è§£æ•°å­—äººå¯¹è¯ç³»ç»Ÿçš„å®Œæ•´æ•°æ®æµ
- âœ… æŒæ¡å„æ¨¡å—çš„å®ç°åŸç†
- âœ… è‡ªå·±åŠ¨æ‰‹æ­å»ºä¸€ä¸ªæ•°å­—äººç³»ç»Ÿ
- âœ… æ ¹æ®éœ€æ±‚å®šåˆ¶åŒ–å„ä¸ªæ¨¡å—

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ·±åº¦è§£æ

### 1. æ ¸å¿ƒæ•°æ®æµ

```python
# å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
ç”¨æˆ·éº¦å…‹é£è¾“å…¥ (PCMéŸ³é¢‘æµ)
  â†“
[VAD] è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (æ£€æµ‹ç”¨æˆ·æ˜¯å¦åœ¨è¯´è¯)
  â†“
[ASR] è¯­éŸ³è¯†åˆ« (éŸ³é¢‘ â†’ æ–‡æœ¬)
  â†“
[LLM] å¤§è¯­è¨€æ¨¡å‹ (ç”Ÿæˆå›å¤æ–‡æœ¬)
  â†“
[TTS] è¯­éŸ³åˆæˆ (æ–‡æœ¬ â†’ éŸ³é¢‘)
  â†“
[Avatar] æ•°å­—äººæ¸²æŸ“ (éŸ³é¢‘ â†’ å£å‹+è¡¨æƒ…)
  â†“
WebRTCè¾“å‡º (è§†é¢‘+éŸ³é¢‘æµ)
```

**å…³é”®è®¾è®¡åŸåˆ™**:
1. **æµå¼å¤„ç†**: æ¯ä¸ªç¯èŠ‚éƒ½æ”¯æŒstreaming,é™ä½å»¶è¿Ÿ
2. **æ¨¡å—è§£è€¦**: é€šè¿‡ç»Ÿä¸€çš„Handleræ¥å£,å„æ¨¡å—å¯ç‹¬ç«‹æ›¿æ¢
3. **å¼‚æ­¥æ‰§è¡Œ**: ä½¿ç”¨asyncioå®ç°å¹¶å‘å¤„ç†

### 2. Handleræ¥å£è®¾è®¡æ¨¡å¼

OpenAvatarChatçš„æ ¸å¿ƒè®¾è®¡æ˜¯**Handler Pattern**:

```python
# æ‰€æœ‰Handlerçš„åŸºç±»æ¥å£
class BaseHandler:
    def __init__(self, config: dict):
        """åˆå§‹åŒ–Handler,ä»configåŠ è½½å‚æ•°"""
        pass

    async def process(self, input_data):
        """æ ¸å¿ƒå¤„ç†é€»è¾‘,è¿”å›å¤„ç†ç»“æœ"""
        pass

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass

# ç¤ºä¾‹: ASR Handleræ¥å£
class ASRHandler(BaseHandler):
    async def process(self, audio_chunk: bytes) -> str:
        """
        è¾“å…¥: PCMéŸ³é¢‘æ•°æ®
        è¾“å‡º: è¯†åˆ«çš„æ–‡æœ¬
        """
        text = await self.recognize(audio_chunk)
        return text

# ç¤ºä¾‹: LLM Handleræ¥å£
class LLMHandler(BaseHandler):
    async def process(self, text: str) -> str:
        """
        è¾“å…¥: ç”¨æˆ·æ–‡æœ¬
        è¾“å‡º: AIå›å¤æ–‡æœ¬
        """
        response = await self.generate(text)
        return response
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡?**

- âœ… **å¯æ›¿æ¢æ€§**: æƒ³æ¢æ¨¡å‹?åªéœ€å®ç°æ–°çš„Handler
- âœ… **å¯æµ‹è¯•æ€§**: æ¯ä¸ªHandlerå¯ä»¥å•ç‹¬æµ‹è¯•
- âœ… **é…ç½®é©±åŠ¨**: é€šè¿‡YAMLé…ç½®é€‰æ‹©Handlerå®ç°

### 3. é…ç½®é©±åŠ¨æ¶æ„

```yaml
# config/chat_with_qwen_omni.yaml ç¤ºä¾‹
client:
  handler: client_handler_rtc  # é€‰æ‹©WebRTCå®¢æˆ·ç«¯
  config:
    server_url: "https://localhost:7860"

vad:
  handler: silero  # é€‰æ‹©Silero VAD
  config:
    speaking_threshold: 0.5
    start_delay: 2048

asr:
  handler: sensevoice  # é€‰æ‹©SenseVoice
  config:
    model_name: "iic/SenseVoiceSmall"

llm:
  handler: qwen_omni  # é€‰æ‹©Qwen-Omni
  config:
    api_key: ${DASHSCOPE_API_KEY}
    model_name: "qwen-audio-chat"

tts:
  handler: cosyvoice_bailian  # é€‰æ‹©ç™¾ç‚¼TTS
  config:
    voice_id: "longwan"

avatar:
  handler: liteavatar  # é€‰æ‹©LiteAvatar
  config:
    avatar_name: "sample_data"
    fps: 25
```

**æ ¸å¿ƒæ€æƒ³**:
```python
# æ ¹æ®é…ç½®åŠ¨æ€åŠ è½½Handler
def load_handler(handler_type: str, config: dict):
    # åŠ¨æ€å¯¼å…¥å¯¹åº”çš„Handlerç±»
    module_path = f"{handler_type}/{config['handler']}"
    HandlerClass = import_module(module_path).Handler

    # åˆå§‹åŒ–å¹¶è¿”å›
    return HandlerClass(config['config'])

# ä½¿ç”¨
vad_handler = load_handler('vad', config['vad'])
asr_handler = load_handler('asr', config['asr'])
llm_handler = load_handler('llm', config['llm'])
```

---

## ğŸ”¬ å„æ¨¡å—æ·±åº¦å®ç°

### æ¨¡å—1: VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹)

#### ä¸ºä»€ä¹ˆéœ€è¦VAD?

```
é—®é¢˜: å¦‚ä½•çŸ¥é“ç”¨æˆ·è¯´å®Œäº†?
- ä¸èƒ½ä¸€ç›´ç­‰(ç”¨æˆ·å¯èƒ½æ€è€ƒ)
- ä¸èƒ½å¤ªå¿«æ‰“æ–­(å¯èƒ½åªæ˜¯åœé¡¿)
- éœ€è¦å®æ—¶æ£€æµ‹è¯­éŸ³æ´»åŠ¨
```

#### Silero VADåŸç†

```python
# vad/silerovad/vad_handler.py æ ¸å¿ƒå®ç°
import torch

class SileroVAD:
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (å°æ¨¡å‹,CPUå°±èƒ½è·‘)
        self.model, self.utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        self.get_speech_timestamps = self.utils[0]

    def detect(self, audio_chunk: torch.Tensor) -> bool:
        """
        æ£€æµ‹è¿™æ®µéŸ³é¢‘æ˜¯å¦åŒ…å«è¯­éŸ³

        å‚æ•°:
            audio_chunk: [1, num_samples] 16kHz PCM
        è¿”å›:
            True/False (æ˜¯å¦æœ‰è¯­éŸ³)
        """
        # æ¨¡å‹è¾“å‡ºè¯­éŸ³æ¦‚ç‡ (0-1)
        speech_prob = self.model(audio_chunk, 16000).item()

        # å¤§äºé˜ˆå€¼è®¤ä¸ºæ˜¯è¯­éŸ³
        return speech_prob > 0.5
```

#### çŠ¶æ€æœºè®¾è®¡

```python
class VADStateMachine:
    """
    çŠ¶æ€æœº:
    - IDLE: ç­‰å¾…è¯­éŸ³
    - SPEAKING: æ£€æµ‹åˆ°è¯­éŸ³
    - SILENCE: è¯­éŸ³ç»“æŸ,ç­‰å¾…ç¡®è®¤
    """

    def __init__(self, start_delay=2048, end_delay=2048):
        self.state = 'IDLE'
        self.start_delay = start_delay  # è¿ç»­å¤šå°‘é‡‡æ ·ç‚¹æ‰ç®—å¼€å§‹è¯´è¯
        self.end_delay = end_delay      # è¿ç»­å¤šå°‘é‡‡æ ·ç‚¹æ‰ç®—è¯´å®Œ
        self.speaking_buffer = []
        self.silence_counter = 0

    def process_chunk(self, audio_chunk, is_speech: bool):
        if self.state == 'IDLE':
            if is_speech:
                self.speaking_buffer.append(audio_chunk)
                if len(self.speaking_buffer) * 512 > self.start_delay:
                    self.state = 'SPEAKING'
                    print("ğŸ¤ User started speaking")

        elif self.state == 'SPEAKING':
            self.speaking_buffer.append(audio_chunk)

            if not is_speech:
                self.silence_counter += 512
                if self.silence_counter > self.end_delay:
                    self.state = 'IDLE'
                    print("âœ… User finished speaking")

                    # è¿”å›å®Œæ•´éŸ³é¢‘ç”¨äºASR
                    full_audio = concatenate(self.speaking_buffer)
                    self.speaking_buffer = []
                    self.silence_counter = 0
                    return full_audio
            else:
                self.silence_counter = 0

        return None
```

**å®æˆ˜æŠ€å·§**:
- `start_delay`: è®¾å¤ªå°å®¹æ˜“è¯¯è§¦å‘,å¤ªå¤§ç”¨æˆ·æ„Ÿè§‰å¡é¡¿
- `end_delay`: è®¾å¤ªå°å®¹æ˜“æ‰“æ–­ç”¨æˆ·,å¤ªå¤§å“åº”æ…¢
- æ¨èå€¼: éƒ½è®¾2048 (16kHzä¸‹çº¦128ms)

---

### æ¨¡å—2: ASR (è¯­éŸ³è¯†åˆ«)

#### SenseVoice vs Whisperå¯¹æ¯”

```python
# OpenAvatarChatä½¿ç”¨SenseVoice,ä¸ºä»€ä¹ˆ?

# SenseVoiceä¼˜åŠ¿:
advantages = {
    "å¤šè¯­è¨€": "æ”¯æŒä¸­è‹±æ—¥ç²¤éŸ©",
    "æƒ…æ„Ÿè¯†åˆ«": "èƒ½æ£€æµ‹æƒ…ç»ª(é«˜å…´/æ„¤æ€’/æ‚²ä¼¤ç­‰)",
    "äº‹ä»¶æ£€æµ‹": "æŒå£°/éŸ³ä¹/ç¬‘å£°ç­‰",
    "é€Ÿåº¦": "å°æ¨¡å‹,æ¨ç†å¿«",
    "å¼€æº": "é˜¿é‡Œå¼€æº,ä¸­æ–‡ä¼˜åŒ–å¥½"
}

# Whisperä¼˜åŠ¿:
whisper_advantages = {
    "å‡†ç¡®ç‡": "å¤§æ¨¡å‹å‡†ç¡®ç‡æ›´é«˜",
    "é²æ£’æ€§": "å™ªéŸ³ç¯å¢ƒæ›´ç¨³å®š",
    "ç¤¾åŒº": "OpenAIå®˜æ–¹,ç”Ÿæ€å®Œå–„"
}
```

#### SenseVoiceå®ç°

```python
# asr/sensevoice/asr_handler_sensevoice.py
from modelscope.pipelines import pipeline

class SenseVoiceHandler:
    def __init__(self, model_name="iic/SenseVoiceSmall"):
        self.pipeline = pipeline(
            task="auto-speech-recognition",
            model=model_name,
            model_revision="master",
            device="cuda:0"  # æˆ–cpu
        )

    async def transcribe(self, audio_data: bytes) -> dict:
        """
        è¯†åˆ«éŸ³é¢‘

        è¿”å›:
        {
            "text": "ç”¨æˆ·è¯´çš„è¯",
            "language": "zh",  # è‡ªåŠ¨æ£€æµ‹çš„è¯­è¨€
            "emotion": "happy",  # æƒ…ç»ª
            "event": None  # èƒŒæ™¯äº‹ä»¶
        }
        """
        # audio_dataæ˜¯PCMå­—èŠ‚æµ,éœ€è¦è½¬æ¢
        import numpy as np
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        audio_array = audio_array.astype(np.float32) / 32768.0

        # æ¨ç†
        result = self.pipeline(
            audio_in=audio_array,
            sampling_rate=16000,
            language="auto",  # è‡ªåŠ¨æ£€æµ‹
            use_itn=True  # åå‘æ–‡æœ¬å½’ä¸€åŒ–(æŠŠ"ä¸€åƒ"è½¬æˆ"1000")
        )

        return {
            "text": result['text'],
            "language": result.get('language', 'zh'),
            "emotion": result.get('emotion'),
            "event": result.get('event')
        }
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# 1. æ¨¡å‹é‡åŒ– (int8,é€Ÿåº¦æå‡2-3å€)
from modelscope import AutoModel
model = AutoModel.from_pretrained(
    "iic/SenseVoiceSmall",
    quantization_config={"bits": 8}
)

# 2. æ‰¹å¤„ç† (å¦‚æœæœ‰å¤šè·¯éŸ³é¢‘)
results = self.pipeline([audio1, audio2, audio3])

# 3. ç¼“å­˜é¢„åŠ è½½ (é¿å…å†·å¯åŠ¨æ…¢)
@lru_cache(maxsize=1)
def get_asr_model():
    return SenseVoiceHandler()
```

---

### æ¨¡å—3: LLM (å¤§è¯­è¨€æ¨¡å‹)

#### å¤šæ¨¡æ€LLM: Qwen-Omni

OpenAvatarChatæ”¯æŒ**Qwen-Omni**,è¿™æ˜¯ä»€ä¹ˆ?

```python
# Qwen-Omniç‰¹ç‚¹:
qwen_omni = {
    "è¾“å…¥": "éŸ³é¢‘ + æ–‡æœ¬ + å›¾åƒ",
    "è¾“å‡º": "æ–‡æœ¬ + éŸ³é¢‘",  # å¯ä»¥ç›´æ¥è¾“å‡ºè¯­éŸ³!
    "ä¼˜åŠ¿": "è·³è¿‡TTSç¯èŠ‚,ç«¯åˆ°ç«¯ç”Ÿæˆ",
    "å»¶è¿Ÿ": "æ¯”ä¼ ç»Ÿpipelineå¿«1-2ç§’"
}

# ä¼ ç»Ÿpipeline:
# ASR â†’ æ–‡æœ¬LLM â†’ TTS (3ä¸ªæ¨¡å‹,3æ¬¡æ¨ç†)

# Qwen-Omni:
# éŸ³é¢‘ â†’ å¤šæ¨¡æ€LLM â†’ éŸ³é¢‘ (1ä¸ªæ¨¡å‹,1æ¬¡æ¨ç†)
```

#### Qwen-Omni APIè°ƒç”¨

```python
# llm/qwen_omni/llm_handler_qwen_omni.py
from openai import OpenAI

class QwenOmniHandler:
    def __init__(self, api_key, model_name="qwen-audio-chat"):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model_name = model_name

    async def chat_with_audio(
        self,
        user_audio: bytes,
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"
    ) -> dict:
        """
        ç›´æ¥ç”¨éŸ³é¢‘å¯¹è¯

        è¿”å›:
        {
            "text": "å›å¤æ–‡æœ¬",
            "audio": "å›å¤éŸ³é¢‘çš„base64"  # å¦‚æœæ¨¡å‹æ”¯æŒ
        }
        """
        import base64
        audio_b64 = base64.b64encode(user_audio).decode()

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "audio_url",
                            "audio_url": {
                                "url": f"data:audio/pcm;base64,{audio_b64}"
                            }
                        }
                    ]
                }
            ],
            stream=False  # æˆ–Trueå®ç°æµå¼
        )

        return {
            "text": response.choices[0].message.content,
            "audio": None  # Qwen-Omniå¯èƒ½ç›´æ¥è¿”å›éŸ³é¢‘
        }
```

#### MiniCPM-o æœ¬åœ°éƒ¨ç½²

```python
# llm/minicpm/llm_handler_minicpm.py
from transformers import AutoModel, AutoTokenizer
import torch

class MiniCPMHandler:
    def __init__(self, model_path="openbmb/MiniCPM-o-2_6"):
        # åŠ è½½æ¨¡å‹ (éœ€è¦20GB+æ˜¾å­˜,æœªé‡åŒ–)
        self.model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…GPU
            torch_dtype=torch.float16  # åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )

        # Int4é‡åŒ– (æ˜¾å­˜é™åˆ°<10GB)
        # self.model = AutoModel.from_pretrained(
        #     model_path,
        #     trust_remote_code=True,
        #     load_in_4bit=True  # éœ€è¦bitsandbytesåº“
        # )

    @torch.no_grad()
    def generate_response(
        self,
        user_text: str,
        image=None,  # PIL.Imageå¯é€‰
        audio=None,  # numpy arrayå¯é€‰
        max_new_tokens=512
    ) -> str:
        """
        å¤šæ¨¡æ€ç”Ÿæˆ
        """
        # æ„å»ºè¾“å…¥
        inputs = []
        if audio is not None:
            inputs.append({"type": "audio", "data": audio})
        if image is not None:
            inputs.append({"type": "image", "data": image})
        inputs.append({"type": "text", "data": user_text})

        # Tokenize
        input_ids = self.tokenizer.apply_chat_template(
            inputs,
            return_tensors="pt"
        ).to(self.model.device)

        # ç”Ÿæˆ
        output_ids = self.model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7
        )

        # è§£ç 
        response = self.tokenizer.decode(
            output_ids[0][len(input_ids[0]):],
            skip_special_tokens=True
        )

        return response
```

---

### æ¨¡å—4: TTS (è¯­éŸ³åˆæˆ)

#### CosyVoiceå®ç°åŸç†

```python
# tts/cosyvoice/tts_handler_cosyvoice.py
from cosyvoice.cli.cosyvoice import CosyVoice

class CosyVoiceHandler:
    def __init__(self, model_dir="iic/CosyVoice-300M"):
        """
        CosyVoiceç‰¹ç‚¹:
        - é›¶æ ·æœ¬è¯­éŸ³å…‹éš† (ç»™3-10ç§’éŸ³é¢‘å°±èƒ½å…‹éš†éŸ³è‰²)
        - å¤šè¯­è¨€æ”¯æŒ (ä¸­è‹±æ—¥éŸ©)
        - æƒ…æ„Ÿæ§åˆ¶
        """
        self.model = CosyVoice(model_dir)

    def synthesize(
        self,
        text: str,
        speaker: str = "ä¸­æ–‡å¥³",  # é¢„ç½®éŸ³è‰²
        speed: float = 1.0
    ) -> bytes:
        """
        åˆæˆè¯­éŸ³

        è¿”å›: PCMéŸ³é¢‘å­—èŠ‚æµ
        """
        # æ¨ç†
        output = self.model.inference_sft(
            text=text,
            speaker=speaker,
            speed=speed
        )

        # outputæ˜¯è¿­ä»£å™¨,éœ€è¦æ‹¼æ¥
        audio_chunks = []
        for chunk in output:
            audio_chunks.append(chunk['tts_speech'])

        import numpy as np
        audio = np.concatenate(audio_chunks)

        # è½¬æ¢ä¸ºPCM int16
        audio_int16 = (audio * 32767).astype(np.int16)
        return audio_int16.tobytes()

    def clone_voice(
        self,
        text: str,
        prompt_audio: bytes,  # 3-10ç§’å‚è€ƒéŸ³é¢‘
        prompt_text: str  # å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬
    ) -> bytes:
        """
        é›¶æ ·æœ¬å…‹éš†éŸ³è‰²
        """
        import numpy as np
        prompt_array = np.frombuffer(prompt_audio, dtype=np.int16)
        prompt_array = prompt_array.astype(np.float32) / 32768.0

        output = self.model.inference_zero_shot(
            text=text,
            prompt_speech_16k=prompt_array,
            prompt_text=prompt_text
        )

        audio_chunks = []
        for chunk in output:
            audio_chunks.append(chunk['tts_speech'])

        audio = np.concatenate(audio_chunks)
        audio_int16 = (audio * 32767).astype(np.int16)
        return audio_int16.tobytes()
```

#### Edge TTS (å…è´¹æ–¹æ¡ˆ)

```python
# tts/edgetts/tts_handler_edgetts.py
import edge_tts
import asyncio

class EdgeTTSHandler:
    """
    å¾®è½¯Edgeæµè§ˆå™¨å†…ç½®çš„TTS,å®Œå…¨å…è´¹!

    ä¼˜ç‚¹:
    - å…è´¹æ— é™åˆ¶
    - éŸ³è´¨å¥½
    - å¤šè¯­è¨€å¤šéŸ³è‰²

    ç¼ºç‚¹:
    - éœ€è¦è”ç½‘
    - ä¸èƒ½å…‹éš†éŸ³è‰²
    """

    def __init__(self, voice="zh-CN-XiaoxiaoNeural"):
        self.voice = voice

    async def synthesize(self, text: str) -> bytes:
        """
        åˆæˆè¯­éŸ³
        """
        communicate = edge_tts.Communicate(
            text=text,
            voice=self.voice,
            rate="+0%",  # è¯­é€Ÿè°ƒæ•´
            pitch="+0Hz"  # éŸ³è°ƒè°ƒæ•´
        )

        audio_chunks = []
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_chunks.append(chunk["data"])

        return b"".join(audio_chunks)
```

---

### æ¨¡å—5: Avatar (æ•°å­—äººæ¸²æŸ“)

#### LiteAvatar - 2Då®æ—¶æ•°å­—äºº

```python
# avatar/liteavatar/avatar_handler_liteavatar.py

class LiteAvatarHandler:
    """
    LiteAvataråŸç†:
    1. è¾“å…¥: éŸ³é¢‘ç‰¹å¾
    2. è¾“å‡º: äººè„¸å…³é”®ç‚¹ (68ä¸ªlandmarks)
    3. æ¸²æŸ“: é€šè¿‡å…³é”®ç‚¹é©±åŠ¨2Då›¾ç‰‡å˜å½¢

    ä¼˜åŠ¿:
    - CPUå°±èƒ½è·‘ (i9-13980HXè¾¾åˆ°30FPS)
    - å»¶è¿Ÿä½
    - æ•ˆæœè‡ªç„¶
    """

    def __init__(
        self,
        avatar_name: str = "sample_data",
        fps: int = 25,
        use_gpu: bool = True
    ):
        # åŠ è½½æ¨¡å‹
        from liteavatar import LiteAvatar
        self.model = LiteAvatar(
            avatar_path=f"assets/{avatar_name}",
            device="cuda" if use_gpu else "cpu"
        )
        self.fps = fps

    async def render_from_audio(
        self,
        audio_chunk: bytes,  # æ¯æ¬¡ä¼ å…¥ä¸€å°æ®µéŸ³é¢‘
        emotion: str = "neutral"  # æƒ…ç»ª:neutral/happy/sad/angry
    ) -> bytes:
        """
        ä»éŸ³é¢‘ç”Ÿæˆä¸€å¸§å›¾åƒ

        è¿”å›: JPEGå›¾åƒå­—èŠ‚æµ
        """
        import numpy as np

        # éŸ³é¢‘ç‰¹å¾æå–
        audio_array = np.frombuffer(audio_chunk, dtype=np.int16)
        audio_features = self.extract_audio_features(audio_array)

        # ç”Ÿæˆäººè„¸å…³é”®ç‚¹
        landmarks = self.model.predict_landmarks(
            audio_features,
            emotion=emotion
        )

        # æ¸²æŸ“å›¾åƒ
        frame = self.model.render(landmarks)

        # è½¬JPEG
        import cv2
        _, jpeg_bytes = cv2.imencode('.jpg', frame)
        return jpeg_bytes.tobytes()

    def extract_audio_features(self, audio: np.ndarray):
        """
        æå–éŸ³é¢‘ç‰¹å¾ (Melé¢‘è°±ç­‰)
        """
        import librosa

        # è½¬float32
        audio_float = audio.astype(np.float32) / 32768.0

        # æå–Melé¢‘è°±
        mel = librosa.feature.melspectrogram(
            y=audio_float,
            sr=16000,
            n_mels=80
        )

        return mel
```

#### MuseTalk - è§†é¢‘é©±åŠ¨

```python
# avatar/musetalk/avatar_handler_musetalk.py

class MuseTalkHandler:
    """
    MuseTalkåŸç†:
    1. å‡†å¤‡ä¸€æ®µ"åº•ç‰ˆè§†é¢‘" (ç”¨æˆ·å½•åˆ¶çš„10ç§’è§†é¢‘)
    2. æ ¹æ®æ–°éŸ³é¢‘,æ›¿æ¢è§†é¢‘ä¸­çš„å˜´éƒ¨
    3. ä¿æŒå¤´éƒ¨å§¿æ€ã€çœ¼ç¥ã€èƒŒæ™¯ä¸å˜

    ä¼˜åŠ¿:
    - é«˜åº¦å†™å®
    - ä¿ç•™åŸè§†é¢‘é£æ ¼

    ç¼ºç‚¹:
    - éœ€è¦åº•ç‰ˆè§†é¢‘
    - ç®—åŠ›è¦æ±‚é«˜ (éœ€è¦GPU)
    """

    def __init__(
        self,
        video_path: str,  # åº•ç‰ˆè§†é¢‘è·¯å¾„
        bbox_shift: int = 0,
        fps: int = 25,
        batch_size: int = 8
    ):
        from musetalk import MuseTalk

        # åŠ è½½æ¨¡å‹
        self.model = MuseTalk()

        # é¢„å¤„ç†åº•ç‰ˆè§†é¢‘ (æå–äººè„¸åŒºåŸŸ)
        self.base_video_coords = self.model.prepare_video(
            video_path,
            bbox_shift=bbox_shift
        )

        self.fps = fps
        self.batch_size = batch_size

    async def generate_video(
        self,
        audio_path: str  # æ–°çš„éŸ³é¢‘æ–‡ä»¶
    ) -> str:
        """
        ç”Ÿæˆæ–°è§†é¢‘

        è¿”å›: è§†é¢‘æ–‡ä»¶è·¯å¾„
        """
        # æ¨ç† (æ‰¹å¤„ç†åŠ é€Ÿ)
        output_frames = self.model.inference(
            audio_path=audio_path,
            video_coords=self.base_video_coords,
            batch_size=self.batch_size
        )

        # ä¿å­˜è§†é¢‘
        import cv2
        output_path = "output.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(
            output_path,
            fourcc,
            self.fps,
            (output_frames[0].shape[1], output_frames[0].shape[0])
        )

        for frame in output_frames:
            writer.write(frame)

        writer.release()
        return output_path
```

---

## ğŸ”§ å®æˆ˜: æ­å»ºè‡ªå·±çš„æ•°å­—äººç³»ç»Ÿ

### æ­¥éª¤1: æœ€å°å¯è¿è¡Œç³»ç»Ÿ

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/HumanAIGC-Engineering/OpenAvatarChat.git
cd OpenAvatarChat

# 2. å®‰è£…ä¾èµ– (ä½¿ç”¨uvåŒ…ç®¡ç†å™¨,æ¯”pipå¿«å¾ˆå¤š)
pip install uv
uv sync --all-packages

# 3. é€‰æ‹©æœ€ç®€å•çš„é…ç½® (äº‘ç«¯API,æ— éœ€GPU)
cp config/chat_with_openai_compatible_bailian_cosyvoice.yaml my_config.yaml

# 4. é…ç½®APIå¯†é’¥
export DASHSCOPE_API_KEY="your_api_key"  # é˜¿é‡Œäº‘ç™¾ç‚¼
export OPENAI_API_KEY="your_openai_key"  # æˆ–å…¶ä»–å…¼å®¹API

# 5. è¿è¡Œ
uv run src/demo.py --config my_config.yaml
```

### æ­¥éª¤2: ç†è§£é…ç½®æ–‡ä»¶

```yaml
# my_config.yaml é€è¡Œè§£æ

# WebRTCå®¢æˆ·ç«¯ (è´Ÿè´£éŸ³è§†é¢‘æµä¼ è¾“)
client:
  handler: client_handler_rtc
  config:
    server_url: "https://localhost:7860"  # æœ¬æœºè®¿é—®
    # å¦‚æœå±€åŸŸç½‘è®¿é—®,éœ€è¦é…ç½®SSLè¯ä¹¦

# VAD (æ£€æµ‹ç”¨æˆ·æ˜¯å¦è¯´è¯)
vad:
  handler: silero
  config:
    speaking_threshold: 0.5  # è¯­éŸ³æ¦‚ç‡é˜ˆå€¼ (0-1)
    start_delay: 2048  # å¼€å§‹è¯´è¯å»¶è¿Ÿ (é‡‡æ ·ç‚¹æ•°)
    end_delay: 2048  # ç»“æŸè¯´è¯å»¶è¿Ÿ

# ASR (è¯­éŸ³è½¬æ–‡å­—)
asr:
  handler: sensevoice
  config:
    model_name: "iic/SenseVoiceSmall"
    # é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (~500MB)

# LLM (å¯¹è¯æ¨¡å‹)
llm:
  handler: openai_compatible
  config:
    api_key: ${OPENAI_API_KEY}
    api_url: "https://api.openai.com/v1"
    model_name: "gpt-4o-mini"  # æˆ–å…¶ä»–æ¨¡å‹
    system_prompt: "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"

# TTS (æ–‡å­—è½¬è¯­éŸ³)
tts:
  handler: cosyvoice_bailian  # é˜¿é‡Œäº‘ç™¾ç‚¼API
  config:
    api_key: ${DASHSCOPE_API_KEY}
    voice_id: "longwan"  # éŸ³è‰²é€‰æ‹©

# Avatar (æ•°å­—äººæ¸²æŸ“)
avatar:
  handler: liteavatar
  config:
    avatar_name: "sample_data"  # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    fps: 25
    use_gpu: true  # CPUä¹Ÿèƒ½è·‘,ä½†æ…¢
```

### æ­¥éª¤3: è‡ªå®šä¹‰LLM

```yaml
# ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹
llm:
  handler: openai_compatible
  config:
    api_key: "ollama"  # éšä¾¿å¡«
    api_url: "http://localhost:11434/v1"  # Ollamaé»˜è®¤ç«¯å£
    model_name: "qwen2.5:32b"  # æˆ–å…¶ä»–æœ¬åœ°æ¨¡å‹
    system_prompt: "ä½ æ˜¯ä¼Šè•¾å¨œ,ä¸€ä¸ªæ—…è¡Œä¸­çš„é­”å¥³"  # è‡ªå®šä¹‰äººè®¾
```

### æ­¥éª¤4: è‡ªå®šä¹‰Avatar

```python
# åˆ›å»ºè‡ªå·±çš„Avatar Handler

# avatar/my_custom/avatar_handler_custom.py
class MyCustomAvatarHandler:
    """
    è‡ªå®šä¹‰Avatarå®ç°

    éœ€æ±‚:
    1. æœ‰ä¸€å¼ è§’è‰²å›¾ç‰‡ (PNG,åŒ…å«é€æ˜é€šé“)
    2. æˆ–è€…æœ‰Live2Dæ¨¡å‹
    """

    def __init__(self, config: dict):
        self.image_path = config['image_path']
        self.fps = config.get('fps', 25)

        # åŠ è½½å›¾ç‰‡
        from PIL import Image
        self.base_image = Image.open(self.image_path)

    async def render(self, audio_chunk: bytes) -> bytes:
        """
        æ ¹æ®éŸ³é¢‘ç”Ÿæˆä¸€å¸§

        ç®€å•å®ç°: æ£€æµ‹éŸ³é‡,å˜´å·´å¼ å¼€/é—­åˆ
        """
        import numpy as np

        # è®¡ç®—éŸ³é‡
        audio_array = np.frombuffer(audio_chunk, dtype=np.int16)
        volume = np.abs(audio_array).mean()

        # å¦‚æœéŸ³é‡å¤§,å˜´å·´å¼ å¼€ (è¿™é‡Œç®€åŒ–å¤„ç†)
        if volume > 1000:
            # å®é™…åº”è¯¥æ ¹æ®éŸ³ç´ ç”Ÿæˆå˜´å‹
            mouth_open = True
        else:
            mouth_open = False

        # æ¸²æŸ“ (è¿™é‡Œçœç•¥å®é™…çš„å›¾åƒå¤„ç†)
        frame = self.render_frame(mouth_open)

        return frame
```

### æ­¥éª¤5: ä¼˜åŒ–å»¶è¿Ÿ

```python
# æ ¸å¿ƒä¼˜åŒ–: æµå¼å¤„ç†

class StreamingPipeline:
    """
    ä¼˜åŒ–å‰:
    ç”¨æˆ·è¯´å®Œ â†’ ASR(1s) â†’ LLM(2s) â†’ TTS(1s) â†’ Avatar(0.5s)
    æ€»å»¶è¿Ÿ: 4.5ç§’

    ä¼˜åŒ–å:
    ASRæµå¼è¾“å‡º â†’ LLMæµå¼ç”Ÿæˆ â†’ TTSæµå¼åˆæˆ â†’ Avataræµå¼æ¸²æŸ“
    æ€»å»¶è¿Ÿ: 1-2ç§’ (åªéœ€ç­‰ç¬¬ä¸€ä¸ªtoken)
    """

    async def process_streaming(self, audio_input):
        # 1. ASRæµå¼è¯†åˆ«
        async for partial_text in self.asr.stream(audio_input):

            # 2. LLMæµå¼ç”Ÿæˆ (ä¸ç­‰ASRå®Œæˆ)
            async for token in self.llm.stream(partial_text):

                # 3. ç´¯ç§¯åˆ°å¥å­çº§åˆ«
                if self.is_sentence_end(token):
                    sentence = self.buffer + token

                    # 4. TTSæµå¼åˆæˆ
                    async for audio_chunk in self.tts.stream(sentence):

                        # 5. Avatarå®æ—¶æ¸²æŸ“
                        frame = await self.avatar.render(audio_chunk)

                        # 6. ç«‹å³è¾“å‡º (WebRTC)
                        await self.send_frame(frame)

                    self.buffer = ""
                else:
                    self.buffer += token
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### OpenAvatarChatå®˜æ–¹æ•°æ®

```
æµ‹è¯•ç¯å¢ƒ:
- CPU: i9-13900KF
- GPU: RTX 4090
- é…ç½®: chat_with_minicpm.yaml

å»¶è¿Ÿåˆ†æ:
- VADæ£€æµ‹: ~100ms
- ASR (SenseVoice): ~200ms
- LLM (MiniCPM-o): ~800ms
- TTS (CosyVoice): ~500ms
- Avatar (LiteAvatar): ~300ms
- RTCä¼ è¾“: ~300ms

æ€»è®¡: ~2.2ç§’
```

### ä¼˜åŒ–ç›®æ ‡

```
ç›®æ ‡: é™åˆ°1ç§’ä»¥å†…

ä¼˜åŒ–æ‰‹æ®µ:
1. ä½¿ç”¨Qwen-Omni (è·³è¿‡TTS)
2. VADå‚æ•°è°ƒä¼˜ (å‡å°‘end_delay)
3. LLMç”¨å°æ¨¡å‹ (Qwen2.5-7B)
4. å¯ç”¨æµå¼å¤„ç†
5. æ¨¡å‹é‡åŒ– (int8/int4)

é¢„æœŸ:
- VAD: ~50ms
- ASR: ~150ms
- Qwen-Omni: ~500ms (ç›´æ¥è¾“å‡ºéŸ³é¢‘)
- Avatar: ~200ms
- RTC: ~200ms

æ€»è®¡: ~1.1ç§’
```

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦SSLè¯ä¹¦?

```
A: WebRTCè¦æ±‚HTTPSæ‰èƒ½è®¿é—®éº¦å…‹é£
- localhostå¯ä»¥ä¸ç”¨
- å±€åŸŸç½‘/å…¬ç½‘å¿…é¡»ç”¨HTTPS
- å¯ä»¥ç”¨è‡ªç­¾åè¯ä¹¦ (ä½†æµè§ˆå™¨ä¼šè­¦å‘Š)
```

### Q2: TURNæœåŠ¡å™¨æ˜¯ä»€ä¹ˆ?

```
A: NATç©¿é€æœåŠ¡å™¨

åœºæ™¯:
- å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¸åœ¨åŒä¸€å±€åŸŸç½‘
- é˜²ç«å¢™é˜»æ­¢ç›´è¿
- éœ€è¦ä¸­ç»§æœåŠ¡å™¨è½¬å‘æµé‡

å…è´¹TURN:
- Google STUN: stun:stun.l.google.com:19302
- Coturn (è‡ªå»º)
```

### Q3: å¦‚ä½•é™ä½æ˜¾å­˜å ç”¨?

```python
# 1. æ¨¡å‹é‡åŒ–
model = AutoModel.from_pretrained(
    "model_name",
    load_in_4bit=True  # 20GB â†’ 5GB
)

# 2. å‡å°‘batch_size
config['avatar']['batch_size'] = 1

# 3. ä½¿ç”¨å°æ¨¡å‹
# MiniCPM-o-2_6 (20GB) â†’ Qwen2.5-7B (7GB)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **æ·±å…¥AvataræŠ€æœ¯**
   - ç ”ç©¶LiteAvatarè®ºæ–‡
   - å­¦ä¹ MuseTalkå®ç°
   - å°è¯•LivePortraité›†æˆ

2. **ä¼˜åŒ–å®æ—¶æ€§**
   - å®ç°å®Œæ•´çš„æµå¼pipeline
   - ç ”ç©¶WebRTCä¼˜åŒ–
   - å­¦ä¹ TURNæœåŠ¡å™¨æ­å»º

3. **å•†ä¸šåŒ–è€ƒè™‘**
   - Live2Då•†ä¸šæˆæƒ
   - éŸ³è‰²ç‰ˆæƒé—®é¢˜
   - äº‘ç«¯éƒ¨ç½²æ–¹æ¡ˆ

---

**æ€»ç»“**: OpenAvatarChatæ˜¯ä¸€ä¸ª**æ¨¡å—åŒ–ã€å¯æ‰©å±•**çš„æ•°å­—äººå¯¹è¯ç³»ç»Ÿã€‚ç†è§£å…¶Handleræ¨¡å¼å,ä½ å¯ä»¥è½»æ¾æ›¿æ¢ä»»ä½•æ¨¡å—,æ‰“é€ è‡ªå·±çš„æ•°å­—äºº!

**æœ€åæ›´æ–°**: 2025-11-20
