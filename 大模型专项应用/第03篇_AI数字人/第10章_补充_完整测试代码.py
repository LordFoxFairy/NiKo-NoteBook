#!/usr/bin/env python3
"""
ç¬¬10ç« å®Œæ•´æµ‹è¯•ä»£ç 
éªŒè¯æ‰€æœ‰æ¨¡å—çš„æ­£ç¡®æ€§ï¼Œå¹¶æä¾›è¿è¡Œç¤ºä¾‹
"""

import numpy as np
import torch
import torch.nn as nn
import cv2
import librosa
from scipy import signal
import matplotlib.pyplot as plt

# ============================================================================
# 1. VADæ¨¡å—æµ‹è¯•
# ============================================================================

class SimpleVAD:
    """ä»é›¶å®ç°çš„VADæ£€æµ‹å™¨"""
    def __init__(
        self,
        sample_rate=16000,
        frame_length=400,
        hop_length=160,
        energy_threshold=0.02,
        zcr_threshold=0.1
    ):
        self.sr = sample_rate
        self.frame_length = frame_length
        self.hop_length = hop_length
        self.energy_threshold = energy_threshold
        self.zcr_threshold = zcr_threshold

        # è®¾è®¡å¸¦é€šæ»¤æ³¢å™¨
        self.b, self.a = signal.butter(
            N=5,
            Wn=[80, 3400],
            btype='band',
            fs=sample_rate
        )

    def extract_features(self, audio):
        """æå–VADç‰¹å¾"""
        # é¢„å¤„ç†
        filtered = signal.filtfilt(self.b, self.a, audio)

        # åˆ†å¸§
        frames = librosa.util.frame(
            filtered,
            frame_length=self.frame_length,
            hop_length=self.hop_length
        )

        # çŸ­æ—¶èƒ½é‡
        energy = np.sum(frames ** 2, axis=0) / self.frame_length

        # è¿‡é›¶ç‡
        zcr = np.sum(np.abs(np.diff(np.sign(frames), axis=0)), axis=0) / (2 * self.frame_length)

        # é¢‘è°±å¹³å¦åº¦
        fft_frames = np.fft.rfft(frames, axis=0)
        power_spectrum = np.abs(fft_frames) ** 2

        geo_mean = np.exp(np.mean(np.log(power_spectrum + 1e-10), axis=0))
        arith_mean = np.mean(power_spectrum, axis=0)
        sfm = geo_mean / (arith_mean + 1e-10)

        return {
            'energy': energy,
            'zcr': zcr,
            'sfm': sfm
        }

    def detect(self, audio):
        """æ£€æµ‹æ˜¯å¦åŒ…å«è¯­éŸ³"""
        features = self.extract_features(audio)

        # å½’ä¸€åŒ–èƒ½é‡
        energy_norm = features['energy'] / (np.max(features['energy']) + 1e-10)

        # è¯­éŸ³å¸§åˆ¤å®š
        speech_frames = (
            (energy_norm > self.energy_threshold) &
            (features['zcr'] > 0.02) &
            (features['zcr'] < 0.3) &
            (features['sfm'] < 0.5)
        )

        speech_ratio = np.sum(speech_frames) / len(speech_frames)
        is_speech = speech_ratio > 0.3
        confidence = speech_ratio

        return is_speech, confidence


def test_vad():
    """æµ‹è¯•VADæ¨¡å—"""
    print("=" * 50)
    print("æµ‹è¯•1: VADæ¨¡å—")
    print("=" * 50)

    # ç”Ÿæˆæµ‹è¯•ä¿¡å·
    sr = 16000
    duration = 1.0

    # çº¯è¯­éŸ³ä¿¡å·ï¼ˆæ¨¡æ‹Ÿï¼‰
    t = np.linspace(0, duration, int(sr * duration))
    speech_signal = np.sin(2 * np.pi * 200 * t) + 0.5 * np.sin(2 * np.pi * 400 * t)
    speech_signal += 0.1 * np.random.randn(len(t))

    # çº¯å™ªå£°ä¿¡å·
    noise_signal = 0.1 * np.random.randn(int(sr * duration))

    # åˆå§‹åŒ–VAD
    vad = SimpleVAD(sample_rate=sr)

    # æµ‹è¯•è¯­éŸ³
    is_speech, confidence = vad.detect(speech_signal)
    print(f"è¯­éŸ³ä¿¡å·: is_speech={is_speech}, confidence={confidence:.3f}")
    assert is_speech, "åº”è¯¥æ£€æµ‹åˆ°è¯­éŸ³"

    # æµ‹è¯•å™ªå£°
    is_speech, confidence = vad.detect(noise_signal)
    print(f"å™ªå£°ä¿¡å·: is_speech={is_speech}, confidence={confidence:.3f}")
    assert not is_speech, "ä¸åº”è¯¥æ£€æµ‹åˆ°è¯­éŸ³"

    print("âœ… VADæµ‹è¯•é€šè¿‡\n")


# ============================================================================
# 2. Melé¢‘è°±æå–æµ‹è¯•
# ============================================================================

class MelSpectrogramExtractor:
    """ä»é›¶å®ç°Melé¢‘è°±æå–"""
    def __init__(
        self,
        sample_rate=16000,
        n_fft=512,
        hop_length=160,
        n_mels=80,
        fmin=0,
        fmax=8000
    ):
        self.sr = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels

        # åˆ›å»ºMelæ»¤æ³¢å™¨ç»„
        self.mel_filters = self.create_mel_filterbank(
            n_fft, n_mels, sample_rate, fmin, fmax
        )

    @staticmethod
    def hz_to_mel(hz):
        return 2595 * np.log10(1 + hz / 700)

    @staticmethod
    def mel_to_hz(mel):
        return 700 * (10 ** (mel / 2595) - 1)

    def create_mel_filterbank(self, n_fft, n_mels, sr, fmin, fmax):
        """åˆ›å»ºMelæ»¤æ³¢å™¨ç»„"""
        # Melå°ºåº¦ä¸Šçš„å‡åŒ€ç‚¹
        mel_min = self.hz_to_mel(fmin)
        mel_max = self.hz_to_mel(fmax)
        mel_points = np.linspace(mel_min, mel_max, n_mels + 2)

        # è½¬å›èµ«å…¹
        hz_points = self.mel_to_hz(mel_points)

        # è½¬æ¢ä¸ºFFT binç´¢å¼•
        bin_points = np.floor((n_fft + 1) * hz_points / sr).astype(int)

        # åˆ›å»ºæ»¤æ³¢å™¨ç»„
        filters = np.zeros((n_mels, n_fft // 2 + 1))

        for i in range(n_mels):
            left = bin_points[i]
            center = bin_points[i + 1]
            right = bin_points[i + 2]

            # ä¸Šå‡æ²¿
            for j in range(left, center):
                if center != left:
                    filters[i, j] = (j - left) / (center - left)

            # ä¸‹é™æ²¿
            for j in range(center, right):
                if right != center:
                    filters[i, j] = (right - j) / (right - center)

        return filters

    def extract(self, audio):
        """æå–Melé¢‘è°±"""
        # åˆ†å¸§
        frames = self._frame_audio(audio)

        # åŠ çª—
        window = np.hanning(self.n_fft)
        frames_windowed = frames * window[:, None]

        # FFT
        fft_result = np.fft.rfft(frames_windowed, n=self.n_fft, axis=0)

        # åŠŸç‡è°±
        power_spectrum = np.abs(fft_result) ** 2

        # åº”ç”¨Melæ»¤æ³¢å™¨
        mel_spectrum = self.mel_filters @ power_spectrum

        # å–å¯¹æ•°
        log_mel = 10 * np.log10(mel_spectrum + 1e-10)

        return log_mel

    def _frame_audio(self, audio):
        """éŸ³é¢‘åˆ†å¸§"""
        num_samples = len(audio)
        num_frames = 1 + (num_samples - self.n_fft) // self.hop_length

        frames = np.zeros((self.n_fft, num_frames))

        for i in range(num_frames):
            start = i * self.hop_length
            end = start + self.n_fft
            if end <= num_samples:
                frames[:, i] = audio[start:end]

        return frames


def test_mel_spectrogram():
    """æµ‹è¯•Melé¢‘è°±æå–"""
    print("=" * 50)
    print("æµ‹è¯•2: Melé¢‘è°±æå–")
    print("=" * 50)

    # ç”Ÿæˆæµ‹è¯•ä¿¡å·ï¼ˆå•é¢‘éŸ³ï¼‰
    sr = 16000
    duration = 1.0
    freq = 440  # A4éŸ³ç¬¦

    t = np.linspace(0, duration, int(sr * duration))
    audio = np.sin(2 * np.pi * freq * t)

    # æå–Melé¢‘è°±
    extractor = MelSpectrogramExtractor(sample_rate=sr)
    mel_spec = extractor.extract(audio)

    print(f"Melé¢‘è°±shape: {mel_spec.shape}")
    print(f"Melé¢‘è°±èŒƒå›´: [{mel_spec.min():.2f}, {mel_spec.max():.2f}] dB")

    # éªŒè¯shape
    assert mel_spec.shape[0] == 80, "åº”è¯¥æœ‰80ä¸ªMel bins"
    assert mel_spec.shape[1] > 0, "åº”è¯¥æœ‰æ—¶é—´å¸§"

    # éªŒè¯æ•°å€¼èŒƒå›´
    assert not np.isnan(mel_spec).any(), "ä¸åº”è¯¥æœ‰NaN"
    assert not np.isinf(mel_spec).any(), "ä¸åº”è¯¥æœ‰Inf"

    print("âœ… Melé¢‘è°±æµ‹è¯•é€šè¿‡\n")

    # å¯é€‰ï¼šå¯è§†åŒ–
    try:
        plt.figure(figsize=(10, 4))
        plt.imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')
        plt.colorbar(label='dB')
        plt.xlabel('Time (frames)')
        plt.ylabel('Mel frequency bins')
        plt.title('Mel Spectrogram')
        plt.tight_layout()
        plt.savefig('/tmp/mel_spectrogram_test.png', dpi=150)
        print("ğŸ“Š Melé¢‘è°±å›¾å·²ä¿å­˜åˆ° /tmp/mel_spectrogram_test.png")
    except:
        print("âš ï¸  æ— æ³•ä¿å­˜å¯è§†åŒ–å›¾ç‰‡ï¼ˆå¯èƒ½æ²¡æœ‰å›¾å½¢ç•Œé¢ï¼‰")


# ============================================================================
# 3. äººè„¸å¯¹é½æµ‹è¯•
# ============================================================================

class FaceAligner:
    """äººè„¸å¯¹é½"""
    def __init__(self, target_size=256):
        self.target_size = target_size

        # æ ‡å‡†äººè„¸å…³é”®ç‚¹ä½ç½®
        self.standard_landmarks = np.array([
            [0.31, 0.46],  # å·¦çœ¼
            [0.69, 0.46],  # å³çœ¼
            [0.50, 0.73]   # é¼»å°–
        ]) * target_size

    def align(self, image, landmarks):
        """å¯¹é½äººè„¸"""
        # é€‰æ‹©3ä¸ªå…³é”®ç‚¹
        if landmarks.shape[0] == 68:
            # 68ç‚¹æ ‡å‡†
            src_pts = np.array([
                landmarks[36:42].mean(axis=0),  # å·¦çœ¼ä¸­å¿ƒ
                landmarks[42:48].mean(axis=0),  # å³çœ¼ä¸­å¿ƒ
                landmarks[30]                    # é¼»å°–
            ], dtype=np.float32)
        else:
            # ç®€åŒ–: å‡è®¾æ˜¯3ä¸ªç‚¹
            src_pts = landmarks[:3].astype(np.float32)

        dst_pts = self.standard_landmarks.astype(np.float32)

        # è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µ
        M = cv2.getAffineTransform(src_pts, dst_pts)

        # åº”ç”¨å˜æ¢
        aligned = cv2.warpAffine(
            image,
            M,
            (self.target_size, self.target_size),
            flags=cv2.INTER_LINEAR
        )

        return aligned, M


def test_face_alignment():
    """æµ‹è¯•äººè„¸å¯¹é½"""
    print("=" * 50)
    print("æµ‹è¯•3: äººè„¸å¯¹é½")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)

    # æ¨¡æ‹Ÿå…³é”®ç‚¹ï¼ˆéæ ‡å‡†ä½ç½®ï¼‰
    landmarks = np.array([
        [150, 200],  # å·¦çœ¼
        [350, 220],  # å³çœ¼
        [250, 350]   # é¼»å°–
    ], dtype=np.float32)

    # å¯¹é½
    aligner = FaceAligner(target_size=256)
    aligned, M = aligner.align(image, landmarks)

    print(f"åŸå›¾shape: {image.shape}")
    print(f"å¯¹é½åshape: {aligned.shape}")
    print(f"ä»¿å°„çŸ©é˜µ:\n{M}")

    # éªŒè¯
    assert aligned.shape == (256, 256, 3), "å¯¹é½ååº”è¯¥æ˜¯256x256"
    assert M.shape == (2, 3), "ä»¿å°„çŸ©é˜µåº”è¯¥æ˜¯2x3"

    # éªŒè¯å…³é”®ç‚¹ç¡®å®è¢«å¯¹é½
    ones = np.ones((3, 1))
    src_homo = np.hstack([landmarks, ones])
    transformed_pts = (M @ src_homo.T).T

    expected_pts = aligner.standard_landmarks

    error = np.abs(transformed_pts - expected_pts).mean()
    print(f"å¯¹é½è¯¯å·®: {error:.2f} åƒç´ ")
    assert error < 1.0, "å¯¹é½è¯¯å·®åº”è¯¥å°äº1åƒç´ "

    print("âœ… äººè„¸å¯¹é½æµ‹è¯•é€šè¿‡\n")


# ============================================================================
# 4. ç®€å•GANæµ‹è¯•
# ============================================================================

class SimpleGenerator(nn.Module):
    """ç®€åŒ–çš„ç”Ÿæˆå™¨"""
    def __init__(self, latent_dim=100, output_channels=3):
        super().__init__()

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256 * 8 * 8),
            nn.ReLU(),
            nn.Unflatten(1, (256, 8, 8)),

            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.ConvTranspose2d(64, output_channels, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z)


def test_generator():
    """æµ‹è¯•ç”Ÿæˆå™¨"""
    print("=" * 50)
    print("æµ‹è¯•4: GANç”Ÿæˆå™¨")
    print("=" * 50)

    # åˆå§‹åŒ–æ¨¡å‹
    generator = SimpleGenerator(latent_dim=100, output_channels=3)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 4
    z = torch.randn(batch_size, 100)

    with torch.no_grad():
        fake_images = generator(z)

    print(f"è¾“å…¥shape: {z.shape}")
    print(f"è¾“å‡ºshape: {fake_images.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{fake_images.min():.3f}, {fake_images.max():.3f}]")

    # éªŒè¯
    assert fake_images.shape == (batch_size, 3, 64, 64), "è¾“å‡ºshapeåº”è¯¥æ˜¯(B,3,64,64)"
    assert -1 <= fake_images.min() <= 1, "Tanhè¾“å‡ºåº”è¯¥åœ¨[-1,1]"
    assert -1 <= fake_images.max() <= 1, "Tanhè¾“å‡ºåº”è¯¥åœ¨[-1,1]"

    print("âœ… ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡\n")


# ============================================================================
# 5. é›†æˆæµ‹è¯•
# ============================================================================

def test_integration():
    """é›†æˆæµ‹è¯•ï¼šæ¨¡æ‹Ÿå®Œæ•´pipeline"""
    print("=" * 50)
    print("æµ‹è¯•5: å®Œæ•´Pipelineé›†æˆ")
    print("=" * 50)

    # 1. VADæ£€æµ‹
    print("æ­¥éª¤1: VADæ£€æµ‹...")
    sr = 16000
    duration = 1.0
    t = np.linspace(0, duration, int(sr * duration))
    audio = np.sin(2 * np.pi * 200 * t)

    vad = SimpleVAD(sample_rate=sr)
    is_speech, _ = vad.detect(audio)
    print(f"  âœ“ æ£€æµ‹åˆ°è¯­éŸ³: {is_speech}")

    # 2. éŸ³é¢‘ç‰¹å¾æå–
    print("æ­¥éª¤2: æå–Melé¢‘è°±...")
    mel_extractor = MelSpectrogramExtractor(sample_rate=sr)
    mel_spec = mel_extractor.extract(audio)
    print(f"  âœ“ Melé¢‘è°±shape: {mel_spec.shape}")

    # 3. äººè„¸å¤„ç†
    print("æ­¥éª¤3: äººè„¸å¯¹é½...")
    image = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    landmarks = np.array([[150, 200], [350, 220], [250, 350]], dtype=np.float32)

    aligner = FaceAligner(target_size=256)
    aligned_face, _ = aligner.align(image, landmarks)
    print(f"  âœ“ å¯¹é½åshape: {aligned_face.shape}")

    # 4. ç”Ÿæˆ
    print("æ­¥éª¤4: ç”Ÿæˆäººè„¸...")
    generator = SimpleGenerator()
    z = torch.randn(1, 100)
    with torch.no_grad():
        generated = generator(z)
    print(f"  âœ“ ç”Ÿæˆshape: {generated.shape}")

    print("\nâœ… å®Œæ•´Pipelineé›†æˆæµ‹è¯•é€šè¿‡")


# ============================================================================
# ä¸»æµ‹è¯•å‡½æ•°
# ============================================================================

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 50)
    print("å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    print("=" * 50 + "\n")

    try:
        test_vad()
        test_mel_spectrogram()
        test_face_alignment()
        test_generator()
        test_integration()

        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 50 + "\n")

        return True

    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = run_all_tests()

    if success:
        print("âœ… ä»£ç éªŒè¯å®Œæˆï¼Œæ‰€æœ‰æ¨¡å—å·¥ä½œæ­£å¸¸ï¼")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. ä½¿ç”¨çœŸå®éŸ³é¢‘æ–‡ä»¶æµ‹è¯•VAD")
        print("2. ä½¿ç”¨çœŸå®äººè„¸å›¾ç‰‡æµ‹è¯•å¯¹é½")
        print("3. åœ¨GPUä¸Šè®­ç»ƒå®Œæ•´GANæ¨¡å‹")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")

    exit(0 if success else 1)
