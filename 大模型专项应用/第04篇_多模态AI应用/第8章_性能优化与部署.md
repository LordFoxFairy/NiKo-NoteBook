# ç¬¬8ç«  æ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½²

> ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½²æ–¹æ¡ˆ

## 8.1 æ‰¹é‡å¤„ç†ä¼˜åŒ–

### 8.1.1 å¹¶å‘å¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Callable
import time

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
    
    async def process_async(
        self,
        items: List[str],
        process_func: Callable,
        batch_size: int = 10
    ) -> List:
        """
        å¼‚æ­¥æ‰¹é‡å¤„ç†
        
        Args:
            items: å¾…å¤„ç†é¡¹åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]
            
            # å¹¶å‘å¤„ç†æ‰¹æ¬¡
            tasks = [
                asyncio.to_thread(process_func, item)
                for item in batch
            ]
            
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)
        
        return results
    
    def process_parallel(
        self,
        items: List[str],
        process_func: Callable
    ) -> List:
        """
        å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
        
        Args:
            items: å¾…å¤„ç†é¡¹åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(process_func, items))
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def analyze_image(image_path: str) -> dict:
    """åˆ†æå›¾ç‰‡"""
    analyzer = GPT4VisionAnalyzer(api_key="your-key")
    result = analyzer.analyze_image(image_path, "æè¿°å›¾ç‰‡")
    return {"path": image_path, "result": result}

# æ‰¹é‡å¤„ç†1000å¼ å›¾ç‰‡
image_paths = [f"image_{i}.jpg" for i in range(1000)]

processor = BatchProcessor(max_workers=8)

start = time.time()
results = asyncio.run(
    processor.process_async(image_paths, analyze_image, batch_size=20)
)
end = time.time()

print(f"å¤„ç†1000å¼ å›¾ç‰‡è€—æ—¶: {end-start:.2f}ç§’")
print(f"å¹³å‡é€Ÿåº¦: {len(image_paths)/(end-start):.2f} å›¾ç‰‡/ç§’")
```

### 8.1.2 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib
import json
from redis import Redis

class MultimodalCache:
    """å¤šæ¨¡æ€ç¼“å­˜"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = Redis.from_url(redis_url)
        self.ttl = 3600  # 1å°æ—¶
    
    def _get_cache_key(self, prefix: str, content: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"{prefix}:{content_hash}"
    
    def get_image_analysis(self, image_path: str, prompt: str) -> Optional[str]:
        """è·å–å›¾ç‰‡åˆ†æç¼“å­˜"""
        # è¯»å–å›¾ç‰‡å†…å®¹è®¡ç®—hash
        with open(image_path, "rb") as f:
            image_hash = hashlib.md5(f.read()).hexdigest()
        
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        key = f"image_analysis:{image_hash}:{prompt_hash}"
        
        cached = self.redis.get(key)
        return cached.decode() if cached else None
    
    def set_image_analysis(
        self,
        image_path: str,
        prompt: str,
        result: str
    ):
        """è®¾ç½®å›¾ç‰‡åˆ†æç¼“å­˜"""
        with open(image_path, "rb") as f:
            image_hash = hashlib.md5(f.read()).hexdigest()
        
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        key = f"image_analysis:{image_hash}:{prompt_hash}"
        
        self.redis.setex(key, self.ttl, result)
    
    def get_clip_embedding(self, content: str, content_type: str) -> Optional[list]:
        """è·å–CLIP embeddingç¼“å­˜"""
        key = self._get_cache_key(f"clip_{content_type}", content)
        cached = self.redis.get(key)
        
        if cached:
            return json.loads(cached)
        return None
    
    def set_clip_embedding(
        self,
        content: str,
        content_type: str,
        embedding: list
    ):
        """è®¾ç½®CLIP embeddingç¼“å­˜"""
        key = self._get_cache_key(f"clip_{content_type}", content)
        self.redis.setex(key, self.ttl * 24, json.dumps(embedding))  # 24å°æ—¶

# ä½¿ç”¨ç¤ºä¾‹
cache = MultimodalCache()

def cached_analyze_image(image_path: str, prompt: str) -> str:
    """å¸¦ç¼“å­˜çš„å›¾ç‰‡åˆ†æ"""
    # æ£€æŸ¥ç¼“å­˜
    cached = cache.get_image_analysis(image_path, prompt)
    if cached:
        print("å‘½ä¸­ç¼“å­˜")
        return cached
    
    # åˆ†æ
    analyzer = GPT4VisionAnalyzer(api_key="your-key")
    result = analyzer.analyze_image(image_path, prompt)
    
    # å†™å…¥ç¼“å­˜
    cache.set_image_analysis(image_path, prompt, result)
    
    return result
```

## 8.2 GPUèµ„æºç®¡ç†

```python
import torch
from contextlib import contextmanager

class GPUResourceManager:
    """GPUèµ„æºç®¡ç†å™¨"""
    
    @staticmethod
    @contextmanager
    def allocate_gpu(gpu_id: int = 0):
        """
        åˆ†é…GPU
        
        Args:
            gpu_id: GPUç¼–å·
        """
        if torch.cuda.is_available():
            device = f"cuda:{gpu_id}"
            print(f"ä½¿ç”¨GPU: {device}")
        else:
            device = "cpu"
            print("ä½¿ç”¨CPU")
        
        try:
            yield device
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("é‡Šæ”¾GPUç¼“å­˜")
    
    @staticmethod
    def get_gpu_memory():
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not torch.cuda.is_available():
            return {}
        
        info = {}
        for i in range(torch.cuda.device_count()):
            info[f"GPU {i}"] = {
                "total": torch.cuda.get_device_properties(i).total_memory / 1024**3,
                "allocated": torch.cuda.memory_allocated(i) / 1024**3,
                "cached": torch.cuda.memory_reserved(i) / 1024**3
            }
        
        return info

# ä½¿ç”¨ç¤ºä¾‹
with GPUResourceManager.allocate_gpu(gpu_id=0) as device:
    model = CLIPSearchEngine()
    # æ‰§è¡ŒGPUè®¡ç®—
    features = model.encode_images(["img1.jpg", "img2.jpg"])

# GPUè‡ªåŠ¨é‡Šæ”¾
memory_info = GPUResourceManager.get_gpu_memory()
print(memory_info)
```

## 8.3 æˆæœ¬æ§åˆ¶

```python
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict

@dataclass
class UsageRecord:
    """ä½¿ç”¨è®°å½•"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    cost: float

class CostMonitor:
    """æˆæœ¬ç›‘æ§"""
    
    def __init__(self, daily_budget: float = 100.0):
        self.daily_budget = daily_budget
        self.records: List[UsageRecord] = []
        
        # å®šä»·
        self.pricing = {
            "gpt-4-vision-preview": {
                "input": 10 / 1_000_000,
                "output": 30 / 1_000_000
            },
            "whisper-1": {
                "per_minute": 0.006
            }
        }
    
    def record_usage(
        self,
        model: str,
        input_tokens: int = 0,
        output_tokens: int = 0,
        minutes: float = 0.0
    ):
        """è®°å½•ä½¿ç”¨"""
        if model in ["gpt-4-vision-preview"]:
            cost = (
                input_tokens * self.pricing[model]["input"] +
                output_tokens * self.pricing[model]["output"]
            )
        elif model == "whisper-1":
            cost = minutes * self.pricing[model]["per_minute"]
        else:
            cost = 0
        
        record = UsageRecord(
            timestamp=datetime.now(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost=cost
        )
        
        self.records.append(record)
        
        # æ£€æŸ¥é¢„ç®—
        self._check_budget()
    
    def _check_budget(self):
        """æ£€æŸ¥é¢„ç®—"""
        today = datetime.now().date()
        today_records = [
            r for r in self.records
            if r.timestamp.date() == today
        ]
        
        today_cost = sum(r.cost for r in today_records)
        
        if today_cost >= self.daily_budget * 0.9:
            print(f"âš ï¸ è­¦å‘Š: ä»Šæ—¥æˆæœ¬ ${today_cost:.2f} å·²è¾¾é¢„ç®—çš„90%")
        
        if today_cost >= self.daily_budget:
            print(f"ğŸš« æˆæœ¬è¶…é™: ä»Šæ—¥æˆæœ¬ ${today_cost:.2f} è¶…è¿‡é¢„ç®— ${self.daily_budget}")
            raise Exception("Daily budget exceeded")
    
    def get_stats(self, days: int = 7) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        cutoff = datetime.now() - timedelta(days=days)
        recent_records = [
            r for r in self.records
            if r.timestamp >= cutoff
        ]
        
        total_cost = sum(r.cost for r in recent_records)
        total_tokens = sum(r.input_tokens + r.output_tokens for r in recent_records)
        
        model_stats = {}
        for record in recent_records:
            if record.model not in model_stats:
                model_stats[record.model] = {"count": 0, "cost": 0}
            model_stats[record.model]["count"] += 1
            model_stats[record.model]["cost"] += record.cost
        
        return {
            "period_days": days,
            "total_cost": f"${total_cost:.2f}",
            "total_tokens": total_tokens,
            "model_stats": model_stats,
            "avg_cost_per_day": f"${total_cost/days:.2f}"
        }

# ä½¿ç”¨ç¤ºä¾‹
cost_monitor = CostMonitor(daily_budget=50.0)

# è®°å½•APIè°ƒç”¨
cost_monitor.record_usage(
    model="gpt-4-vision-preview",
    input_tokens=1500,
    output_tokens=500
)

# æŸ¥çœ‹ç»Ÿè®¡
stats = cost_monitor.get_stats(days=7)
print(stats)
```

## 8.4 ç”Ÿäº§éƒ¨ç½²

### 8.4.1 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsm6 \
    libxext6 \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
      - qdrant
    volumes:
      - ./uploads:/app/uploads
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  qdrant_data:
```

### 8.4.2 Kuberneteséƒ¨ç½²

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multimodal-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multimodal-api
  template:
    metadata:
      labels:
        app: multimodal-api
    spec:
      containers:
      - name: api
        image: multimodal-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
apiVersion: v1
kind: Service
metadata:
  name: multimodal-api
spec:
  selector:
    app: multimodal-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

## 8.5 ç›‘æ§å‘Šè­¦

```python
from prometheus_client import Counter, Histogram, Gauge
import logging

# PrometheusæŒ‡æ ‡
request_count = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint'])
request_duration = Histogram('api_request_duration_seconds', 'Request duration')
error_count = Counter('api_errors_total', 'Total errors', ['error_type'])
cost_gauge = Gauge('api_cost_total', 'Total API cost')

class MonitoringMiddleware:
    """ç›‘æ§ä¸­é—´ä»¶"""
    
    @staticmethod
    def track_request(method: str, endpoint: str):
        """è·Ÿè¸ªè¯·æ±‚"""
        request_count.labels(method=method, endpoint=endpoint).inc()
    
    @staticmethod
    def track_error(error_type: str):
        """è·Ÿè¸ªé”™è¯¯"""
        error_count.labels(error_type=error_type).inc()
    
    @staticmethod
    @request_duration.time()
    def track_duration():
        """è·Ÿè¸ªè¯·æ±‚æ—¶é•¿"""
        pass
    
    @staticmethod
    def update_cost(cost: float):
        """æ›´æ–°æˆæœ¬"""
        cost_gauge.set(cost)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

## æœ¬ç« å°ç»“

- æ‰¹é‡å¤„ç†é€šè¿‡å¹¶å‘ä¸ç¼“å­˜æ˜¾è‘—æå‡æ€§èƒ½
- GPUèµ„æºç®¡ç†é¿å…å†…å­˜æ³„æ¼
- æˆæœ¬ç›‘æ§ç¡®ä¿é¢„ç®—å¯æ§
- Docker/K8så®ç°æ ‡å‡†åŒ–éƒ¨ç½²
- Prometheusç›‘æ§ä¿éšœç³»ç»Ÿå¯è§‚æµ‹æ€§

---

**ä¸‹ä¸€ç« **: [é™„å½• å¤šæ¨¡æ€åº”ç”¨æ¡ˆä¾‹](./é™„å½•_å¤šæ¨¡æ€åº”ç”¨æ¡ˆä¾‹.md)
