# 第4章 语音分析与识别

> 掌握Whisper语音识别与音频处理技术

## 4.1 Whisper语音识别

### 4.1.1 API使用

```python
from openai import OpenAI
from pathlib import Path

class WhisperTranscriber:
    """Whisper语音转文本"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
    
    def transcribe(
        self,
        audio_path: str,
        language: str = None,
        response_format: str = "json"
    ) -> dict:
        """
        语音转文本
        
        Args:
            audio_path: 音频文件路径
            language: 语言代码(如'zh', 'en'),None为自动检测
            response_format: 返回格式(json/text/srt/vtt)
            
        Returns:
            转录结果
        """
        with open(audio_path, "rb") as audio_file:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language=language,
                response_format=response_format
            )
        
        return transcript
    
    def translate_to_english(self, audio_path: str) -> str:
        """
        翻译音频为英文
        
        Args:
            audio_path: 音频文件路径
            
        Returns:
            英文翻译
        """
        with open(audio_path, "rb") as audio_file:
            translation = self.client.audio.translations.create(
                model="whisper-1",
                file=audio_file
            )
        
        return translation.text

# 使用示例
transcriber = WhisperTranscriber(api_key="your-key")

# 中文转录
result = transcriber.transcribe("chinese_audio.mp3", language="zh")
print(result['text'])

# 翻译为英文
english_text = transcriber.translate_to_english("chinese_audio.mp3")
print(english_text)
```

### 4.1.2 本地部署Whisper

```python
import whisper
import torch

class LocalWhisperTranscriber:
    """本地Whisper模型"""
    
    def __init__(self, model_size: str = "base"):
        """
        初始化
        
        Args:
            model_size: 模型大小(tiny/base/small/medium/large)
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = whisper.load_model(model_size, device=device)
    
    def transcribe(
        self,
        audio_path: str,
        language: str = None
    ) -> dict:
        """
        转录音频
        
        Args:
            audio_path: 音频路径
            language: 语言代码
            
        Returns:
            转录结果
        """
        result = self.model.transcribe(
            audio_path,
            language=language,
            verbose=False
        )
        
        return {
            'text': result['text'],
            'segments': result['segments'],
            'language': result['language']
        }

# 使用示例
local_transcriber = LocalWhisperTranscriber(model_size="base")
result = local_transcriber.transcribe("audio.mp3", language="zh")
print(result['text'])

# 获取时间戳
for segment in result['segments']:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['text']}")
```

## 4.2 语音情感分析

```python
from transformers import pipeline

class SpeechEmotionAnalyzer:
    """语音情感分析"""
    
    def __init__(self):
        self.transcriber = WhisperTranscriber(api_key="your-key")
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
    
    def analyze_emotion(self, audio_path: str) -> dict:
        """
        分析语音情感
        
        Args:
            audio_path: 音频路径
            
        Returns:
            情感分析结果
        """
        # 转录
        transcript = self.transcriber.transcribe(audio_path)
        text = transcript['text']
        
        # 情感分析
        sentiment = self.sentiment_analyzer(text)[0]
        
        return {
            'text': text,
            'emotion': sentiment['label'],
            'confidence': sentiment['score']
        }

# 使用示例
emotion_analyzer = SpeechEmotionAnalyzer()
result = emotion_analyzer.analyze_emotion("customer_call.mp3")
print(f"情感: {result['emotion']}, 置信度: {result['confidence']:.2f}")
```

## 4.3 实时语音处理

```python
import pyaudio
import wave
from io import BytesIO

class RealtimeSpeechProcessor:
    """实时语音处理"""
    
    def __init__(self, transcriber):
        self.transcriber = transcriber
        self.audio = pyaudio.PyAudio()
        self.stream = None
    
    def start_recording(self, duration: int = 5):
        """
        开始录音
        
        Args:
            duration: 录音时长(秒)
        """
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        
        self.stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            frames_per_buffer=CHUNK
        )
        
        print("开始录音...")
        frames = []
        
        for _ in range(0, int(RATE / CHUNK * duration)):
            data = self.stream.read(CHUNK)
            frames.append(data)
        
        print("录音结束")
        
        # 保存并转录
        audio_data = b''.join(frames)
        return self._transcribe_audio_data(audio_data, RATE, CHANNELS)
    
    def _transcribe_audio_data(
        self,
        audio_data: bytes,
        rate: int,
        channels: int
    ) -> str:
        """转录音频数据"""
        # 保存为临时WAV文件
        temp_path = "temp_recording.wav"
        with wave.open(temp_path, 'wb') as wf:
            wf.setnchannels(channels)
            wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))
            wf.setframerate(rate)
            wf.writeframes(audio_data)
        
        # 转录
        result = self.transcriber.transcribe(temp_path)
        return result['text']
    
    def stop(self):
        """停止录音"""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.audio.terminate()

# 使用示例
processor = RealtimeSpeechProcessor(transcriber)
text = processor.start_recording(duration=5)
print(f"识别结果: {text}")
processor.stop()
```

## 本章小结

- Whisper支持多语言高精度转录
- 本地部署适合隐私敏感场景
- 实时处理需要合理的音频分段策略

---

**下一章**: [第5章 CLIP跨模态检索](./第5章_CLIP跨模态检索.md)
