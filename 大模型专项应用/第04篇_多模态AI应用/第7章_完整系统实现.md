# ç¬¬7ç«  å®Œæ•´ç³»ç»Ÿå®ç°

> æ„å»ºç”Ÿäº§çº§å¤šæ¨¡æ€AIåº”ç”¨ç³»ç»Ÿ

## 7.1 æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ

### 7.1.1 ç³»ç»Ÿæ¶æ„

```python
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Optional
import asyncio

class AlertLevel(str, Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

@dataclass
class MonitoringEvent:
    """ç›‘æ§äº‹ä»¶"""
    timestamp: datetime
    camera_id: str
    event_type: str
    description: str
    alert_level: AlertLevel
    image_path: Optional[str] = None
    confidence: float = 0.0

class IntelligentMonitoringSystem:
    """æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(
        self,
        vision_analyzer,
        alert_callback=None
    ):
        self.vision_analyzer = vision_analyzer
        self.alert_callback = alert_callback
        self.monitoring_rules = self._load_rules()
    
    def _load_rules(self) -> dict:
        """åŠ è½½ç›‘æ§è§„åˆ™"""
        return {
            "suspicious_person": {
                "keywords": ["suspicious", "unauthorized", "intruder"],
                "alert_level": AlertLevel.CRITICAL
            },
            "safety_violation": {
                "keywords": ["not wearing helmet", "safety violation"],
                "alert_level": AlertLevel.WARNING
            },
            "abnormal_behavior": {
                "keywords": ["running", "fighting", "falling"],
                "alert_level": AlertLevel.WARNING
            }
        }
    
    async def analyze_frame(
        self,
        frame_path: str,
        camera_id: str
    ) -> Optional[MonitoringEvent]:
        """
        åˆ†æç›‘æ§å¸§
        
        Args:
            frame_path: å¸§å›¾ç‰‡è·¯å¾„
            camera_id: æ‘„åƒå¤´ID
            
        Returns:
            ç›‘æ§äº‹ä»¶(å¦‚æœ‰å¼‚å¸¸)
        """
        # åˆ†æå›¾ç‰‡
        analysis_prompt = """
        åˆ†æè¿™ä¸ªç›‘æ§ç”»é¢,æ£€æµ‹:
        1. æ˜¯å¦æœ‰å¯ç–‘äººå‘˜æˆ–æœªæˆæƒè®¿é—®
        2. æ˜¯å¦æœ‰å®‰å…¨è¿è§„è¡Œä¸º(å¦‚æœªæˆ´å®‰å…¨å¸½)
        3. æ˜¯å¦æœ‰å¼‚å¸¸è¡Œä¸º(å¥”è·‘ã€æ‰“æ–—ã€æ‘”å€’ç­‰)
        
        è¿”å›JSONæ ¼å¼: {
            "has_anomaly": bool,
            "anomaly_type": str,
            "description": str,
            "confidence": float
        }
        """
        
        result_text = self.vision_analyzer.analyze_image(
            frame_path,
            analysis_prompt
        )
        
        # è§£æç»“æœ
        import json
        try:
            result = json.loads(result_text)
        except:
            return None
        
        if not result.get("has_anomaly"):
            return None
        
        # åŒ¹é…å‘Šè­¦è§„åˆ™
        anomaly_type = result["anomaly_type"]
        alert_level = AlertLevel.INFO
        
        for rule_name, rule_config in self.monitoring_rules.items():
            if any(kw in anomaly_type.lower() for kw in rule_config["keywords"]):
                alert_level = rule_config["alert_level"]
                break
        
        # åˆ›å»ºäº‹ä»¶
        event = MonitoringEvent(
            timestamp=datetime.now(),
            camera_id=camera_id,
            event_type=anomaly_type,
            description=result["description"],
            alert_level=alert_level,
            image_path=frame_path,
            confidence=result["confidence"]
        )
        
        # è§¦å‘å‘Šè­¦
        if self.alert_callback and alert_level in [AlertLevel.WARNING, AlertLevel.CRITICAL]:
            await self.alert_callback(event)
        
        return event
    
    async def monitor_camera(
        self,
        camera_id: str,
        video_source: str,
        analysis_interval: int = 30
    ):
        """
        ç›‘æ§æ‘„åƒå¤´
        
        Args:
            camera_id: æ‘„åƒå¤´ID
            video_source: è§†é¢‘æº
            analysis_interval: åˆ†æé—´éš”(å¸§)
        """
        import cv2
        
        cap = cv2.VideoCapture(video_source)
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            if frame_count % analysis_interval == 0:
                # ä¿å­˜å¸§
                frame_path = f"temp/camera_{camera_id}_frame_{frame_count}.jpg"
                cv2.imwrite(frame_path, frame)
                
                # åˆ†æ
                event = await self.analyze_frame(frame_path, camera_id)
                
                if event:
                    print(f"[{event.alert_level.value.upper()}] {event.description}")
        
        cap.release()

# ä½¿ç”¨ç¤ºä¾‹
async def alert_handler(event: MonitoringEvent):
    """å‘Šè­¦å¤„ç†"""
    print(f"ğŸš¨ å‘Šè­¦: {event.description}")
    # å‘é€é€šçŸ¥ã€ä¿å­˜è®°å½•ç­‰

monitoring_system = IntelligentMonitoringSystem(
    vision_analyzer,
    alert_callback=alert_handler
)

# å¯åŠ¨ç›‘æ§
asyncio.run(
    monitoring_system.monitor_camera(
        camera_id="CAM001",
        video_source="rtsp://camera_url",
        analysis_interval=30
    )
)
```

## 7.2 å†…å®¹å®¡æ ¸å¹³å°

```python
from typing import List, Dict
from enum import Enum

class ContentType(str, Enum):
    """å†…å®¹ç±»å‹"""
    IMAGE = "image"
    VIDEO = "video"
    AUDIO = "audio"
    TEXT = "text"

class ModerationResult(str, Enum):
    """å®¡æ ¸ç»“æœ"""
    PASS = "pass"
    REVIEW = "review"
    REJECT = "reject"

@dataclass
class ModerationReport:
    """å®¡æ ¸æŠ¥å‘Š"""
    content_id: str
    content_type: ContentType
    result: ModerationResult
    violations: List[str]
    confidence: float
    details: str

class ContentModerationPlatform:
    """å†…å®¹å®¡æ ¸å¹³å°"""
    
    def __init__(
        self,
        vision_analyzer,
        audio_transcriber
    ):
        self.vision_analyzer = vision_analyzer
        self.audio_transcriber = audio_transcriber
        self.violation_rules = self._load_violation_rules()
    
    def _load_violation_rules(self) -> dict:
        """åŠ è½½è¿è§„è§„åˆ™"""
        return {
            "violence": ["æš´åŠ›", "è¡€è…¥", "æ‰“æ–—"],
            "sexual": ["è‰²æƒ…", "è£¸éœ²"],
            "political": ["æ•æ„Ÿæ”¿æ²»"],
            "hate_speech": ["ä»‡æ¨è¨€è®º", "æ­§è§†"],
            "illegal": ["è¿æ³•", "æ¯’å“", "èµŒåš"]
        }
    
    def moderate_image(self, image_path: str, content_id: str) -> ModerationReport:
        """
        å®¡æ ¸å›¾ç‰‡
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            content_id: å†…å®¹ID
            
        Returns:
            å®¡æ ¸æŠ¥å‘Š
        """
        prompt = """
        å®¡æ ¸è¿™å¼ å›¾ç‰‡æ˜¯å¦åŒ…å«ä»¥ä¸‹è¿è§„å†…å®¹:
        1. æš´åŠ›è¡€è…¥å†…å®¹
        2. è‰²æƒ…ä½ä¿—å†…å®¹
        3. æ•æ„Ÿæ”¿æ²»å†…å®¹
        4. ä»‡æ¨æ­§è§†è¨€è®º
        5. è¿æ³•è¿è§„å†…å®¹
        
        è¿”å›JSON: {
            "has_violation": bool,
            "violations": ["è¿è§„ç±»å‹åˆ—è¡¨"],
            "severity": "low/medium/high",
            "confidence": float,
            "explanation": "è¯´æ˜"
        }
        """
        
        result_text = self.vision_analyzer.analyze_image(image_path, prompt)
        
        import json
        result = json.loads(result_text)
        
        # åˆ¤æ–­å®¡æ ¸ç»“æœ
        if not result["has_violation"]:
            moderation_result = ModerationResult.PASS
        elif result["severity"] == "high":
            moderation_result = ModerationResult.REJECT
        else:
            moderation_result = ModerationResult.REVIEW
        
        return ModerationReport(
            content_id=content_id,
            content_type=ContentType.IMAGE,
            result=moderation_result,
            violations=result["violations"],
            confidence=result["confidence"],
            details=result["explanation"]
        )
    
    def moderate_audio(self, audio_path: str, content_id: str) -> ModerationReport:
        """å®¡æ ¸éŸ³é¢‘"""
        # è½¬å½•
        transcript = self.audio_transcriber.transcribe(audio_path)
        text = transcript['text']
        
        # æ–‡æœ¬å®¡æ ¸
        violations = []
        for rule_name, keywords in self.violation_rules.items():
            if any(kw in text for kw in keywords):
                violations.append(rule_name)
        
        if not violations:
            result = ModerationResult.PASS
        elif len(violations) >= 2:
            result = ModerationResult.REJECT
        else:
            result = ModerationResult.REVIEW
        
        return ModerationReport(
            content_id=content_id,
            content_type=ContentType.AUDIO,
            result=result,
            violations=violations,
            confidence=0.85,
            details=f"è½¬å½•æ–‡æœ¬: {text}"
        )
    
    def batch_moderate(
        self,
        contents: List[Dict[str, str]]
    ) -> List[ModerationReport]:
        """
        æ‰¹é‡å®¡æ ¸
        
        Args:
            contents: å†…å®¹åˆ—è¡¨ [{"type": "image/audio", "path": "...", "id": "..."}]
            
        Returns:
            å®¡æ ¸æŠ¥å‘Šåˆ—è¡¨
        """
        reports = []
        
        for content in contents:
            content_type = content["type"]
            content_path = content["path"]
            content_id = content["id"]
            
            if content_type == "image":
                report = self.moderate_image(content_path, content_id)
            elif content_type == "audio":
                report = self.moderate_audio(content_path, content_id)
            else:
                continue
            
            reports.append(report)
        
        return reports

# ä½¿ç”¨ç¤ºä¾‹
moderation_platform = ContentModerationPlatform(
    vision_analyzer,
    audio_transcriber
)

# å•ä¸ªå®¡æ ¸
report = moderation_platform.moderate_image("user_upload.jpg", "IMG001")
print(f"å®¡æ ¸ç»“æœ: {report.result.value}")
if report.violations:
    print(f"è¿è§„ç±»å‹: {', '.join(report.violations)}")

# æ‰¹é‡å®¡æ ¸
contents = [
    {"type": "image", "path": "img1.jpg", "id": "IMG001"},
    {"type": "image", "path": "img2.jpg", "id": "IMG002"},
    {"type": "audio", "path": "audio1.mp3", "id": "AUD001"},
]

reports = moderation_platform.batch_moderate(contents)
for r in reports:
    print(f"{r.content_id}: {r.result.value}")
```

## 7.3 å›¾æ–‡äº’æœå¼•æ“

å·²åœ¨ç¬¬5ç« è¯¦ç»†ä»‹ç»,è¿™é‡Œå±•ç¤ºç³»ç»Ÿé›†æˆ:

```python
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
import uvicorn

app = FastAPI(title="å¤šæ¨¡æ€æœç´¢å¼•æ“")

# åˆå§‹åŒ–ç»„ä»¶
clip_engine = CLIPSearchEngine()
clip_db = CLIPVectorDB(clip_engine)

@app.post("/index/image")
async def index_image(file: UploadFile = File(...)):
    """ç´¢å¼•å›¾ç‰‡"""
    # ä¿å­˜æ–‡ä»¶
    file_path = f"uploads/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())
    
    # ç´¢å¼•
    clip_db.index_images([file_path])
    
    return {"message": "ç´¢å¼•æˆåŠŸ", "path": file_path}

@app.get("/search")
async def search(query: str, limit: int = 10):
    """æœç´¢"""
    results = clip_db.search(query, limit=limit)
    return {"results": results}

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## æœ¬ç« å°ç»“

- æ™ºèƒ½ç›‘æ§ç³»ç»Ÿå®ç°å®æ—¶å¼‚å¸¸æ£€æµ‹ä¸å‘Šè­¦
- å†…å®¹å®¡æ ¸å¹³å°æ”¯æŒå¤šæ¨¡æ€æ‰¹é‡å®¡æ ¸
- å›¾æ–‡äº’æœå¼•æ“æä¾›RESTful APIæœåŠ¡

---

**ä¸‹ä¸€ç« **: [ç¬¬8ç«  æ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½²](./ç¬¬8ç« _æ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½².md)
