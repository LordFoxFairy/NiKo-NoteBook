# ç¬¬04ç¯‡_è§†é¢‘ç”Ÿæˆ(11)_VGenï¼šé˜¿é‡Œ8åˆ1è§†é¢‘ç”Ÿæˆç”Ÿæ€å®Œå…¨æŒ‡å—

> **æ›´æ–°æ—¶é—´**: 2025-11-30
> **GitHub**: https://github.com/ali-vilab/VGen
> **å¼€å‘å›¢é˜Ÿ**: é˜¿é‡Œå·´å·´é€šä¹‰å®éªŒå®¤ (Alibaba Tongyi Lab)
> **è®¸å¯è¯**: Apache-2.0ï¼ˆæ¨æµ‹ï¼Œä»“åº“æœªæ˜ç¡®æ ‡æ³¨ï¼‰
> **æ ¸å¿ƒä»·å€¼**: 8ä¸ªæ¨¡å‹çš„å®Œæ•´è§†é¢‘ç”Ÿæˆç”Ÿæ€ç³»ç»Ÿ

---

## ğŸ“‹ ç›®å½•

1. [VGenç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ](#1-vgenç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ)
2. [I2VGen-XLï¼šé«˜è´¨é‡å›¾ç”Ÿè§†é¢‘](#2-i2vgen-xlé«˜è´¨é‡å›¾ç”Ÿè§†é¢‘)
3. [DreamVideoï¼šä¸»ä½“+è¿åŠ¨å®šåˆ¶](#3-dreamvideoä¸»ä½“è¿åŠ¨å®šåˆ¶)
4. [VideoComposerï¼šå¯æ§è¿åŠ¨åˆæˆ](#4-videocomposerå¯æ§è¿åŠ¨åˆæˆ)
5. [HiGenï¼šæ—¶ç©ºè§£è€¦ç”Ÿæˆ](#5-higenæ—¶ç©ºè§£è€¦ç”Ÿæˆ)
6. [InstructVideoï¼šäººç±»åé¦ˆæŒ‡å¯¼](#6-instructvideoäººç±»åé¦ˆæŒ‡å¯¼)
7. [ModelScope T2Vï¼šåŸºç¡€æ–‡ç”Ÿè§†é¢‘](#7-modelscope-t2våŸºç¡€æ–‡ç”Ÿè§†é¢‘)
8. [TF-T2Vä¸VideoLCMï¼šæ•ˆç‡ä¼˜åŒ–](#8-tf-t2vä¸videolcmæ•ˆç‡ä¼˜åŒ–)
9. [ç¯å¢ƒæ­å»ºä¸ç»Ÿä¸€æ¥å£](#9-ç¯å¢ƒæ­å»ºä¸ç»Ÿä¸€æ¥å£)
10. [å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹](#10-å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹)
11. [å¤šå¹³å°éƒ¨ç½²æŒ‡å—](#11-å¤šå¹³å°éƒ¨ç½²æŒ‡å—)
12. [ä¸å…¶ä»–ç”Ÿæ€å¯¹æ¯”](#12-ä¸å…¶ä»–ç”Ÿæ€å¯¹æ¯”)
13. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#13-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## 1. VGenç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ

### 1.1 ä¸ºä»€ä¹ˆé€‰æ‹©VGen

VGenæ˜¯é˜¿é‡Œå·´å·´é€šä¹‰å®éªŒå®¤å¼€å‘çš„**å¼€æºè§†é¢‘ç”Ÿæˆä»£ç åº“**ï¼Œå…·æœ‰ä»¥ä¸‹ç‹¬ç‰¹ä¼˜åŠ¿ï¼š

#### **ğŸ¯ 8åˆ1å®Œæ•´ç”Ÿæ€**

```
VGenç”Ÿæ€åœ°å›¾:

å›¾ç”Ÿè§†é¢‘ (I2V):
  â”œâ”€ I2VGen-XL: çº§è”æ‰©æ•£ï¼Œ1280Ã—720é«˜æ¸…

æ–‡ç”Ÿè§†é¢‘ (T2V):
  â”œâ”€ ModelScope T2V: åŸºç¡€æ–‡ç”Ÿè§†é¢‘
  â”œâ”€ HiGen: æ—¶ç©ºåˆ†å±‚è§£è€¦
  â”œâ”€ TF-T2V: æ— æ–‡æœ¬è§†é¢‘æ‰©å±•è®­ç»ƒ
  â””â”€ VideoLCM: æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆé«˜æ•ˆï¼‰

å¯æ§ç”Ÿæˆ:
  â”œâ”€ VideoComposer: è¿åŠ¨å¯æ§åˆæˆ
  â””â”€ InstructVideo: äººç±»åé¦ˆæŒ‡å¯¼

è‡ªå®šä¹‰ç”Ÿæˆ:
  â””â”€ DreamVideo: ä¸»ä½“+è¿åŠ¨å®šåˆ¶ â­ï¸
```

#### **ğŸ”§ æ¨¡å—åŒ–è®¾è®¡**

```python
# VGenæ ¸å¿ƒä¼˜åŠ¿
VGEN_ADVANTAGES = {
    "æ¨¡å—åŒ–": "8ä¸ªæ¨¡å‹ç‹¬ç«‹è¿è¡Œï¼ŒæŒ‰éœ€é€‰æ‹©",
    "æ‰©å±•æ€§": "æ˜“äºç®¡ç†è‡ªå·±çš„å®éªŒ",
    "å·¥å…·é“¾": "å¯è§†åŒ–ã€é‡‡æ ·ã€è®­ç»ƒã€æ¨ç†ä¸€ä½“åŒ–",
    "å¤šå¹³å°": "HuggingFace + ModelScope + Replicate"
}
```

#### **ğŸ“Š æ¨¡å‹å¯¹æ¯”æ¦‚è§ˆ**

| æ¨¡å‹ | ç±»å‹ | åˆ†è¾¨ç‡ | æ ¸å¿ƒç‰¹ç‚¹ | æ¨èåœºæ™¯ |
|------|------|--------|---------|---------|
| **I2VGen-XL** | I2V | 1280Ã—720 | çº§è”æ‰©æ•£ï¼Œé«˜è´¨é‡ | å›¾ç‰‡åŠ¨ç”»åŒ– |
| **DreamVideo** | å®šåˆ¶ | å¯å˜ | ä¸»ä½“/è¿åŠ¨å®šåˆ¶ | ä¸ªæ€§åŒ–è§†é¢‘ |
| **VideoComposer** | å¯æ§ | å¯å˜ | è¿åŠ¨æ¡ä»¶æ§åˆ¶ | ç²¾ç¡®æ§åˆ¶ |
| **HiGen** | T2V | 600+ | æ—¶ç©ºè§£è€¦+è¶…åˆ† | é«˜åˆ†è¾¨ç‡T2V |
| **InstructVideo** | åé¦ˆ | å¯å˜ | äººç±»åé¦ˆä¼˜åŒ– | è´¨é‡æå‡ |
| **ModelScope T2V** | T2V | 256Ã—256 | åŸºç¡€æ–‡ç”Ÿè§†é¢‘ | å¿«é€ŸåŸå‹ |
| **TF-T2V** | è®­ç»ƒ | - | æ— æ–‡æœ¬æ‰©å±•è®­ç»ƒ | æ¨¡å‹ä¼˜åŒ– |
| **VideoLCM** | é«˜æ•ˆ | å¯å˜ | ä¸€è‡´æ€§æ¨¡å‹åŠ é€Ÿ | å¿«é€Ÿç”Ÿæˆ |

### 1.2 å‘å¸ƒæ—¶é—´çº¿

```
2023.11: I2VGen-XLé«˜è´¨é‡æ¨¡å‹å‘å¸ƒ
2023.12: I2VGen-XL + ModelScope T2Vä»£ç å¼€æº
2024.01: HuggingFace/ModelScopeæ¼”ç¤ºä¸Šçº¿
2024.03: DreamVideo + HiGenå‘å¸ƒ
2024.04: DreamVideoè®­ç»ƒä»£ç  + ModelScope T2V V1.5
        TF-T2V + VideoLCMå‘å¸ƒ
2024.06: InstructVideo + LoRAå¾®è°ƒæ”¯æŒ
2025.01: è¯„æµ‹æŒ‡æ ‡ä»£ç  (CLIP-T/I, DINO-I, Temporal Consistency)
```

### 1.3 ç”Ÿæ€æ¶æ„

```python
# VGenç»Ÿä¸€æ¶æ„

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  User Prompt    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  I2V    â”‚    â”‚  T2V    â”‚    â”‚ Control â”‚
    â”‚  Path   â”‚    â”‚  Path   â”‚    â”‚  Path   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚
    I2VGen-XL      ModelScope      VideoComposer
    DreamVideo      HiGen          InstructVideo
                    TF-T2V
                    VideoLCM
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   VAE Decoder   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚  Video  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. I2VGen-XLï¼šé«˜è´¨é‡å›¾ç”Ÿè§†é¢‘

### 2.1 æŠ€æœ¯ç‰¹ç‚¹

**è®ºæ–‡**: "High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"

#### **çº§è”æ‰©æ•£æ¶æ„**

```python
# ä¸¤é˜¶æ®µçº§è”æµç¨‹

Stage 1: Base Generation
  Input: å›¾åƒ (ä»»æ„åˆ†è¾¨ç‡)
  Process: ä½åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆ (256Ã—256, 16å¸§)
  Output: ç²—ç³™è§†é¢‘

Stage 2: Super-Resolution
  Input: Stage 1è¾“å‡º
  Process: è¶…åˆ†è¾¨ç‡è‡³1280Ã—720
  Output: é«˜æ¸…è§†é¢‘ (1280Ã—720, 16å¸§)
```

æ•°å­¦æ¨¡å‹ï¼š

Stage 1åŸºç¡€ç”Ÿæˆï¼š
$$
p_{\theta_1}(x_{0:T}^{low} | I, c) = \prod_{t=1}^{T} p_{\theta_1}(x_{t-1}^{low} | x_t^{low}, I, c)
$$

Stage 2è¶…åˆ†è¾¨ç‡ï¼š
$$
p_{\theta_2}(x_{0:T}^{high} | x_0^{low}, I, c) = \prod_{t=1}^{T} p_{\theta_2}(x_{t-1}^{high} | x_t^{high}, x_0^{low}, I, c)
$$

å…¶ä¸­ $I$ ä¸ºè¾“å…¥å›¾åƒï¼Œ$c$ ä¸ºæ–‡æœ¬æ¡ä»¶ã€‚

### 2.2 å®‰è£…ä¸æ¨ç†

#### **ç¯å¢ƒå‡†å¤‡**

```bash
# å…‹éš†VGenä»“åº“
git clone https://github.com/ali-vilab/VGen.git
cd VGen

# åˆ›å»ºç¯å¢ƒ
conda create -n vgen python=3.8
conda activate vgen

# å®‰è£…ä¾èµ–
pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
pip install -r requirements.txt
```

#### **ä¸‹è½½I2VGen-XLæ¨¡å‹**

```python
# æ–¹æ³•1: ModelScope
from modelscope.hub.snapshot_download import snapshot_download

model_dir = snapshot_download(
    'damo/I2VGen-XL',
    cache_dir='models/'
)

# æ–¹æ³•2: HuggingFace
from huggingface_hub import snapshot_download

model_dir = snapshot_download(
    repo_id="ali-vilab/i2vgen-xl",
    cache_dir="models/"
)
```

#### **Pythonæ¨ç†API**

```python
# i2vgen_xl_inference.py

import torch
from vgen.models import I2VGenXL
from PIL import Image
import numpy as np

# åŠ è½½æ¨¡å‹
model = I2VGenXL(
    model_path="models/i2vgen_xl_00854500.pth",
    device="cuda"
)

# åŠ è½½è¾“å…¥å›¾åƒ
image = Image.open("squat_start.jpg")

# æ¨ç†å‚æ•°
config = {
    "prompt": "å¥èº«æ•™ç»ƒä»å‡†å¤‡å§¿åŠ¿å¼€å§‹æ·±è¹²ï¼ŒåŠ¨ä½œæµç•…æ ‡å‡†",
    "negative_prompt": "é™æ­¢ï¼Œæ¨¡ç³Šï¼Œä½è´¨é‡",
    "num_frames": 16,
    "height": 720,
    "width": 1280,
    "num_inference_steps": 50,
    "guidance_scale": 7.5,
    "seed": 42
}

# ç”Ÿæˆè§†é¢‘
video = model.generate(
    image=image,
    **config
)

# ä¿å­˜
model.save_video(video, "squat_i2v.mp4", fps=8)

print(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: squat_i2v.mp4")
print(f"åˆ†è¾¨ç‡: {config['width']}Ã—{config['height']}")
print(f"å¸§æ•°: {config['num_frames']}")
print(f"æ—¶é•¿: {config['num_frames'] / 8}ç§’")
```

#### **å‘½ä»¤è¡Œæ¨ç†**

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶æ¨ç†
python inference.py --cfg configs/i2vgen_xl_infer.yaml

# è‡ªå®šä¹‰å‚æ•°
python inference.py \
  --cfg configs/i2vgen_xl_infer.yaml \
  --test_list_path data/test_list.txt \
  --test_model models/i2vgen_xl_00854500.pth \
  --seed 2024
```

### 2.3 æ€§èƒ½ä¸å±€é™

#### **æ€§èƒ½æ•°æ®**

```
åˆ†è¾¨ç‡: 1280Ã—720
å¸§æ•°: 16å¸§
æ¨ç†æ—¶é—´: ~120ç§’ (A100 40GB)
æ˜¾å­˜éœ€æ±‚: 18GB
```

#### **å·²çŸ¥å±€é™ï¼ˆå®˜æ–¹è¯´æ˜ï¼‰**

```python
# I2VGen-XLå½“å‰é™åˆ¶

é™åˆ¶1: åŠ¨æ¼«é£æ ¼å›¾åƒæ•ˆæœä¸ä½³
  åŸå› : è®­ç»ƒæ•°æ®ç¼ºä¹åŠ¨æ¼«å†…å®¹
  ç¼“è§£: å›¢é˜Ÿæ­£åœ¨ä¼˜åŒ–

é™åˆ¶2: é»‘è‰²èƒŒæ™¯å›¾åƒæ•ˆæœä¸ä½³
  åŸå› : æ•°æ®é›†åå‘è‡ªç„¶èƒŒæ™¯
  è§£å†³: é¿å…çº¯é»‘èƒŒæ™¯è¾“å…¥

é™åˆ¶3: å¤§å¹…è¿åŠ¨æ§åˆ¶æœ‰é™
  åŸå› : çº§è”æ¶æ„çº¦æŸ
  å»ºè®®: ä½¿ç”¨DreamVideoè¿›è¡Œè¿åŠ¨å®šåˆ¶
```

---

## 3. DreamVideoï¼šä¸»ä½“+è¿åŠ¨å®šåˆ¶

### 3.1 æ ¸å¿ƒåˆ›æ–°

**è®ºæ–‡**: "Composing Your Dream Videos with Customized Subject and Motion"

DreamVideoå®ç°äº†è§†é¢‘ç”Ÿæˆçš„**ä¸‰ç»´å®šåˆ¶**ï¼š

```
DreamVideoä¸‰ç§å®šåˆ¶æ¨¡å¼:

1. ä¸»ä½“å®šåˆ¶ (Subject Customization)
   Input: 3-5å¼ ä¸»ä½“å›¾ç‰‡
   Output: ä¿æŒä¸»ä½“ä¸€è‡´æ€§çš„è§†é¢‘
   æŠ€æœ¯: Textual Inversion + Identity Adapter

2. è¿åŠ¨å®šåˆ¶ (Motion Customization)
   Input: å‚è€ƒè¿åŠ¨è§†é¢‘
   Output: å­¦ä¹ ç‰¹å®šè¿åŠ¨æ¨¡å¼
   æŠ€æœ¯: Motion LoRA

3. è”åˆå®šåˆ¶ (Joint Customization)
   Input: ä¸»ä½“å›¾ç‰‡ + è¿åŠ¨è§†é¢‘
   Output: ç‰¹å®šä¸»ä½“æ‰§è¡Œç‰¹å®šè¿åŠ¨
   æŠ€æœ¯: Subject + Motionè”åˆè®­ç»ƒ
```

### 3.2 æ¶æ„è®¾è®¡

```python
# DreamVideoå®Œæ•´æµç¨‹

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: Subject Learning (ä¸¤æ­¥)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Step 1: Textual Inversion            â”‚
â”‚    - å­¦ä¹ ä¸»ä½“åµŒå…¥ (text embedding)    â”‚
â”‚    - è¾“å…¥: 3-5å¼ å‚è€ƒå›¾                â”‚
â”‚    - è¾“å‡º: <subject> token            â”‚
â”‚                                        â”‚
â”‚  Step 2: Identity Adapter             â”‚
â”‚    - å­¦ä¹ ä¸»ä½“é€‚é…å™¨                   â”‚
â”‚    - è¾“å…¥: å‚è€ƒå›¾ + <subject> token   â”‚
â”‚    - è¾“å‡º: identity_adapter.pth       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: Motion Learning              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Motion LoRA Training                  â”‚
â”‚    - å­¦ä¹ è¿åŠ¨æ¨¡å¼                     â”‚
â”‚    - è¾“å…¥: å‚è€ƒè¿åŠ¨è§†é¢‘                â”‚
â”‚    - è¾“å‡º: motion_lora.pth             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: Joint Inference              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ModelScope T2V Base Model             â”‚
â”‚    + Subject Adapter                   â”‚
â”‚    + Motion LoRA                       â”‚
â”‚    â†’ ç”Ÿæˆå®šåˆ¶è§†é¢‘                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 ä¸»ä½“å®šåˆ¶å®æˆ˜

#### **Step 1: Textual Inversionè®­ç»ƒ**

```bash
# å‡†å¤‡æ•°æ®
data/
  subject_learning/
    dog2/
      images/
        00.jpg  # æ­£é¢ç…§
        01.jpg  # ä¾§é¢ç…§
        02.jpg  # èƒŒé¢ç…§
        03.jpg  # åŠ¨ä½œç…§
        04.jpg  # ç‰¹å†™ç…§

# é…ç½®æ–‡ä»¶: configs/dreamvideo/subjectLearning/dog2_step1.yaml
model:
  pretrained_model: "models/modelscope_t2v.pth"
  learning_rate: 0.0001
  max_train_steps: 500

data:
  train_data_dir: "data/subject_learning/dog2/images"
  subject_token: "<dog2>"
  prompt_template: "a photo of <dog2>"

# è®­ç»ƒ
python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/dog2_step1.yaml

# è¾“å‡º
outputs/dog2_step1/
  text_inv_embeddings.pth  # Textual InversionåµŒå…¥
```

#### **Step 2: Identity Adapterè®­ç»ƒ**

```bash
# é…ç½®æ–‡ä»¶: configs/dreamvideo/subjectLearning/dog2_step2.yaml
model:
  pretrained_model: "models/modelscope_t2v.pth"
  text_inv_path: "outputs/dog2_step1/text_inv_embeddings.pth"
  learning_rate: 0.00005
  max_train_steps: 1000

# è®­ç»ƒ
python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/dog2_step2.yaml

# è¾“å‡º
outputs/dog2_step2/
  identity_adapter.pth  # èº«ä»½é€‚é…å™¨
```

#### **æ¨ç†ï¼šä½¿ç”¨å®šåˆ¶ä¸»ä½“**

```python
# dream_subject_inference.py

from vgen.models import DreamVideo

# åŠ è½½æ¨¡å‹
model = DreamVideo(
    base_model="models/modelscope_t2v.pth",
    subject_adapter="outputs/dog2_step2/identity_adapter.pth",
    subject_token="<dog2>"
)

# ç”Ÿæˆè§†é¢‘
video = model.generate(
    prompt="<dog2> running in the park, sunny day",
    negative_prompt="blurry, low quality",
    num_frames=16,
    height=256,
    width=256,
    seed=42
)

model.save_video(video, "dog2_running.mp4", fps=8)
```

### 3.4 è¿åŠ¨å®šåˆ¶å®æˆ˜

```bash
# å‡†å¤‡è¿åŠ¨å‚è€ƒè§†é¢‘
data/
  motion_learning/
    carTurn/
      reference.mp4  # æ±½è½¦è½¬å¼¯è§†é¢‘

# é…ç½®æ–‡ä»¶: configs/dreamvideo/motionLearning/carTurn.yaml
model:
  pretrained_model: "models/modelscope_t2v.pth"
  learning_rate: 0.0001
  max_train_steps: 800

data:
  train_video_path: "data/motion_learning/carTurn/reference.mp4"
  motion_name: "carTurn"

# è®­ç»ƒMotion LoRA
python train_net.py \
  --cfg configs/dreamvideo/motionLearning/carTurn.yaml

# è¾“å‡º
outputs/carTurn/
  motion_lora.pth  # è¿åŠ¨LoRA
```

#### **æ¨ç†ï¼šä½¿ç”¨å®šåˆ¶è¿åŠ¨**

```python
# dream_motion_inference.py

model = DreamVideo(
    base_model="models/modelscope_t2v.pth",
    motion_lora="outputs/carTurn/motion_lora.pth"
)

video = model.generate(
    prompt="A sports car on the highway",
    motion_strength=1.0,  # è¿åŠ¨LoRAå¼ºåº¦
    num_frames=16,
    seed=42
)

model.save_video(video, "sportscar_turn.mp4", fps=8)
```

### 3.5 è”åˆå®šåˆ¶å®æˆ˜

**åœºæ™¯**: è®©å®šåˆ¶çš„å¥èº«æ•™ç»ƒæ‰§è¡Œæ·±è¹²åŠ¨ä½œ

```python
# 1. è®­ç»ƒå¥èº«æ•™ç»ƒä¸»ä½“
python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/coach_step1.yaml

python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/coach_step2.yaml

# 2. è®­ç»ƒæ·±è¹²è¿åŠ¨
python train_net.py \
  --cfg configs/dreamvideo/motionLearning/squat.yaml

# 3. è”åˆæ¨ç†
from vgen.models import DreamVideo

model = DreamVideo(
    base_model="models/modelscope_t2v.pth",
    subject_adapter="outputs/coach_step2/identity_adapter.pth",
    subject_token="<coach>",
    motion_lora="outputs/squat/motion_lora.pth"
)

video = model.generate(
    prompt="<coach> doing squat in the gym",
    motion_strength=0.9,
    num_frames=16,
    seed=2024
)

model.save_video(video, "coach_squat.mp4", fps=8)
```

### 3.6 è®­ç»ƒæˆæœ¬ä¼°ç®—

| è®­ç»ƒé˜¶æ®µ | GPU | æ—¶é—´ | æ˜¾å­˜ | æˆæœ¬ (A100) |
|---------|-----|------|------|------------|
| Textual Inversion | A100 | 30åˆ†é’Ÿ | 16GB | $1.5 |
| Identity Adapter | A100 | 1å°æ—¶ | 18GB | $3 |
| Motion LoRA | A100 | 45åˆ†é’Ÿ | 18GB | $2.25 |
| **æ€»è®¡** | - | **~2.25å°æ—¶** | - | **~$6.75** |

---

## 4. VideoComposerï¼šå¯æ§è¿åŠ¨åˆæˆ

### 4.1 æ ¸å¿ƒç‰¹ç‚¹

**è®ºæ–‡**: "Compositional Video Synthesis with Motion Controllability"

VideoComposeræ”¯æŒ**å¤šç§è¿åŠ¨æ§åˆ¶æ¡ä»¶çš„ç»„åˆ**ï¼š

```python
# æ”¯æŒçš„æ§åˆ¶æ¡ä»¶

CONTROL_CONDITIONS = {
    "depth": "æ·±åº¦å›¾æ§åˆ¶",
    "canny": "è¾¹ç¼˜æ£€æµ‹æ§åˆ¶",
    "sketch": "è‰å›¾æ§åˆ¶",
    "motion": "å…‰æµæ§åˆ¶",
    "single_sketch": "å•å¸§è‰å›¾",
    "mask": "é®ç½©æ§åˆ¶",
    "image": "å‚è€ƒå›¾åƒ"
}

# ç»„åˆæ§åˆ¶ç¤ºä¾‹
controls = {
    "depth": depth_map,      # æ§åˆ¶ç©ºé—´ç»“æ„
    "motion": optical_flow,  # æ§åˆ¶è¿åŠ¨æ–¹å‘
    "mask": foreground_mask  # æ§åˆ¶ç”ŸæˆåŒºåŸŸ
}
```

### 4.2 æ¨ç†ç¤ºä¾‹

```python
# video_composer_inference.py

from vgen.models import VideoComposer

# åŠ è½½æ¨¡å‹
model = VideoComposer(
    model_path="models/video_composer.pth"
)

# å‡†å¤‡æ§åˆ¶æ¡ä»¶
from PIL import Image
import cv2

# 1. æ·±åº¦å›¾æ§åˆ¶
depth_map = cv2.imread("gym_depth.png", cv2.IMREAD_GRAYSCALE)

# 2. å…‰æµæ§åˆ¶ï¼ˆè¿åŠ¨æ–¹å‘ï¼‰
optical_flow = compute_optical_flow("reference_motion.mp4")

# ç”Ÿæˆå¯æ§è§†é¢‘
video = model.generate(
    prompt="å¥èº«æˆ¿å™¨æå±•ç¤ºï¼Œé•œå¤´ä»å·¦å‘å³ç§»åŠ¨",
    depth=depth_map,
    motion=optical_flow,
    num_frames=16,
    height=256,
    width=256
)

model.save_video(video, "gym_tour_controlled.mp4", fps=8)
```

---

## 5. HiGenï¼šæ—¶ç©ºè§£è€¦ç”Ÿæˆ

### 5.1 æŠ€æœ¯åˆ›æ–°

**è®ºæ–‡**: "Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation"

HiGené‡‡ç”¨**ä¸¤é˜¶æ®µæ—¶ç©ºåˆ†å±‚è§£è€¦**æ¶æ„ï¼š

```
Stage 1: Base Generation (æ—¶ç©ºè€¦åˆ)
  Input: Text prompt
  Output: ä½åˆ†è¾¨ç‡è§†é¢‘ (256Ã—256, 16å¸§)

Stage 2: Super-Resolution (ç©ºé—´è§£è€¦)
  Input: Stage 1è¾“å‡º
  Output: è¶…åˆ†è¾¨ç‡è§†é¢‘ (600+åˆ†è¾¨ç‡)
```

### 5.2 æ¨ç†æµç¨‹

```bash
# æ–‡æœ¬ç”Ÿæˆè§†é¢‘
python inference.py --cfg configs/higen_infer.yaml

# è¶…åˆ†è¾¨ç‡ï¼ˆç‹¬ç«‹ä½¿ç”¨ï¼‰
python inference.py --cfg configs/sr600_infer.yaml
```

#### **Python API**

```python
from vgen.models import HiGen

# åŠ è½½æ¨¡å‹
higen = HiGen(
    base_model="models/higen_base.pth",
    sr_model="models/higen_sr600.pth"
)

# ä¸¤é˜¶æ®µç”Ÿæˆ
video_low = higen.generate_base(
    prompt="å¥èº«æ•™ç»ƒæ¼”ç¤ºä¿¯å§æ’‘",
    num_frames=16
)

video_high = higen.super_resolution(
    video_low,
    target_resolution=600
)

higen.save_video(video_high, "pushup_hires.mp4", fps=8)
```

---

## 6. InstructVideoï¼šäººç±»åé¦ˆæŒ‡å¯¼

### 6.1 æ ¸å¿ƒæ€æƒ³

**è®ºæ–‡**: "Instructing Video Diffusion Models with Human Feedback"

InstructVideoé€šè¿‡**äººç±»åé¦ˆ**ä¼˜åŒ–è§†é¢‘ç”Ÿæˆï¼š

```python
# InstructVideoå·¥ä½œæµ

1. åˆå§‹ç”Ÿæˆ
   prompt â†’ base_model â†’ video_v1

2. äººç±»åé¦ˆ
   video_v1 â†’ human_rating (1-5åˆ†)
           â†’ feedback_text ("è¿åŠ¨å¤ªå¿«", "é¢œè‰²ä¸å¯¹"ç­‰)

3. LoRAå¾®è°ƒ
   feedback_data â†’ LoRA_training â†’ instruct_lora.pth

4. ä¼˜åŒ–ç”Ÿæˆ
   prompt + instruct_lora â†’ video_v2 (æ”¹è¿›ç‰ˆ)
```

### 6.2 LoRAå¾®è°ƒ

```bash
# å‡†å¤‡åé¦ˆæ•°æ®
data/
  instruct/
    feedback.json:
      [
        {
          "prompt": "å¥èº«æ•™ç»ƒæ·±è¹²",
          "video_path": "squat_v1.mp4",
          "rating": 3,
          "feedback": "åŠ¨ä½œé€Ÿåº¦å¤ªå¿«ï¼Œéœ€è¦æ›´æ…¢æ›´å¹³ç¨³"
        },
        ...
      ]

# é…ç½®æ–‡ä»¶: configs/instruct_video_lora.yaml
model:
  base_model: "models/modelscope_t2v.pth"
  learning_rate: 0.0001
  lora_rank: 16

data:
  feedback_json: "data/instruct/feedback.json"

# è®­ç»ƒLoRA
python train_net.py \
  --cfg configs/instruct_video_lora.yaml

# è¾“å‡º
outputs/instruct_lora/
  lora_weights.pth
```

#### **ä½¿ç”¨åé¦ˆä¼˜åŒ–çš„LoRA**

```python
from vgen.models import InstructVideo

model = InstructVideo(
    base_model="models/modelscope_t2v.pth",
    lora_path="outputs/instruct_lora/lora_weights.pth"
)

video = model.generate(
    prompt="å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²ï¼ŒåŠ¨ä½œç¼“æ…¢æ ‡å‡†",
    num_frames=16
)

model.save_video(video, "squat_improved.mp4", fps=8)
```

---

## 7. ModelScope T2Vï¼šåŸºç¡€æ–‡ç”Ÿè§†é¢‘

### 7.1 æ¨¡å‹ç‰ˆæœ¬

```python
# ModelScope T2Vç‰ˆæœ¬

V1.0 (2023.03):
  - åŸºç¡€æ–‡ç”Ÿè§†é¢‘æ¨¡å‹
  - 256Ã—256åˆ†è¾¨ç‡
  - 16å¸§ç”Ÿæˆ

V1.5 (2024.04):
  - 365kè¿­ä»£å¾®è°ƒ
  - æ›´å¤šæ•°æ®è®­ç»ƒ
  - è´¨é‡æå‡15-20%
```

### 7.2 å¿«é€Ÿæ¨ç†

```bash
# ä½¿ç”¨V1.5
python inference.py \
  --cfg configs/modelscope_t2v_v1.5_infer.yaml \
  --prompt "å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œ" \
  --seed 2024
```

#### **Python API**

```python
from vgen.models import ModelScopeT2V

model = ModelScopeT2V(
    model_path="models/modelscope_t2v_v1.5.pth"
)

video = model.generate(
    prompt="å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼Œå¥èº«æˆ¿èƒŒæ™¯",
    negative_prompt="æ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œé™æ­¢",
    num_frames=16,
    height=256,
    width=256,
    num_inference_steps=50,
    guidance_scale=9.0
)

model.save_video(video, "squat_t2v.mp4", fps=8)
```

---

## 8. TF-T2Vä¸VideoLCMï¼šæ•ˆç‡ä¼˜åŒ–

### 8.1 TF-T2V: æ— æ–‡æœ¬è§†é¢‘æ‰©å±•è®­ç»ƒ

**è®ºæ–‡**: "A Recipe for Scaling up Text-to-Video Generation with Text-free Videos"

æ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨**æ— æ–‡æœ¬æ ‡æ³¨çš„è§†é¢‘**æ‰©å±•è®­ç»ƒï¼š

```python
# ä¼ ç»ŸT2Vè®­ç»ƒ
è®­ç»ƒæ•°æ®: (video, text_caption) é…å¯¹æ•°æ®
é™åˆ¶: éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨

# TF-T2Væ–¹æ³•
è®­ç»ƒæ•°æ®:
  - (video, text) é…å¯¹æ•°æ®
  + video-onlyæ•°æ®ï¼ˆæ— æ–‡æœ¬ï¼‰ â­ï¸

æ–¹æ³•:
  - ä½¿ç”¨é¢„è®­ç»ƒCLIPæå–è§†è§‰ç‰¹å¾
  - å¯¹é½è§†è§‰-æ–‡æœ¬ç©ºé—´
  - æ‰©å±•è®­ç»ƒæ•°æ®10Ã—
```

### 8.2 VideoLCM: æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹

**è®ºæ–‡**: "Video Latent Consistency Model"

VideoLCMé€šè¿‡**ä¸€è‡´æ€§è’¸é¦**å®ç°å¿«é€Ÿç”Ÿæˆï¼š

```python
# ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹
æ¨ç†æ­¥æ•°: 50-100æ­¥
ç”Ÿæˆæ—¶é—´: 120ç§’

# VideoLCM
æ¨ç†æ­¥æ•°: 4-8æ­¥ â­ï¸
ç”Ÿæˆæ—¶é—´: 15ç§’
è´¨é‡æŸå¤±: <5%
```

#### **ä½¿ç”¨VideoLCMåŠ é€Ÿ**

```python
from vgen.models import VideoLCM

# åŠ è½½LCMæ¨¡å‹
model = VideoLCM(
    base_model="models/modelscope_t2v.pth",
    lcm_weights="models/video_lcm.pth"
)

video = model.generate(
    prompt="å¥èº«æ•™ç»ƒæ·±è¹²",
    num_frames=16,
    num_inference_steps=4,  # ä»…éœ€4æ­¥ï¼
    guidance_scale=1.0       # LCMæ¨èä½guidance
)

model.save_video(video, "squat_lcm.mp4", fps=8)

print("ç”Ÿæˆæ—¶é—´: ~15ç§’ (vs ä¼ ç»Ÿ120ç§’)")
```

---

## 9. ç¯å¢ƒæ­å»ºä¸ç»Ÿä¸€æ¥å£

### 9.1 å®Œæ•´å®‰è£…

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/ali-vilab/VGen.git
cd VGen

# 2. åˆ›å»ºç¯å¢ƒ
conda create -n vgen python=3.8
conda activate vgen

# 3. å®‰è£…PyTorch
pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 \
    --extra-index-url https://download.pytorch.org/whl/cu113

# 4. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 5. å®‰è£…xformersï¼ˆå¯é€‰ï¼ŒåŠ é€Ÿï¼‰
pip install xformers==0.0.22
```

### 9.2 æ¨¡å‹ä¸‹è½½

```python
# download_models.py

from modelscope.hub.snapshot_download import snapshot_download

# ä¸‹è½½æ‰€æœ‰VGenæ¨¡å‹
models = {
    "I2VGen-XL": "damo/I2VGen-XL",
    "ModelScope T2V": "damo/text-to-video-synthesis",
    "VideoComposer": "damo/VideoComposer",
    # å…¶ä»–æ¨¡å‹...
}

for name, repo_id in models.items():
    print(f"ä¸‹è½½ {name}...")
    model_dir = snapshot_download(
        repo_id,
        cache_dir=f"models/{name}"
    )
    print(f"  â†’ {model_dir}")
```

### 9.3 ç»Ÿä¸€æ¨ç†æ¥å£

```python
# vgen_unified_api.py

class VGenUnified:
    """VGenç»Ÿä¸€æ¨ç†æ¥å£"""

    def __init__(self, model_name):
        self.model_name = model_name
        self.model = self._load_model()

    def _load_model(self):
        if self.model_name == "i2vgen-xl":
            from vgen.models import I2VGenXL
            return I2VGenXL("models/i2vgen_xl_00854500.pth")
        elif self.model_name == "dreamvideo":
            from vgen.models import DreamVideo
            return DreamVideo("models/modelscope_t2v.pth")
        # ... å…¶ä»–æ¨¡å‹

    def generate(self, **kwargs):
        """ç»Ÿä¸€ç”Ÿæˆæ¥å£"""
        return self.model.generate(**kwargs)

# ä½¿ç”¨ç¤ºä¾‹
vgen = VGenUnified("i2vgen-xl")

video = vgen.generate(
    image="squat.jpg",
    prompt="å¥èº«æ•™ç»ƒæ·±è¹²åŠ¨ä½œ",
    num_frames=16
)
```

### 9.4 Gradioæœ¬åœ°æ¼”ç¤º

```bash
# å¯åŠ¨Gradioç•Œé¢
python gradio_app.py

# è®¿é—®: http://localhost:7860
```

Gradioç•Œé¢åŠŸèƒ½ï¼š
- é€‰æ‹©æ¨¡å‹ï¼ˆI2VGen-XL, DreamVideoç­‰ï¼‰
- ä¸Šä¼ å›¾ç‰‡/è§†é¢‘
- è¾“å…¥æç¤ºè¯
- è°ƒæ•´å‚æ•°
- å®æ—¶é¢„è§ˆç”Ÿæˆç»“æœ

---

## 10. å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹

### 10.1 æ¡ˆä¾‹1ï¼šI2VGen-XLå›¾ç”Ÿè§†é¢‘

```python
# åœºæ™¯: å°†é™æ€æ·±è¹²å›¾ç‰‡è½¬ä¸ºåŠ¨ä½œè§†é¢‘

from vgen.models import I2VGenXL
from PIL import Image

# åŠ è½½æ¨¡å‹
model = I2VGenXL("models/i2vgen_xl_00854500.pth")

# åŠ è½½å‚è€ƒå›¾
image = Image.open("squat_prepare.jpg").resize((1280, 720))

# ç”Ÿæˆé«˜æ¸…è§†é¢‘
video = model.generate(
    image=image,
    prompt="å¥èº«æ•™ç»ƒä»å‡†å¤‡å§¿åŠ¿å¼€å§‹æ·±è¹²ï¼ŒåŠ¨ä½œæµç•…æ ‡å‡†ï¼Œå¤§è…¿ä¸åœ°é¢å¹³è¡Œåç«™èµ·",
    negative_prompt="é™æ­¢ï¼Œæ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œé”™è¯¯å§¿åŠ¿",
    num_frames=16,
    height=720,
    width=1280,
    num_inference_steps=50,
    guidance_scale=7.5,
    seed=2024
)

model.save_video(video, "squat_i2v_hd.mp4", fps=8)

print("âœ“ ç”Ÿæˆå®Œæˆ: squat_i2v_hd.mp4")
print("  åˆ†è¾¨ç‡: 1280Ã—720")
print("  å¸§æ•°: 16å¸§ (2ç§’@8fps)")
```

### 10.2 æ¡ˆä¾‹2ï¼šDreamVideoå®šåˆ¶å¥èº«æ•™ç»ƒ

```bash
# Step 1: å‡†å¤‡æ•™ç»ƒç…§ç‰‡
data/subject_learning/coach_john/images/
  00.jpg  # æ­£é¢
  01.jpg  # ä¾§é¢
  02.jpg  # åŠ¨ä½œ1
  03.jpg  # åŠ¨ä½œ2

# Step 2: Textual Inversion
python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/coach_john_step1.yaml

# Step 3: Identity Adapter
python train_net.py \
  --cfg configs/dreamvideo/subjectLearning/coach_john_step2.yaml

# Step 4: æ¨ç†
python
```

```python
from vgen.models import DreamVideo

model = DreamVideo(
    base_model="models/modelscope_t2v.pth",
    subject_adapter="outputs/coach_john_step2/identity_adapter.pth",
    subject_token="<coach_john>"
)

# ç”Ÿæˆå¤šä¸ªåŠ¨ä½œè§†é¢‘
exercises = [
    "æ·±è¹² (squat)",
    "ç¡¬æ‹‰ (deadlift)",
    "å§æ¨ (bench press)"
]

for exercise in exercises:
    video = model.generate(
        prompt=f"<coach_john> demonstrating {exercise} in the gym",
        num_frames=16,
        seed=42
    )

    filename = f"coach_john_{exercise.split()[0]}.mp4"
    model.save_video(video, filename, fps=8)
    print(f"âœ“ {filename}")
```

### 10.3 æ¡ˆä¾‹3ï¼šVideoComposerç²¾ç¡®æ§åˆ¶

```python
# åœºæ™¯: ä½¿ç”¨æ·±åº¦å›¾æ§åˆ¶å¥èº«æˆ¿æ¼«æ¸¸

from vgen.models import VideoComposer
import cv2

# åŠ è½½æ¨¡å‹
composer = VideoComposer("models/video_composer.pth")

# å‡†å¤‡æ·±åº¦å›¾åºåˆ—ï¼ˆ16å¸§ï¼‰
depth_maps = []
for i in range(16):
    depth = cv2.imread(f"gym_depth_sequence/frame_{i:03d}.png", cv2.IMREAD_GRAYSCALE)
    depth_maps.append(depth)

# ç”Ÿæˆæ§åˆ¶è§†é¢‘
video = composer.generate(
    prompt="å¥èº«æˆ¿å†…éƒ¨ç¯å¢ƒå±•ç¤ºï¼Œç°ä»£è®¾å¤‡ï¼Œæ˜äº®å…‰çº¿",
    depth=depth_maps,
    num_frames=16,
    height=256,
    width=256,
    guidance_scale=7.5
)

composer.save_video(video, "gym_tour_depth_control.mp4", fps=8)
```

### 10.4 æ¡ˆä¾‹4ï¼šHiGené«˜åˆ†è¾¨ç‡ç”Ÿæˆ

```python
# åœºæ™¯: ç”Ÿæˆé«˜åˆ†è¾¨ç‡å¥èº«æ•™å­¦è§†é¢‘

from vgen.models import HiGen

higen = HiGen(
    base_model="models/higen_base.pth",
    sr_model="models/higen_sr600.pth"
)

# Stage 1: åŸºç¡€ç”Ÿæˆ
video_base = higen.generate_base(
    prompt="Professional fitness coach teaching proper squat form, gym background",
    negative_prompt="blurry, low quality, wrong posture",
    num_frames=16,
    num_inference_steps=50
)

# Stage 2: è¶…åˆ†è¾¨ç‡
video_hd = higen.super_resolution(
    video_base,
    target_resolution=600  # 600Ã—600
)

higen.save_video(video_hd, "squat_hd_higen.mp4", fps=8)

print("âœ“ é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆå®Œæˆ")
print("  åˆ†è¾¨ç‡: 600Ã—600 (vs 256Ã—256)")
```

---

## 11. å¤šå¹³å°éƒ¨ç½²æŒ‡å—

### 11.1 HuggingFace Spaces

```python
# app.py (Gradioéƒ¨ç½²)

import gradio as gr
from vgen.models import I2VGenXL, DreamVideo, ModelScopeT2V

# åŠ è½½æ¨¡å‹
i2vgen = I2VGenXL("models/i2vgen_xl_00854500.pth")
t2v = ModelScopeT2V("models/modelscope_t2v_v1.5.pth")

def generate_i2v(image, prompt):
    video = i2vgen.generate(
        image=image,
        prompt=prompt,
        num_frames=16
    )
    return video

def generate_t2v(prompt):
    video = t2v.generate(
        prompt=prompt,
        num_frames=16
    )
    return video

# Gradioç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# VGenè§†é¢‘ç”Ÿæˆæ¼”ç¤º")

    with gr.Tab("I2VGen-XL"):
        image_input = gr.Image(type="pil")
        prompt_input = gr.Textbox(label="æç¤ºè¯")
        i2v_button = gr.Button("ç”Ÿæˆè§†é¢‘")
        i2v_output = gr.Video()

        i2v_button.click(
            generate_i2v,
            inputs=[image_input, prompt_input],
            outputs=i2v_output
        )

    with gr.Tab("ModelScope T2V"):
        t2v_prompt = gr.Textbox(label="æç¤ºè¯")
        t2v_button = gr.Button("ç”Ÿæˆè§†é¢‘")
        t2v_output = gr.Video()

        t2v_button.click(
            generate_t2v,
            inputs=t2v_prompt,
            outputs=t2v_output
        )

demo.launch()
```

### 11.2 ModelScope Studio

```python
# ç›´æ¥ä½¿ç”¨ModelScopeéƒ¨ç½²

from modelscope.pipelines import pipeline
from modelscope.outputs import OutputKeys

# I2VGen-XLç®¡é“
i2v_pipeline = pipeline(
    task='image-to-video',
    model='damo/I2VGen-XL'
)

# æ¨ç†
output = i2v_pipeline({
    'image': 'squat.jpg',
    'text': 'å¥èº«æ•™ç»ƒæ·±è¹²åŠ¨ä½œ'
})

video_path = output[OutputKeys.OUTPUT_VIDEO]
print(f"è§†é¢‘ç”Ÿæˆè‡³: {video_path}")
```

åœ¨çº¿æ¼”ç¤ºï¼š
- HuggingFace: https://huggingface.co/spaces/damo-vilab/I2VGen-XL
- ModelScope: https://modelscope.cn/studios/damo/I2VGen-XL

### 11.3 Replicateéƒ¨ç½²

```python
# replicateéƒ¨ç½²é…ç½®

# cog.yaml
build:
  gpu: true
  python_version: "3.8"
  python_packages:
    - torch==1.12.0
    - torchvision==0.13.0
predict: "predict.py:Predictor"

# predict.py
from cog import BasePredictor, Input, Path
from vgen.models import I2VGenXL

class Predictor(BasePredictor):
    def setup(self):
        self.model = I2VGenXL("models/i2vgen_xl_00854500.pth")

    def predict(
        self,
        image: Path = Input(description="Input image"),
        prompt: str = Input(description="Text prompt")
    ) -> Path:
        video = self.model.generate(
            image=str(image),
            prompt=prompt,
            num_frames=16
        )

        output_path = "/tmp/output.mp4"
        self.model.save_video(video, output_path, fps=8)
        return Path(output_path)
```

---

## 12. ä¸å…¶ä»–ç”Ÿæ€å¯¹æ¯”

### 12.1 ç»¼åˆå¯¹æ¯”

| ç»´åº¦ | VGen | HunyuanVideo | CogVideoX | LTX-Video |
|------|------|-------------|-----------|-----------|
| æ¨¡å‹æ•°é‡ | **8ä¸ª** â­ï¸ | 1ä¸ª | 1ä¸ª | 3ä¸ª |
| æœ€é«˜åˆ†è¾¨ç‡ | 1280Ã—720 | 720p | 768p | **4K** â­ï¸ |
| å®šåˆ¶èƒ½åŠ› | **DreamVideo** â­ï¸ | âŒ | âŒ | LoRA |
| è¿åŠ¨æ§åˆ¶ | **VideoComposer** â­ï¸ | âŒ | âŒ | Depth/Pose/Canny |
| äººç±»åé¦ˆ | **InstructVideo** â­ï¸ | âŒ | âŒ | âŒ |
| é«˜æ•ˆç”Ÿæˆ | **VideoLCM** (4æ­¥) | âŒ | âŒ | **è’¸é¦ç‰ˆ** (8æ­¥) |
| æ¨¡å—åŒ– | âœ… | âŒ | âŒ | âœ… |
| å­¦ä¹ æ›²çº¿ | é™¡å³­ | ä¸­ç­‰ | æ˜“ | ä¸­ç­‰ |

### 12.2 ä½¿ç”¨åœºæ™¯æ¨è

```
å†³ç­–æ ‘:

éœ€è¦å®šåˆ¶ä¸»ä½“ï¼Ÿ
â”œâ”€ æ˜¯ â†’ VGen (DreamVideo) âœ…
â””â”€ å¦
    â””â”€ éœ€è¦ç²¾ç¡®è¿åŠ¨æ§åˆ¶ï¼Ÿ
        â”œâ”€ æ˜¯ â†’ VGen (VideoComposer) âœ…
        â””â”€ å¦
            â””â”€ éœ€è¦æœ€é«˜åˆ†è¾¨ç‡ï¼Ÿ
                â”œâ”€ æ˜¯ â†’ LTX-Video (4K) âœ…
                â””â”€ å¦
                    â””â”€ è¿½æ±‚æœ€ä½³è¿åŠ¨è´¨é‡ï¼Ÿ
                        â”œâ”€ æ˜¯ â†’ HunyuanVideo âœ…
                        â””â”€ å¦ â†’ CogVideoX (Apache 2.0)
```

### 12.3 VGenç‹¬ç‰¹ä¼˜åŠ¿

**å¼ºçƒˆæ¨èVGençš„åœºæ™¯**:
- âœ… éœ€è¦å®šåˆ¶ç‰¹å®šä¸»ä½“ï¼ˆDreamVideoï¼‰
- âœ… éœ€è¦ç²¾ç¡®è¿åŠ¨æ§åˆ¶ï¼ˆVideoComposerï¼‰
- âœ… éœ€è¦äººç±»åé¦ˆä¼˜åŒ–ï¼ˆInstructVideoï¼‰
- âœ… ç ”ç©¶å¤šç§æŠ€æœ¯è·¯çº¿
- âœ… æ¨¡å—åŒ–å®éªŒç¯å¢ƒ

**VGençš„åŠ£åŠ¿**:
- âš ï¸ å­¦ä¹ æ›²çº¿é™¡å³­ï¼ˆ8ä¸ªæ¨¡å‹ï¼‰
- âš ï¸ æ–‡æ¡£ç›¸å¯¹è¾ƒå°‘
- âš ï¸ åˆ†è¾¨ç‡ä¸å¦‚LTX-Video
- âš ï¸ è¿åŠ¨è´¨é‡ä¸å¦‚HunyuanVideo

---

## 13. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 13.1 å®‰è£…é—®é¢˜

#### **Q1: PyTorchç‰ˆæœ¬å†²çª**

```bash
# é”™è¯¯
ERROR: torch 2.0.0 is not compatible with this project

# è§£å†³
conda create -n vgen python=3.8 -y
conda activate vgen
pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 \
    --extra-index-url https://download.pytorch.org/whl/cu113
```

#### **Q2: xformerså®‰è£…å¤±è´¥**

```bash
# xformerså¯é€‰ï¼Œä½†èƒ½æ˜¾è‘—åŠ é€Ÿ

# æ–¹æ³•1: é¢„ç¼–è¯‘åŒ…
pip install xformers==0.0.22

# æ–¹æ³•2: è·³è¿‡xformers
# åœ¨requirements.txtä¸­æ³¨é‡Šæ‰xformers
# æ€§èƒ½ä¼šé™ä½20-30%ï¼Œä½†ä¸å½±å“åŠŸèƒ½
```

### 13.2 æ¨¡å‹ä¸‹è½½é—®é¢˜

#### **Q3: ModelScopeä¸‹è½½è¶…æ—¶**

```python
# é—®é¢˜: ç½‘ç»œè¿æ¥ModelScopeè¶…æ—¶

# è§£å†³: ä½¿ç”¨HuggingFaceé•œåƒ
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from huggingface_hub import snapshot_download

model_dir = snapshot_download(
    repo_id="ali-vilab/i2vgen-xl",
    cache_dir="models/"
)
```

### 13.3 è®­ç»ƒé—®é¢˜

#### **Q4: DreamVideoè®­ç»ƒæ˜¾å­˜ä¸è¶³**

```python
# é”™è¯¯
CUDA out of memory during DreamVideo training

# è§£å†³æ–¹æ¡ˆ1: é™ä½æ‰¹é‡å¤§å°
# configs/dreamvideo/subjectLearning/xxx.yaml
train_batch_size: 1  # ä»2é™è‡³1

# è§£å†³æ–¹æ¡ˆ2: å¯ç”¨gradient checkpointing
model:
  gradient_checkpointing: true

# è§£å†³æ–¹æ¡ˆ3: å‡å°‘è®­ç»ƒæ­¥æ•°
max_train_steps: 300  # ä»500é™è‡³300
```

#### **Q5: ä¸»ä½“å­¦ä¹ æ•ˆæœä¸ä½³**

```python
# é—®é¢˜: ç”Ÿæˆè§†é¢‘ä¸»ä½“ä¸ä¸€è‡´

# è§£å†³æ–¹æ¡ˆ1: å¢åŠ å‚è€ƒå›¾æ•°é‡
data/subject_learning/subject/images/
  00.jpg  # è‡³å°‘5å¼ ï¼Œè¦†ç›–ä¸åŒè§’åº¦
  01.jpg
  02.jpg
  03.jpg
  04.jpg

# è§£å†³æ–¹æ¡ˆ2: å¢åŠ è®­ç»ƒæ­¥æ•°
max_train_steps: 800  # ä»500å¢è‡³800

# è§£å†³æ–¹æ¡ˆ3: è°ƒæ•´å­¦ä¹ ç‡
learning_rate: 0.00005  # ä»0.0001é™è‡³0.00005ï¼ˆæ›´ç¨³å®šï¼‰
```

### 13.4 æ¨ç†é—®é¢˜

#### **Q6: I2VGen-XLç”Ÿæˆé»‘å±**

```python
# é—®é¢˜: è¾“å‡ºè§†é¢‘å…¨é»‘

# åŸå› : è¾“å…¥å›¾åƒèƒŒæ™¯ä¸ºçº¯é»‘è‰²ï¼ˆå®˜æ–¹å·²çŸ¥é™åˆ¶ï¼‰

# è§£å†³: é¿å…é»‘è‰²èƒŒæ™¯
from PIL import Image
import numpy as np

def add_background(image):
    # å°†é»‘è‰²èƒŒæ™¯æ›¿æ¢ä¸ºç°è‰²
    img_array = np.array(image)
    mask = (img_array.sum(axis=2) < 30)  # è¯†åˆ«é»‘è‰²åŒºåŸŸ
    img_array[mask] = [50, 50, 50]  # æ›¿æ¢ä¸ºæ·±ç°è‰²
    return Image.fromarray(img_array)

image = Image.open("input.jpg")
image = add_background(image)
video = model.generate(image=image, ...)
```

#### **Q7: ç”Ÿæˆè§†é¢‘é—ªçƒä¸¥é‡**

```python
# è§£å†³æ–¹æ¡ˆ1: å¢åŠ æ¨ç†æ­¥æ•°
num_inference_steps: 100  # ä»50å¢è‡³100

# è§£å†³æ–¹æ¡ˆ2: é™ä½guidance_scale
guidance_scale: 5.0  # ä»9.0é™è‡³5.0

# è§£å†³æ–¹æ¡ˆ3: ä½¿ç”¨å›ºå®šç§å­
seed: 42  # ç¡®ä¿å¯å¤ç°
```

### 13.5 æ€§èƒ½ä¼˜åŒ–

#### **Q8: æ¨ç†é€Ÿåº¦æ…¢**

```python
# æ–¹æ³•1: ä½¿ç”¨VideoLCMåŠ é€Ÿ
from vgen.models import VideoLCM

model = VideoLCM("models/modelscope_t2v.pth", "models/video_lcm.pth")
video = model.generate(
    prompt="...",
    num_inference_steps=4  # ä»…éœ€4æ­¥ï¼Œé€Ÿåº¦æå‡10Ã—
)

# æ–¹æ³•2: é™ä½åˆ†è¾¨ç‡
height: 128  # ä»256é™è‡³128
width: 128

# æ–¹æ³•3: å‡å°‘å¸§æ•°
num_frames: 8  # ä»16é™è‡³8
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **VGenå®šä½**: é˜¿é‡Œå·´å·´çš„**8åˆ1è§†é¢‘ç”Ÿæˆç”Ÿæ€ç³»ç»Ÿ**
   - I2VGen-XL: 1280Ã—720é«˜æ¸…å›¾ç”Ÿè§†é¢‘
   - DreamVideo: ä¸»ä½“+è¿åŠ¨å®šåˆ¶ï¼ˆç‹¬ç‰¹ï¼‰
   - VideoComposer: å¤šæ¡ä»¶è¿åŠ¨æ§åˆ¶
   - HiGen: æ—¶ç©ºè§£è€¦+è¶…åˆ†è¾¨ç‡
   - InstructVideo: äººç±»åé¦ˆä¼˜åŒ–
   - ModelScope T2V: åŸºç¡€æ–‡ç”Ÿè§†é¢‘
   - TF-T2V + VideoLCM: è®­ç»ƒå’Œæ¨ç†åŠ é€Ÿ

2. **DreamVideoç‹¬ç‰¹ä»·å€¼**:
   - ä¸‰ç§å®šåˆ¶æ¨¡å¼: ä¸»ä½“/è¿åŠ¨/è”åˆ
   - Textual Inversion + Identity Adapteræ¶æ„
   - é€‚åˆä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆ

3. **é€‚ç”¨åœºæ™¯**:
   - éœ€è¦å®šåˆ¶ä¸»ä½“ â†’ **DreamVideo**
   - éœ€è¦ç²¾ç¡®æ§åˆ¶ â†’ **VideoComposer**
   - éœ€è¦é«˜æ¸…I2V â†’ **I2VGen-XL**
   - éœ€è¦å¿«é€Ÿç”Ÿæˆ â†’ **VideoLCM**

4. **æŒ‘æˆ˜**:
   - å­¦ä¹ æ›²çº¿é™¡å³­ï¼ˆ8ä¸ªæ¨¡å‹ï¼‰
   - æ–‡æ¡£ç›¸å¯¹è¾ƒå°‘
   - éœ€è¦ä¸€å®šè®­ç»ƒæˆæœ¬ï¼ˆDreamVideoï¼‰

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ“– é˜…è¯»VGen GitHub: https://github.com/ali-vilab/VGen
- ğŸ¨ å°è¯•Gradioæœ¬åœ°æ¼”ç¤º
- ğŸ”§ å®è·µDreamVideoä¸»ä½“å®šåˆ¶
- ğŸš€ æ¢ç´¢VideoComposerå¤šæ¡ä»¶æ§åˆ¶

---

**æ›´æ–°æ—¥å¿—**:
- 2025-11-30: åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºäºVGen 2025.01æœ€æ–°æ›´æ–°
