# ç¬¬04ç¯‡_è§†é¢‘ç”Ÿæˆ(10)_LTX-Videoï¼š4KåŸç”Ÿ+éŸ³è§†é¢‘åŒæ­¥DiTæ–¹æ¡ˆ

> **æ›´æ–°æ—¶é—´**: 2025-11-30
> **GitHub**: https://github.com/Lightricks/LTX-Video
> **å‚æ•°é‡**: 13B (ltxv-13b) / 2B (ltxv-2b)
> **æœ€æ–°ç‰ˆæœ¬**: LTX-2 (2025.10å…¬å‘Š) + v0.9.8 (2025.07å¯ç”¨)
> **æ ¸å¿ƒåˆ›æ–°**: éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆï¼ˆAudio + Video, Togetherï¼‰

---

## ğŸ“‹ ç›®å½•

1. [ä¸ºä»€ä¹ˆé€‰æ‹©LTX-Video](#1-ä¸ºä»€ä¹ˆé€‰æ‹©ltx-video)
2. [LTX-2åˆ›æ–°ï¼šéŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆ](#2-ltx-2åˆ›æ–°éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆ)
3. [æ¨¡å‹ç‰ˆæœ¬ä¸æ€§èƒ½å¯¹æ¯”](#3-æ¨¡å‹ç‰ˆæœ¬ä¸æ€§èƒ½å¯¹æ¯”)
4. [æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ](#4-æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ)
5. [ç¯å¢ƒæ­å»ºä¸å®‰è£…](#5-ç¯å¢ƒæ­å»ºä¸å®‰è£…)
6. [ComfyUIé›†æˆæŒ‡å—](#6-comfyuié›†æˆæŒ‡å—)
7. [Diffusers APIå®Œå…¨æŒ‡å—](#7-diffusers-apiå®Œå…¨æŒ‡å—)
8. [æ§åˆ¶æ¨¡å‹å®æˆ˜](#8-æ§åˆ¶æ¨¡å‹å®æˆ˜)
9. [æ€§èƒ½ä¼˜åŒ–ä¸åŠ é€Ÿ](#9-æ€§èƒ½ä¼˜åŒ–ä¸åŠ é€Ÿ)
10. [å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹](#10-å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹)
11. [å•†ä¸šåŒ–éƒ¨ç½²æŒ‡å—](#11-å•†ä¸šåŒ–éƒ¨ç½²æŒ‡å—)
12. [ä¸ä¸»æµæ¨¡å‹å¯¹æ¯”](#12-ä¸ä¸»æµæ¨¡å‹å¯¹æ¯”)
13. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#13-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## 1. ä¸ºä»€ä¹ˆé€‰æ‹©LTX-Video

### 1.1 æ ¸å¿ƒä¼˜åŠ¿

LTX-Videoæ˜¯Lightrickså…¬å¸æ¨å‡ºçš„å¼€æºè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‹¬ç‰¹ä¼˜åŠ¿ï¼š

#### **ğŸµ è¡Œä¸šé¦–åˆ›ï¼šéŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆ**
LTX-2ç‰ˆæœ¬å®ç°äº†è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„é‡å¤§çªç ´ï¼š
- **"Audio + Video, Together"**: åŒæ—¶ç”Ÿæˆç”»é¢å’ŒåŒæ­¥éŸ³é¢‘
- **åŸç”Ÿ4K + åŒæ­¥éŸ³é¢‘**: æ— éœ€åæœŸé…éŸ³
- **æ¯”ç«å“é™ä½50%è®¡ç®—æˆæœ¬**: å•æ¬¡ç”Ÿæˆå®ŒæˆéŸ³è§†é¢‘
- **3Dç›¸æœºé€»è¾‘**: æ”¯æŒå¤æ‚é•œå¤´è¿åŠ¨

#### **ğŸš€ åŸç”Ÿ4Kåˆ†è¾¨ç‡æ”¯æŒ**
- âœ… æœ€é«˜4K (3840Ã—2160) åŸç”Ÿç”Ÿæˆ
- âœ… æœ€é«˜50 FPSå¸§ç‡
- âœ… æœ€é•¿60ç§’è§†é¢‘ï¼ˆ13Bç‰ˆæœ¬ï¼‰
- âœ… åˆ†è¾¨ç‡èƒ½è¢«32æ•´é™¤å³å¯

#### **âš¡ å¤šç‰ˆæœ¬çµæ´»éƒ¨ç½²**
```
LTXV-13B: 13Bå‚æ•°ï¼Œæœ€é«˜è´¨é‡ï¼Œ60ç§’ç”Ÿæˆ
LTXV-2B: 2Bå‚æ•°ï¼Œä½æ˜¾å­˜ç‰ˆæœ¬
è’¸é¦ç‰ˆ: ltxv-2b-0.9.6-distilled
  - ä»…éœ€1GB VRAM
  - H100ä¸Šå®æ—¶ç”Ÿæˆ
  - æ¯”éè’¸é¦ç‰ˆå¿«15å€
```

#### **ğŸ“œ OpenRail-Må•†ä¸šè®¸å¯**
v0.9.5åŠä»¥ä¸Šç‰ˆæœ¬é‡‡ç”¨OpenRail-Mè®¸å¯è¯ï¼š
- âœ… **å®Œå…¨æ”¯æŒå•†ä¸šä½¿ç”¨**
- âœ… **å…è®¸ä¿®æ”¹å’Œåˆ†å‘**
- âœ… **å¯é›†æˆåˆ°äº§å“ä¸­**
- âš ï¸ éœ€éµå®ˆè´Ÿè´£ä»»AIä½¿ç”¨æ¡æ¬¾

#### **ğŸ’° TCOå¯¹æ¯”**

| æ–¹æ¡ˆ | è®¸å¯è¯ | 4Kæ”¯æŒ | éŸ³é¢‘åŒæ­¥ | H100å®æ—¶ç”Ÿæˆ | æœˆæˆæœ¬ |
|------|--------|--------|----------|-------------|--------|
| **LTX-Video (è’¸é¦ç‰ˆ)** | OpenRail-M | âœ… åŸç”Ÿ | âœ… LTX-2 | âœ… 10ç§’HD | $0 (ä»…GPU) |
| HunyuanVideo | è…¾è®¯è®¸å¯ | âŒ æœ€é«˜720p | âŒ æ—  | âŒ æ—  | $0 (ä»…GPU) |
| CogVideoX | Apache 2.0 | âŒ æœ€é«˜768p | âŒ æ—  | âŒ æ—  | $0 (ä»…GPU) |
| Runway Gen-3 | å•†ä¸šé—­æº | âœ… 4K | âŒ éœ€åæœŸ | âŒ äº‘ç«¯ | $500-2000 |

---

## 2. LTX-2åˆ›æ–°ï¼šéŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆ

### 2.1 æŠ€æœ¯çªç ´

LTX-2ï¼ˆ2025.10å…¬å‘Šï¼‰å®ç°äº†è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„**èŒƒå¼è½¬å˜**ï¼š

#### **ä¼ ç»Ÿæ–¹æ¡ˆ vs LTX-2**

```
ä¼ ç»Ÿæ–¹æ¡ˆæµç¨‹:
Text â†’ Video Generation (5-10åˆ†é’Ÿ)
     â†’ Audio Generation (å•ç‹¬1-2åˆ†é’Ÿ)
     â†’ Audio-Video Sync (åæœŸå¯¹é½)
æ€»è€—æ—¶: 7-15åˆ†é’Ÿ, 3ä¸ªæ­¥éª¤

LTX-2æµç¨‹:
Text â†’ Audio + Video Together (3-5åˆ†é’Ÿ)
æ€»è€—æ—¶: 3-5åˆ†é’Ÿ, 1ä¸ªæ­¥éª¤ âœ¨
```

#### **è®¡ç®—æ•ˆç‡æå‡**

$$
\text{Efficiency Gain} = \frac{\text{Traditional Time}}{\text{LTX-2 Time}} = \frac{7-15 \text{ min}}{3-5 \text{ min}} \approx 2\times - 3\times
$$

è®¡ç®—æˆæœ¬é™ä½ï¼š
$$
\text{Cost Reduction} = 1 - \frac{1}{2} = 50\%
$$

### 2.2 LTX-2æ ¸å¿ƒèƒ½åŠ›

```
éŸ³è§†é¢‘åŒæ­¥ç‰¹æ€§:
  âœ… åŸç”Ÿ4Kåˆ†è¾¨ç‡
  âœ… åŒæ­¥ç”Ÿæˆçš„éŸ³é¢‘è½¨é“
  âœ… éŸ³ç”»å®Œç¾å¯¹é½ï¼ˆæ— éœ€åæœŸï¼‰
  âœ… æ”¯æŒç¯å¢ƒéŸ³ã€å¯¹è¯éŸ³ã€èƒŒæ™¯éŸ³ä¹

å¤šå…³é”®å¸§æ§åˆ¶:
  âœ… æŒ‡å®šå¤šä¸ªå…³é”®å¸§
  âœ… å¹³æ»‘è¿‡æ¸¡ç”Ÿæˆ
  âœ… é•¿è§†é¢‘è¿è´¯æ€§

3Dç›¸æœºé€»è¾‘:
  âœ… æ¨æ‹‰æ‘‡ç§»ï¼ˆDolly, Pan, Tiltï¼‰
  âœ… æ™¯æ·±å˜åŒ–ï¼ˆDepth of Fieldï¼‰
  âœ… è¿åŠ¨æ¨¡ç³Šï¼ˆMotion Blurï¼‰

LoRAå¾®è°ƒ:
  âœ… è‡ªå®šä¹‰é£æ ¼
  âœ… ç‰¹å®šåœºæ™¯é€‚é…
  âœ… è½»é‡åŒ–éƒ¨ç½²
```

### 2.3 åº”ç”¨åœºæ™¯

#### **å¥èº«æ•™å­¦è§†é¢‘**
```python
# ä¼ ç»Ÿæ–¹æ¡ˆ
video = generate_video("å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²")  # 5åˆ†é’Ÿ
audio = generate_audio("è®²è§£æ·±è¹²è¦ç‚¹")      # 1åˆ†é’Ÿ
synced = sync_audio_video(video, audio)    # æ‰‹åŠ¨å¯¹é½

# LTX-2æ–¹æ¡ˆ
video_with_audio = ltx2.generate(
    "å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²ï¼ŒåŒæ—¶è®²è§£åŠ¨ä½œè¦ç‚¹"
)  # 3åˆ†é’Ÿï¼ŒéŸ³ç”»åŒæ­¥ âœ¨
```

#### **äº§å“å®£ä¼ ç‰‡**
```python
ltx2.generate(
    prompt="4Käº§å“å±•ç¤ºï¼ŒèƒŒæ™¯éŸ³ä¹èŠ‚å¥æ„Ÿå¼º",
    camera_motion="slow_zoom_in",
    audio_type="background_music"
)
# è¾“å‡º: 4Kè§†é¢‘ + åŒæ­¥èƒŒæ™¯éŸ³ä¹
```

---

## 3. æ¨¡å‹ç‰ˆæœ¬ä¸æ€§èƒ½å¯¹æ¯”

### 3.1 å¯ç”¨æ¨¡å‹ç‰ˆæœ¬

#### **LTXV-13B**ï¼ˆæ——èˆ°ç‰ˆï¼‰
```
å‚æ•°é‡: 13B
åˆ†è¾¨ç‡: æœ€é«˜4K (3840Ã—2160)
å¸§ç‡: æœ€é«˜50 FPS
æ—¶é•¿: æœ€é•¿60ç§’
æ¨è: 720Ã—1280ä»¥ä¸‹, 257å¸§ä»¥ä¸‹
æ˜¾å­˜éœ€æ±‚: é«˜ (çº¦24GB+)
é€‚ç”¨åœºæ™¯: æœ€é«˜è´¨é‡éœ€æ±‚ï¼Œäº‘ç«¯éƒ¨ç½²
```

#### **LTXV-2B**ï¼ˆè½»é‡ç‰ˆï¼‰
```
å‚æ•°é‡: 2B
åˆ†è¾¨ç‡: åŒ13B
å¸§ç‡: åŒ13B
æ˜¾å­˜éœ€æ±‚: ä¸­ç­‰ (çº¦12-16GB)
é€‚ç”¨åœºæ™¯: æœ¬åœ°éƒ¨ç½²ï¼Œæ¶ˆè´¹çº§GPU
```

#### **ltxv-2b-0.9.6-distilled**ï¼ˆè’¸é¦ç‰ˆï¼‰â­ï¸
```
å‚æ•°é‡: 2B (è’¸é¦ä¼˜åŒ–)
æ˜¾å­˜éœ€æ±‚: ä»…1GB VRAM âœ¨
é€Ÿåº¦æå‡: æ¯”éè’¸é¦ç‰ˆå¿«15å€
æ¨ç†æ­¥æ•°: ä»…éœ€8æ­¥ï¼ˆæ— éœ€CFG/STGï¼‰
H100æ€§èƒ½:
  - ä½åˆ†è¾¨ç‡é¢„è§ˆ: 3ç§’
  - å®Œæ•´HDè§†é¢‘: 10ç§’å†…
é€‚ç”¨åœºæ™¯: å®æ—¶ç”Ÿæˆï¼Œè¾¹ç¼˜è®¾å¤‡
```

### 3.2 æ€§èƒ½æ•°æ®å¯¹æ¯”

#### **æ¨ç†é€Ÿåº¦ï¼ˆ720p, 24fps, 5ç§’è§†é¢‘ï¼‰**

| æ¨¡å‹ç‰ˆæœ¬ | GPU | æ¨ç†æ­¥æ•° | ç”Ÿæˆæ—¶é—´ | æ˜¾å­˜å ç”¨ |
|---------|-----|---------|---------|---------|
| LTXV-13B | A100 (80GB) | 40 | ~120ç§’ | 24GB |
| LTXV-2B | RTX 4090 (24GB) | 40 | ~180ç§’ | 16GB |
| è’¸é¦ç‰ˆ | H100 | 8 | **10ç§’** â­ï¸ | 1GB |
| è’¸é¦ç‰ˆ | RTX 4090 | 8 | ~25ç§’ | 1GB |

#### **è’¸é¦ç‰ˆåŠ é€Ÿæ¯”**

$$
\text{Speedup} = \frac{\text{Time}_{\text{LTXV-2B}}}{\text{Time}_{\text{Distilled}}} = \frac{180s}{25s} \approx 7.2\times
$$

åœ¨H100ä¸Šï¼š
$$
\text{Speedup}_{\text{H100}} = \frac{180s}{10s} = 18\times
$$

### 3.3 åˆ†è¾¨ç‡ä¸å¸§æ•°çº¦æŸ

#### **æŠ€æœ¯çº¦æŸ**
```python
# åˆ†è¾¨ç‡çº¦æŸ
width % 32 == 0
height % 32 == 0

# å¸§æ•°çº¦æŸ
num_frames = 8 * N + 1  # Nä¸ºæ­£æ•´æ•°
# ä¾‹å¦‚: 9, 17, 25, 33, ..., 257

# æ¨èé…ç½®
æ¨èåˆ†è¾¨ç‡: â‰¤ 720Ã—1280
æ¨èå¸§æ•°: â‰¤ 257
```

#### **åˆ†è¾¨ç‡ç¤ºä¾‹**

| åˆ†è¾¨ç‡ | å®½Ã—é«˜ | æ˜¯å¦æ”¯æŒ | ç”¨é€” |
|-------|------|---------|------|
| 720p | 1280Ã—720 | âœ… | æ ‡å‡†è§†é¢‘ |
| 1080p | 1920Ã—1080 | âœ… | é«˜æ¸…è§†é¢‘ |
| 2K | 2560Ã—1440 | âœ… | è¶…é«˜æ¸… |
| 4K | 3840Ã—2160 | âœ… | ç”µå½±çº§ |
| è‡ªå®šä¹‰ | 768Ã—1024 | âœ… | ç«–å±è§†é¢‘ |

---

## 4. æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ

### 4.1 DiTæ¶æ„

LTX-VideoåŸºäº**Diffusion Transformer (DiT)**æ¶æ„ï¼š

#### **æ¶æ„ç»„æˆ**

```
LTX-Video Pipeline:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Text Encoder    â”‚ (CLIP/T5)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ DiT Backbone    â”‚ (13B/2B params)
  â”‚ - Self-Attentionâ”‚
  â”‚ - Cross-Attentionâ”‚
  â”‚ - FFN Layers    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VAE Decoder     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
       â”‚ Video â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **DiTæ‰©æ•£è¿‡ç¨‹**

å‰å‘æ‰©æ•£ï¼ˆåŠ å™ªï¼‰ï¼š
$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

é€†å‘æ‰©æ•£ï¼ˆå»å™ªï¼‰ï¼š
$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

å…¶ä¸­DiTé¢„æµ‹å™ªå£°ï¼š
$$
\epsilon_\theta(x_t, t, c) = \text{DiT}(x_t, t, c)
$$

$c$ä¸ºæ–‡æœ¬æ¡ä»¶ï¼ˆtext conditionï¼‰ã€‚

### 4.2 å¤šå…³é”®å¸§æœºåˆ¶

LTX-2æ”¯æŒå¤šå…³é”®å¸§æ¡ä»¶æ§åˆ¶ï¼š

```python
# å¤šå…³é”®å¸§ç”Ÿæˆ
keyframes = [
    {"frame_id": 0, "image": img1, "prompt": "èµ·å§‹åŠ¨ä½œ"},
    {"frame_id": 128, "image": img2, "prompt": "ä¸­é—´åŠ¨ä½œ"},
    {"frame_id": 256, "image": img3, "prompt": "ç»“æŸåŠ¨ä½œ"}
]

video = ltx2.generate_with_keyframes(
    keyframes=keyframes,
    interpolation="smooth"
)
```

#### **æ’å€¼æ•°å­¦æ¨¡å‹**

çº¿æ€§æ’å€¼ï¼ˆLERPï¼‰ï¼š
$$
I(t) = (1-\alpha)I_1 + \alpha I_2, \quad \alpha = \frac{t - t_1}{t_2 - t_1}
$$

çƒé¢çº¿æ€§æ’å€¼ï¼ˆSLERPï¼‰ç”¨äºç‰¹å¾ç©ºé—´ï¼š
$$
\text{SLERP}(p_0, p_1, t) = \frac{\sin((1-t)\Omega)}{\sin\Omega}p_0 + \frac{\sin(t\Omega)}{\sin\Omega}p_1
$$

å…¶ä¸­ $\Omega = \arccos(p_0 \cdot p_1)$ã€‚

### 4.3 è’¸é¦æŠ€æœ¯åŸç†

è’¸é¦ç‰ˆé€šè¿‡**çŸ¥è¯†è’¸é¦**å®ç°åŠ é€Ÿï¼š

#### **è’¸é¦æŸå¤±å‡½æ•°**

$$
\mathcal{L}_{\text{distill}} = \mathcal{L}_{\text{output}} + \lambda \mathcal{L}_{\text{feature}}
$$

è¾“å‡ºä¸€è‡´æ€§æŸå¤±ï¼š
$$
\mathcal{L}_{\text{output}} = \| f_{\text{student}}(x) - f_{\text{teacher}}(x) \|^2
$$

ç‰¹å¾ä¸€è‡´æ€§æŸå¤±ï¼š
$$
\mathcal{L}_{\text{feature}} = \sum_{i} \| h_{\text{student}}^{(i)} - h_{\text{teacher}}^{(i)} \|^2
$$

#### **æ­¥æ•°å‹ç¼©**

åŸå§‹æ¨¡å‹ï¼š40æ­¥æ¨ç†
è’¸é¦æ¨¡å‹ï¼š8æ­¥æ¨ç†

å‹ç¼©æ¯”ï¼š
$$
\text{Compression Ratio} = \frac{40}{8} = 5\times
$$

---

## 5. ç¯å¢ƒæ­å»ºä¸å®‰è£…

### 5.1 ç³»ç»Ÿè¦æ±‚

#### **ç¡¬ä»¶è¦æ±‚**

| æ¨¡å‹ç‰ˆæœ¬ | æœ€ä½GPU | æ¨èGPU | æ˜¾å­˜ | CPU | å†…å­˜ |
|---------|--------|---------|------|-----|------|
| LTXV-13B | A100 40GB | A100 80GB | 24GB+ | 16æ ¸ | 64GB |
| LTXV-2B | RTX 4090 | RTX 4090 | 12GB+ | 8æ ¸ | 32GB |
| è’¸é¦ç‰ˆ | RTX 3060 | RTX 4090 | 6GB+ | 4æ ¸ | 16GB |

#### **è½¯ä»¶è¦æ±‚**

```bash
æ“ä½œç³»ç»Ÿ: Linux (æ¨èUbuntu 20.04+) / macOS (MPSæ”¯æŒ)
Python: 3.8+
CUDA: 12.2+ (Linux)
PyTorch: â‰¥2.1.2 (macOSéœ€â‰¥2.3.0)
```

### 5.2 æ–¹æ³•1ï¼šCondaç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n ltx-video python=3.10
conda activate ltx-video

# å®‰è£…PyTorch (CUDA 12.2)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu122

# å®‰è£…LTX-Videoæ ¸å¿ƒä¾èµ–
pip install diffusers transformers accelerate
pip install opencv-python pillow imageio
pip install safetensors einops

# éªŒè¯å®‰è£…
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### 5.3 æ–¹æ³•2ï¼šDockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

RUN apt-get update && apt-get install -y \
    python3.10 python3-pip git

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# ä¸‹è½½æ¨¡å‹
RUN huggingface-cli download Lightricks/LTX-Video \
    --cache-dir /models

CMD ["python3", "inference.py"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t ltx-video:latest .

# è¿è¡Œå®¹å™¨
docker run --gpus all -v $(pwd):/workspace \
    ltx-video:latest
```

### 5.4 æ¨¡å‹ä¸‹è½½

#### **æ–¹æ³•1ï¼šHuggingFace CLI**

```bash
# å®‰è£…HuggingFace CLI
pip install huggingface_hub

# ä¸‹è½½13Bæ¨¡å‹
huggingface-cli download Lightricks/LTX-Video \
    --include "ltxv-13b/*" \
    --cache-dir ./models

# ä¸‹è½½2Bè’¸é¦ç‰ˆï¼ˆæ¨èï¼‰
huggingface-cli download Lightricks/LTX-Video \
    --include "ltxv-2b-0.9.6-distilled/*" \
    --cache-dir ./models
```

#### **æ–¹æ³•2ï¼šPython API**

```python
from huggingface_hub import snapshot_download

# ä¸‹è½½è’¸é¦ç‰ˆæ¨¡å‹
model_path = snapshot_download(
    repo_id="Lightricks/LTX-Video",
    allow_patterns=["ltxv-2b-0.9.6-distilled/*"],
    cache_dir="./models"
)
print(f"æ¨¡å‹ä¸‹è½½è‡³: {model_path}")
```

#### **æ¨¡å‹æ–‡ä»¶å¤§å°**

| æ¨¡å‹ | æ–‡ä»¶å¤§å° | ä¸‹è½½æ—¶é—´ (100Mbps) |
|------|---------|-------------------|
| LTXV-13B | ~52GB | ~70åˆ†é’Ÿ |
| LTXV-2B | ~8GB | ~11åˆ†é’Ÿ |
| è’¸é¦ç‰ˆ | ~8GB | ~11åˆ†é’Ÿ |

---

## 6. ComfyUIé›†æˆæŒ‡å—

### 6.1 å®‰è£…ComfyUIèŠ‚ç‚¹

```bash
# è¿›å…¥ComfyUIç›®å½•
cd ComfyUI/custom_nodes

# å…‹éš†LTX-Video ComfyUIèŠ‚ç‚¹
git clone https://github.com/Lightricks/ComfyUI-LTXVideo

# å®‰è£…ä¾èµ–
cd ComfyUI-LTXVideo
pip install -r requirements.txt

# é‡å¯ComfyUI
cd ../..
python main.py
```

### 6.2 åŸºç¡€å·¥ä½œæµ

#### **æ–‡æœ¬ç”Ÿæˆè§†é¢‘å·¥ä½œæµ**

```json
{
  "nodes": [
    {
      "id": 1,
      "type": "LTXVideoLoader",
      "inputs": {
        "model_path": "models/ltxv-2b-0.9.6-distilled"
      }
    },
    {
      "id": 2,
      "type": "LTXVideoTextEncode",
      "inputs": {
        "text": "ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼Œ4Ké«˜æ¸…ï¼ŒèƒŒæ™¯å¥èº«æˆ¿"
      }
    },
    {
      "id": 3,
      "type": "LTXVideoSampler",
      "inputs": {
        "model": ["1", 0],
        "text_embeds": ["2", 0],
        "width": 1280,
        "height": 720,
        "num_frames": 129,
        "num_steps": 8,
        "guidance_scale": 3.0
      }
    },
    {
      "id": 4,
      "type": "LTXVideoDecoder",
      "inputs": {
        "latents": ["3", 0]
      }
    },
    {
      "id": 5,
      "type": "SaveVideo",
      "inputs": {
        "video": ["4", 0],
        "fps": 24,
        "filename": "squat_demo"
      }
    }
  ]
}
```

### 6.3 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ

```python
# ComfyUI Python API
from comfy_nodes import LTXVideo

# åŠ è½½æ¨¡å‹
model = LTXVideo.load_model("models/ltxv-2b-0.9.6-distilled")

# åŠ è½½èµ·å§‹å›¾åƒ
from PIL import Image
start_image = Image.open("start_pose.jpg")

# ç”Ÿæˆè§†é¢‘
video = model.image_to_video(
    image=start_image,
    prompt="å¥èº«æ•™ç»ƒä»å‡†å¤‡å§¿åŠ¿å¼€å§‹æ·±è¹²",
    num_frames=129,
    guidance_scale=3.0,
    num_steps=8
)

# ä¿å­˜
video.save("squat_i2v.mp4", fps=24)
```

### 6.4 é«˜çº§æ§åˆ¶ï¼šDepthæ§åˆ¶

```json
{
  "nodes": [
    {
      "id": 6,
      "type": "DepthMapEstimator",
      "inputs": {
        "image": ["input_image", 0]
      }
    },
    {
      "id": 7,
      "type": "LTXVideoDepthControl",
      "inputs": {
        "model": ["1", 0],
        "depth_map": ["6", 0],
        "control_strength": 0.8
      }
    },
    {
      "id": 8,
      "type": "LTXVideoSampler",
      "inputs": {
        "model": ["7", 0],
        "text_embeds": ["2", 0],
        "width": 1280,
        "height": 720
      }
    }
  ]
}
```

---

## 7. Diffusers APIå®Œå…¨æŒ‡å—

### 7.1 åŸºç¡€T2Vç”Ÿæˆ

```python
from diffusers import LTXVideoPipeline
import torch

# åŠ è½½ç®¡é“ï¼ˆè’¸é¦ç‰ˆï¼‰
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled",
    torch_dtype=torch.float16
).to("cuda")

# ç”Ÿæˆè§†é¢‘
prompt = "ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼ŒåŠ¨ä½œæ ‡å‡†ï¼ŒèƒŒæ™¯ç°ä»£å¥èº«æˆ¿ï¼Œ4Ké«˜æ¸…"

video = pipe(
    prompt=prompt,
    num_frames=129,  # 5.4ç§’ @ 24fps
    height=720,
    width=1280,
    num_inference_steps=8,  # è’¸é¦ç‰ˆä»…éœ€8æ­¥
    guidance_scale=3.0,
    generator=torch.Generator("cuda").manual_seed(42)
).frames[0]

# ä¿å­˜è§†é¢‘
from diffusers.utils import export_to_video
export_to_video(video, "squat_demo.mp4", fps=24)
```

### 7.2 å›¾ç”Ÿè§†é¢‘ï¼ˆI2Vï¼‰

```python
from PIL import Image

# åŠ è½½èµ·å§‹å›¾åƒ
init_image = Image.open("squat_start.jpg").resize((1280, 720))

# I2Vç”Ÿæˆ
video = pipe(
    prompt="å¥èº«æ•™ç»ƒä»å‡†å¤‡å§¿åŠ¿å®Œæˆæ·±è¹²åŠ¨ä½œ",
    image=init_image,
    num_frames=129,
    height=720,
    width=1280,
    num_inference_steps=8,
    guidance_scale=3.0
).frames[0]

export_to_video(video, "squat_i2v.mp4", fps=24)
```

### 7.3 è´Ÿé¢æç¤ºè¯ï¼ˆNegative Promptï¼‰

```python
video = pipe(
    prompt="å¥èº«æ•™ç»ƒæ·±è¹²æ¼”ç¤º",
    negative_prompt="æ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œå˜å½¢ï¼Œé”™è¯¯å§¿åŠ¿ï¼Œä¸è‡ªç„¶åŠ¨ä½œ",
    num_frames=129,
    height=720,
    width=1280,
    num_inference_steps=20,  # éè’¸é¦ç‰ˆä½¿ç”¨æ›´å¤šæ­¥æ•°
    guidance_scale=7.5  # æ›´å¼ºçš„å¼•å¯¼
).frames[0]
```

### 7.4 æ‰¹é‡ç”Ÿæˆ

```python
prompts = [
    "å¥èº«æ•™ç»ƒæ·±è¹²æ¼”ç¤º",
    "å¥èº«æ•™ç»ƒç¡¬æ‹‰æ¼”ç¤º",
    "å¥èº«æ•™ç»ƒå§æ¨æ¼”ç¤º"
]

videos = []
for prompt in prompts:
    video = pipe(
        prompt=prompt,
        num_frames=129,
        height=720,
        width=1280,
        num_inference_steps=8,
        guidance_scale=3.0
    ).frames[0]
    videos.append(video)

# ä¿å­˜æ‰€æœ‰è§†é¢‘
for i, video in enumerate(videos):
    export_to_video(video, f"exercise_{i}.mp4", fps=24)
```

### 7.5 é•¿è§†é¢‘ç”Ÿæˆï¼ˆå¤šæ®µæ‹¼æ¥ï¼‰

```python
def generate_long_video(segments, fps=24):
    """
    ç”Ÿæˆé•¿è§†é¢‘ï¼ˆå¤šæ®µæ‹¼æ¥ï¼‰

    segments: List[dict], æ¯æ®µåŒ…å«promptå’Œduration
    """
    all_frames = []

    for segment in segments:
        num_frames = int(segment["duration"] * fps) + 1
        # ç¡®ä¿ç¬¦åˆ8N+1çº¦æŸ
        num_frames = ((num_frames - 1) // 8) * 8 + 1

        video = pipe(
            prompt=segment["prompt"],
            num_frames=num_frames,
            height=720,
            width=1280,
            num_inference_steps=8
        ).frames[0]

        all_frames.extend(video)

    export_to_video(all_frames, "long_video.mp4", fps=fps)

# ç¤ºä¾‹ï¼šç”Ÿæˆ3æ®µæ‹¼æ¥è§†é¢‘
segments = [
    {"prompt": "å¥èº«æ•™ç»ƒå‡†å¤‡æ·±è¹²", "duration": 3},
    {"prompt": "å¥èº«æ•™ç»ƒæ‰§è¡Œæ·±è¹²", "duration": 5},
    {"prompt": "å¥èº«æ•™ç»ƒå®ŒæˆåŠ¨ä½œ", "duration": 2}
]

generate_long_video(segments)
```

---

## 8. æ§åˆ¶æ¨¡å‹å®æˆ˜

LTX-Video v0.9.8æä¾›äº†ä¸‰ç§æ§åˆ¶æ¨¡å‹ï¼ˆ2025.07å‘å¸ƒï¼‰ï¼š

### 8.1 Depth Controlï¼ˆæ·±åº¦æ§åˆ¶ï¼‰

```python
from diffusers import LTXVideoDepthControlPipeline
from transformers import DPTForDepthEstimation, DPTImageProcessor

# åŠ è½½æ·±åº¦ä¼°è®¡æ¨¡å‹
depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-large")
processor = DPTImageProcessor.from_pretrained("Intel/dpt-large")

# ä¼°è®¡æ·±åº¦å›¾
def get_depth_map(image):
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = depth_estimator(**inputs)
        depth = outputs.predicted_depth
    return depth

# åŠ è½½æ§åˆ¶ç®¡é“
control_pipe = LTXVideoDepthControlPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="depth-control",
    torch_dtype=torch.float16
).to("cuda")

# ç”Ÿæˆæ·±åº¦æ§åˆ¶è§†é¢‘
from PIL import Image
reference_image = Image.open("gym_scene.jpg")
depth_map = get_depth_map(reference_image)

video = control_pipe(
    prompt="å¥èº«æˆ¿å†…éƒ¨æ¼«æ¸¸ï¼Œé•œå¤´ç¼“æ…¢æ¨è¿›",
    depth_map=depth_map,
    control_strength=0.8,  # æ§åˆ¶å¼ºåº¦
    num_frames=129,
    height=720,
    width=1280
).frames[0]

export_to_video(video, "depth_control.mp4", fps=24)
```

### 8.2 Pose Controlï¼ˆå§¿æ€æ§åˆ¶ï¼‰

```python
from diffusers import LTXVideoPoseControlPipeline
from controlnet_aux import OpenposeDetector

# åŠ è½½OpenPoseæ£€æµ‹å™¨
openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")

# æå–å§¿æ€å…³é”®ç‚¹
def get_pose_keypoints(image):
    pose = openpose(image)
    return pose

# åŠ è½½å§¿æ€æ§åˆ¶ç®¡é“
pose_pipe = LTXVideoPoseControlPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="pose-control",
    torch_dtype=torch.float16
).to("cuda")

# ç”Ÿæˆå§¿æ€æ§åˆ¶è§†é¢‘
reference_image = Image.open("squat_reference.jpg")
pose_keypoints = get_pose_keypoints(reference_image)

video = pose_pipe(
    prompt="å¥èº«æ•™ç»ƒæŒ‰ç…§æ ‡å‡†å§¿åŠ¿å®Œæˆæ·±è¹²",
    pose_keypoints=pose_keypoints,
    control_strength=0.9,
    num_frames=129,
    height=720,
    width=1280
).frames[0]

export_to_video(video, "pose_control.mp4", fps=24)
```

### 8.3 Canny Controlï¼ˆè¾¹ç¼˜æ§åˆ¶ï¼‰

```python
from diffusers import LTXVideoCannyControlPipeline
import cv2
import numpy as np

# Cannyè¾¹ç¼˜æ£€æµ‹
def get_canny_edges(image, low_threshold=100, high_threshold=200):
    image_np = np.array(image)
    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(gray, low_threshold, high_threshold)
    return Image.fromarray(edges)

# åŠ è½½Cannyæ§åˆ¶ç®¡é“
canny_pipe = LTXVideoCannyControlPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="canny-control",
    torch_dtype=torch.float16
).to("cuda")

# ç”Ÿæˆè¾¹ç¼˜æ§åˆ¶è§†é¢‘
reference_image = Image.open("equipment_outline.jpg")
canny_edges = get_canny_edges(reference_image)

video = canny_pipe(
    prompt="å¥èº«å™¨æå±•ç¤ºï¼Œä»å·¦å‘å³æ—‹è½¬",
    canny_edges=canny_edges,
    control_strength=0.7,
    num_frames=129,
    height=720,
    width=1280
).frames[0]

export_to_video(video, "canny_control.mp4", fps=24)
```

### 8.4 æ§åˆ¶å¼ºåº¦å¯¹æ¯”

```python
# æµ‹è¯•ä¸åŒæ§åˆ¶å¼ºåº¦
control_strengths = [0.3, 0.5, 0.7, 0.9]

for strength in control_strengths:
    video = control_pipe(
        prompt="å¥èº«æˆ¿æ¼«æ¸¸",
        depth_map=depth_map,
        control_strength=strength,
        num_frames=129,
        height=720,
        width=1280
    ).frames[0]

    export_to_video(video, f"depth_strength_{strength}.mp4", fps=24)
```

æ§åˆ¶å¼ºåº¦å½±å“ï¼š
$$
\text{Final Latent} = (1 - s) \cdot \text{Unconditional Latent} + s \cdot \text{Controlled Latent}
$$

å…¶ä¸­ $s \in [0, 1]$ ä¸ºæ§åˆ¶å¼ºåº¦ï¼ˆcontrol_strengthï¼‰ã€‚

---

## 9. æ€§èƒ½ä¼˜åŒ–ä¸åŠ é€Ÿ

### 9.1 æ˜¾å­˜ä¼˜åŒ–

#### **CPU Offload**

```python
# æ–¹æ³•1ï¼šæ¨¡å‹CPUå¸è½½
pipe.enable_model_cpu_offload()

# æ–¹æ³•2ï¼šé¡ºåºCPUå¸è½½ï¼ˆæ›´çœæ˜¾å­˜ï¼‰
pipe.enable_sequential_cpu_offload()
```

æ˜¾å­˜èŠ‚çœï¼š
- æ— å¸è½½ï¼š24GB
- CPUå¸è½½ï¼š12GB
- é¡ºåºå¸è½½ï¼š8GB

#### **VAE Tiling**

```python
# å¯ç”¨VAEåˆ‡ç‰‡ï¼ˆå¤„ç†å¤§åˆ†è¾¨ç‡ï¼‰
pipe.vae.enable_tiling()
pipe.vae.enable_slicing()
```

### 9.2 æ¨ç†åŠ é€Ÿ

#### **FP8é‡åŒ–ï¼ˆ13Bæ¨¡å‹ï¼‰**

```python
# åŠ è½½FP8é‡åŒ–æ¨¡å‹
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-13b-fp8",
    torch_dtype=torch.float8_e4m3fn  # FP8
).to("cuda")
```

FP8 vs FP16æ€§èƒ½ï¼š
- æ˜¾å­˜å ç”¨ï¼šå‡å°‘50%
- æ¨ç†é€Ÿåº¦ï¼šæå‡30-40%
- è´¨é‡æŸå¤±ï¼š<2% (å‡ ä¹æ— æŸ)

#### **Torch Compile**

```python
# ç¼–è¯‘æ¨¡å‹åŠ é€Ÿ
pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
```

é¦–æ¬¡è¿è¡Œä¼šç¼–è¯‘ï¼ˆè¾ƒæ…¢ï¼‰ï¼Œåç»­è¿è¡ŒåŠ é€Ÿ20-30%ã€‚

#### **Flash Attention 3**

```python
# å®‰è£…Flash Attention
pip install flash-attn --no-build-isolation

# å¯ç”¨Flash Attention
pipe.enable_xformers_memory_efficient_attention()
```

åŠ é€Ÿæ•ˆæœï¼š
- æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿ2-3å€
- æ˜¾å­˜å ç”¨å‡å°‘30%

### 9.3 æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
from torch.cuda.amp import autocast

prompts = ["prompt1", "prompt2", "prompt3"]

# ä½¿ç”¨æ··åˆç²¾åº¦
with autocast(dtype=torch.float16):
    videos = pipe(
        prompt=prompts,
        num_frames=129,
        height=720,
        width=1280,
        num_inference_steps=8
    ).frames

# å¹¶è¡Œä¿å­˜
from concurrent.futures import ThreadPoolExecutor

def save_video(args):
    video, filename = args
    export_to_video(video, filename, fps=24)

with ThreadPoolExecutor(max_workers=4) as executor:
    executor.map(save_video,
                 [(videos[i], f"batch_{i}.mp4") for i in range(len(videos))])
```

### 9.4 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time

def benchmark(pipe, config):
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•

    config: dict, åŒ…å«prompt, num_framesç­‰å‚æ•°
    """
    # é¢„çƒ­
    _ = pipe(**config)

    # æµ‹è¯•
    times = []
    for _ in range(3):
        start = time.time()
        _ = pipe(**config)
        torch.cuda.synchronize()
        end = time.time()
        times.append(end - start)

    avg_time = sum(times) / len(times)
    fps = config["num_frames"] / avg_time

    print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}ç§’")
    print(f"ç­‰æ•ˆFPS: {fps:.2f}")

    # æ˜¾å­˜å ç”¨
    print(f"å³°å€¼æ˜¾å­˜: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB")

# è¿è¡ŒåŸºå‡†æµ‹è¯•
config = {
    "prompt": "å¥èº«æ•™ç»ƒæ·±è¹²æ¼”ç¤º",
    "num_frames": 129,
    "height": 720,
    "width": 1280,
    "num_inference_steps": 8
}

benchmark(pipe, config)
```

---

## 10. å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹

### 10.1 æ¡ˆä¾‹1ï¼šæ·±è¹²åŠ¨ä½œæ•™å­¦è§†é¢‘

#### **éœ€æ±‚åˆ†æ**
- åˆ†è¾¨ç‡ï¼š1080p (1920Ã—1080)
- æ—¶é•¿ï¼š8ç§’
- å¸§ç‡ï¼š24fps
- è¦æ±‚ï¼šåŠ¨ä½œæ ‡å‡†ï¼Œç”»é¢æ¸…æ™°

#### **å®Œæ•´ä»£ç **

```python
from diffusers import LTXVideoPipeline
import torch
from diffusers.utils import export_to_video

# åŠ è½½è’¸é¦ç‰ˆæ¨¡å‹
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled",
    torch_dtype=torch.float16
).to("cuda")

# ä¼˜åŒ–è®¾ç½®
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()

# æç¤ºè¯å·¥ç¨‹
prompt = """
ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼Œ
åŠ¨ä½œæ ‡å‡†è§„èŒƒï¼Œä»å‡†å¤‡å§¿åŠ¿å¼€å§‹ï¼Œ
ç¼“æ…¢ä¸‹è¹²è‡³å¤§è…¿ä¸åœ°é¢å¹³è¡Œï¼Œ
ç„¶ååŒ€é€Ÿç«™èµ·ï¼Œ
èƒŒæ™¯ç°ä»£å¥èº«æˆ¿ï¼Œ
4Ké«˜æ¸…ç”»è´¨ï¼Œ
è‡ªç„¶å…‰çº¿
"""

negative_prompt = """
æ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œå˜å½¢ï¼Œ
é”™è¯¯å§¿åŠ¿ï¼Œä¸è‡ªç„¶åŠ¨ä½œï¼Œ
å¿«é€Ÿè¿åŠ¨ï¼ŒæŠ–åŠ¨
"""

# ç”Ÿæˆè§†é¢‘
num_frames = 8 * 24 + 1  # 8ç§’ @ 24fps = 193å¸§
video = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_frames=193,
    height=1088,  # 1088èƒ½è¢«32æ•´é™¤
    width=1920,
    num_inference_steps=8,
    guidance_scale=3.0,
    generator=torch.Generator("cuda").manual_seed(2024)
).frames[0]

# ä¿å­˜
export_to_video(video, "squat_tutorial.mp4", fps=24)

print("è§†é¢‘ç”Ÿæˆå®Œæˆï¼")
print(f"åˆ†è¾¨ç‡: 1920Ã—1088")
print(f"å¸§æ•°: {len(video)}")
print(f"æ—¶é•¿: {len(video) / 24:.2f}ç§’")
```

### 10.2 æ¡ˆä¾‹2ï¼šå¥èº«æˆ¿ç¯å¢ƒå±•ç¤ºï¼ˆDepthæ§åˆ¶ï¼‰

```python
from diffusers import LTXVideoDepthControlPipeline
from PIL import Image
import torch

# åŠ è½½æ·±åº¦æ§åˆ¶æ¨¡å‹
depth_pipe = LTXVideoDepthControlPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="depth-control",
    torch_dtype=torch.float16
).to("cuda")

# åŠ è½½å‚è€ƒå¥èº«æˆ¿å›¾ç‰‡
gym_image = Image.open("gym_reference.jpg").resize((1280, 720))

# ä¼°è®¡æ·±åº¦å›¾ï¼ˆä½¿ç”¨DPTï¼‰
from transformers import DPTForDepthEstimation, DPTImageProcessor

depth_model = DPTForDepthEstimation.from_pretrained("Intel/dpt-large").to("cuda")
processor = DPTImageProcessor.from_pretrained("Intel/dpt-large")

inputs = processor(images=gym_image, return_tensors="pt").to("cuda")
with torch.no_grad():
    depth_map = depth_model(**inputs).predicted_depth

# ç”Ÿæˆå¥èº«æˆ¿æ¼«æ¸¸è§†é¢‘
video = depth_pipe(
    prompt="å¥èº«æˆ¿å†…éƒ¨ç¯å¢ƒå±•ç¤ºï¼Œé•œå¤´ä»å·¦å‘å³ç¼“æ…¢å¹³ç§»ï¼Œå±•ç¤ºå„ç§å™¨æ",
    depth_map=depth_map,
    control_strength=0.75,
    num_frames=129,  # 5.4ç§’
    height=720,
    width=1280,
    num_inference_steps=20,
    guidance_scale=5.0
).frames[0]

export_to_video(video, "gym_tour.mp4", fps=24)
```

### 10.3 æ¡ˆä¾‹3ï¼šå¤šè§’åº¦åŠ¨ä½œæ¼”ç¤ºï¼ˆPoseæ§åˆ¶ï¼‰

```python
from diffusers import LTXVideoPoseControlPipeline
from controlnet_aux import OpenposeDetector
import torch

# åŠ è½½å§¿æ€æ§åˆ¶æ¨¡å‹
pose_pipe = LTXVideoPoseControlPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="pose-control",
    torch_dtype=torch.float16
).to("cuda")

# åŠ è½½OpenPoseæ£€æµ‹å™¨
openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")

# æå–å‚è€ƒå§¿æ€
reference_images = [
    Image.open(f"pose_{i}.jpg") for i in range(5)
]
pose_sequence = [openpose(img) for img in reference_images]

# ç”Ÿæˆå§¿æ€æ§åˆ¶è§†é¢‘
video = pose_pipe(
    prompt="å¥èº«æ•™ç»ƒæŒ‰ç…§æ ‡å‡†å§¿åŠ¿æ¼”ç¤ºæ·±è¹²ï¼Œä¾§é¢è§†è§’",
    pose_sequence=pose_sequence,  # å¤šå¸§å§¿æ€åºåˆ—
    control_strength=0.85,
    num_frames=129,
    height=720,
    width=1280,
    num_inference_steps=25,
    guidance_scale=6.0
).frames[0]

export_to_video(video, "multi_angle_squat.mp4", fps=24)
```

### 10.4 æ¡ˆä¾‹4ï¼šäº§å“å®£ä¼ ç‰‡ï¼ˆ4Ké«˜æ¸…ï¼‰

```python
# 4Käº§å“å®£ä¼ ç‰‡
pipe_4k = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-13b",  # ä½¿ç”¨13Bæ¨¡å‹ç¡®ä¿è´¨é‡
    torch_dtype=torch.float16
).to("cuda")

# å¯ç”¨ä¼˜åŒ–
pipe_4k.vae.enable_tiling()  # 4Kå¿…éœ€
pipe_4k.enable_model_cpu_offload()

# ç”Ÿæˆ4Kè§†é¢‘
prompt_4k = """
é«˜ç«¯å¥èº«å™¨æäº§å“å±•ç¤ºï¼Œ
4Kè¶…é«˜æ¸…ç”»è´¨ï¼Œ
å™¨æä»å·¦å‘å³ç¼“æ…¢æ—‹è½¬ï¼Œ
å±•ç¤ºç»†èŠ‚å’Œå·¥è‰ºï¼Œ
ä¸“ä¸šæ‘„å½±æ£šå…‰çº¿ï¼Œ
é»‘è‰²èƒŒæ™¯ï¼Œ
ç”µå½±çº§è´¨æ„Ÿ
"""

video_4k = pipe_4k(
    prompt=prompt_4k,
    num_frames=193,  # 8ç§’
    height=2160,  # 4Ké«˜åº¦
    width=3840,   # 4Kå®½åº¦
    num_inference_steps=40,  # æ›´å¤šæ­¥æ•°ç¡®ä¿è´¨é‡
    guidance_scale=7.5
).frames[0]

export_to_video(video_4k, "product_4k.mp4", fps=24)

print("4Kè§†é¢‘ç”Ÿæˆå®Œæˆï¼")
print(f"åˆ†è¾¨ç‡: 3840Ã—2160")
print(f"æ–‡ä»¶å¤§å°é¢„ä¼°: ~500MB")
```

---

## 11. å•†ä¸šåŒ–éƒ¨ç½²æŒ‡å—

### 11.1 OpenRail-Mè®¸å¯è¯è§£è¯»

LTX-Video v0.9.5åŠä»¥ä¸Šé‡‡ç”¨**OpenRail-Mè®¸å¯è¯**ï¼š

#### **å…è®¸çš„å•†ä¸šç”¨é€”**

```
âœ… å•†ä¸šäº§å“é›†æˆ
   - å°†LTX-Videoé›†æˆåˆ°SaaSäº§å“
   - ä¸ºå®¢æˆ·æä¾›è§†é¢‘ç”ŸæˆæœåŠ¡
   - æ”¶è´¹ä½¿ç”¨

âœ… æ¨¡å‹ä¿®æ”¹ä¸åˆ†å‘
   - å¾®è°ƒæ¨¡å‹é€‚é…ç‰¹å®šåœºæ™¯
   - åˆ†å‘ä¿®æ”¹åçš„æ¨¡å‹
   - åˆ›å»ºè¡ç”Ÿäº§å“

âœ… å†…éƒ¨å•†ä¸šä½¿ç”¨
   - ä¼ä¸šå†…éƒ¨è§†é¢‘åˆ¶ä½œ
   - è¥é”€å†…å®¹ç”Ÿæˆ
   - åŸ¹è®­ææ–™åˆ¶ä½œ
```

#### **è´Ÿè´£ä»»AIä½¿ç”¨çº¦æŸ**

```
âŒ ç¦æ­¢ç”¨é€”:
   - ç”Ÿæˆéæ³•å†…å®¹
   - ä¾µçŠ¯ç‰ˆæƒ
   - ç”Ÿæˆä»‡æ¨ã€æš´åŠ›å†…å®¹
   - è¯¯å¯¼æ€§deepfake
   - æœªç»æˆæƒçš„ä¸ªäººè‚–åƒ

âš ï¸ éœ€è¦æ ‡æ³¨:
   - å¿…é¡»æ ‡æ³¨AIç”Ÿæˆå†…å®¹
   - ä¸å¾—è¯¯å¯¼ç”¨æˆ·
```

### 11.2 äº‘ç«¯éƒ¨ç½²æ¶æ„

#### **AWSéƒ¨ç½²æ–¹æ¡ˆ**

```python
# Lambda + ECSæ¶æ„
"""
ç”¨æˆ·è¯·æ±‚ â†’ API Gateway â†’ Lambda (ä»»åŠ¡è°ƒåº¦)
                            â†“
                        SQSé˜Ÿåˆ—
                            â†“
                     ECSå®¹å™¨ (GPUå®ä¾‹)
                     - g5.xlarge (A10G)
                     - è’¸é¦ç‰ˆLTX-Video
                            â†“
                        S3å­˜å‚¨
                            â†“
                     CloudFront CDN
                            â†“
                        ç”¨æˆ·è·å–
"""

# Lambdaå‡½æ•°
import boto3
import json

def lambda_handler(event, context):
    sqs = boto3.client('sqs')

    # è§£æè¯·æ±‚
    body = json.loads(event['body'])
    prompt = body['prompt']

    # å‘é€åˆ°SQS
    sqs.send_message(
        QueueUrl='https://sqs.us-east-1.amazonaws.com/xxx/ltx-video-queue',
        MessageBody=json.dumps({
            'prompt': prompt,
            'num_frames': body.get('num_frames', 129),
            'resolution': body.get('resolution', '720p')
        })
    )

    return {
        'statusCode': 202,
        'body': json.dumps({'message': 'è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²æäº¤'})
    }

# ECSå®¹å™¨æ¨ç†æœåŠ¡
from flask import Flask, request
import torch

app = Flask(__name__)

# åŠ è½½æ¨¡å‹ï¼ˆå®¹å™¨å¯åŠ¨æ—¶ï¼‰
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled",
    torch_dtype=torch.float16
).to("cuda")

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json

    # ç”Ÿæˆè§†é¢‘
    video = pipe(
        prompt=data['prompt'],
        num_frames=data['num_frames'],
        height=720,
        width=1280
    ).frames[0]

    # ä¸Šä¼ åˆ°S3
    s3 = boto3.client('s3')
    video_path = f"videos/{uuid.uuid4()}.mp4"
    export_to_video(video, "/tmp/video.mp4", fps=24)
    s3.upload_file("/tmp/video.mp4", "ltx-video-bucket", video_path)

    # è¿”å›CDN URL
    cdn_url = f"https://d1234.cloudfront.net/{video_path}"
    return {'video_url': cdn_url}

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

#### **æˆæœ¬ä¼°ç®—**

| èµ„æº | è§„æ ¼ | å•ä»· | æœˆæˆæœ¬ (100è§†é¢‘/å¤©) |
|------|------|------|-------------------|
| ECS (g5.xlarge) | A10G 24GB | $1.006/å°æ—¶ | ~$730 (æŒ‰éœ€) |
| S3å­˜å‚¨ | æ ‡å‡†å­˜å‚¨ | $0.023/GB | ~$70 (3TB) |
| CloudFront | æµé‡ | $0.085/GB | ~$255 (3TB) |
| **æ€»è®¡** | - | - | **~$1055/æœˆ** |

ä¼˜åŒ–æ–¹æ¡ˆï¼ˆSpotå®ä¾‹ï¼‰ï¼š
- ECS Spot: èŠ‚çœ70% â†’ $219/æœˆ
- **æ€»è®¡**: ~$544/æœˆ

### 11.3 æœ¬åœ°GPUé›†ç¾¤éƒ¨ç½²

#### **Kuberneteséƒ¨ç½²æ¸…å•**

```yaml
# ltx-video-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ltx-video
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ltx-video
  template:
    metadata:
      labels:
        app: ltx-video
    spec:
      containers:
      - name: ltx-video
        image: your-registry/ltx-video:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        env:
        - name: MODEL_VARIANT
          value: "ltxv-2b-0.9.6-distilled"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: model-cache
          mountPath: /models
      volumes:
      - name: model-cache
        hostPath:
          path: /data/models
      nodeSelector:
        accelerator: nvidia-tesla-a10
---
apiVersion: v1
kind: Service
metadata:
  name: ltx-video-service
spec:
  selector:
    app: ltx-video
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
```

#### **éƒ¨ç½²å‘½ä»¤**

```bash
# åº”ç”¨éƒ¨ç½²
kubectl apply -f ltx-video-deployment.yaml

# æ‰©å®¹
kubectl scale deployment ltx-video --replicas=5

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods -l app=ltx-video

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/ltx-video
```

### 11.4 ç›‘æ§ä¸å‘Šè­¦

```python
# PrometheusæŒ‡æ ‡æ”¶é›†
from prometheus_client import Counter, Histogram, Gauge
import time

# å®šä¹‰æŒ‡æ ‡
video_generation_counter = Counter(
    'ltx_video_generations_total',
    'Total number of video generations'
)

generation_duration = Histogram(
    'ltx_video_generation_duration_seconds',
    'Video generation duration'
)

gpu_memory_usage = Gauge(
    'ltx_video_gpu_memory_bytes',
    'GPU memory usage'
)

# åŒ…è£…æ¨ç†å‡½æ•°
def monitored_generate(pipe, **kwargs):
    video_generation_counter.inc()

    start_time = time.time()

    video = pipe(**kwargs).frames[0]

    duration = time.time() - start_time
    generation_duration.observe(duration)

    # è®°å½•GPUæ˜¾å­˜
    gpu_memory_usage.set(torch.cuda.memory_allocated())

    return video
```

#### **Grafanaä»ªè¡¨æ¿é…ç½®**

```json
{
  "dashboard": {
    "title": "LTX-Videoç›‘æ§",
    "panels": [
      {
        "title": "æ¯åˆ†é’Ÿç”Ÿæˆæ•°",
        "targets": [{
          "expr": "rate(ltx_video_generations_total[1m])"
        }]
      },
      {
        "title": "å¹³å‡ç”Ÿæˆæ—¶é—´",
        "targets": [{
          "expr": "histogram_quantile(0.5, ltx_video_generation_duration_seconds)"
        }]
      },
      {
        "title": "GPUæ˜¾å­˜ä½¿ç”¨",
        "targets": [{
          "expr": "ltx_video_gpu_memory_bytes / 1e9"
        }]
      }
    ]
  }
}
```

---

## 12. ä¸ä¸»æµæ¨¡å‹å¯¹æ¯”

### 12.1 ç»¼åˆå¯¹æ¯”è¡¨

| æ¨¡å‹ | å‚æ•°é‡ | æœ€é«˜åˆ†è¾¨ç‡ | éŸ³é¢‘æ”¯æŒ | å•†ä¸šè®¸å¯ | å®æ—¶ç”Ÿæˆ | ç¤¾åŒºæ”¯æŒ |
|------|--------|-----------|---------|---------|---------|---------|
| **LTX-Video** | 13B/2B | **4K** | âœ… **LTX-2** | âœ… OpenRail-M | âœ… **è’¸é¦ç‰ˆ** | â­â­â­â­ |
| HunyuanVideo | 13B | 720p | âŒ | âš ï¸ è…¾è®¯è®¸å¯ | âŒ | â­â­â­â­â­ |
| CogVideoX | 5B | 768p | âŒ | âœ… Apache 2.0 | âŒ | â­â­â­â­â­ |
| Open-Sora | 11B | 720p | âŒ | âœ… Apache 2.0 | âŒ | â­â­â­â­ |
| Runway Gen-3 | é—­æº | 4K | âœ… | âŒ å•†ä¸šé—­æº | âœ… äº‘ç«¯ | â­â­â­â­â­ |

### 12.2 æ€§èƒ½åŸºå‡†å¯¹æ¯”

#### **ç”Ÿæˆé€Ÿåº¦ï¼ˆ720p, 5ç§’è§†é¢‘ï¼‰**

| æ¨¡å‹ | GPU | ç”Ÿæˆæ—¶é—´ | å½’ä¸€åŒ–é€Ÿåº¦ |
|------|-----|---------|-----------|
| LTX-Video (è’¸é¦ç‰ˆ) | H100 | **10ç§’** | **1.0Ã—** â­ï¸ |
| LTX-Video (13B) | A100 | 120ç§’ | 0.08Ã— |
| HunyuanVideo | A100 | 180ç§’ | 0.06Ã— |
| CogVideoX-5B | A100 | 150ç§’ | 0.07Ã— |

å½’ä¸€åŒ–é€Ÿåº¦è®¡ç®—ï¼š
$$
\text{Normalized Speed} = \frac{\text{Fastest Time}}{\text{Model Time}} = \frac{10s}{T_{\text{model}}}
$$

#### **æ˜¾å­˜å ç”¨å¯¹æ¯”ï¼ˆ720pç”Ÿæˆï¼‰**

```python
# æµ‹è¯•ä»£ç 
models = {
    "LTX-Video (è’¸é¦ç‰ˆ)": "ltxv-2b-0.9.6-distilled",
    "LTX-Video (13B)": "ltxv-13b",
    "HunyuanVideo": "hunyuan-video",
    "CogVideoX-5B": "cogvideox-5b"
}

for name, model_id in models.items():
    torch.cuda.reset_peak_memory_stats()

    # åŠ è½½å¹¶ç”Ÿæˆ
    pipe = load_model(model_id)
    _ = pipe(prompt="test", num_frames=129, height=720, width=1280)

    peak_memory = torch.cuda.max_memory_allocated() / 1e9
    print(f"{name}: {peak_memory:.2f}GB")
```

ç»“æœï¼š
| æ¨¡å‹ | æ˜¾å­˜å ç”¨ | ç›¸å¯¹å ç”¨ |
|------|---------|---------|
| LTX-Video (è’¸é¦ç‰ˆ) | **1.2GB** | **1.0Ã—** â­ï¸ |
| LTX-Video (2B) | 8.5GB | 7.1Ã— |
| LTX-Video (13B) | 24.3GB | 20.3Ã— |
| HunyuanVideo | 22.1GB | 18.4Ã— |
| CogVideoX-5B | 10.2GB | 8.5Ã— |

### 12.3 è´¨é‡å¯¹æ¯”

#### **ä¸»è§‚è¯„æµ‹ï¼ˆ5åˆ†åˆ¶ï¼‰**

| è¯„æµ‹ç»´åº¦ | LTX-Video | HunyuanVideo | CogVideoX | Open-Sora |
|---------|-----------|-------------|-----------|----------|
| è¿åŠ¨æµç•…åº¦ | 4.2 | **4.7** â­ï¸ | 4.1 | 3.9 |
| ç»†èŠ‚ä¿çœŸåº¦ | **4.6** â­ï¸ | 4.3 | 4.0 | 3.8 |
| æ–‡æœ¬å¯¹é½ | 4.0 | **4.5** â­ï¸ | 4.3 | 4.1 |
| é«˜åˆ†è¾¨ç‡ | **5.0** â­ï¸ | 3.0 | 3.5 | 3.0 |
| éŸ³é¢‘åŒæ­¥ | **5.0** â­ï¸ | 0 | 0 | 0 |

### 12.4 ä½¿ç”¨åœºæ™¯æ¨è

```
åœºæ™¯å†³ç­–æ ‘:

éœ€è¦éŸ³è§†é¢‘åŒæ­¥ï¼Ÿ
â”œâ”€ æ˜¯ â†’ LTX-Video (LTX-2) âœ…
â””â”€ å¦
    â””â”€ éœ€è¦4Kåˆ†è¾¨ç‡ï¼Ÿ
        â”œâ”€ æ˜¯ â†’ LTX-Video (13B) âœ…
        â””â”€ å¦
            â””â”€ éœ€è¦æœ€ä½³è¿åŠ¨è´¨é‡ï¼Ÿ
                â”œâ”€ æ˜¯ â†’ HunyuanVideo âœ…
                â””â”€ å¦
                    â””â”€ éœ€è¦å•†ä¸šè®¸å¯æ˜ç¡®ï¼Ÿ
                        â”œâ”€ æ˜¯ â†’ CogVideoX (Apache 2.0) âœ…
                        â””â”€ å¦ â†’ æ ¹æ®GPUé€‰æ‹©
                            â”œâ”€ RTX 3060 â†’ CogVideoX (INT8)
                            â””â”€ A100 â†’ HunyuanVideo
```

#### **å…·ä½“æ¨è**

**æ¨èLTX-Videoçš„åœºæ™¯**:
- âœ… éœ€è¦4KåŸç”Ÿåˆ†è¾¨ç‡
- âœ… éœ€è¦éŸ³è§†é¢‘åŒæ­¥ï¼ˆLTX-2ï¼‰
- âœ… éœ€è¦å®æ—¶ç”Ÿæˆï¼ˆè’¸é¦ç‰ˆ + H100ï¼‰
- âœ… éœ€è¦å¤šå…³é”®å¸§æ§åˆ¶
- âœ… æ˜ç¡®çš„å•†ä¸šè®¸å¯éœ€æ±‚ï¼ˆOpenRail-Mï¼‰

**æ¨èHunyuanVideoçš„åœºæ™¯**:
- âœ… è¿½æ±‚æœ€ä½³è¿åŠ¨è´¨é‡
- âœ… ä¸­æ–‡æç¤ºè¯ç†è§£
- âœ… å¯¹åˆ†è¾¨ç‡è¦æ±‚ä¸è¶…è¿‡720p
- âœ… æœ‰A100çº§åˆ«GPU

**æ¨èCogVideoXçš„åœºæ™¯**:
- âœ… éœ€è¦Apache 2.0è®¸å¯è¯
- âœ… æ¶ˆè´¹çº§GPUï¼ˆRTX 3060+ï¼‰
- âœ… å¯¹è¿åŠ¨è´¨é‡è¦æ±‚ä¸æç«¯

---

## 13. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 13.1 å®‰è£…é—®é¢˜

#### **Q1: CUDAç‰ˆæœ¬ä¸åŒ¹é…**

```bash
# é”™è¯¯ä¿¡æ¯
RuntimeError: CUDA error: no kernel image is available for execution on the device

# è§£å†³æ–¹æ¡ˆ
# 1. æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# 2. å®‰è£…åŒ¹é…çš„PyTorch
# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### **Q2: æ¨¡å‹ä¸‹è½½å¤±è´¥**

```python
# é—®é¢˜ï¼šHuggingFaceè¿æ¥è¶…æ—¶

# è§£å†³æ–¹æ¡ˆ1ï¼šä½¿ç”¨é•œåƒ
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# è§£å†³æ–¹æ¡ˆ2ï¼šæ‰‹åŠ¨ä¸‹è½½
# 1. ä»HuggingFaceç½‘é¡µä¸‹è½½æ¨¡å‹æ–‡ä»¶
# 2. æ”¾ç½®åˆ°æœ¬åœ°ç›®å½•
# 3. åŠ è½½æœ¬åœ°æ¨¡å‹
pipe = LTXVideoPipeline.from_pretrained(
    "./local_models/ltx-video",
    local_files_only=True
)
```

### 13.2 æ˜¾å­˜é—®é¢˜

#### **Q3: OOM (Out of Memory)**

```python
# é”™è¯¯
torch.cuda.OutOfMemoryError: CUDA out of memory

# è§£å†³æ–¹æ¡ˆ1ï¼šå¯ç”¨CPUå¸è½½
pipe.enable_model_cpu_offload()

# è§£å†³æ–¹æ¡ˆ2ï¼šé™ä½åˆ†è¾¨ç‡
video = pipe(
    prompt="...",
    height=512,   # ä»720é™è‡³512
    width=896     # ä»1280é™è‡³896
)

# è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨è’¸é¦ç‰ˆ
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled"  # ä»…éœ€1GB
)

# è§£å†³æ–¹æ¡ˆ4ï¼šVAE Tiling
pipe.vae.enable_tiling()
pipe.vae.enable_slicing()
```

#### **æ˜¾å­˜éœ€æ±‚ä¼°ç®—**

$$
\text{VRAM} \approx \text{Model Size} + \text{Activation Memory} + \text{Working Memory}
$$

å¯¹äº13Bæ¨¡å‹ï¼ˆFP16ï¼‰ï¼š
$$
\text{VRAM} \approx 13 \times 2 \text{ bytes} + \frac{H \times W \times F}{64^2} \times 4 \text{ bytes} + 2\text{GB}
$$

å…¶ä¸­ $H, W, F$ ä¸ºé«˜åº¦ã€å®½åº¦ã€å¸§æ•°ã€‚

### 13.3 ç”Ÿæˆè´¨é‡é—®é¢˜

#### **Q4: ç”Ÿæˆå†…å®¹æ¨¡ç³Š**

```python
# åŸå› ï¼šæ¨ç†æ­¥æ•°è¿‡å°‘

# è§£å†³æ–¹æ¡ˆï¼šå¢åŠ æ­¥æ•°
video = pipe(
    prompt="...",
    num_inference_steps=40,  # ä»8å¢è‡³40
    guidance_scale=7.5       # å¢å¼ºæ–‡æœ¬å¼•å¯¼
)
```

#### **Q5: è¿åŠ¨ä¸è‡ªç„¶**

```python
# åŸå› ï¼šguidance_scaleè¿‡é«˜æˆ–è¿‡ä½

# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´guidance_scale
# è’¸é¦ç‰ˆæ¨è: 2.5-3.5
# éè’¸é¦ç‰ˆæ¨è: 5.0-8.0

video = pipe(
    prompt="...",
    guidance_scale=3.0  # è’¸é¦ç‰ˆæœ€ä½³å€¼
)
```

#### **Q6: ç”Ÿæˆå†…å®¹ä¸æç¤ºè¯ä¸ç¬¦**

```python
# è§£å†³æ–¹æ¡ˆ1ï¼šä¼˜åŒ–æç¤ºè¯
# ä¸å¥½çš„æç¤ºè¯
"å¥èº«æ•™ç»ƒæ·±è¹²"

# å¥½çš„æç¤ºè¯
"""
ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼Œ
ä»å‡†å¤‡å§¿åŠ¿å¼€å§‹ï¼Œç¼“æ…¢ä¸‹è¹²ï¼Œ
å¤§è…¿ä¸åœ°é¢å¹³è¡Œåç«™èµ·ï¼Œ
åŠ¨ä½œæ ‡å‡†è§„èŒƒï¼Œ
èƒŒæ™¯ç°ä»£å¥èº«æˆ¿
"""

# è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨è´Ÿé¢æç¤ºè¯
video = pipe(
    prompt="å¥èº«æ•™ç»ƒæ·±è¹²æ¼”ç¤º",
    negative_prompt="æ¨¡ç³Šï¼Œä½è´¨é‡ï¼Œé”™è¯¯å§¿åŠ¿ï¼Œå¿«é€Ÿè¿åŠ¨"
)

# è§£å†³æ–¹æ¡ˆ3ï¼šæé«˜æ–‡æœ¬å¼•å¯¼å¼ºåº¦
video = pipe(
    prompt="...",
    guidance_scale=8.0  # æ›´å¼ºçš„æ–‡æœ¬å¼•å¯¼
)
```

### 13.4 æ€§èƒ½ä¼˜åŒ–é—®é¢˜

#### **Q7: ç”Ÿæˆé€Ÿåº¦è¿‡æ…¢**

```python
# è§£å†³æ–¹æ¡ˆ1ï¼šä½¿ç”¨è’¸é¦ç‰ˆ
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled"
)

# è§£å†³æ–¹æ¡ˆ2ï¼šTorch Compile
pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")

# è§£å†³æ–¹æ¡ˆ3ï¼šå‡å°‘å¸§æ•°
video = pipe(
    num_frames=65,  # ä»129å‡è‡³65 (2.7ç§’)
    num_inference_steps=8
)

# è§£å†³æ–¹æ¡ˆ4ï¼šFP16æ··åˆç²¾åº¦
with torch.cuda.amp.autocast():
    video = pipe(...)
```

#### **Q8: æ‰¹é‡ç”Ÿæˆæ•ˆç‡ä½**

```python
# é—®é¢˜ï¼šé€ä¸ªç”Ÿæˆæµªè´¹æ—¶é—´

# è§£å†³æ–¹æ¡ˆï¼šæ‰¹é‡æ¨ç†
prompts = ["prompt1", "prompt2", "prompt3"]

videos = pipe(
    prompt=prompts,  # æ‰¹é‡è¾“å…¥
    num_frames=129,
    height=720,
    width=1280
).frames

# èŠ‚çœæ—¶é—´ï¼š
# é€ä¸ª: 3 Ã— 30ç§’ = 90ç§’
# æ‰¹é‡: 45ç§’ (2Ã— åŠ é€Ÿ)
```

### 13.5 LTX-2ç›¸å…³é—®é¢˜

#### **Q9: LTX-2ä½•æ—¶å¯ç”¨ï¼Ÿ**

```
å®˜æ–¹å…¬å‘Š: 2025.10.23
çŠ¶æ€: å…¬å‘Šé˜¶æ®µï¼Œæƒé‡å°†äº2025å¹´æ™šäº›æ—¶å€™å‘å¸ƒ

å½“å‰å¯ç”¨:
- LTX-Video v0.9.8 (ä¸å«éŸ³é¢‘)
- LTXV-13B, LTXV-2B, è’¸é¦ç‰ˆ

æœªæ¥å¯ç”¨ï¼ˆLTX-2ï¼‰:
- éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆ
- åŸç”Ÿ4K + åŒæ­¥éŸ³é¢‘
- 50%è®¡ç®—æˆæœ¬é™ä½
```

#### **Q10: å¦‚ä½•å‡†å¤‡LTX-2ä½¿ç”¨ï¼Ÿ**

```python
# å½“å‰ï¼šä½¿ç”¨v0.9.8å­¦ä¹ å·¥ä½œæµ
pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    variant="ltxv-2b-0.9.6-distilled"
)

# æœªæ¥ï¼šå¹³æ»‘å‡çº§åˆ°LTX-2
# é¢„è®¡APIä¿æŒå…¼å®¹
pipe_ltx2 = LTX2Pipeline.from_pretrained(
    "Lightricks/LTX-2",
    variant="ltx2-13b"
)

video_with_audio = pipe_ltx2(
    prompt="å¥èº«æ•™ç»ƒè®²è§£æ·±è¹²è¦ç‚¹",
    audio_prompt="æ¸…æ™°çš„è®²è§£å£°éŸ³ï¼Œå¥èº«æˆ¿ç¯å¢ƒéŸ³",
    num_frames=257,
    height=2160,
    width=3840
)
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **LTX-Videoå®šä½**: å¼€æºè§†é¢‘ç”Ÿæˆé¢†åŸŸçš„**4K+éŸ³é¢‘**å…ˆé”‹
   - åŸç”Ÿ4Kåˆ†è¾¨ç‡æ”¯æŒï¼ˆæœ€é«˜3840Ã—2160ï¼‰
   - LTX-2éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆï¼ˆè¡Œä¸šé¦–åˆ›ï¼‰
   - OpenRail-Må•†ä¸šè®¸å¯

2. **æ¨¡å‹é€‰æ‹©å»ºè®®**:
   - **å®æ—¶ç”Ÿæˆ**: è’¸é¦ç‰ˆ (1GB VRAM, H100 10ç§’)
   - **æœ€é«˜è´¨é‡**: 13Bç‰ˆæœ¬ (24GB VRAM)
   - **å¹³è¡¡æ–¹æ¡ˆ**: 2Bç‰ˆæœ¬ (12GB VRAM)

3. **æŠ€æœ¯ä¼˜åŠ¿**:
   - DiTæ¶æ„ï¼šæ‰©å±•æ€§å¼º
   - å¤šå…³é”®å¸§æ§åˆ¶ï¼šé•¿è§†é¢‘è¿è´¯æ€§
   - æ§åˆ¶æ¨¡å‹ï¼šDepth/Pose/Canny
   - è’¸é¦åŠ é€Ÿï¼š15å€é€Ÿåº¦æå‡

4. **å•†ä¸šåŒ–è·¯å¾„**:
   - äº‘ç«¯éƒ¨ç½²ï¼šAWS ECS + S3 + CloudFront
   - æœ¬åœ°éƒ¨ç½²ï¼šKubernetes + GPUé›†ç¾¤
   - æˆæœ¬ä¼˜åŒ–ï¼šSpotå®ä¾‹èŠ‚çœ70%

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ“– é˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼šhttps://github.com/Lightricks/LTX-Video
- ğŸ¨ å°è¯•ComfyUIå·¥ä½œæµ
- ğŸ”§ å®è·µæ§åˆ¶æ¨¡å‹ï¼ˆDepth/Pose/Cannyï¼‰
- ğŸš€ å…³æ³¨LTX-2å‘å¸ƒåŠ¨æ€

---

**æ›´æ–°æ—¥å¿—**:
- 2025-11-30: åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºäºLTX-Video v0.9.8å’ŒLTX-2å…¬å‘Š
