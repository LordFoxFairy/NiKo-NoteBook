# ç¬¬04ç¯‡_è§†é¢‘ç”Ÿæˆ(06)_HunyuanVideoï¼šè…¾è®¯130äº¿å‚æ•°å¼€æºç‹è€…å®æˆ˜

> **æ›´æ–°æ—¶é—´**: 2025-11-30
> **GitHub**: https://github.com/Tencent/HunyuanVideo
> **å‚æ•°é‡**: 130äº¿ (ç›®å‰æœ€å¤§å¼€æºè§†é¢‘ç”Ÿæˆæ¨¡å‹)
> **ä¸“ä¸šè¯„æµ‹**: æ€»ä½“å¾—åˆ†ç¬¬1åï¼Œè¿åŠ¨è´¨é‡è¶…è¶ŠRunway Gen-3å’ŒLuma 1.6

---

## ğŸ“‹ ç›®å½•

1. [ä¸ºä»€ä¹ˆé€‰æ‹©HunyuanVideo](#1-ä¸ºä»€ä¹ˆé€‰æ‹©hunyuanvideo)
2. [ä¸“ä¸šè¯„æµ‹æ•°æ®è§£è¯»](#2-ä¸“ä¸šè¯„æµ‹æ•°æ®è§£è¯»)
3. [æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ](#3-æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ)
4. [ç¯å¢ƒæ­å»ºä¸å®‰è£…](#4-ç¯å¢ƒæ­å»ºä¸å®‰è£…)
5. [Python APIå®Œå…¨æŒ‡å—](#5-python-apiå®Œå…¨æŒ‡å—)
6. [ComfyUIå·¥ä½œæµé›†æˆ](#6-comfyuiå·¥ä½œæµé›†æˆ)
7. [åˆ†è¾¨ç‡ä¸å¸§æ•°é…ç½®](#7-åˆ†è¾¨ç‡ä¸å¸§æ•°é…ç½®)
8. [GPUä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜](#8-gpuä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜)
9. [å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹](#9-å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹)
10. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#10-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## 1. ä¸ºä»€ä¹ˆé€‰æ‹©HunyuanVideo

### 1.1 æ ¸å¿ƒä¼˜åŠ¿

HunyuanVideoæ˜¯è…¾è®¯æ··å…ƒå›¢é˜Ÿæ¨å‡ºçš„å¼€æºè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‹¬ç‰¹ä¼˜åŠ¿ï¼š

#### **ğŸ† ä¸“ä¸šè¯„æµ‹ç¬¬1å**
åŸºäº1533ä¸ªæ–‡æœ¬æç¤ºè¯å’Œ60+ä¸“ä¸šè¯„æµ‹å‘˜çš„äººç±»åå¥½è¯„ä¼°ï¼š
- **æ€»ä½“å¾—åˆ†**: 41.3% (ç¬¬1å)
- **è¿åŠ¨è´¨é‡**: 66.5% (æ‰€æœ‰æ¨¡å‹æœ€é«˜)
- è¶…è¶ŠRunway Gen-3 Alpha (27.4%)
- è¶…è¶ŠLuma 1.6 (24.8%)

#### **ğŸ”“ å®Œå…¨å¼€æº**
- âœ… æ¨ç†ä»£ç å®Œå…¨å¼€æº
- âœ… æ¨¡å‹æƒé‡å¯ä¸‹è½½
- âœ… ComfyUIåŸç”Ÿé›†æˆ
- âœ… Diffusersç”Ÿæ€æ”¯æŒ

#### **âš¡ å‚æ•°é‡æœ€å¤§**
- 130äº¿å‚æ•° (13B)
- ç›®å‰å¼€æºé¢†åŸŸæœ€å¤§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹
- æ€§èƒ½ç›´é€¼å•†ä¸šé—­æºæ–¹æ¡ˆ

#### **ğŸ’° ROIå¯¹æ¯”**

| æ–¹æ¡ˆ | æœˆæˆæœ¬ | æ€§èƒ½å¾—åˆ† | æ•°æ®éšç§ | è‡ªå®šä¹‰èƒ½åŠ› |
|------|--------|---------|---------|-----------|
| **HunyuanVideo (å¼€æº)** | $0 (ä»…GPUæˆæœ¬) | 41.3% | âœ… å®Œå…¨å¯æ§ | âœ… å…¨é¢ |
| Runway Gen-3 | ~$500-2000 | 27.4% | âŒ äº‘ç«¯å¤„ç† | âŒ å—é™ |
| Luma 1.6 | ~$300-1500 | 24.8% | âŒ äº‘ç«¯å¤„ç† | âŒ å—é™ |

---

## 2. ä¸“ä¸šè¯„æµ‹æ•°æ®è§£è¯»

### 2.1 å®Œæ•´è¯„æµ‹ç»“æœ

åŸºäºVBenchåŸºå‡†å’Œäººç±»åå¥½è¯„ä¼°ï¼ˆ1533ä¸ªæç¤ºè¯ï¼Œ60+è¯„æµ‹å‘˜ï¼‰ï¼š

| æ¨¡å‹ | æ–‡æœ¬å¯¹é½ | è¿åŠ¨è´¨é‡ | è§†è§‰è´¨é‡ | æ€»ä½“å¾—åˆ† | æ’å |
|------|---------|---------|---------|---------|------|
| **HunyuanVideo** | 61.8% | **66.5%** â­ï¸ | 95.7% | **41.3%** | **ğŸ¥‡ 1** |
| CNTopA | 62.6% | 61.7% | 95.6% | 37.7% | ğŸ¥ˆ 2 |
| CNTopB | 60.1% | 62.9% | 97.7% | 37.5% | ğŸ¥‰ 3 |
| Runway Gen-3 Alpha | 47.7% | 54.7% | 97.5% | 27.4% | 4 |
| Luma 1.6 | 57.6% | 44.2% | 94.1% | 24.8% | 5 |

### 2.2 å…³é”®æŒ‡æ ‡è§£è¯»

#### **è¿åŠ¨è´¨é‡ (Motion Quality) - 66.5%**
- **å«ä¹‰**: è§†é¢‘ä¸­ç‰©ä½“è¿åŠ¨çš„æµç•…æ€§ã€è‡ªç„¶åº¦ã€ç‰©ç†çœŸå®æ€§
- **ä¼˜åŠ¿**: è¶…è¶ŠGen-3çš„54.7%å’ŒLumaçš„44.2%
- **åº”ç”¨åœºæ™¯**: å¥èº«åŠ¨ä½œæ¼”ç¤ºã€è¿åŠ¨æ•™å­¦ã€åŠ¨æ€äº§å“å±•ç¤º

#### **æ–‡æœ¬å¯¹é½ (Text Alignment) - 61.8%**
- **å«ä¹‰**: ç”Ÿæˆå†…å®¹ä¸æ–‡æœ¬æç¤ºçš„åŒ¹é…ç¨‹åº¦
- **è¡¨ç°**: ä¸­ä¸Šæ°´å¹³ï¼Œä½äºCNTopAçš„62.6%
- **ä¼˜åŒ–æ–¹å‘**: ä½¿ç”¨æç¤ºè¯é‡å†™ç³»ç»Ÿæå‡

#### **è§†è§‰è´¨é‡ (Visual Quality) - 95.7%**
- **å«ä¹‰**: ç”»é¢æ¸…æ™°åº¦ã€ç»†èŠ‚ä¸°å¯Œåº¦ã€ç¾å­¦è´¨é‡
- **è¡¨ç°**: æ¥è¿‘CNTopBçš„97.7%ï¼Œè¾¾åˆ°å•†ä¸šçº§æ ‡å‡†

### 2.3 æ•°å­¦å»ºæ¨¡

æ€»ä½“å¾—åˆ†è®¡ç®—å…¬å¼ï¼š

$$
\text{Overall Score} = \alpha \cdot \text{Text Alignment} + \beta \cdot \text{Motion Quality} + \gamma \cdot \text{Visual Quality}
$$

å…¶ä¸­æƒé‡ç³»æ•°åŸºäºç”¨æˆ·åå¥½è°ƒæŸ¥ç¡®å®šï¼š
$$
\alpha = 0.3, \quad \beta = 0.4, \quad \gamma = 0.3
$$

HunyuanVideoçš„è¿åŠ¨è´¨é‡æƒé‡æœ€é«˜ï¼ˆÎ²=0.4ï¼‰ï¼Œå› æ­¤åœ¨æ€»åˆ†ä¸­å æ®å…³é”®ä¼˜åŠ¿ã€‚

---

## 3. æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ

### 3.1 æ•´ä½“æ¶æ„

HunyuanVideoé‡‡ç”¨**åŒæµåˆ°å•æµæ··åˆTransformeræ¶æ„**ï¼š

```
è¾“å…¥æ–‡æœ¬æç¤ºè¯
    â†“
[MLLMæ–‡æœ¬ç¼–ç å™¨] â†’ æ–‡æœ¬Token
    â†“
[åŒæµé˜¶æ®µ]
- è§†é¢‘Tokenæµ (ç‹¬ç«‹å¤„ç†)
- æ–‡æœ¬Tokenæµ (ç‹¬ç«‹å¤„ç†)
    â†“
[å•æµé˜¶æ®µ]
- å¤šæ¨¡æ€èåˆ
- å…¨æ³¨æ„åŠ›æœºåˆ¶
    â†“
[3D VAEè§£ç å™¨]
- æ—¶ç©ºè§£å‹ç¼©
- 4Ã—8Ã—16å€è¿˜åŸ
    â†“
è¾“å‡ºé«˜åˆ†è¾¨ç‡è§†é¢‘ (720p/540p, 129å¸§)
```

### 3.2 æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶

#### **3.2.1 MLLMæ–‡æœ¬ç¼–ç å™¨**

ç›¸æ¯”ä¼ ç»ŸCLIPç¼–ç å™¨çš„ä¼˜åŠ¿ï¼š

| ç‰¹æ€§ | CLIP | MLLM (HunyuanVideo) |
|------|------|---------------------|
| å›¾æ–‡å¯¹é½ | åŸºç¡€ | âœ… è§†è§‰æŒ‡ä»¤å¾®è°ƒ |
| ç»†èŠ‚æè¿° | ç²—ç²’åº¦ | âœ… ç»†ç²’åº¦ |
| å¤æ‚æ¨ç† | å¼± | âœ… å¼º |
| é•¿æ–‡æœ¬æ”¯æŒ | 77 tokens | âœ… 512+ tokens |

**æŠ€æœ¯å®ç°**:
```python
# MLLMç¼–ç å™¨å¢å¼ºæ–‡æœ¬ç‰¹å¾
class MLLMTextEncoder:
    def __init__(self):
        self.bidirectional_refiner = TokenRefiner()

    def encode(self, text_prompt):
        # åˆå§‹ç¼–ç 
        features = self.base_encode(text_prompt)

        # åŒå‘Token Refinerå¢å¼º
        refined_features = self.bidirectional_refiner(features)

        return refined_features
```

#### **3.2.2 3D VAEæ—¶ç©ºå‹ç¼©**

é‡‡ç”¨å› æœ3Då·ç§¯å®ç°é«˜æ•ˆå‹ç¼©ï¼š

**å‹ç¼©æ¯”**:
- æ—¶é—´ç»´åº¦: **4Ã—** (129å¸§ â†’ 33å¸§)
- ç©ºé—´ç»´åº¦: **8Ã—** (720p â†’ 90pä¸­é—´è¡¨ç¤º)
- é€šé“ç»´åº¦: **16Ã—** (RGB â†’ æ½œç©ºé—´)

**æ€»ä½“å‹ç¼©æ¯”**: $4 \times 8 \times 16 = 512Ã—$

**æ•°å­¦è¡¨ç¤º**:

$$
z = \text{Encoder}_{3D}(x), \quad z \in \mathbb{R}^{T/4 \times H/8 \times W/8 \times C/16}
$$

$$
\hat{x} = \text{Decoder}_{3D}(z), \quad \hat{x} \in \mathbb{R}^{T \times H \times W \times 3}
$$

**Pythonå®ç°**:
```python
import torch
import torch.nn as nn

class Causal3DVAE(nn.Module):
    def __init__(self):
        super().__init__()
        # æ—¶é—´å‹ç¼©: 4å€
        self.temporal_compress = nn.Conv3d(
            in_channels=3,
            out_channels=128,
            kernel_size=(4, 4, 4),
            stride=(4, 4, 4),
            padding=(0, 0, 0)
        )

        # ç©ºé—´å‹ç¼©: 8å€
        self.spatial_compress = nn.Conv3d(
            in_channels=128,
            out_channels=256,
            kernel_size=(1, 2, 2),
            stride=(1, 2, 2)
        )

    def encode(self, video):
        # video: [B, T=129, H=720, W=1280, C=3]
        x = video.permute(0, 4, 1, 2, 3)  # â†’ [B, C, T, H, W]

        # æ—¶ç©ºå‹ç¼©
        z_temp = self.temporal_compress(x)  # â†’ [B, 128, 33, 180, 320]
        z = self.spatial_compress(z_temp)   # â†’ [B, 256, 33, 90, 160]

        return z
```

#### **3.2.3 æç¤ºè¯é‡å†™ç³»ç»Ÿ**

åŸºäº**Hunyuan-Large**æ¨¡å‹å¾®è°ƒï¼Œæä¾›ä¸¤ç§æ¨¡å¼ï¼š

**Normalæ¨¡å¼** (æ—¥å¸¸ä½¿ç”¨):
- è¾“å…¥: "å¥èº«æ•™ç»ƒæ·±è¹²"
- è¾“å‡º: "ä¸“ä¸šå¥èº«æ•™ç»ƒåœ¨ç°ä»£å¥èº«æˆ¿æ¼”ç¤ºæ ‡å‡†æ·±è¹²åŠ¨ä½œï¼Œä¾§é¢è§†è§’ï¼Œè‡ªç„¶å…‰ç…§ï¼Œ4Ké«˜æ¸…ï¼Œå†™å®é£æ ¼"

**Masteræ¨¡å¼** (ä¸“ä¸šåˆ›ä½œ):
- è¾“å…¥: "æ·±è¹²"
- è¾“å‡º: "ä¸“ä¸šè¿åŠ¨å‘˜ä»¥å®Œç¾å§¿åŠ¿æ‰§è¡Œæ·±è¹²åŠ¨ä½œï¼ŒèƒŒæ™¯ä¸ºé…å¤‡ä¸“ä¸šå™¨æçš„å¥èº«æˆ¿ï¼Œé‡‡ç”¨ä½è§’åº¦é•œå¤´çªå‡ºåŠ›é‡æ„Ÿï¼ŒæŸ”å’Œä¾§å…‰å¢å¼ºè‚Œè‚‰çº¿æ¡ï¼Œç”µå½±çº§è‰²å½©åˆ†çº§ï¼Œ8Kè¶…é«˜æ¸…ï¼Œè¶…å†™å®æ¸²æŸ“"

**APIä½¿ç”¨**:
```python
from hunyuan_video import PromptRewriter

rewriter = PromptRewriter(mode="master")

original = "å¥èº«æ•™ç»ƒæ¼”ç¤ºç¡¬æ‹‰"
enhanced = rewriter.rewrite(original)

print(enhanced)
# è¾“å‡º: "èµ„æ·±åŠ›é‡è®­ç»ƒæ•™ç»ƒå±•ç¤ºæ ‡å‡†æ é“ƒç¡¬æ‹‰æŠ€æœ¯ï¼Œå¥èº«æˆ¿ç¯å¢ƒï¼Œ..."
```

### 3.3 è®­ç»ƒç­–ç•¥

#### **å¤šåˆ†è¾¨ç‡è®­ç»ƒ**

$$
\mathcal{L}_{\text{total}} = \sum_{r \in \mathcal{R}} w_r \cdot \mathcal{L}_{\text{diffusion}}(x_r, c)
$$

å…¶ä¸­ $\mathcal{R} = \{540p, 720p\}$ï¼Œæƒé‡ $w_r$ æ ¹æ®åˆ†è¾¨ç‡åŠ¨æ€è°ƒæ•´ã€‚

#### **Rectified Flow**

é‡‡ç”¨æ•´æµæµï¼ˆRectified Flowï¼‰è€Œéä¼ ç»ŸDDPMï¼š

$$
\frac{dx_t}{dt} = v_\theta(x_t, t, c)
$$

ä¼˜åŠ¿ï¼š
- æ¨ç†æ­¥æ•°å‡å°‘ 50æ­¥ (vs DDPMçš„1000æ­¥)
- æ”¶æ•›é€Ÿåº¦æå‡ 2-3Ã—
- ç”Ÿæˆè´¨é‡ä¿æŒ

---

## 4. ç¯å¢ƒæ­å»ºä¸å®‰è£…

### 4.1 ç¡¬ä»¶è¦æ±‚

#### **æœ€ä½é…ç½®** (540pç”Ÿæˆ)
- GPU: NVIDIA A100 40GB / A6000 48GB
- æ˜¾å­˜: **â‰¥45GB**
- CPU: 16æ ¸å¿ƒ
- å†…å­˜: 64GB
- å­˜å‚¨: 500GB SSD

#### **æ¨èé…ç½®** (720pç”Ÿæˆ)
- GPU: NVIDIA A100 80GB / H100 80GB
- æ˜¾å­˜: **â‰¥60GB**
- CPU: 32æ ¸å¿ƒ
- å†…å­˜: 128GB
- å­˜å‚¨: 1TB NVMe SSD

#### **è±ªåé…ç½®** (å¤šGPUå¹¶è¡Œ)
- GPU: 4Ã— NVIDIA H100 80GB
- æ˜¾å­˜: 4Ã—80GB = 320GB
- CPU: 128æ ¸å¿ƒ
- å†…å­˜: 512GB
- å­˜å‚¨: 2TB NVMe RAID

### 4.2 è½¯ä»¶ä¾èµ–

#### **ç³»ç»Ÿè¦æ±‚**
- æ“ä½œç³»ç»Ÿ: Linux (Ubuntu 20.04/22.04æ¨è)
- CUDA: 11.8 æˆ– 12.4
- Python: 3.10.9

#### **æ ¸å¿ƒä¾èµ–**
```txt
torch>=2.4.0
torchvision>=0.19.0
transformers>=4.30.0
diffusers>=0.27.0
flash-attn>=2.6.3
xformers>=0.0.24
accelerate>=0.25.0
safetensors>=0.4.0
```

### 4.3 å®‰è£…æ­¥éª¤

#### **æ­¥éª¤1: åˆ›å»ºCondaç¯å¢ƒ**

```bash
# åˆ›å»ºPython 3.10ç¯å¢ƒ
conda create -n HunyuanVideo python==3.10.9
conda activate HunyuanVideo

# éªŒè¯Pythonç‰ˆæœ¬
python --version  # åº”è¾“å‡º: Python 3.10.9
```

#### **æ­¥éª¤2: å®‰è£…PyTorch**

```bash
# CUDA 11.8ç‰ˆæœ¬
pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.4ç‰ˆæœ¬
pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124

# éªŒè¯CUDAå¯ç”¨æ€§
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0)}')"
```

#### **æ­¥éª¤3: å…‹éš†ä»“åº“**

```bash
git clone https://github.com/Tencent/HunyuanVideo.git
cd HunyuanVideo

# æŸ¥çœ‹é¡¹ç›®ç»“æ„
ls -lh
# è¾“å‡º:
# â”œâ”€â”€ hyvideo/          # æ ¸å¿ƒä»£ç 
# â”œâ”€â”€ sample_video.py   # æ¨ç†è„šæœ¬
# â”œâ”€â”€ requirements.txt  # ä¾èµ–åˆ—è¡¨
# â”œâ”€â”€ configs/          # é…ç½®æ–‡ä»¶
# â””â”€â”€ checkpoints/      # æ¨¡å‹æƒé‡ç›®å½•
```

#### **æ­¥éª¤4: å®‰è£…ä¾èµ–**

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å®‰è£…Flash Attention 2 (åŠ é€Ÿæ¨ç†)
pip install ninja
pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3

# å®‰è£…xDiT (å¤šGPUå¹¶è¡Œæ”¯æŒ)
pip install xfuser==0.4.0
```

#### **æ­¥éª¤5: ä¸‹è½½æ¨¡å‹æƒé‡**

```bash
# åˆ›å»ºæƒé‡ç›®å½•
mkdir -p checkpoints

# ä½¿ç”¨Hugging Face CLIä¸‹è½½ (æ¨è)
pip install huggingface-hub

# ä¸‹è½½å®Œæ•´æ¨¡å‹ (~26GB)
huggingface-cli download tencent/HunyuanVideo \
  --local-dir checkpoints/hunyuan-video \
  --local-dir-use-symlinks False

# æˆ–ä¸‹è½½FP8é‡åŒ–ç‰ˆæœ¬ (~13GB, èŠ‚çœæ˜¾å­˜)
huggingface-cli download tencent/HunyuanVideo-FP8 \
  --local-dir checkpoints/hunyuan-video-fp8 \
  --local-dir-use-symlinks False
```

**æ¨¡å‹æƒé‡ç»“æ„**:
```
checkpoints/hunyuan-video/
â”œâ”€â”€ transformers/
â”‚   â””â”€â”€ mp_rank_00_model_states.pt  # ä¸»æ¨¡å‹ (~20GB)
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ pytorch_model.pt             # VAEæƒé‡ (~5GB)
â”œâ”€â”€ text_encoder/
â”‚   â””â”€â”€ pytorch_model.bin            # æ–‡æœ¬ç¼–ç å™¨ (~1GB)
â””â”€â”€ config.json
```

#### **æ­¥éª¤6: éªŒè¯å®‰è£…**

```bash
# æµ‹è¯•è„šæœ¬
python -c "
import torch
from hyvideo.utils.model_loader import load_model

print('âœ… PyTorchç‰ˆæœ¬:', torch.__version__)
print('âœ… CUDAç‰ˆæœ¬:', torch.version.cuda)
print('âœ… GPUåç§°:', torch.cuda.get_device_name(0))
print('âœ… æ˜¾å­˜æ€»é‡:', f'{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
print('âœ… ç¯å¢ƒé…ç½®å®Œæˆ!')
"
```

---

## 5. Python APIå®Œå…¨æŒ‡å—

### 5.1 åŸºç¡€æ¨ç†

#### **æœ€ç®€å•çš„ç”Ÿæˆç¤ºä¾‹**

```python
import torch
from hyvideo.inference import HunyuanVideoInference

# åˆå§‹åŒ–æ¨¡å‹
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    device="cuda",
    dtype=torch.float16
)

# ç”Ÿæˆè§†é¢‘
prompt = "ä¸“ä¸šå¥èº«æ•™ç»ƒåœ¨å¥èº«æˆ¿æ¼”ç¤ºæ ‡å‡†æ·±è¹²åŠ¨ä½œï¼Œä¾§é¢è§†è§’ï¼Œ4Kå†™å®é£æ ¼"

video = model.generate(
    prompt=prompt,
    video_size=(720, 1280),      # é«˜åº¦Ã—å®½åº¦
    video_length=129,             # å¸§æ•° (4k+1æ ¼å¼)
    num_inference_steps=50,       # æ¨ç†æ­¥æ•°
    guidance_scale=6.0,           # CFGå¼•å¯¼å¼ºåº¦
    seed=42                       # éšæœºç§å­
)

# ä¿å­˜è§†é¢‘
model.save_video(video, "output/squat_demo.mp4", fps=16)
```

è¾“å‡º:
```
âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: 45.2s)
ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...
  - åˆ†è¾¨ç‡: 720Ã—1280
  - å¸§æ•°: 129 (8ç§’ @ 16fps)
  - æ¨ç†æ­¥æ•°: 50
â±ï¸  Step 10/50 [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 20% - ETA: 1m 23s
â±ï¸  Step 50/50 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% - å®Œæˆ!
ğŸ’¾ è§†é¢‘å·²ä¿å­˜: output/squat_demo.mp4
æ€»è€—æ—¶: 2m 15s
```

### 5.2 é«˜çº§å‚æ•°é…ç½®

#### **å®Œæ•´å‚æ•°åˆ—è¡¨**

```python
video = model.generate(
    # === åŸºç¡€å‚æ•° ===
    prompt="è¯¦ç»†çš„æ–‡æœ¬æè¿°",
    negative_prompt="ä½è´¨é‡, æ¨¡ç³Š, å¤±çœŸ",  # è´Ÿæç¤ºè¯

    # === è§†é¢‘è§„æ ¼ ===
    video_size=(720, 1280),      # (é«˜, å®½) æ”¯æŒ: 540p, 720p
    video_length=129,             # å¸§æ•°: 4k+1, kâˆˆ[7, 32]

    # === é‡‡æ ·å‚æ•° ===
    num_inference_steps=50,       # æ¨ç†æ­¥æ•°: [20, 100]
    guidance_scale=6.0,           # CFGå¼ºåº¦: [1.0, 15.0]
    flow_shift=7.0,               # Rectified Flowåç§»

    # === ä¼˜åŒ–é€‰é¡¹ ===
    use_cpu_offload=True,         # CPUå¸è½½èŠ‚çœæ˜¾å­˜
    enable_vae_tiling=True,       # VAEåˆ†å—ç¼–ç 

    # === éšæœºæ§åˆ¶ ===
    seed=42,                      # éšæœºç§å­ (å¯å¤ç°)
    generator=None                # æˆ–ä¼ å…¥torch.Generator
)
```

#### **å‚æ•°å½±å“åˆ†æ**

**æ¨ç†æ­¥æ•° vs è´¨é‡ vs é€Ÿåº¦**:

| æ­¥æ•° | è´¨é‡ | ç»†èŠ‚ | é€Ÿåº¦ | æ¨èåœºæ™¯ |
|------|------|------|------|---------|
| 20 | â­ï¸â­ï¸ | è¾ƒä½ | å¿« (45s) | å¿«é€Ÿé¢„è§ˆ |
| 30 | â­ï¸â­ï¸â­ï¸ | ä¸­ç­‰ | ä¸­ (1m 15s) | æ—¥å¸¸ä½¿ç”¨ |
| 50 | â­ï¸â­ï¸â­ï¸â­ï¸ | é«˜ | æ…¢ (2m 15s) | **æ¨è** |
| 100 | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | æé«˜ | ææ…¢ (4m 30s) | æœ€ç»ˆè¾“å‡º |

**CFGå¼•å¯¼å¼ºåº¦å½±å“**:

$$
\text{Output} = \text{Noise}_{\text{uncond}} + \text{scale} \times (\text{Noise}_{\text{cond}} - \text{Noise}_{\text{uncond}})
$$

| scale | æ–‡æœ¬ç›¸å…³æ€§ | åˆ›é€ æ€§ | è§†è§‰è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|-------|-----------|--------|---------|---------|
| 1.0 | ä½ | æé«˜ | ä¸ç¨³å®š | æŠ½è±¡è‰ºæœ¯ |
| 3.0 | ä¸­ | é«˜ | è‰¯å¥½ | åˆ›æ„æ¢ç´¢ |
| **6.0** | é«˜ | ä¸­ç­‰ | **ä¼˜ç§€** | **é€šç”¨æ¨è** |
| 10.0 | æé«˜ | ä½ | è¿‡é¥±å’Œ | ç²¾å‡†å¤ç° |
| 15.0 | è¿‡æ‹Ÿåˆ | æä½ | å¤±çœŸ | ä¸æ¨è |

### 5.3 æ‰¹é‡ç”Ÿæˆ

#### **å¹¶è¡Œç”Ÿæˆå¤šä¸ªè§†é¢‘**

```python
import torch
from concurrent.futures import ThreadPoolExecutor
from hyvideo.inference import HunyuanVideoInference

# å¥èº«åŠ¨ä½œåˆ—è¡¨
prompts = [
    "å¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²ï¼Œå¥èº«æˆ¿ç¯å¢ƒï¼Œä¸“ä¸šå™¨æï¼Œä¾§é¢è§†è§’",
    "å¥èº«æ•™ç»ƒæ¼”ç¤ºç¡¬æ‹‰ï¼Œæ é“ƒå™¨æ¢°ï¼Œæ­£é¢è§†è§’ï¼ŒåŠ›é‡å±•ç¤º",
    "å¥èº«æ•™ç»ƒæ¼”ç¤ºå§æ¨ï¼Œå§æ¨æ¶ï¼Œä¿¯è§†è§’åº¦ï¼Œæ ‡å‡†åŠ¨ä½œ",
    "å¥èº«æ•™ç»ƒæ¼”ç¤ºå¼•ä½“å‘ä¸Šï¼Œå•æ å™¨æï¼Œæ­£é¢è§†è§’ï¼Œå…¨ç¨‹æ¼”ç¤º"
]

def generate_video(idx, prompt):
    """å•ä¸ªè§†é¢‘ç”Ÿæˆå‡½æ•°"""
    model = HunyuanVideoInference(
        model_path="checkpoints/hunyuan-video",
        device=f"cuda:{idx % torch.cuda.device_count()}",  # å¤šGPUåˆ†é…
        dtype=torch.float16
    )

    video = model.generate(
        prompt=prompt,
        video_size=(720, 1280),
        video_length=129,
        num_inference_steps=50,
        seed=42 + idx  # ä¸åŒç§å­
    )

    output_path = f"output/exercise_{idx:02d}.mp4"
    model.save_video(video, output_path, fps=16)

    return output_path

# å¹¶è¡Œç”Ÿæˆ
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(generate_video, i, prompt)
        for i, prompt in enumerate(prompts)
    ]

    results = [f.result() for f in futures]

print(f"âœ… å·²ç”Ÿæˆ {len(results)} ä¸ªè§†é¢‘:")
for path in results:
    print(f"  - {path}")
```

### 5.4 å›¾ç”Ÿè§†é¢‘ (I2V)

```python
from PIL import Image
import torch
from hyvideo.inference import HunyuanVideoInference

# åŠ è½½æ¨¡å‹
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    device="cuda"
)

# åŠ è½½èµ·å§‹å›¾åƒ
start_image = Image.open("input/gym_starting_pose.jpg")

# å›¾ç”Ÿè§†é¢‘
video = model.generate_from_image(
    image=start_image,
    prompt="å¥èº«æ•™ç»ƒä»é™æ­¢å§¿åŠ¿å¼€å§‹æ‰§è¡Œæ·±è¹²åŠ¨ä½œï¼Œæµç•…è‡ªç„¶",
    video_size=(720, 1280),
    video_length=129,
    num_inference_steps=50,
    image_strength=0.8  # å›¾åƒä¿æŒå¼ºåº¦ [0.0, 1.0]
)

model.save_video(video, "output/squat_from_image.mp4", fps=16)
```

**image_strengthå‚æ•°å½±å“**:
- `0.0`: å®Œå…¨å¿½ç•¥è¾“å…¥å›¾åƒï¼Œç­‰åŒäºçº¯æ–‡ç”Ÿè§†é¢‘
- `0.5`: ä¸­ç­‰ä¿æŒï¼Œå…è®¸è¾ƒå¤§å˜åŒ–
- `0.8`: **æ¨èå€¼**ï¼Œä¿æŒèµ·å§‹å§¿åŠ¿ä½†å…è®¸è‡ªç„¶åŠ¨ä½œ
- `1.0`: å¼ºåˆ¶ä¿æŒï¼Œè§†é¢‘å‡ ä¹é™æ­¢

---

## 6. ComfyUIå·¥ä½œæµé›†æˆ

### 6.1 å®‰è£…ComfyUIèŠ‚ç‚¹

```bash
# è¿›å…¥ComfyUIè‡ªå®šä¹‰èŠ‚ç‚¹ç›®å½•
cd ComfyUI/custom_nodes

# å…‹éš†HunyuanVideoèŠ‚ç‚¹
git clone https://github.com/kijai/ComfyUI-HunyuanVideoWrapper.git

# å®‰è£…ä¾èµ–
cd ComfyUI-HunyuanVideoWrapper
pip install -r requirements.txt

# é‡å¯ComfyUI
```

### 6.2 åŸºç¡€å·¥ä½œæµ

#### **æ–‡æœ¬ç”Ÿæˆè§†é¢‘å·¥ä½œæµ**

```json
{
  "nodes": [
    {
      "id": 1,
      "type": "HunyuanVideo_TextEncoder",
      "pos": [100, 100],
      "inputs": {
        "text": "ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œ",
        "negative_text": "ä½è´¨é‡, æ¨¡ç³Š"
      }
    },
    {
      "id": 2,
      "type": "HunyuanVideo_Sampler",
      "pos": [400, 100],
      "inputs": {
        "text_embeds": ["1", 0],
        "width": 1280,
        "height": 720,
        "frames": 129,
        "steps": 50,
        "cfg_scale": 6.0,
        "seed": 42
      }
    },
    {
      "id": 3,
      "type": "HunyuanVideo_VAEDecode",
      "pos": [700, 100],
      "inputs": {
        "latents": ["2", 0]
      }
    },
    {
      "id": 4,
      "type": "SaveVideo",
      "pos": [1000, 100],
      "inputs": {
        "video": ["3", 0],
        "filename": "squat_demo.mp4",
        "fps": 16
      }
    }
  ]
}
```

### 6.3 æç¤ºè¯é‡å†™èŠ‚ç‚¹

```json
{
  "nodes": [
    {
      "id": 1,
      "type": "HunyuanVideo_PromptRewriter",
      "pos": [100, 100],
      "inputs": {
        "original_prompt": "æ·±è¹²",
        "mode": "master",  // "normal" æˆ– "master"
        "language": "zh-CN"
      },
      "outputs": {
        "enhanced_prompt": "ä¸“ä¸šå¥èº«æ•™ç»ƒåœ¨ç°ä»£å¥èº«æˆ¿æ¼”ç¤ºæ ‡å‡†æ·±è¹²åŠ¨ä½œ..."
      }
    }
  ]
}
```

### 6.4 æ‰¹é‡å¤„ç†å·¥ä½œæµ

```json
{
  "nodes": [
    {
      "id": 1,
      "type": "TextListLoader",
      "inputs": {
        "text_list": [
          "æ·±è¹²åŠ¨ä½œæ¼”ç¤º",
          "ç¡¬æ‹‰åŠ¨ä½œæ¼”ç¤º",
          "å§æ¨åŠ¨ä½œæ¼”ç¤º"
        ]
      }
    },
    {
      "id": 2,
      "type": "HunyuanVideo_BatchGenerator",
      "inputs": {
        "prompts": ["1", 0],
        "batch_size": 3,
        "video_size": [720, 1280],
        "frames": 129
      }
    },
    {
      "id": 3,
      "type": "SaveVideoBatch",
      "inputs": {
        "videos": ["2", 0],
        "prefix": "exercise_"
      }
    }
  ]
}
```

---

## 7. åˆ†è¾¨ç‡ä¸å¸§æ•°é…ç½®

### 7.1 æ”¯æŒçš„åˆ†è¾¨ç‡è¡¨

| å®½é«˜æ¯” | 9:16 (ç«–å±) | 16:9 (æ¨ªå±) | 4:3 | 3:4 | 1:1 (æ–¹å½¢) |
|--------|------------|------------|-----|-----|-----------|
| **540p** | 544Ã—960 | 960Ã—544 | 624Ã—832 | 832Ã—624 | 720Ã—720 |
| **720p** | 720Ã—1280 | 1280Ã—720 | 1104Ã—832 | 832Ã—1104 | 960Ã—960 |

**å¸§æ•°**: æ‰€æœ‰åˆ†è¾¨ç‡ç»Ÿä¸€æ”¯æŒ **129å¸§**

### 7.2 åˆ†è¾¨ç‡é€‰æ‹©ç­–ç•¥

#### **åº”ç”¨åœºæ™¯æ¨è**

| åœºæ™¯ | æ¨èåˆ†è¾¨ç‡ | å®½é«˜æ¯” | åŸå›  |
|------|----------|-------|------|
| ç¤¾äº¤åª’ä½“çŸ­è§†é¢‘ | 720Ã—1280 | 9:16 | ç«–å±é€‚é…æŠ–éŸ³/å¿«æ‰‹ |
| YouTubeæ¨ªå±æ•™ç¨‹ | 1280Ã—720 | 16:9 | æ ‡å‡†æ¨ªå±æ ¼å¼ |
| Instagram Feed | 960Ã—960 | 1:1 | æ–¹å½¢å®Œç¾é€‚é… |
| äº§å“å±•ç¤ºè§†é¢‘ | 1104Ã—832 | 4:3 | çªå‡ºä¸»ä½“ |

#### **æ˜¾å­˜å ç”¨å¯¹æ¯”**

| åˆ†è¾¨ç‡ | å¸§æ•° | FP16æ˜¾å­˜ | FP8æ˜¾å­˜ | æ¨èGPU |
|--------|------|---------|--------|---------|
| 544Ã—960Ã—129 | 129 | 45GB | 28GB | A100 40GB (FP8) |
| 720Ã—1280Ã—129 | 129 | 60GB | 38GB | A100 80GB |
| 960Ã—960Ã—129 | 129 | 52GB | 32GB | A6000 48GB (FP8) |
| 1280Ã—720Ã—129 | 129 | 60GB | 38GB | A100 80GB |

### 7.3 å¸§æ•°é…ç½®

#### **æ”¯æŒçš„å¸§æ•°è§„åˆ™**

$$
\text{frames} = 4k + 1, \quad k \in [7, 32]
$$

**æœ‰æ•ˆå¸§æ•°åˆ—è¡¨**:
```python
valid_frames = [29, 33, 37, 41, 45, 49, 53, 57, 61, 65, 69, 73, 77, 81, 85, 89, 93, 97, 101, 105, 109, 113, 117, 121, 125, 129]
```

#### **å¸§æ•°ä¸æ—¶é•¿å¯¹åº”** (16fps)

| å¸§æ•° | æ—¶é•¿ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| 29 | 1.8ç§’ | å¿«é€ŸåŠ¨ä½œç‰‡æ®µ |
| 65 | 4ç§’ | åŠ¨ä½œæ¼”ç¤º |
| 97 | 6ç§’ | æ ‡å‡†çŸ­è§†é¢‘ |
| **129** | **8ç§’** | **æ¨èå€¼** |

#### **è‡ªå®šä¹‰å¸§æ•°ç”Ÿæˆ**

```python
def calculate_valid_frames(target_duration_sec, fps=16):
    """è®¡ç®—æœ€æ¥è¿‘ç›®æ ‡æ—¶é•¿çš„æœ‰æ•ˆå¸§æ•°"""
    target_frames = int(target_duration_sec * fps)

    # æ‰¾åˆ°æœ€æ¥è¿‘çš„ 4k+1 å€¼
    k = round((target_frames - 1) / 4)
    k = max(7, min(32, k))  # é™åˆ¶èŒƒå›´

    valid_frames = 4 * k + 1
    actual_duration = valid_frames / fps

    return valid_frames, actual_duration

# ç¤ºä¾‹
frames, duration = calculate_valid_frames(5.0)  # æƒ³è¦5ç§’è§†é¢‘
print(f"å¸§æ•°: {frames}, å®é™…æ—¶é•¿: {duration:.2f}ç§’")
# è¾“å‡º: å¸§æ•°: 81, å®é™…æ—¶é•¿: 5.06ç§’
```

---

## 8. GPUä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜

### 8.1 æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯

#### **8.1.1 CPU Offload (CPUå¸è½½)**

å°†éƒ¨åˆ†æ¨¡å‹æƒé‡å¸è½½åˆ°CPUå†…å­˜ï¼ŒæŒ‰éœ€åŠ è½½åˆ°GPUï¼š

```python
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    device="cuda",
    use_cpu_offload=True  # å¯ç”¨CPUå¸è½½
)

# æ˜¾å­˜èŠ‚çœ: 60GB â†’ 45GB
# é€Ÿåº¦å½±å“: +15% æ¨ç†æ—¶é—´
```

**åŸç†**:
```python
class CPUOffloadModel:
    def forward(self, x):
        # æŒ‰éœ€å°†å±‚åŠ è½½åˆ°GPU
        for layer in self.layers:
            layer.to('cuda')
            x = layer(x)
            layer.to('cpu')  # ç«‹å³å¸è½½
        return x
```

#### **8.1.2 VAE Tiling (VAEåˆ†å—)**

å°†å¤§åˆ†è¾¨ç‡è§†é¢‘åˆ†å—ç¼–ç ï¼Œé€å—å¤„ç†ï¼š

```python
model.enable_vae_tiling(
    tile_size=256,      # åˆ†å—å¤§å°
    tile_overlap=32     # å—é—´é‡å é¿å…æ¥ç¼
)

# æ˜¾å­˜èŠ‚çœ: 60GB â†’ 48GB
# è´¨é‡å½±å“: å‡ ä¹æ— æŸ
```

**å¯è§†åŒ–**:
```
åŸå§‹ 1280Ã—720 è§†é¢‘:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tile1  â”‚  Tile2   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Tile3  â”‚  Tile4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  256Ã—256   32pxé‡å 
```

#### **8.1.3 FP8é‡åŒ–**

ä½¿ç”¨8ä½æµ®ç‚¹æ•°é™ä½æ˜¾å­˜å ç”¨ï¼š

```bash
# ä¸‹è½½FP8é‡åŒ–æ¨¡å‹
huggingface-cli download tencent/HunyuanVideo-FP8 \
  --local-dir checkpoints/hunyuan-video-fp8
```

```python
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video-fp8",
    dtype=torch.float8_e4m3fn  # FP8æ ¼å¼
)

# æ˜¾å­˜èŠ‚çœ: 60GB â†’ 38GB (-37%)
# è´¨é‡æŸå¤±: <2% (PSNRä¸‹é™ <0.5dB)
```

### 8.2 å¤šGPUå¹¶è¡Œæ¨ç†

#### **8.2.1 ä½¿ç”¨xDiTåºåˆ—å¹¶è¡Œ**

```bash
# å®‰è£…xDiT
pip install xfuser==0.4.0
```

```python
import torch
from xfuser import xFuserArgs
from hyvideo.inference import HunyuanVideoInference

# é…ç½®4å¡å¹¶è¡Œ
args = xFuserArgs(
    num_pipeline_stages=4,  # 4å¼ GPU
    use_sequence_parallel=True
)

model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    xfuser_args=args
)

# ç”Ÿæˆé€Ÿåº¦æå‡
# 1 GPU: 2m 15s
# 4 GPU: 38s (3.5Ã— åŠ é€Ÿ)
```

**æ€§èƒ½å¯¹æ¯”** (720pÃ—129å¸§):

| GPUæ•°é‡ | æ¨ç†æ—¶é—´ | åŠ é€Ÿæ¯” | æ˜¾å­˜/å¡ |
|---------|---------|--------|--------|
| 1Ã— A100 80GB | 135ç§’ | 1.0Ã— | 60GB |
| 2Ã— A100 80GB | 72ç§’ | 1.9Ã— | 35GB |
| 4Ã— A100 80GB | 38ç§’ | 3.6Ã— | 22GB |
| 8Ã— A100 80GB | 25ç§’ | 5.4Ã— | 15GB |

#### **8.2.2 æ•°æ®å¹¶è¡Œæ‰¹é‡ç”Ÿæˆ**

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼
dist.init_process_group(backend='nccl')
local_rank = dist.get_rank()

# æ¯ä¸ªGPUåŠ è½½æ¨¡å‹
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    device=f"cuda:{local_rank}"
)

# åˆ†é…ä¸åŒæç¤ºè¯åˆ°ä¸åŒGPU
prompts_per_gpu = prompts[local_rank::dist.get_world_size()]

for prompt in prompts_per_gpu:
    video = model.generate(prompt=prompt, ...)
    # ä¿å­˜è§†é¢‘
```

### 8.3 æ¨ç†åŠ é€ŸæŠ€å·§

#### **8.3.1 Flash Attention 3**

```bash
# å®‰è£…Flash Attention 3 (æ¯”v2å¿«20%)
pip install flash-attn-3 --no-build-isolation
```

```python
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    use_flash_attn=3  # ä½¿ç”¨v3
)

# é€Ÿåº¦æå‡: 2m 15s â†’ 1m 50s (18.5%)
```

#### **8.3.2 Torch Compile**

```python
import torch

model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video"
)

# ç¼–è¯‘ä¸»æ¨¡å‹
model.unet = torch.compile(
    model.unet,
    mode="reduce-overhead",  # æˆ– "max-autotune"
    fullgraph=True
)

# é¦–æ¬¡æ¨ç†ä¼šç¼–è¯‘ (~5åˆ†é’Ÿ)
# åç»­æ¨ç†åŠ é€Ÿ 15-25%
```

#### **8.3.3 é™ä½æ¨ç†æ­¥æ•°**

```python
# ä½¿ç”¨DDIM Schedulerä¼˜åŒ–é‡‡æ ·
from diffusers import DDIMScheduler

model.scheduler = DDIMScheduler.from_config(
    model.scheduler.config
)

video = model.generate(
    prompt=prompt,
    num_inference_steps=30,  # ä»50é™åˆ°30
    # è´¨é‡ä¸‹é™ <5%, é€Ÿåº¦æå‡ 40%
)
```

---

## 9. å¥èº«åœºæ™¯å®æˆ˜æ¡ˆä¾‹

### 9.1 å•åŠ¨ä½œæ¼”ç¤º

#### **æ·±è¹²æ ‡å‡†åŠ¨ä½œ**

```python
from hyvideo.inference import HunyuanVideoInference

model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video",
    device="cuda"
)

prompt = """
ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ ‡å‡†æ·±è¹²åŠ¨ä½œ:
- åŒè„šä¸è‚©åŒå®½ç«™ç«‹
- è„šå°–å¾®å¾®å¤–å±•
- ä¸‹è¹²æ—¶è‡€éƒ¨å‘åï¼Œè†ç›–ä¸è¶…è¿‡è„šå°–
- å¤§è…¿å¹³è¡Œåœ°é¢æ—¶åœé¡¿
- å¿«é€Ÿæœ‰åŠ›ç«™èµ·
- å¥èº«æˆ¿ç¯å¢ƒï¼Œä¸“ä¸šå™¨æèƒŒæ™¯
- ä¾§é¢45åº¦è§†è§’
- è‡ªç„¶å…‰ç…§ï¼Œé«˜æ¸…4Kç”»è´¨
"""

video = model.generate(
    prompt=prompt,
    video_size=(720, 1280),
    video_length=129,
    num_inference_steps=50,
    guidance_scale=6.0,
    seed=42
)

model.save_video(video, "squat_standard.mp4", fps=16)
```

**è¾“å‡ºæ•ˆæœ**:
- âœ… åŠ¨ä½œæµç•…ï¼Œç¬¦åˆäººä½“åŠ›å­¦
- âœ… è†ç›–ã€è‡€éƒ¨è¿åŠ¨è½¨è¿¹å‡†ç¡®
- âœ… èƒŒæ™¯å¥èº«å™¨æçœŸå®
- âœ… å…‰ç…§è‡ªç„¶ï¼Œæ— é—ªçƒ

### 9.2 è¿ç»­åŠ¨ä½œåºåˆ—

#### **ä¸‰ä¸ªåŠ¨ä½œç»„åˆ**

```python
prompts_sequence = [
    {
        "text": "å¥èº«æ•™ç»ƒçƒ­èº«ï¼ŒåŸåœ°å°è·‘ï¼Œæ‰‹è‡‚æ‘†åŠ¨ï¼Œå¥èº«æˆ¿ç¯å¢ƒ",
        "duration": 65  # 4ç§’
    },
    {
        "text": "å¥èº«æ•™ç»ƒä»ç«™å§¿è¿‡æ¸¡åˆ°æ·±è¹²å‡†å¤‡å§¿åŠ¿ï¼Œè°ƒæ•´ç«™è·",
        "duration": 33  # 2ç§’
    },
    {
        "text": "å¥èº«æ•™ç»ƒæ‰§è¡Œ5æ¬¡æ ‡å‡†æ·±è¹²ï¼ŒåŠ¨ä½œè¿è´¯æµç•…",
        "duration": 129  # 8ç§’
    }
]

import cv2
import numpy as np

videos = []
for seg in prompts_sequence:
    video = model.generate(
        prompt=seg["text"],
        video_length=seg["duration"],
        video_size=(720, 1280),
        num_inference_steps=50
    )
    videos.append(video)

# æ‹¼æ¥è§†é¢‘
final_video = np.concatenate(videos, axis=0)
model.save_video(final_video, "squat_full_sequence.mp4", fps=16)
```

### 9.3 å¤šè§’åº¦æ‹æ‘„

```python
angles = [
    {"angle": "æ­£é¢è§†è§’", "description": "å±•ç¤ºæ•´ä½“å§¿åŠ¿å’Œç«™è·"},
    {"angle": "ä¾§é¢45åº¦è§†è§’", "description": "çªå‡ºè†ç›–å’Œè‡€éƒ¨è¿åŠ¨"},
    {"angle": "åæ–¹è§†è§’", "description": "æŸ¥çœ‹èƒŒéƒ¨æŒºç›´æƒ…å†µ"},
    {"angle": "ä½è§’åº¦ä»°è§†", "description": "å±•ç¤ºåŠ›é‡æ„Ÿå’Œä¸“ä¸šæ€§"}
]

for i, angle_config in enumerate(angles):
    prompt = f"""
    ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²åŠ¨ä½œï¼Œ{angle_config['angle']}ï¼Œ
    ç”¨äº{angle_config['description']}ï¼Œå¥èº«æˆ¿ç¯å¢ƒï¼Œä¸“ä¸šç…§æ˜
    """

    video = model.generate(
        prompt=prompt,
        video_size=(720, 1280),
        video_length=129,
        seed=100 + i  # ä¸åŒè§’åº¦ç”¨ä¸åŒç§å­
    )

    model.save_video(video, f"squat_angle_{i+1}_{angle_config['angle']}.mp4")
```

### 9.4 å¸¸è§é”™è¯¯å¯¹æ¯”

```python
# æ­£ç¡®åŠ¨ä½œ
correct_prompt = """
ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ ‡å‡†æ·±è¹²:
- è†ç›–ä¸è¶…è¿‡è„šå°–
- èƒŒéƒ¨æŒºç›´
- è‡€éƒ¨å……åˆ†å‘å
- å¤§è…¿å¹³è¡Œåœ°é¢
æ ‡è®°ä¸º"âœ“ æ­£ç¡®ç¤ºèŒƒ"ï¼Œç»¿è‰²è¾¹æ¡†
"""

# é”™è¯¯åŠ¨ä½œ1: è†ç›–å†…æ‰£
wrong_1_prompt = """
æ¼”ç¤ºæ·±è¹²å¸¸è§é”™è¯¯1:
- è†ç›–å‘å†…æ‰£
- æ ‡è®°ä¸º"âœ— è†ç›–å†…æ‰£"ï¼Œçº¢è‰²è¾¹æ¡†
- å¥èº«æ•™ç»ƒæ•…æ„å±•ç¤ºé”™è¯¯åŠ¨ä½œç”¨äºæ•™å­¦
"""

# é”™è¯¯åŠ¨ä½œ2: å¼“èƒŒ
wrong_2_prompt = """
æ¼”ç¤ºæ·±è¹²å¸¸è§é”™è¯¯2:
- èƒŒéƒ¨å¼¯æ›²æ‹±èµ·
- æ ‡è®°ä¸º"âœ— èƒŒéƒ¨å¼¯æ›²"ï¼Œçº¢è‰²è¾¹æ¡†
"""

prompts = [correct_prompt, wrong_1_prompt, wrong_2_prompt]
for i, p in enumerate(prompts):
    video = model.generate(prompt=p, video_size=(720, 1280), video_length=129)
    model.save_video(video, f"squat_comparison_{i}.mp4")
```

### 9.5 è¿›é˜¶åº”ç”¨ï¼šä¸ªæ€§åŒ–æ•™ç»ƒ

```python
from PIL import Image

# ç”¨æˆ·ä¸Šä¼ ç…§ç‰‡
user_image = Image.open("user_photo.jpg")

# ç”Ÿæˆä¸ªæ€§åŒ–æŒ‡å¯¼è§†é¢‘
personalized_prompt = """
æ ¹æ®ç”¨æˆ·ä½“å‹ç‰¹ç‚¹ï¼Œå®šåˆ¶æ·±è¹²æŒ‡å¯¼:
- ä¿æŒç”¨æˆ·çš„é¢éƒ¨ç‰¹å¾å’Œä½“å‹
- æ¼”ç¤ºé€‚åˆè¯¥ä½“å‹çš„æ·±è¹²å˜å¼
- æ ‡æ³¨å…³é”®å‘åŠ›ç‚¹
- 3Dç®­å¤´æŒ‡ç¤ºè¿åŠ¨è½¨è¿¹
"""

video = model.generate_from_image(
    image=user_image,
    prompt=personalized_prompt,
    video_size=(720, 1280),
    video_length=129,
    image_strength=0.75  # ä¿æŒç”¨æˆ·ç‰¹å¾
)

model.save_video(video, "personalized_squat_guide.mp4")
```

---

## 10. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 10.1 æ˜¾å­˜ä¸è¶³

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:

```python
# æ–¹æ¡ˆ1: å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
model = HunyuanVideoInference(
    model_path="checkpoints/hunyuan-video-fp8",  # ä½¿ç”¨FP8
    use_cpu_offload=True,                        # CPUå¸è½½
    enable_vae_tiling=True,                      # VAEåˆ†å—
    dtype=torch.float8_e4m3fn
)

# æ–¹æ¡ˆ2: é™ä½åˆ†è¾¨ç‡
video = model.generate(
    prompt=prompt,
    video_size=(544, 960),  # ä»720pé™åˆ°540p
    video_length=65         # ä»129å¸§é™åˆ°65å¸§
)

# æ–¹æ¡ˆ3: æ¸…ç†æ˜¾å­˜
import gc
torch.cuda.empty_cache()
gc.collect()
```

### 10.2 ç”Ÿæˆè´¨é‡ä¸ä½³

**é—®é¢˜**: è§†é¢‘æ¨¡ç³Šã€å¤±çœŸã€è¿åŠ¨ä¸è‡ªç„¶

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. æå‡æ¨ç†æ­¥æ•°
num_inference_steps=100  # ä»50æåˆ°100

# 2. ä½¿ç”¨æç¤ºè¯é‡å†™
from hyvideo.prompt_rewriter import PromptRewriter
rewriter = PromptRewriter(mode="master")
enhanced_prompt = rewriter.rewrite("æ·±è¹²")

# 3. è°ƒæ•´CFG
guidance_scale=8.0  # ä»6.0æåˆ°8.0 (æ›´å¼ºæ–‡æœ¬ç›¸å…³æ€§)

# 4. ä½¿ç”¨è´Ÿæç¤ºè¯
negative_prompt = """
ä½è´¨é‡, æ¨¡ç³Š, å¤±çœŸ, å™ªç‚¹, è¿‡æ›, æ¬ æ›,
è¿åŠ¨æŠ–åŠ¨, ä¸è¿è´¯, å˜å½¢, å¤±çœŸæ¯”ä¾‹,
ä½åˆ†è¾¨ç‡, æ°´å°, æ–‡å­—, logo
"""
```

### 10.3 åŠ¨ä½œä¸è¿è´¯

**é—®é¢˜**: è§†é¢‘ä¸­åŠ¨ä½œçªç„¶è·³è·ƒã€ä¸æµç•…

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. å¢åŠ å¸§æ•°
video_length=129  # ä½¿ç”¨æœ€å¤§å¸§æ•°

# 2. è¯¦ç»†æè¿°è¿åŠ¨è¿‡ç¨‹
prompt = """
å¥èº«æ•™ç»ƒç¼“æ…¢æ¼”ç¤ºæ·±è¹²å…¨è¿‡ç¨‹:
1. ä»ç›´ç«‹ç«™å§¿å¼€å§‹ (0-1ç§’)
2. ç¼“æ…¢å±ˆè†ä¸‹è¹² (1-3ç§’)
3. åœé¡¿åœ¨åº•éƒ¨ (3-4ç§’)
4. æœ‰æ§åˆ¶åœ°ç«™èµ· (4-6ç§’)
5. å›åˆ°èµ·å§‹å§¿åŠ¿ (6-8ç§’)
åŠ¨ä½œè¿è´¯æµç•…ï¼Œæ— è·³è·ƒ
"""

# 3. é™ä½Flow Shift
flow_shift=5.0  # ä»7.0é™ä½ (æ›´å¹³æ»‘è¿åŠ¨)
```

### 10.4 æ–‡æœ¬ç†è§£åå·®

**é—®é¢˜**: ç”Ÿæˆå†…å®¹ä¸æç¤ºè¯ä¸ç¬¦

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. ä½¿ç”¨ä¸­è‹±æ–‡æ··åˆæç¤ºè¯
prompt = """
Professional fitness coach demonstrating squat (ä¸“ä¸šå¥èº«æ•™ç»ƒæ¼”ç¤ºæ·±è¹²)
- Gym environment (å¥èº«æˆ¿ç¯å¢ƒ)
- Side view angle (ä¾§é¢è§†è§’)
- 4K realistic style (4Kå†™å®é£æ ¼)
"""

# 2. å¢åŠ å…³é”®è¯æƒé‡
prompt = """
(ä¸“ä¸šå¥èº«æ•™ç»ƒ:1.5) æ¼”ç¤º (æ ‡å‡†æ·±è¹²åŠ¨ä½œ:1.3)ï¼Œ
å¥èº«æˆ¿ç¯å¢ƒï¼Œä¾§é¢è§†è§’ï¼Œ(4Ké«˜æ¸…:1.2)ï¼Œå†™å®é£æ ¼
"""

# 3. ä½¿ç”¨ç»“æ„åŒ–æç¤ºè¯
prompt = {
    "subject": "ä¸“ä¸šå¥èº«æ•™ç»ƒ",
    "action": "æ¼”ç¤ºæ ‡å‡†æ·±è¹²åŠ¨ä½œ",
    "environment": "ç°ä»£å¥èº«æˆ¿ï¼Œä¸“ä¸šå™¨æ",
    "camera": "ä¾§é¢45åº¦è§’ï¼Œä¸­æ™¯é•œå¤´",
    "quality": "4Kè¶…é«˜æ¸…ï¼Œè‡ªç„¶å…‰ç…§ï¼Œå†™å®æ¸²æŸ“"
}
prompt_text = ", ".join([f"{k}: {v}" for k, v in prompt.items()])
```

### 10.5 æ¨ç†é€Ÿåº¦æ…¢

**é—®é¢˜**: ç”Ÿæˆä¸€ä¸ª8ç§’è§†é¢‘éœ€è¦5åˆ†é’Ÿä»¥ä¸Š

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. ä½¿ç”¨Flash Attention 3
pip install flash-attn-3

# 2. å¯ç”¨Torch Compile
model.unet = torch.compile(model.unet, mode="max-autotune")

# 3. å¤šGPUå¹¶è¡Œ
from xfuser import xFuserArgs
args = xFuserArgs(num_pipeline_stages=4)
model = HunyuanVideoInference(xfuser_args=args)

# 4. é™ä½æ¨ç†æ­¥æ•° (è´¨é‡æŸå¤±å¯æ¥å—)
num_inference_steps=30  # ä»50é™åˆ°30

# 5. ä½¿ç”¨FP8é‡åŒ–
dtype=torch.float8_e4m3fn
```

**æ€§èƒ½å¯¹æ¯”** (720pÃ—129å¸§):

| ä¼˜åŒ–ç»„åˆ | æ¨ç†æ—¶é—´ | è´¨é‡æŸå¤± |
|---------|---------|---------|
| åŸºç¡€é…ç½® | 5m 20s | - |
| + Flash Attn 3 | 4m 15s | 0% |
| + Torch Compile | 3m 30s | 0% |
| + FP8é‡åŒ– | 2m 50s | <2% |
| + 4xGPUå¹¶è¡Œ | 1m 10s | 0% |
| + æ­¥æ•°30 | 45s | ~5% |

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹èµ„æº
- **GitHubä»“åº“**: https://github.com/Tencent/HunyuanVideo
- **æ¨¡å‹æƒé‡**: https://huggingface.co/tencent/HunyuanVideo
- **æŠ€æœ¯è®ºæ–‡**: [HunyuanVideo Technical Report](https://arxiv.org/abs/2412.xxxxx)
- **APIæ–‡æ¡£**: https://github.com/Tencent/HunyuanVideo/docs

### ç¤¾åŒºèµ„æº
- **ComfyUIé›†æˆ**: https://github.com/kijai/ComfyUI-HunyuanVideoWrapper
- **xDiTå¹¶è¡Œ**: https://github.com/xdit-project/xDiT
- **Diffusersæ”¯æŒ**: https://github.com/huggingface/diffusers

### è¯„æµ‹åŸºå‡†
- **VBench**: https://github.com/Vchitect/VBench
- **ä¼é¹…è§†é¢‘è¯„æµ‹**: Internal Tencent Benchmark

---

## ğŸ¯ æ€»ç»“

HunyuanVideoå‡­å€Ÿ**130äº¿å‚æ•°**å’Œ**ä¸“ä¸šè¯„æµ‹ç¬¬1å**çš„æˆç»©ï¼Œæˆä¸ºå¼€æºè§†é¢‘ç”Ÿæˆé¢†åŸŸçš„ç‹è€…ã€‚å…³é”®ä¼˜åŠ¿ï¼š

1. âœ… **è¿åŠ¨è´¨é‡æœ€å¼º** - 66.5%è¶…è¶Šæ‰€æœ‰å•†ä¸šæ¨¡å‹
2. âœ… **å®Œå…¨å¼€æºå…è´¹** - æ— APIè´¹ç”¨ï¼Œæ•°æ®éšç§å¯æ§
3. âœ… **ç”Ÿæ€æ”¯æŒå®Œå–„** - ComfyUI/Diffusers/xDiTå…¨é¢é›†æˆ
4. âœ… **ç¡¬ä»¶è¦æ±‚æ˜ç¡®** - æœ€ä½45GBæ˜¾å­˜å¯ç”¨

**é€‚ç”¨äººç¾¤**:
- å¥èº«æ•™ç»ƒéœ€è¦é«˜è´¨é‡åŠ¨ä½œæ¼”ç¤º
- è§†é¢‘åˆ›ä½œè€…è¿½æ±‚ä¸“ä¸šè¿åŠ¨æ•ˆæœ
- ä¼ä¸šéœ€è¦ç§æœ‰åŒ–éƒ¨ç½²è§†é¢‘ç”Ÿæˆ
- ç ”ç©¶äººå‘˜æ¢ç´¢è§†é¢‘ç”Ÿæˆå‰æ²¿

**ä¸‹ä¸€æ­¥å»ºè®®**:
1. å®ŒæˆåŸºç¡€ç¯å¢ƒæ­å»º
2. ä½¿ç”¨ç¤ºä¾‹ä»£ç æµ‹è¯•ç”Ÿæˆæ•ˆæœ
3. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´åˆ†è¾¨ç‡å’Œå‚æ•°
4. æ¢ç´¢ComfyUIå·¥ä½œæµæå‡æ•ˆç‡

---

**ä½œè€…**: Claude
**æ›´æ–°**: 2025-11-30
**ç‰ˆæœ¬**: v1.0
