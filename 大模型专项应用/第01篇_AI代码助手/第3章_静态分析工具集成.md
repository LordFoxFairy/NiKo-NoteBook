# 第3章 静态分析工具集成

> 从Ruff的100x性能提升到Bandit的安全防护:构建多维度代码质量体系

## 3.1 Ruff深度集成

### 3.1.1 为什么Ruff如此之快?

**架构对比**:

```
传统工具链 (Flake8 + Black + isort):
Python解释器启动
  ↓
加载flake8及所有插件 (pycodestyle/pyflakes/mccabe...)
  ↓
遍历AST检查 (Python速度)
  ↓
启动Black进行格式化 (又一次解析)
  ↓
启动isort排序导入 (又一次解析)
总耗时: 数十秒到数分钟

Ruff:
Rust编译的单一二进制
  ↓
并行解析所有文件 (多线程)
  ↓
同时进行Lint + Format (共享AST)
  ↓
缓存未变更文件
总耗时: 数百毫秒
```

**性能数据(CPython代码库测试)**:

| 工具 | 耗时 | 相对速度 |
|------|------|---------|
| Flake8 | 60.0s | 1x |
| Pylint | 180.0s | 0.33x |
| Black | 8.0s | 7.5x |
| isort | 6.0s | 10x |
| **Ruff (Lint+Format)** | **0.3s** | **200x** |

### 3.1.2 Ruff基础使用

**安装**:
```bash
pip install ruff==0.14.5
```

**最简单使用**:
```bash
# Lint检查
ruff check .

# 自动修复
ruff check --fix .

# 代码格式化
ruff format .

# 一步完成
ruff check --fix . && ruff format .
```

**配置文件(pyproject.toml)**:

```toml
[tool.ruff]
# 行宽限制
line-length = 88

# 目标Python版本
target-version = "py310"

# 排除目录
exclude = [
    ".git",
    ".venv",
    "__pycache__",
    "build",
    "dist",
]

[tool.ruff.lint]
# 启用规则集
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "N",    # pep8-naming
    "UP",   # pyupgrade
    "B",    # flake8-bugbear
    "C4",   # flake8-comprehensions
    "SIM",  # flake8-simplify
    "PTH",  # flake8-use-pathlib
]

# 忽略特定规则
ignore = [
    "E501",  # 行长度(由formatter处理)
    "B008",  # 函数调用在参数默认值中
]

# 每个文件最多忽略的错误数
per-file-ignores = {"__init__.py" = ["F401"]}  # 允许未使用的导入

[tool.ruff.lint.mccabe]
# 圈复杂度阈值
max-complexity = 10

[tool.ruff.format]
# 使用单引号
quote-style = "single"

# 缩进样式
indent-style = "space"
```

### 3.1.3 Python API集成

**基础封装**:

```python
import subprocess
import json
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class RuffIssue:
    """Ruff检测到的问题"""
    code: str           # 规则代码(如E501)
    message: str        # 错误描述
    file: str           # 文件路径
    line: int           # 行号
    column: int         # 列号
    end_line: int       # 结束行号
    end_column: int     # 结束列号
    fix: Optional[Dict] # 修复信息(如果可自动修复)
    url: str            # 规则文档URL

class RuffAnalyzer:
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path

    def check_file(self, file_path: str, fix: bool = False) -> List[RuffIssue]:
        """检查单个文件"""
        cmd = ["ruff", "check", file_path, "--output-format=json"]

        if fix:
            cmd.append("--fix")

        if self.config_path:
            cmd.extend(["--config", self.config_path])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False  # 不抛出异常(有问题时返回非0)
        )

        if not result.stdout:
            return []

        try:
            data = json.loads(result.stdout)
            return [self._parse_issue(item) for item in data]
        except json.JSONDecodeError:
            # 可能没有JSON输出(无问题或错误)
            return []

    def check_directory(self, dir_path: str) -> List[RuffIssue]:
        """检查整个目录"""
        cmd = ["ruff", "check", dir_path, "--output-format=json"]

        if self.config_path:
            cmd.extend(["--config", self.config_path])

        result = subprocess.run(cmd, capture_output=True, text=True, check=False)

        if not result.stdout:
            return []

        data = json.loads(result.stdout)
        return [self._parse_issue(item) for item in data]

    def format_file(self, file_path: str, check_only: bool = False) -> bool:
        """格式化文件"""
        cmd = ["ruff", "format", file_path]

        if check_only:
            cmd.append("--check")

        result = subprocess.run(cmd, capture_output=True, check=False)
        return result.returncode == 0

    def get_rules(self) -> List[Dict[str, str]]:
        """获取所有可用规则"""
        result = subprocess.run(
            ["ruff", "rule", "--all", "--output-format=json"],
            capture_output=True,
            text=True
        )

        return json.loads(result.stdout)

    def _parse_issue(self, item: dict) -> RuffIssue:
        """解析JSON输出为Issue对象"""
        location = item["location"]
        end_location = item.get("end_location", location)

        return RuffIssue(
            code=item["code"],
            message=item["message"],
            file=item["filename"],
            line=location["row"],
            column=location["column"],
            end_line=end_location["row"],
            end_column=end_location["column"],
            fix=item.get("fix"),
            url=item.get("url", "")
        )
```

**使用示例**:

```python
# 创建分析器
analyzer = RuffAnalyzer()

# 检查单个文件
issues = analyzer.check_file("src/main.py")

for issue in issues:
    print(f"{issue.file}:{issue.line}:{issue.column}: {issue.code} {issue.message}")
    if issue.fix:
        print(f"  可自动修复!")

# 自动修复
analyzer.check_file("src/main.py", fix=True)

# 格式化
analyzer.format_file("src/main.py")
```

### 3.1.4 高级应用:增量分析

```python
from datetime import datetime
from typing import Set
import hashlib

class IncrementalRuffAnalyzer:
    """支持增量分析的Ruff封装"""

    def __init__(self):
        self.analyzer = RuffAnalyzer()
        self._file_cache = {}  # 文件路径 -> (哈希, 问题列表, 时间戳)
        self._dirty_files: Set[str] = set()

    def mark_dirty(self, file_path: str):
        """标记文件为需要重新分析"""
        self._dirty_files.add(file_path)

    def analyze_project(
        self,
        project_path: str,
        force_full: bool = False
    ) -> Dict[str, List[RuffIssue]]:
        """增量分析整个项目"""
        from pathlib import Path

        all_issues = {}
        py_files = list(Path(project_path).rglob("*.py"))

        for file_path in py_files:
            file_str = str(file_path)

            # 计算文件哈希
            with open(file_path, "rb") as f:
                content = f.read()
                file_hash = hashlib.md5(content).hexdigest()

            # 检查缓存
            if not force_full and file_str in self._file_cache:
                cached_hash, cached_issues, _ = self._file_cache[file_str]

                if cached_hash == file_hash and file_str not in self._dirty_files:
                    # 缓存命中,直接使用
                    all_issues[file_str] = cached_issues
                    continue

            # 执行分析
            issues = self.analyzer.check_file(file_str)
            all_issues[file_str] = issues

            # 更新缓存
            self._file_cache[file_str] = (file_hash, issues, datetime.now())

            # 清除dirty标记
            self._dirty_files.discard(file_str)

        return all_issues

    def get_statistics(self, issues_by_file: Dict[str, List[RuffIssue]]) -> Dict:
        """生成统计信息"""
        total_issues = sum(len(issues) for issues in issues_by_file.values())
        fixable = sum(
            1 for issues in issues_by_file.values()
            for issue in issues if issue.fix
        )

        # 按规则分类
        by_rule = {}
        for issues in issues_by_file.values():
            for issue in issues:
                by_rule[issue.code] = by_rule.get(issue.code, 0) + 1

        return {
            "total_files": len(issues_by_file),
            "total_issues": total_issues,
            "fixable_issues": fixable,
            "by_rule": sorted(by_rule.items(), key=lambda x: x[1], reverse=True),
            "avg_issues_per_file": total_issues / len(issues_by_file) if issues_by_file else 0
        }
```

**使用示例**:

```python
analyzer = IncrementalRuffAnalyzer()

# 首次全量分析
print("全量分析...")
issues = analyzer.analyze_project("./src")
stats = analyzer.get_statistics(issues)

print(f"总文件数: {stats['total_files']}")
print(f"总问题数: {stats['total_issues']}")
print(f"可自动修复: {stats['fixable_issues']}")
print("\n前5大问题:")
for code, count in stats['by_rule'][:5]:
    print(f"  {code}: {count}次")

# 模拟文件修改
analyzer.mark_dirty("./src/main.py")

# 增量分析(仅分析修改的文件)
print("\n增量分析...")
issues = analyzer.analyze_project("./src")
# 大部分文件从缓存读取,速度极快!
```

## 3.2 mypy类型检查集成

### 3.2.1 mypy核心概念

**类型检查的价值**:

```python
# 无类型注解 - mypy无法检查
def calculate_discount(price, rate):
    return price * rate

result = calculate_discount("100", 0.1)  # Bug! 字符串*浮点数
# 运行时才会出错

# 有类型注解 - mypy能提前发现
def calculate_discount(price: float, rate: float) -> float:
    return price * rate

result = calculate_discount("100", 0.1)
# mypy报错: Argument 1 has incompatible type "str"; expected "float"
```

**mypy检查级别**:

| 模式 | 检查强度 | 适用场景 |
|------|---------|---------|
| `--no-strict` | 宽松 | 逐步引入类型注解 |
| (默认) | 中等 | 日常开发 |
| `--strict` | 严格 | 新项目/高质量要求 |

### 3.2.2 mypy配置

**pyproject.toml配置**:

```toml
[tool.mypy]
# 基础选项
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true

# 严格模式选项
disallow_untyped_defs = true        # 禁止无类型注解的函数
disallow_any_generics = true        # 禁止Any泛型
no_implicit_optional = true         # 禁止隐式Optional
warn_redundant_casts = true         # 警告冗余类型转换
warn_unused_ignores = true          # 警告未使用的# type: ignore
warn_unreachable = true             # 警告不可达代码

# 第三方库存根文件
ignore_missing_imports = false

# 每个模块的特定配置
[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false       # 测试代码可以放宽要求
```

### 3.2.3 Python API集成

```python
import subprocess
import re
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class MypyIssue:
    """mypy检测到的类型问题"""
    file: str
    line: int
    column: int
    severity: str  # error/warning/note
    message: str
    code: str      # 错误代码(如arg-type)

class MypyAnalyzer:
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path

    def check_file(self, file_path: str) -> List[MypyIssue]:
        """检查单个文件"""
        cmd = ["mypy", file_path]

        if self.config_path:
            cmd.extend(["--config-file", self.config_path])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False
        )

        return self._parse_output(result.stdout)

    def check_directory(
        self,
        dir_path: str,
        follow_imports: str = "normal"
    ) -> List[MypyIssue]:
        """检查整个目录

        Args:
            follow_imports: "normal"(默认) | "silent" | "skip"
                - normal: 检查所有导入
                - silent: 检查但不报告导入的模块错误
                - skip: 完全跳过导入的模块
        """
        cmd = ["mypy", dir_path, f"--follow-imports={follow_imports}"]

        if self.config_path:
            cmd.extend(["--config-file", self.config_path])

        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        return self._parse_output(result.stdout)

    def generate_stubs(self, package_name: str):
        """为第三方库生成类型存根"""
        subprocess.run(
            ["stubgen", "-p", package_name, "-o", "stubs"],
            check=True
        )

    def _parse_output(self, output: str) -> List[MypyIssue]:
        """解析mypy输出"""
        issues = []

        # mypy输出格式: file.py:line:column: severity: message [code]
        pattern = r'(.+?):(\d+):(\d+): (\w+): (.+?)(\[[\w-]+\])?$'

        for line in output.split('\n'):
            match = re.match(pattern, line)
            if match:
                file, line_no, col, severity, message, code = match.groups()
                issues.append(MypyIssue(
                    file=file,
                    line=int(line_no),
                    column=int(col),
                    severity=severity,
                    message=message,
                    code=code.strip('[]') if code else ''
                ))

        return issues
```

**使用示例**:

```python
analyzer = MypyAnalyzer()

# 检查文件
issues = analyzer.check_file("src/main.py")

for issue in issues:
    print(f"{issue.file}:{issue.line}:{issue.column}: {issue.severity}: {issue.message}")
    if issue.code:
        print(f"  错误码: {issue.code}")

# 统计错误类型
error_types = {}
for issue in issues:
    if issue.code:
        error_types[issue.code] = error_types.get(issue.code, 0) + 1

print("\n错误统计:")
for code, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
    print(f"  {code}: {count}次")
```

### 3.2.4 与Tree-sitter结合

```python
from tree_sitter import Language, Parser, Query
import tree_sitter_python as tspython

class TypeHintAnalyzer:
    """分析代码的类型注解覆盖率"""

    def __init__(self):
        self.language = Language(tspython.language())
        self.parser = Parser(self.language)

    def analyze_coverage(self, code: bytes) -> Dict:
        """分析类型注解覆盖率"""
        tree = self.parser.parse(code)

        # 查找所有函数定义
        func_query = self.language.query("""
        (function_definition
          name: (identifier) @func_name
          parameters: (parameters) @params
          return_type: (type)? @return_type)
        """)

        captures = func_query.captures(tree.root_node)

        total_funcs = 0
        funcs_with_return_type = 0
        total_params = 0
        params_with_type = 0

        # 提取函数信息
        current_func = None
        for node, capture_name in captures:
            if capture_name == "func_name":
                total_funcs += 1
                current_func = code[node.start_byte:node.end_byte].decode()

            elif capture_name == "return_type":
                funcs_with_return_type += 1

            elif capture_name == "params":
                # 统计参数
                param_query = self.language.query("""
                (parameters
                  (identifier) @param_name
                  (type)? @param_type)
                """)

                for child, param_capture in param_query.captures(node):
                    if param_capture == "param_name":
                        param_name = code[child.start_byte:child.end_byte].decode()
                        if param_name != "self" and param_name != "cls":
                            total_params += 1

                    elif param_capture == "param_type":
                        params_with_type += 1

        return {
            "total_functions": total_funcs,
            "functions_with_return_type": funcs_with_return_type,
            "return_type_coverage": funcs_with_return_type / total_funcs if total_funcs > 0 else 0,
            "total_parameters": total_params,
            "parameters_with_type": params_with_type,
            "parameter_type_coverage": params_with_type / total_params if total_params > 0 else 0,
            "overall_coverage": (funcs_with_return_type + params_with_type) /
                               (total_funcs + total_params) if (total_funcs + total_params) > 0 else 0
        }

# 测试
code = b"""
def func1(a: int, b: str) -> bool:  # 完整类型注解
    return True

def func2(x, y):  # 无类型注解
    return x + y

def func3(name: str):  # 仅参数有类型注解
    print(name)
"""

analyzer = TypeHintAnalyzer()
coverage = analyzer.analyze_coverage(code)

print(f"函数返回类型覆盖率: {coverage['return_type_coverage']:.1%}")
print(f"参数类型覆盖率: {coverage['parameter_type_coverage']:.1%}")
print(f"总体覆盖率: {coverage['overall_coverage']:.1%}")
```

## 3.3 Bandit安全扫描

### 3.3.1 Bandit核心规则

**常见安全问题分类**:

| 类别 | 示例规则 | 风险 |
|------|---------|------|
| 注入攻击 | B608: SQL注入 | 高 |
| 密码学 | B301: pickle使用 | 高 |
| 输入验证 | B307: eval()使用 | 高 |
| 配置 | B104: 硬编码密码 | 中 |
| 随机数 | B311: random而非secrets | 中 |
| 文件操作 | B110: try-except-pass | 低 |

**安装**:
```bash
pip install bandit==1.9.1
```

### 3.3.2 Bandit配置与使用

**配置文件(.bandit)**:

```yaml
# 排除目录
exclude_dirs:
  - /test
  - /venv
  - /.venv

# 跳过特定测试
skips:
  - B101  # assert_used (测试中经常使用)
  - B601  # paramiko_calls

# 仅运行特定测试
tests:
  - B201  # flask_debug_true
  - B301  # pickle
  - B302  # marshal
  - B303  # md5
  - B304  # ciphers
  - B305  # cipher_modes
  - B306  # mktemp_q
  - B307  # eval
  - B308  # mark_safe
  - B309  # httpsconnection
  - B310  # urllib_urlopen
  - B311  # random
  - B312  # telnetlib
  - B313  # xml_bad_cElementTree
  - B314  # xml_bad_ElementTree
  - B315  # xml_bad_expatreader
  - B316  # xml_bad_expatbuilder
  - B317  # xml_bad_sax
  - B318  # xml_bad_minidom
  - B319  # xml_bad_pulldom
  - B320  # xml_bad_etree
  - B321  # ftplib
  - B322  # input
  - B323  # unverified_context
  - B324  # hashlib_new_insecure_functions
  - B325  # tempnam
  - B401  # import_telnetlib
  - B402  # import_ftplib
  - B403  # import_pickle
  - B404  # import_subprocess
  - B405  # import_xml_etree
  - B406  # import_xml_sax
  - B407  # import_xml_expat
  - B408  # import_xml_minidom
  - B409  # import_xml_pulldom
  - B410  # import_lxml
  - B411  # import_xmlrpclib
  - B412  # import_httpoxy
  - B413  # import_pycrypto
  - B501  # request_with_no_cert_validation
  - B502  # ssl_with_bad_version
  - B503  # ssl_with_bad_defaults
  - B504  # ssl_with_no_version
  - B505  # weak_cryptographic_key
  - B506  # yaml_load
  - B507  # ssh_no_host_key_verification
  - B601  # paramiko_calls
  - B602  # shell_injection_subprocess
  - B603  # subprocess_without_shell_equals_true
  - B604  # any_other_function_with_shell_equals_true
  - B605  # start_process_with_a_shell
  - B606  # start_process_with_no_shell
  - B607  # start_process_with_partial_path
  - B608  # sql_statements_without_placeholders
  - B609  # linux_commands_wildcard_injection
  - B610  # django_extra_used
  - B611  # django_rawsql_used
  - B701  # jinja2_autoescape_false
  - B702  # use_of_mako_templates
  - B703  # django_mark_safe

# 严重性级别
severity: 'LOW'  # LOW/MEDIUM/HIGH
confidence: 'LOW'  # LOW/MEDIUM/HIGH
```

### 3.3.3 Python API集成

```python
import subprocess
import json
from typing import List
from dataclasses import dataclass

@dataclass
class BanditIssue:
    """Bandit检测到的安全问题"""
    test_id: str        # 如B608
    test_name: str      # 如sql_injection
    issue_severity: str # LOW/MEDIUM/HIGH
    issue_confidence: str
    issue_text: str
    filename: str
    line_number: int
    line_range: List[int]
    code: str          # 有问题的代码片段

class BanditAnalyzer:
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path

    def check_file(self, file_path: str) -> List[BanditIssue]:
        """检查单个文件"""
        cmd = ["bandit", "-f", "json", file_path]

        if self.config_path:
            cmd.extend(["-c", self.config_path])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False
        )

        return self._parse_output(result.stdout)

    def check_directory(
        self,
        dir_path: str,
        recursive: bool = True,
        severity_level: str = "LOW"
    ) -> List[BanditIssue]:
        """检查目录"""
        cmd = ["bandit", "-f", "json", "-ll"]

        if recursive:
            cmd.append("-r")

        cmd.append(dir_path)

        if self.config_path:
            cmd.extend(["-c", self.config_path])

        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        return self._parse_output(result.stdout)

    def _parse_output(self, output: str) -> List[BanditIssue]:
        """解析JSON输出"""
        try:
            data = json.loads(output)
        except json.JSONDecodeError:
            return []

        issues = []
        for result in data.get("results", []):
            issues.append(BanditIssue(
                test_id=result["test_id"],
                test_name=result["test_name"],
                issue_severity=result["issue_severity"],
                issue_confidence=result["issue_confidence"],
                issue_text=result["issue_text"],
                filename=result["filename"],
                line_number=result["line_number"],
                line_range=result["line_range"],
                code=result["code"]
            ))

        return issues

    def get_statistics(self, issues: List[BanditIssue]) -> Dict:
        """生成统计信息"""
        severity_count = {"LOW": 0, "MEDIUM": 0, "HIGH": 0}
        test_count = {}

        for issue in issues:
            severity_count[issue.issue_severity] += 1
            test_count[issue.test_id] = test_count.get(issue.test_id, 0) + 1

        return {
            "total": len(issues),
            "by_severity": severity_count,
            "by_test": sorted(test_count.items(), key=lambda x: x[1], reverse=True)
        }
```

**使用示例**:

```python
analyzer = BanditAnalyzer()

# 检查项目
issues = analyzer.check_directory("./src", recursive=True)

# 按严重性分组
high_risk = [i for i in issues if i.issue_severity == "HIGH"]
medium_risk = [i for i in issues if i.issue_severity == "MEDIUM"]

print(f"发现 {len(high_risk)} 个高风险问题:")
for issue in high_risk:
    print(f"\n{issue.filename}:{issue.line_number}")
    print(f"  [{issue.test_id}] {issue.issue_text}")
    print(f"  代码: {issue.code.strip()}")

# 统计
stats = analyzer.get_statistics(issues)
print(f"\n总问题数: {stats['total']}")
print(f"高风险: {stats['by_severity']['HIGH']}")
print(f"中风险: {stats['by_severity']['MEDIUM']}")
print(f"低风险: {stats['by_severity']['LOW']}")
```

### 3.3.4 常见安全问题示例

```python
# 1. SQL注入风险 (B608)
def bad_query(user_id):
    query = f"SELECT * FROM users WHERE id = {user_id}"  # 危险!
    cursor.execute(query)

def good_query(user_id):
    query = "SELECT * FROM users WHERE id = %s"  # 安全
    cursor.execute(query, (user_id,))

# 2. 硬编码密码 (B105)
password = "admin123"  # 危险!

# 应该从环境变量读取
import os
password = os.environ.get("DB_PASSWORD")

# 3. 不安全的随机数 (B311)
import random
token = random.randint(1000, 9999)  # 可预测!

# 应该使用secrets模块
import secrets
token = secrets.randbelow(9999)

# 4. eval()使用 (B307)
user_input = request.GET['expr']
result = eval(user_input)  # 极度危险!

# 应该使用ast.literal_eval或专用解析器
import ast
result = ast.literal_eval(user_input)  # 只能解析字面量

# 5. pickle不安全 (B301)
import pickle
data = pickle.loads(untrusted_input)  # 可能执行任意代码!

# 应该使用JSON
import json
data = json.loads(untrusted_input)

# 6. SSL验证禁用 (B501)
import requests
requests.get("https://example.com", verify=False)  # 危险!

# 应该始终验证证书
requests.get("https://example.com", verify=True)
```

## 3.4 多工具结果聚合

### 3.4.1 统一问题模型

```python
from enum import Enum
from dataclasses import dataclass
from typing import Optional, List

class IssueSeverity(Enum):
    INFO = 1
    LOW = 2
    MEDIUM = 3
    HIGH = 4
    CRITICAL = 5

class IssueCategory(Enum):
    STYLE = "style"           # 代码风格
    TYPE = "type"             # 类型错误
    SECURITY = "security"     # 安全问题
    BUG = "bug"               # 潜在Bug
    COMPLEXITY = "complexity" # 复杂度
    PERFORMANCE = "performance"

@dataclass
class UnifiedIssue:
    """统一的代码问题模型"""
    tool: str  # ruff/mypy/bandit
    category: IssueCategory
    severity: IssueSeverity
    code: str
    message: str
    file: str
    line: int
    column: int
    end_line: Optional[int] = None
    end_column: Optional[int] = None
    fix_available: bool = False
    fix_description: Optional[str] = None
    url: Optional[str] = None
```

### 3.4.2 聚合器实现

```python
class CodeAnalysisAggregator:
    """聚合多个静态分析工具的结果"""

    def __init__(self):
        self.ruff = RuffAnalyzer()
        self.mypy = MypyAnalyzer()
        self.bandit = BanditAnalyzer()

    def analyze_file(self, file_path: str) -> List[UnifiedIssue]:
        """分析单个文件(所有工具)"""
        unified_issues = []

        # Ruff
        ruff_issues = self.ruff.check_file(file_path)
        unified_issues.extend(self._convert_ruff_issues(ruff_issues))

        # mypy
        mypy_issues = self.mypy.check_file(file_path)
        unified_issues.extend(self._convert_mypy_issues(mypy_issues))

        # Bandit
        bandit_issues = self.bandit.check_file(file_path)
        unified_issues.extend(self._convert_bandit_issues(bandit_issues))

        # 去重 + 排序
        return self._deduplicate_and_sort(unified_issues)

    def analyze_directory(self, dir_path: str) -> Dict[str, List[UnifiedIssue]]:
        """分析整个目录"""
        from pathlib import Path
        from concurrent.futures import ThreadPoolExecutor

        py_files = list(Path(dir_path).rglob("*.py"))

        results = {}
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self.analyze_file, str(f)): str(f)
                for f in py_files
            }

            for future in futures:
                file_path = futures[future]
                try:
                    results[file_path] = future.result()
                except Exception as e:
                    print(f"分析 {file_path} 失败: {e}")

        return results

    def _convert_ruff_issues(self, issues: List[RuffIssue]) -> List[UnifiedIssue]:
        """转换Ruff问题"""
        unified = []
        for issue in issues:
            # 判断类别
            category = IssueCategory.STYLE
            if issue.code.startswith("F"):
                category = IssueCategory.BUG
            elif issue.code.startswith("E"):
                category = IssueCategory.STYLE
            elif issue.code.startswith("C"):
                category = IssueCategory.COMPLEXITY

            # 判断严重性
            severity = IssueSeverity.LOW
            if issue.code in ["F841", "F821"]:  # 未使用/未定义变量
                severity = IssueSeverity.MEDIUM

            unified.append(UnifiedIssue(
                tool="ruff",
                category=category,
                severity=severity,
                code=issue.code,
                message=issue.message,
                file=issue.file,
                line=issue.line,
                column=issue.column,
                end_line=issue.end_line,
                end_column=issue.end_column,
                fix_available=issue.fix is not None,
                url=issue.url
            ))

        return unified

    def _convert_mypy_issues(self, issues: List[MypyIssue]) -> List[UnifiedIssue]:
        """转换mypy问题"""
        unified = []
        for issue in issues:
            severity = IssueSeverity.MEDIUM if issue.severity == "error" else IssueSeverity.LOW

            unified.append(UnifiedIssue(
                tool="mypy",
                category=IssueCategory.TYPE,
                severity=severity,
                code=issue.code or "type-error",
                message=issue.message,
                file=issue.file,
                line=issue.line,
                column=issue.column,
                fix_available=False
            ))

        return unified

    def _convert_bandit_issues(self, issues: List[BanditIssue]) -> List[UnifiedIssue]:
        """转换Bandit问题"""
        unified = []
        severity_map = {
            "LOW": IssueSeverity.LOW,
            "MEDIUM": IssueSeverity.MEDIUM,
            "HIGH": IssueSeverity.HIGH
        }

        for issue in issues:
            unified.append(UnifiedIssue(
                tool="bandit",
                category=IssueCategory.SECURITY,
                severity=severity_map[issue.issue_severity],
                code=issue.test_id,
                message=issue.issue_text,
                file=issue.filename,
                line=issue.line_number,
                column=1,
                fix_available=False
            ))

        return unified

    def _deduplicate_and_sort(self, issues: List[UnifiedIssue]) -> List[UnifiedIssue]:
        """去重并按严重性排序"""
        # 简单去重: 同一位置的同一问题只保留一个
        seen = set()
        unique_issues = []

        for issue in issues:
            key = (issue.file, issue.line, issue.column, issue.code)
            if key not in seen:
                seen.add(key)
                unique_issues.append(issue)

        # 排序: 严重性 > 文件 > 行号
        return sorted(
            unique_issues,
            key=lambda x: (-x.severity.value, x.file, x.line)
        )

    def generate_report(self, issues_by_file: Dict[str, List[UnifiedIssue]]) -> str:
        """生成汇总报告"""
        total = sum(len(issues) for issues in issues_by_file.values())

        by_severity = {s: 0 for s in IssueSeverity}
        by_category = {c: 0 for c in IssueCategory}
        by_tool = {"ruff": 0, "mypy": 0, "bandit": 0}

        for issues in issues_by_file.values():
            for issue in issues:
                by_severity[issue.severity] += 1
                by_category[issue.category] += 1
                by_tool[issue.tool] += 1

        report = f"""
代码质量分析报告
{'='*50}
总文件数: {len(issues_by_file)}
总问题数: {total}

按严重性分类:
  CRITICAL: {by_severity[IssueSeverity.CRITICAL]}
  HIGH:     {by_severity[IssueSeverity.HIGH]}
  MEDIUM:   {by_severity[IssueSeverity.MEDIUM]}
  LOW:      {by_severity[IssueSeverity.LOW]}
  INFO:     {by_severity[IssueSeverity.INFO]}

按类别分类:
  安全问题:   {by_category[IssueCategory.SECURITY]}
  类型错误:   {by_category[IssueCategory.TYPE]}
  潜在Bug:    {by_category[IssueCategory.BUG]}
  代码风格:   {by_category[IssueCategory.STYLE]}
  复杂度:     {by_category[IssueCategory.COMPLEXITY]}
  性能:       {by_category[IssueCategory.PERFORMANCE]}

按工具分类:
  Ruff:   {by_tool['ruff']}
  mypy:   {by_tool['mypy']}
  Bandit: {by_tool['bandit']}
"""
        return report
```

**使用示例**:

```python
# 创建聚合器
aggregator = CodeAnalysisAggregator()

# 分析整个项目
print("分析项目...")
results = aggregator.analyze_directory("./src")

# 生成报告
report = aggregator.generate_report(results)
print(report)

# 导出详细结果
for file_path, issues in results.items():
    if not issues:
        continue

    print(f"\n{file_path}:")
    for issue in issues[:5]:  # 只显示前5个
        print(f"  行{issue.line}: [{issue.tool}:{issue.code}] {issue.message}")
        if issue.fix_available:
            print("    ✓ 可自动修复")
```

## 3.5 本章小结

### 核心要点

1. **Ruff的100-200x性能提升来自**:
   - Rust实现 + 并行处理
   - 共享AST(Lint + Format)
   - 智能缓存

2. **mypy类型检查价值**:
   - 编译期发现Bug
   - 提高代码可维护性
   - 配合IDE实现智能提示

3. **Bandit安全扫描**:
   - 检测SQL注入/XSS等漏洞
   - 发现硬编码密码
   - 识别不安全的库使用

4. **多工具聚合策略**:
   - 统一问题模型
   - 去重与优先级排序
   - 综合报告生成

### 最佳实践

1. **持续集成**: 在CI/CD中自动运行所有工具
2. **增量分析**: 仅检查变更文件
3. **分级修复**: 优先修复高严重性问题
4. **自动修复**: 优先使用工具的自动修复功能

### 下一章预告

第4章将实现智能代码补全:
- FIM(Fill-in-the-Middle)模式
- AST驱动的候选生成
- Claude 3.5 Sonnet代码能力测试
- 实时响应优化

---

**下一章**: [第4章 智能代码补全](./第4章_智能代码补全.md) →
