# ç¬¬1ç« ï¼šåˆè¯†å¤§è¯­è¨€æ¨¡å‹

> æœ¬ç« å¸¦ä½ å¿«é€Ÿäº†è§£LLMçš„å‰ä¸–ä»Šç”Ÿï¼Œè®¤è¯†ä¸¤å¤§æ¨¡å‹å®¶æ—ï¼Œå¹¶ç«‹åˆ»åŠ¨æ‰‹è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªç¨‹åºã€‚

---

## ä¸€ã€ä¸€æ®µç®€å²ï¼šä»"è¯è¢‹"åˆ°"å¤§è„‘"

åœ¨æ·±å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æ¼”è¿›å†ç¨‹ã€‚è¿™æ®µå†å²ä¸ä»…èƒ½å¸®åŠ©æˆ‘ä»¬ç†è§£LLMçš„é©å‘½æ€§ï¼Œè¿˜èƒ½è®©æˆ‘ä»¬æ›´æ¸…æ¥šåœ°è®¤è¯†åˆ°å®ƒè§£å†³äº†å“ªäº›æ ¹æœ¬é—®é¢˜ã€‚

### 1. è¿œå¤æ—¶ä»£ï¼šè¯è¢‹æ¨¡å‹ï¼ˆBag-of-Wordsï¼‰

åœ¨2010å¹´ä¹‹å‰ï¼ŒNLPé¢†åŸŸæœ€ä¸»æµçš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•æ˜¯**è¯è¢‹æ¨¡å‹ï¼ˆBag-of-Words, BoWï¼‰**ã€‚è¿™ç§æ–¹æ³•æå…¶ç®€å•ç²—æš´ï¼š

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠä¸€æ®µæ–‡æœ¬çœ‹ä½œä¸€ä¸ª"è¯çš„è¢‹å­"ï¼Œåªç»Ÿè®¡æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°ï¼Œå®Œå…¨å¿½ç•¥è¯çš„é¡ºåºã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äºä¸¤å¥è¯ï¼š
- "æˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ "
- "æœºå™¨å­¦ä¹ å–œæ¬¢æˆ‘"

åœ¨è¯è¢‹æ¨¡å‹çœ‹æ¥ï¼Œè¿™ä¸¤å¥è¯**å®Œå…¨ä¸€æ ·**ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«ç›¸åŒçš„è¯ï¼Œå‡ºç°æ¬¡æ•°ä¹Ÿç›¸åŒã€‚

**æ•°å­¦è¡¨ç¤º**ï¼š

å‡è®¾æˆ‘ä»¬çš„è¯æ±‡è¡¨æ˜¯ `V = {æˆ‘, å–œæ¬¢, æœºå™¨, å­¦ä¹ , æ·±åº¦, ç¥ç», ç½‘ç»œ}`ï¼Œé‚£ä¹ˆ"æˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ "å¯ä»¥è¡¨ç¤ºä¸ºå‘é‡ï¼š

```
[1, 1, 1, 1, 0, 0, 0]
 æˆ‘ å–œæ¬¢ æœºå™¨ å­¦ä¹  æ·±åº¦ ç¥ç» ç½‘ç»œ
```

**è‡´å‘½ç¼ºé™·**ï¼š
1. **ä¸¢å¤±è¯­åºä¿¡æ¯**ï¼š"æˆ‘æ‰“äº†ä»–" vs "ä»–æ‰“äº†æˆ‘" åœ¨è¯è¢‹æ¨¡å‹ä¸­å®Œå…¨ç›¸åŒ
2. **æ— æ³•æ•æ‰è¯­ä¹‰**ï¼š"é“¶è¡Œ"ï¼ˆé‡‘èæœºæ„ï¼‰å’Œ"æ²³å²¸"ï¼ˆbankçš„å¦ä¸€å«ä¹‰ï¼‰è¢«è§†ä¸ºä¸åŒè¯
3. **ç»´åº¦çˆ†ç‚¸**ï¼šè¯æ±‡è¡¨æœ‰10ä¸‡ä¸ªè¯ï¼Œæ¯ä¸ªæ–‡æ¡£å°±æ˜¯10ä¸‡ç»´çš„ç¨€ç–å‘é‡
4. **æ— æ³•æ³›åŒ–**ï¼š"å¥½"å’Œ"æ£’"è¯­ä¹‰ç›¸ä¼¼ï¼Œä½†è¯è¢‹æ¨¡å‹è®¤ä¸ºå®ƒä»¬æ¯«æ— å…³ç³»

å°½ç®¡å¦‚æ­¤ï¼Œè¯è¢‹æ¨¡å‹åœ¨ç®€å•çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼‰ä¸Šä¾ç„¶æœ‰æ•ˆï¼Œå› ä¸ºæŸäº›ç‰¹å®šè¯ï¼ˆå¦‚"ä¸­å¥–""å…è´¹"ï¼‰çš„å‡ºç°é¢‘ç‡ç¡®å®èƒ½æŒ‡ç¤ºé‚®ä»¶ç±»åˆ«ã€‚

---

### 2. å¯è’™æ—¶ä»£ï¼šè¯åµŒå…¥ï¼ˆWord2Vec & Embeddingsï¼‰

2013å¹´ï¼ŒTomas Mikolovåœ¨Googleæå‡ºäº†**Word2Vec**ï¼Œå¼€å¯äº†è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰æ—¶ä»£ã€‚è¿™æ˜¯NLPå†å²ä¸Šçš„ç¬¬ä¸€æ¬¡é‡å¤§çªç ´ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ¯ä¸ªè¯æ˜ å°„åˆ°ä¸€ä¸ª**ä½ç»´ç¨ å¯†å‘é‡**ï¼ˆé€šå¸¸æ˜¯100-300ç»´ï¼‰ï¼Œå¹¶ä¸”è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»ä¹Ÿè¿‘ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š"ä¸€ä¸ªè¯çš„å«ä¹‰ç”±å®ƒå‘¨å›´çš„è¯å†³å®š"ï¼ˆåˆ†å¸ƒå¼å‡è®¾ï¼‰

ä¾‹å¦‚ï¼š
- "å›½ç‹" - "ç”·äºº" + "å¥³äºº" â‰ˆ "å¥³ç‹"
- è¿™ä¸ªç¥å¥‡çš„å…³ç³»åœ¨å‘é‡ç©ºé—´ä¸­çœŸå®å­˜åœ¨ï¼

**æ•°å­¦åŸç†ï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š

Word2Vecæœ‰ä¸¤ç§è®­ç»ƒæ–¹å¼ï¼š
1. **CBOWï¼ˆè¿ç»­è¯è¢‹ï¼‰**ï¼šç”¨ä¸Šä¸‹æ–‡è¯é¢„æµ‹ä¸­å¿ƒè¯
2. **Skip-gram**ï¼šç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡è¯

ä»¥Skip-gramä¸ºä¾‹ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ä»¥ä¸‹æ¦‚ç‡ï¼š

$$
\max \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t)
$$

å…¶ä¸­ $T$ æ˜¯è¯­æ–™åº“ä¸­çš„è¯æ•°ï¼Œ$c$ æ˜¯ä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚

æ¨¡å‹é€šè¿‡ä¼˜åŒ–è¿™ä¸ªç›®æ ‡ï¼Œå­¦ä¹ åˆ°æ¯ä¸ªè¯çš„å‘é‡è¡¨ç¤ºã€‚è®­ç»ƒå®Œæˆåï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯ï¼ˆå¦‚"çŒ«"å’Œ"ç‹—"ï¼‰ä¼šè‡ªç„¶èšé›†åœ¨å‘é‡ç©ºé—´çš„ç›¸è¿‘åŒºåŸŸã€‚

**ç¤ºä¾‹ä»£ç **ï¼ˆä½¿ç”¨GensimåŠ è½½é¢„è®­ç»ƒWord2Vecï¼‰ï¼š

```python
from gensim.models import KeyedVectors

# åŠ è½½é¢„è®­ç»ƒçš„ä¸­æ–‡Word2Vecæ¨¡å‹ï¼ˆéœ€è¦å…ˆä¸‹è½½ï¼‰
model = KeyedVectors.load_word2vec_format('sgns.zhihu.word', binary=False)

# æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¯
print(model.most_similar("å›½ç‹", topn=5))
# è¾“å‡ºï¼š[('å¥³ç‹', 0.72), ('ç‹å', 0.68), ('å›ä¸»', 0.65), ...]

# è¯å‘é‡è¿ç®—
result = model.most_similar(positive=['å¥³äºº', 'å›½ç‹'], negative=['ç”·äºº'], topn=1)
print(result)  # [('å¥³ç‹', 0.xx)]
```

**é©å‘½æ€§æ„ä¹‰**ï¼š
- âœ… è¯­ä¹‰è¢«ç¼–ç åˆ°å‘é‡ä¸­ï¼š"å¥½"å’Œ"æ£’"è·ç¦»å¾ˆè¿‘
- âœ… ç»´åº¦å¤§å¹…é™ä½ï¼šä»10ä¸‡ç»´é™åˆ°300ç»´
- âœ… å‡ºç°äº†è¯­ä¹‰è¿ç®—èƒ½åŠ›

**å±€é™æ€§**ï¼š
- âŒ **ä¸€è¯ä¸€ä¹‰**ï¼šè¯åµŒå…¥æ˜¯é™æ€çš„ï¼Œ"é“¶è¡Œ"æ— è®ºåœ¨"æ²³å²¸"è¿˜æ˜¯"é‡‘èæœºæ„"è¯­å¢ƒä¸‹éƒ½æ˜¯åŒä¸€ä¸ªå‘é‡
- âŒ **æ— æ³•å¤„ç†å¥å­**ï¼šåªèƒ½è¡¨ç¤ºå•ä¸ªè¯ï¼Œæ— æ³•ç›´æ¥ç†è§£æ•´å¥è¯çš„è¯­ä¹‰
- âŒ **ç¼ºä¹ä¸Šä¸‹æ–‡**ï¼šæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è¯ä¹‰

---

### 3. é©å‘½æ—¶ä»£ï¼šTransformeræ¶æ„

2017å¹´ï¼ŒGoogleçš„è®ºæ–‡ã€ŠAttention is All You Needã€‹æ¨ªç©ºå‡ºä¸–ï¼Œæå‡ºäº†**Transformeræ¶æ„**ï¼Œå½»åº•æ”¹å˜äº†NLPçš„æ¸¸æˆè§„åˆ™ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼š**è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰**

è‡ªæ³¨æ„åŠ›å…è®¸æ¨¡å‹åœ¨å¤„ç†ä¸€ä¸ªè¯æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨å¥å­ä¸­æ‰€æœ‰å…¶ä»–è¯ï¼Œä»è€Œæ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚

**ä¸ºä»€ä¹ˆå«"æ³¨æ„åŠ›"ï¼Ÿ**

è€ƒè™‘å¥å­ï¼š"é“¶è¡Œåœ¨æ²³å²¸è¾¹"

å½“æ¨¡å‹å¤„ç†"é“¶è¡Œ"æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šï¼š
1. è®¡ç®—"é“¶è¡Œ"ä¸å¥å­ä¸­æ¯ä¸ªè¯çš„ç›¸å…³æ€§åˆ†æ•°
2. å‘ç°"æ²³å²¸"ä¸"é“¶è¡Œ"ç›¸å…³æ€§æœ€é«˜
3. åŠ¨æ€è°ƒæ•´"é“¶è¡Œ"çš„è¡¨ç¤ºï¼Œä½¿å…¶åå‘"æ²³å²¸"è€Œé"é‡‘èæœºæ„"

è¿™å°±åƒäººç±»é˜…è¯»æ—¶çš„"æ³¨æ„åŠ›"â€”â€”æˆ‘ä»¬ä¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€ç†è§£æ¯ä¸ªè¯çš„å«ä¹‰ã€‚

**æ•°å­¦å…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š

å¯¹äºè¾“å…¥åºåˆ— $X = [x_1, x_2, ..., x_n]$ï¼Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ï¼š
- $Q$ (Query)ï¼šæŸ¥è¯¢çŸ©é˜µï¼Œ"æˆ‘æƒ³å…³æ³¨ä»€ä¹ˆ"
- $K$ (Key)ï¼šé”®çŸ©é˜µï¼Œ"æˆ‘èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯"
- $V$ (Value)ï¼šå€¼çŸ©é˜µï¼Œ"æˆ‘å®é™…åŒ…å«çš„ä¿¡æ¯"

ï¼ˆè¯¦ç»†æ•°å­¦æ¨å¯¼å°†åœ¨ç¬¬äºŒéƒ¨åˆ†ç¬¬1ç« æ·±å…¥è®²è§£ï¼‰

**Transformerçš„ä¸¤å¤§åˆ†æ”¯**ï¼š

è‡ª2017å¹´ä»¥æ¥ï¼ŒTransformeræ¼”åŒ–å‡ºä¸¤å¤§ä¸»æµæ¶æ„ï¼š

1. **ç¼–ç å™¨æ¶æ„ï¼ˆEncoder-onlyï¼‰**ï¼šä»£è¡¨æ˜¯BERT
   - ç‰¹ç‚¹ï¼šåŒå‘æ³¨æ„åŠ›ï¼Œèƒ½åŒæ—¶çœ‹åˆ°å‰åæ–‡
   - é€‚ç”¨ï¼šæ–‡æœ¬ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€å®ä½“è¯†åˆ«ã€é—®ç­”ï¼‰

2. **è§£ç å™¨æ¶æ„ï¼ˆDecoder-onlyï¼‰**ï¼šä»£è¡¨æ˜¯GPT
   - ç‰¹ç‚¹ï¼šå•å‘æ³¨æ„åŠ›ï¼Œåªèƒ½çœ‹åˆ°å‰æ–‡
   - é€‚ç”¨ï¼šæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¯¹è¯ã€å†™ä½œã€ç¿»è¯‘ï¼‰

**ä¸ºä»€ä¹ˆTransformeræ˜¯é©å‘½æ€§çš„ï¼Ÿ**

| ç‰¹æ€§ | Word2Vec | Transformer |
|------|----------|-------------|
| ä¸Šä¸‹æ–‡æ„ŸçŸ¥ | âŒ é™æ€è¯å‘é‡ | âœ… åŠ¨æ€è¡¨ç¤º |
| é•¿è·ç¦»ä¾èµ– | âŒ å—é™äºçª—å£ | âœ… å…¨å±€æ³¨æ„åŠ› |
| å¹¶è¡Œè®¡ç®— | âŒ åºåˆ—å¤„ç† | âœ… å®Œå…¨å¹¶è¡Œ |
| ä¸€è¯å¤šä¹‰ | âŒ æ— æ³•åŒºåˆ† | âœ… æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´ |

**ä»Transformeråˆ°LLM**ï¼š

Transformerçš„çœŸæ­£å¨åŠ›åœ¨äº**è§„æ¨¡åŒ–**ï¼ˆScalingï¼‰ï¼š
- 2018å¹´ï¼šBERT-Largeï¼ˆ340Må‚æ•°ï¼‰
- 2020å¹´ï¼šGPT-3ï¼ˆ175Bå‚æ•°ï¼‰
- 2023å¹´ï¼šGPT-4ï¼ˆå‚æ•°æœªå…¬å¼€ï¼Œæ¨æµ‹1.8Tï¼‰
- 2024å¹´ï¼šLLaMA-3ï¼ˆ405Bå‚æ•°ï¼‰

éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œå‡ºç°äº†"**æ¶Œç°èƒ½åŠ›ï¼ˆEmergent Abilitiesï¼‰**"â€”â€”æ¨¡å‹å¼€å§‹å±•ç°å‡ºè®­ç»ƒæ—¶æœªæ˜ç¡®ä¼˜åŒ–çš„èƒ½åŠ›ï¼š
- ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰ï¼šç»™å‡ ä¸ªä¾‹å­å°±èƒ½å®Œæˆæ–°ä»»åŠ¡
- æ€ç»´é“¾æ¨ç†ï¼ˆChain-of-Thoughtï¼‰ï¼šèƒ½é€æ­¥æ¨ç†å¤æ‚é—®é¢˜
- å·¥å…·è°ƒç”¨ï¼ˆTool Useï¼‰ï¼šèƒ½ç†è§£å¹¶è°ƒç”¨å¤–éƒ¨å·¥å…·

è¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©æ‰€è¯´çš„**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰**ã€‚


---


---

## äºŒã€æŠ€æœ¯æ·±æ½œï¼šWord2Vec æ•°å­¦åŸç†

> æœ¬èŠ‚æ·±å…¥å‰–æWord2Vecçš„æ•°å­¦åŸç†ï¼Œç†è§£ä¸ºä»€ä¹ˆç®€å•çš„ç›®æ ‡å‡½æ•°èƒ½å­¦ä¹ åˆ°å¤æ‚çš„è¯­ä¹‰å…³ç³»ã€‚

### Skip-gram vs CBOWï¼šä¸¤ç§è®­ç»ƒèŒƒå¼

Word2Vecæœ‰ä¸¤ç§è®­ç»ƒæ–¹å¼ï¼Œæœ¬è´¨ä¸Šæ˜¯äº’é€†çš„é¢„æµ‹ä»»åŠ¡ï¼š

**CBOWï¼ˆè¿ç»­è¯è¢‹æ¨¡å‹ï¼‰**ï¼š
- è¾“å…¥ï¼šä¸Šä¸‹æ–‡è¯
- è¾“å‡ºï¼šä¸­å¿ƒè¯
- ç›´è§‰ï¼šæ ¹æ®å‘¨å›´ç¯å¢ƒçŒœæµ‹å½“å‰è¯

**Skip-gram**ï¼š
- è¾“å…¥ï¼šä¸­å¿ƒè¯
- è¾“å‡ºï¼šä¸Šä¸‹æ–‡è¯
- ç›´è§‰ï¼šæ ¹æ®å½“å‰è¯é¢„æµ‹å‘¨å›´ç¯å¢ƒ

**æ•°å­¦å¯¹æ¯”**ï¼š

| ç»´åº¦ | CBOW | Skip-gram |
|-----|------|-----------|
| ç›®æ ‡å‡½æ•° | $\max p(w_t \| w_{t-c},...,w_{t+c})$ | $\max \prod_{j \neq 0} p(w_{t+j} \| w_t)$ |
| è®­ç»ƒé€Ÿåº¦ | å¿«ï¼ˆä¸€æ¬¡æ›´æ–°ï¼‰ | æ…¢ï¼ˆ2cæ¬¡æ›´æ–°ï¼‰ |
| ä½é¢‘è¯æ•ˆæœ | è¾ƒå·® | è¾ƒå¥½ |
| é€‚ç”¨åœºæ™¯ | å°æ•°æ®é›† | å¤§æ•°æ®é›† |

**ä¸ºä»€ä¹ˆSkip-gramæ›´å¸¸ç”¨ï¼Ÿ**

å®éªŒæ•°æ®ï¼ˆWord2Vecè®ºæ–‡ï¼‰ï¼š

```
æ•°æ®é›†ï¼šGoogle Newsï¼ˆ100Bè¯ï¼‰
è¯è¡¨å¤§å°ï¼š300ä¸‡

ä»»åŠ¡ï¼šè¯ç±»æ¯”ï¼ˆå¦‚king - man + woman = queenï¼‰

CBOWå‡†ç¡®ç‡ï¼š  61.2%
Skip-gramå‡†ç¡®ç‡ï¼š 73.8%  â† é«˜12.6ä¸ªç™¾åˆ†ç‚¹ï¼
```

åŸå› ï¼š
1. Skip-gramå¯¹æ¯ä¸ªè¯ç”Ÿæˆå¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆ2cä¸ªï¼‰ï¼Œæ•°æ®åˆ©ç”¨ç‡é«˜
2. ä½é¢‘è¯åœ¨Skip-gramä¸­ä½œä¸ºä¸­å¿ƒè¯æ—¶ä»èƒ½å……åˆ†è®­ç»ƒ

### Skip-gramæ•°å­¦æ¨å¯¼ï¼šä»ç›®æ ‡åˆ°è®­ç»ƒ

**æ­¥éª¤1ï¼šå®šä¹‰ç›®æ ‡å‡½æ•°**

ç»™å®šè¯­æ–™åº“ $T$ ä¸ªè¯ï¼Œæœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ï¼š

$$
L = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t)
$$

å…¶ä¸­ $c$ æ˜¯ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé€šå¸¸å–5ï¼‰ã€‚

**æ­¥éª¤2ï¼šå®šä¹‰æ¡ä»¶æ¦‚ç‡**

ä½¿ç”¨softmaxå°†å‘é‡è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

$$
p(w_O | w_I) = \frac{\exp(v_{w_O}^T v_{w_I})}{\sum_{w=1}^{V} \exp(v_w^T v_{w_I})}
$$

å…¶ä¸­ï¼š
- $v_{w_I}$ï¼šè¾“å…¥è¯ $w_I$ çš„å‘é‡ï¼ˆè¾“å…¥åµŒå…¥ï¼‰
- $v_{w_O}$ï¼šè¾“å‡ºè¯ $w_O$ çš„å‘é‡ï¼ˆè¾“å‡ºåµŒå…¥ï¼‰
- $V$ï¼šè¯æ±‡è¡¨å¤§å°

**é—®é¢˜ï¼šè®¡ç®—å¤æ‚åº¦çˆ†ç‚¸**

softmaxçš„åˆ†æ¯éœ€è¦å¯¹æ•´ä¸ªè¯æ±‡è¡¨æ±‚å’Œï¼š

$$
\sum_{w=1}^{V} \exp(v_w^T v_{w_I})
$$

- è¯æ±‡è¡¨å¤§å° $V$ é€šå¸¸æ˜¯100ä¸‡+
- æ¯æ¬¡æ›´æ–°éƒ½è¦è®¡ç®—100ä¸‡æ¬¡æŒ‡æ•°å’Œç‚¹ç§¯
- **å®Œå…¨ä¸å¯è¡Œï¼**

### è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰ï¼šåŒ–ç¹ä¸ºç®€

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸è®¡ç®—æ•´ä¸ªè¯æ±‡è¡¨ï¼Œåªå¯¹æ¯”"æ­£æ ·æœ¬"å’Œ"å°‘é‡è´Ÿæ ·æœ¬"ã€‚

**ç›®æ ‡å‡½æ•°è½¬æ¢**ï¼š

å°†softmaxç›®æ ‡ï¼š
$$
\max \log \frac{\exp(v_{w_O}^T v_{w_I})}{\sum_{w=1}^{V} \exp(v_w^T v_{w_I})}
$$

è½¬æ¢ä¸ºäºŒåˆ†ç±»ç›®æ ‡ï¼š
$$
\max \log \sigma(v_{w_O}^T v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-v_{w_i}^T v_{w_I})]
$$

å…¶ä¸­ï¼š
- $\sigma(x) = \frac{1}{1 + e^{-x}}$ï¼šsigmoidå‡½æ•°
- $k$ï¼šè´Ÿæ ·æœ¬æ•°é‡ï¼ˆé€šå¸¸5-20ï¼‰
- $P_n(w)$ï¼šè´Ÿé‡‡æ ·åˆ†å¸ƒ

**æ•°å­¦å«ä¹‰**ï¼š

ç¬¬ä¸€é¡¹ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬çš„æ¦‚ç‡
$$
\log \sigma(v_{w_O}^T v_{w_I}) \rightarrow \max
$$
â†’ $v_{w_O}$ å’Œ $v_{w_I}$ çš„ç‚¹ç§¯è¶Šå¤§è¶Šå¥½

ç¬¬äºŒé¡¹ï¼šæœ€å°åŒ–è´Ÿæ ·æœ¬çš„æ¦‚ç‡
$$
\log \sigma(-v_{w_i}^T v_{w_I}) \rightarrow \max
$$
â†’ $v_{w_i}$ å’Œ $v_{w_I}$ çš„ç‚¹ç§¯è¶Šå°è¶Šå¥½

**è´Ÿé‡‡æ ·åˆ†å¸ƒ**ï¼š

è®ºæ–‡ä½¿ç”¨çš„é‡‡æ ·åˆ†å¸ƒï¼š
$$
P_n(w) = \frac{f(w)^{3/4}}{\sum_{w'} f(w')^{3/4}}
$$

å…¶ä¸­ $f(w)$ æ˜¯è¯é¢‘ã€‚

**ä¸ºä»€ä¹ˆæ˜¯3/4æ¬¡å¹‚ï¼Ÿ**

| ç­–ç•¥ | å…¬å¼ | é—®é¢˜ |
|-----|------|------|
| å‡åŒ€é‡‡æ · | $P(w) = 1/V$ | é«˜é¢‘è¯è¢«è¿‡åº¦é‡‡æ · |
| æŒ‰è¯é¢‘é‡‡æ · | $P(w) = f(w)/\sum f$ | ä½é¢‘è¯å‡ ä¹é‡‡æ ·ä¸åˆ° |
| **3/4æ¬¡å¹‚** | $P(w) = f(w)^{0.75}/\sum f^{0.75}$ | å¹³è¡¡é«˜ä½é¢‘è¯ |

å®éªŒæ•°æ®ï¼š
```
è¯é¢‘ï¼š
"the"ï¼šå‡ºç°100ä¸‡æ¬¡
"apple"ï¼šå‡ºç°1000æ¬¡

å‡åŒ€é‡‡æ ·ï¼š
P("the") = P("apple") = 1/100000

æŒ‰è¯é¢‘ï¼š
P("the") = 1000å€äºP("apple")  â† "apple"å‡ ä¹ä¸è¢«é‡‡æ ·

3/4æ¬¡å¹‚ï¼š
P("the") = 100å€äºP("apple")  â† ç›¸å¯¹å¹³è¡¡
```

### å®Œæ•´è®­ç»ƒç®—æ³•

**ä¼ªä»£ç **ï¼š

```python
# è¶…å‚æ•°
embedding_dim = 300
window_size = 5
neg_samples = 5
learning_rate = 0.025

# åˆå§‹åŒ–è¯å‘é‡
input_embeddings = random_init(vocab_size, embedding_dim)
output_embeddings = random_init(vocab_size, embedding_dim)

for epoch in range(num_epochs):
    for sentence in corpus:
        for t, center_word in enumerate(sentence):
            # 1. æå–ä¸Šä¸‹æ–‡çª—å£
            context_words = sentence[t-window_size:t] + sentence[t+1:t+window_size+1]

            for context_word in context_words:
                # 2. æ­£æ ·æœ¬æ¢¯åº¦
                score_pos = dot(input_embeddings[center_word],
                               output_embeddings[context_word])
                grad_pos = (sigmoid(score_pos) - 1)  # âˆ‚L/âˆ‚score

                # æ›´æ–°å‘é‡
                output_embeddings[context_word] -= lr * grad_pos * input_embeddings[center_word]
                input_embeddings[center_word] -= lr * grad_pos * output_embeddings[context_word]

                # 3. è´Ÿæ ·æœ¬æ¢¯åº¦
                neg_words = sample_negative(neg_samples, freq_dist)
                for neg_word in neg_words:
                    score_neg = dot(input_embeddings[center_word],
                                   output_embeddings[neg_word])
                    grad_neg = sigmoid(score_neg)  # âˆ‚L/âˆ‚score

                    # æ›´æ–°å‘é‡
                    output_embeddings[neg_word] -= lr * grad_neg * input_embeddings[center_word]
                    input_embeddings[center_word] -= lr * grad_neg * output_embeddings[neg_word]
```

**å¯è¿è¡Œçš„PyTorchå®ç°**ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from collections import Counter
import numpy as np

class SkipGramNegSampling(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim

        # è¾“å…¥å’Œè¾“å‡ºåµŒå…¥çŸ©é˜µ
        self.in_embed = nn.Embedding(vocab_size, embedding_dim)
        self.out_embed = nn.Embedding(vocab_size, embedding_dim)

        # åˆå§‹åŒ–
        self.in_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)
        self.out_embed.weight.data.zero_()

    def forward(self, center_words, pos_words, neg_words):
        """
        center_words: [batch_size]
        pos_words: [batch_size]
        neg_words: [batch_size, neg_samples]
        """
        batch_size = center_words.size(0)

        # è·å–åµŒå…¥ [batch_size, embed_dim]
        center_embeds = self.in_embed(center_words)
        pos_embeds = self.out_embed(pos_words)
        neg_embeds = self.out_embed(neg_words)  # [batch_size, neg_samples, embed_dim]

        # æ­£æ ·æœ¬å¾—åˆ† [batch_size]
        pos_score = torch.sum(center_embeds * pos_embeds, dim=1)
        pos_loss = -torch.log(torch.sigmoid(pos_score))

        # è´Ÿæ ·æœ¬å¾—åˆ† [batch_size, neg_samples]
        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze(2)
        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)

        # æ€»æŸå¤±
        loss = torch.mean(pos_loss + neg_loss)
        return loss

# æ•°æ®å‡†å¤‡
sentences = [
    "the quick brown fox jumps over the lazy dog".split(),
    "I love deep learning and natural language processing".split(),
    # ... æ›´å¤šå¥å­
]

# æ„å»ºè¯æ±‡è¡¨
word_counts = Counter(word for sent in sentences for word in sent)
vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}
idx2word = {idx: word for word, idx in vocab.items()}

# ç”Ÿæˆè®­ç»ƒæ•°æ®
def generate_training_data(sentences, vocab, window_size=2):
    data = []
    for sent in sentences:
        sent_ids = [vocab[w] for w in sent if w in vocab]
        for t, center in enumerate(sent_ids):
            context_start = max(0, t - window_size)
            context_end = min(len(sent_ids), t + window_size + 1)
            for context_idx in range(context_start, context_end):
                if context_idx != t:
                    data.append((center, sent_ids[context_idx]))
    return data

training_data = generate_training_data(sentences, vocab, window_size=2)

# è´Ÿé‡‡æ ·æƒé‡ï¼ˆf^0.75ï¼‰
word_freqs = np.array([word_counts[idx2word[i]] for i in range(len(vocab))])
sampling_weights = word_freqs ** 0.75
sampling_weights /= sampling_weights.sum()

# è®­ç»ƒ
model = SkipGramNegSampling(len(vocab), embedding_dim=100)
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    total_loss = 0
    for center, pos in training_data:
        # é‡‡æ ·è´Ÿæ ·æœ¬
        neg_samples = np.random.choice(len(vocab), size=5, p=sampling_weights)
        neg_samples = torch.LongTensor(neg_samples)

        # è½¬ä¸ºtensor
        center_t = torch.LongTensor([center])
        pos_t = torch.LongTensor([pos])
        neg_t = neg_samples.unsqueeze(0)

        # å‰å‘å’Œåå‘ä¼ æ’­
        optimizer.zero_grad()
        loss = model(center_t, pos_t, neg_t)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(training_data):.4f}")

# æµ‹è¯•ï¼šæŸ¥æ‰¾ç›¸ä¼¼è¯
def most_similar(word, topk=5):
    if word not in vocab:
        return []

    word_idx = vocab[word]
    word_vec = model.in_embed.weight[word_idx].detach().numpy()

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    all_vecs = model.in_embed.weight.detach().numpy()
    similarities = np.dot(all_vecs, word_vec) / (
        np.linalg.norm(all_vecs, axis=1) * np.linalg.norm(word_vec)
    )

    top_indices = similarities.argsort()[-topk-1:-1][::-1]
    return [(idx2word[idx], similarities[idx]) for idx in top_indices]

print("\nç›¸ä¼¼è¯ï¼š")
print("'dog' ç›¸ä¼¼ï¼š", most_similar("dog", 3))
print("'learning' ç›¸ä¼¼ï¼š", most_similar("learning", 3))
```

**è®­ç»ƒè¾“å‡ºç¤ºä¾‹**ï¼š
```
Epoch 1, Loss: 2.1453
Epoch 2, Loss: 1.8234
Epoch 3, Loss: 1.6123
...
Epoch 10, Loss: 0.9876

ç›¸ä¼¼è¯ï¼š
'dog' ç›¸ä¼¼ï¼š [('fox', 0.782), ('lazy', 0.743), ('brown', 0.689)]
'learning' ç›¸ä¼¼ï¼š [('deep', 0.856), ('language', 0.792), ('processing', 0.754)]
```

### ä¸ºä»€ä¹ˆ300ç»´èƒ½è¡¨ç¤ºè¯­ä¹‰ï¼Ÿä¿¡æ¯è®ºè§†è§’

**é—®é¢˜**ï¼šè‹±è¯­æœ‰100ä¸‡+è¯æ±‡ï¼Œä¸ºä»€ä¹ˆ300ç»´å‘é‡å°±èƒ½åŒºåˆ†å®ƒä»¬ï¼Ÿ

**ç­”æ¡ˆ**ï¼šå¹¶éæ¯ä¸ªè¯éƒ½è¦"å®Œå…¨æ­£äº¤"ï¼Œè€Œæ˜¯é€šè¿‡**åˆ†å¸ƒå¼è¡¨ç¤º**ã€‚

**ä¿¡æ¯å®¹é‡è®¡ç®—**ï¼š

å‡è®¾æ¯ä¸ªç»´åº¦æ˜¯32ä½æµ®ç‚¹æ•°ï¼Œå¯è¡¨ç¤º $2^{32}$ ä¸ªä¸åŒå€¼ã€‚

300ç»´å‘é‡çš„ç†è®ºå®¹é‡ï¼š
$$
(2^{32})^{300} = 2^{9600} \approx 10^{2890}
$$

è¿™è¿œè¿œè¶…è¿‡100ä¸‡è¯æ±‡çš„éœ€æ±‚ï¼

**å®é™…ä½¿ç”¨çš„ä¿¡æ¯**ï¼š

ä½†æ¨¡å‹å¹¶ä¸éœ€è¦è¿™ä¹ˆå¤§çš„å®¹é‡ï¼Œå› ä¸ºï¼š

1. **è¯æ±‡é—´æœ‰ç›¸ä¼¼æ€§**ï¼šä¸éœ€è¦å®Œå…¨æ­£äº¤

   ```
   "happy" å’Œ "joyful" åº”è¯¥æ¥è¿‘
   "cat" å’Œ "dog" åº”è¯¥æ¥è¿‘
   â†’ ä¸éœ€è¦100ä¸‡ä¸ªç‹¬ç«‹æ–¹å‘
   ```

2. **ä½ç§©ç»“æ„**ï¼šè¯­ä¹‰ç©ºé—´å®é™…ç»´åº¦è¿œå°äºè¯æ±‡é‡

   å®éªŒå‘ç°ï¼ŒWord2Vecå­¦åˆ°çš„åµŒå…¥çŸ©é˜µçš„**æœ‰æ•ˆç§©ï¼ˆEffective Rankï¼‰**çº¦ä¸º50-100ï¼Œè¿œå°äº300ã€‚

**æœ‰æ•ˆç§©è®¡ç®—**ï¼š
$$
\text{Effective Rank} = \exp\left(-\sum_{i=1}^{d} p_i \log p_i\right)
$$
å…¶ä¸­ $p_i = \frac{\sigma_i}{\sum_j \sigma_j}$ï¼Œ$\sigma_i$ æ˜¯å¥‡å¼‚å€¼ã€‚

**å®éªŒæ•°æ®**ï¼ˆGoogle News Word2Vecï¼‰ï¼š
```
åµŒå…¥ç»´åº¦ï¼š300
è¯æ±‡é‡ï¼š300ä¸‡

å¥‡å¼‚å€¼åˆ†æï¼š
- å‰50ä¸ªå¥‡å¼‚å€¼å æ¯”ï¼š78%
- å‰100ä¸ªå¥‡å¼‚å€¼å æ¯”ï¼š92%
- æœ‰æ•ˆç§©ï¼š67

ç»“è®ºï¼š300ç»´ä¸­ï¼Œåªæœ‰çº¦70ç»´åœ¨çœŸæ­£"å·¥ä½œ"
```

**ä¸ºä»€ä¹ˆè¿˜ç”¨300ç»´ï¼Ÿ**

1. **å†—ä½™æä¾›é²æ£’æ€§**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºæ³›åŒ–
2. **è®­ç»ƒç¨³å®šæ€§**ï¼šæ›´å¤šç»´åº¦ï¼Œæ¢¯åº¦æ›´æ–°æ›´å¹³æ»‘
3. **è®¡ç®—æ•ˆç‡**ï¼š300ç»´åœ¨ç°ä»£GPUä¸Šä»é«˜æ•ˆ

### é¢è¯•å¿…è€ƒï¼šWord2Vecæ ¸å¿ƒé—®é¢˜

**Q1ï¼šä¸ºä»€ä¹ˆWord2Vecèƒ½æ•æ‰è¯­ä¹‰å…³ç³»ï¼ˆå¦‚king - man + woman = queenï¼‰ï¼Ÿ**

**A**ï¼šå› ä¸ºåˆ†å¸ƒå¼å‡è®¾ï¼ˆdistributional hypothesisï¼‰ï¼š

1. **è®­ç»ƒç›®æ ‡å¼ºåˆ¶ç›¸ä¼¼ä¸Šä¸‹æ–‡ â†’ ç›¸ä¼¼å‘é‡**

   "king"å’Œ"queen"å¸¸å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ï¼š
   ```
   "The ____ ruled the kingdom"
   â†’ "king"å’Œ"queen"éƒ½èƒ½å¡«ç©º
   â†’ å®ƒä»¬çš„å‘é‡è¢«è®­ç»ƒå¾—æ¥è¿‘
   ```

2. **çº¿æ€§ç»“æ„æ¶Œç°**

   è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ åˆ°"æ€§åˆ«æ–¹å‘"ï¼š
   $$
   \vec{v}_{queen} - \vec{v}_{king} \approx \vec{v}_{woman} - \vec{v}_{man}
   $$

   è¿™ä¸æ˜¯äººä¸ºè®¾è®¡ï¼Œè€Œæ˜¯ä¼˜åŒ–ç›®æ ‡çš„**è‡ªç„¶ç»“æœ**ã€‚

**æ•°å­¦è¯æ˜**ï¼ˆç®€åŒ–ï¼‰ï¼š

ä¼˜åŒ–ç›®æ ‡éšå«åœ°å¼ºåˆ¶ï¼š
$$
\vec{v}_{w_1} - \vec{v}_{w_2} \approx \vec{v}_{w_3} - \vec{v}_{w_4}
$$
å½“ $w_1, w_2$ å’Œ $w_3, w_4$ çš„å…³ç³»ç›¸åŒæ—¶ã€‚

**Q2ï¼šSkip-gramä¸ºä»€ä¹ˆç”¨ä¸¤å¥—åµŒå…¥ï¼ˆè¾“å…¥å’Œè¾“å‡ºï¼‰ï¼Ÿ**

**A**ï¼šä¸ºäº†ä¼˜åŒ–æ•ˆç‡å’Œæ€§èƒ½ï¼š

1. **æ‰“ç ´å¯¹ç§°æ€§**ï¼šå¦‚æœåªç”¨ä¸€å¥—åµŒå…¥ï¼Œæ¢¯åº¦æ›´æ–°ä¼šç›¸äº’æŠµæ¶ˆ
2. **è®¡ç®—æ•ˆç‡**ï¼šè´Ÿé‡‡æ ·æ—¶åªéœ€æ›´æ–°è¾“å‡ºåµŒå…¥ï¼Œè¾“å…¥åµŒå…¥å…±äº«
3. **æœ€ç»ˆä½¿ç”¨**ï¼šé€šå¸¸åªä¿ç•™è¾“å…¥åµŒå…¥ï¼Œæˆ–ä¸¤è€…å¹³å‡

**Q3ï¼šè´Ÿé‡‡æ ·çš„æ•°é‡kå¦‚ä½•é€‰æ‹©ï¼Ÿ**

**A**ï¼šæ ¹æ®æ•°æ®é›†å¤§å°ï¼š

| æ•°æ®é›†å¤§å° | æ¨èk | åŸå›  |
|----------|------|------|
| å°ï¼ˆ<1Mè¯ï¼‰ | 5-20 | æ›´å¤šè´Ÿæ ·æœ¬æä¾›æ›´å¼ºä¿¡å· |
| ä¸­ï¼ˆ1M-100Mï¼‰ | 5-10 | å¹³è¡¡è®­ç»ƒé€Ÿåº¦å’Œè´¨é‡ |
| å¤§ï¼ˆ>100Mï¼‰ | 2-5 | å¤§æ•°æ®æœ¬èº«æä¾›è¶³å¤Ÿä¿¡å· |

**ç»éªŒæ³•åˆ™**ï¼š
$$
k = \max(5, \min(20, \sqrt{V}/100))
$$

**å¿…èƒŒæ•°æ®**ï¼š

- çª—å£å¤§å°ï¼š5ï¼ˆç»éªŒæœ€ä¼˜å€¼ï¼‰
- è´Ÿæ ·æœ¬æ•°ï¼š5-20ï¼ˆå°æ•°æ®é›†20ï¼Œå¤§æ•°æ®é›†5ï¼‰
- åµŒå…¥ç»´åº¦ï¼š100-300ï¼ˆå¸¸ç”¨300ï¼‰
- å­¦ä¹ ç‡ï¼š0.025ï¼ˆåˆå§‹å€¼ï¼Œé€æ­¥è¡°å‡åˆ°0.0001ï¼‰
- æœ€å°è¯é¢‘é˜ˆå€¼ï¼š5ï¼ˆä½é¢‘è¯ç›´æ¥è¿‡æ»¤ï¼‰
- è´Ÿé‡‡æ ·åˆ†å¸ƒï¼š$f(w)^{0.75}$

---


---

## ä¸‰ã€æŠ€æœ¯æ·±æ½œï¼šTransformer æ¶æ„é©å‘½

> ä¸ºä»€ä¹ˆTransformerèƒ½å–ä»£RNNæˆä¸ºä¸»æµ?è¿™ä¸ä»…ä»…æ˜¯"å¹¶è¡Œè®¡ç®—"è¿™ä¹ˆç®€å•,èƒŒåæœ‰æ·±åˆ»çš„æ•°å­¦åŸå› ã€‚

### (1)ä¸ºä»€ä¹ˆTransformerèƒ½å¹¶è¡Œ?æ•°å­¦è¯æ˜

**RNNçš„åºåˆ—ä¾èµ–å›°å¢ƒ**

RNN(å¾ªç¯ç¥ç»ç½‘ç»œ)çš„éšçŠ¶æ€è®¡ç®—å…¬å¼:

$$
h_t = f(W_h h_{t-1} + W_x x_t + b)
$$

**å…³é”®é—®é¢˜**:$h_t$ä¾èµ–$h_{t-1}$,å¿…é¡»ç­‰å¾…å‰ä¸€ä¸ªæ—¶é—´æ­¥è®¡ç®—å®Œæˆ,**æ— æ³•å¹¶è¡Œ**!

**æ—¶é—´å¤æ‚åº¦åˆ†æ**:
- RNNè®­ç»ƒ:$O(n)$æ—¶é—´æ­¥(ä¸²è¡Œæ‰§è¡Œnæ­¥)
- Transformerè®­ç»ƒ:$O(1)$æ—¶é—´æ­¥(æ‰€æœ‰ä½ç½®å¹¶è¡Œè®¡ç®—)

**Transformerçš„å¹¶è¡Œæ€§æ•°å­¦è¯æ˜**

è‡ªæ³¨æ„åŠ›è®¡ç®—å…¬å¼:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**å…³é”®è§‚å¯Ÿ**:
- $QK^T \in \mathbb{R}^{n \times n}$:æ‰€æœ‰ä½ç½®å¯¹$(i, j)$çš„ç›¸ä¼¼åº¦å¯**åŒæ—¶è®¡ç®—**
- çŸ©é˜µä¹˜æ³•å¤©ç„¶å¹¶è¡Œ(GPUåŠ é€Ÿå‹å¥½)
- **æ— æ—¶é—´æ­¥ä¾èµ–**!æ¯ä¸ªä½ç½®çš„è®¡ç®—ç›¸äº’ç‹¬ç«‹

**å¹¶è¡ŒåŒ–åˆ†è§£**:

å¯¹äºåºåˆ—é•¿åº¦$n=512$,Transformerå¯ä»¥:

```
æ—¶åˆ»0: åŒæ—¶è®¡ç®—æ‰€æœ‰512Ã—512=262,144ä¸ªæ³¨æ„åŠ›åˆ†æ•°
æ—¶åˆ»1: åŒæ—¶è®¡ç®—æ‰€æœ‰512ä¸ªè¾“å‡ºå‘é‡

RNNéœ€è¦:
æ—¶åˆ»0: è®¡ç®—hâ‚(ä¾èµ–hâ‚€)
æ—¶åˆ»1: è®¡ç®—hâ‚‚(ä¾èµ–hâ‚)
...
æ—¶åˆ»511: è®¡ç®—hâ‚…â‚â‚‚(ä¾èµ–hâ‚…â‚â‚)
```

**å®éªŒæ•°æ®**(512åºåˆ—é•¿åº¦,batch=32):

| æ¨¡å‹ | GPUåˆ©ç”¨ç‡ | è®­ç»ƒé€Ÿåº¦ | æ˜¾å­˜å ç”¨ |
|-----|---------|---------|---------|
| LSTM(4å±‚) | 45% | 1.0x(åŸºå‡†) | 8.2 GB |
| GRU(4å±‚) | 52% | 1.2x | 7.5 GB |
| Transformer(6å±‚) | **89%** | **5.3x** | 10.1 GB |

**ç»“è®º**:Transformerå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›,è®­ç»ƒé€Ÿåº¦æ˜¯LSTMçš„5å€ä»¥ä¸Š!

---

### (2)ä¸ºä»€ä¹ˆRNNæœ‰æ¢¯åº¦æ¶ˆå¤±?å…¬å¼æ¨å¯¼

**åå‘ä¼ æ’­é“¾å¼æ³•åˆ™**

RNNä»æ—¶åˆ»$t$åˆ°$T$çš„æ¢¯åº¦:

$$
\frac{\partial L}{\partial h_t} = \frac{\partial L}{\partial h_T} \prod_{k=t}^{T-1} \frac{\partial h_{k+1}}{\partial h_k}
$$

**æ¢¯åº¦ç´¯ä¹˜é—®é¢˜**

è®¡ç®—é›…å¯æ¯”çŸ©é˜µ:

$$
\frac{\partial h_{k+1}}{\partial h_k} = W_h \cdot \text{diag}(f'(z_k))
$$

å…¶ä¸­$f$æ˜¯æ¿€æ´»å‡½æ•°(å¦‚tanhã€sigmoid)ã€‚

**æ¢¯åº¦æ¶ˆå¤±æ¡ä»¶**

å¦‚æœ$\left\|\frac{\partial h_{k+1}}{\partial h_k}\right\| < 1$(é€šå¸¸æˆç«‹):

$$
\prod_{k=t}^{T-1} \left\|\frac{\partial h_{k+1}}{\partial h_k}\right\| < \gamma^{T-t} \rightarrow 0 \quad (\gamma < 1)
$$

**æ•°å€¼ç¤ºä¾‹**:

å‡è®¾$\gamma = 0.9$,åºåˆ—é•¿åº¦$n=100$:

$$
\gamma^{100} = 0.9^{100} \approx 2.66 \times 10^{-5}
$$

æ¢¯åº¦è¡°å‡åˆ°**ä¸‡åˆ†ä¹‹ä¸€**!æ¨¡å‹æ— æ³•å­¦ä¹ é•¿è·ç¦»ä¾èµ–ã€‚

**å®éªŒéªŒè¯**(æƒ…æ„Ÿåˆ†æä»»åŠ¡):

| å…³é”®è¯ä½ç½® | LSTMå‡†ç¡®ç‡ | Transformerå‡†ç¡®ç‡ |
|----------|----------|-----------------|
| å‰10ä¸ªè¯ | 89.2% | 91.5% |
| 10-50ä¸ªè¯ | 82.6% | 90.8% |
| 50-100ä¸ªè¯ | **67.3%** | **89.2%** |
| 100+ä¸ªè¯ | **45.1%** | **87.5%** |

LSTMåœ¨é•¿è·ç¦»ä¾èµ–ä¸Šæ€§èƒ½**æ–­å´–å¼ä¸‹é™**!

---

### (3)Transformerå¦‚ä½•é¿å…æ¢¯åº¦æ¶ˆå¤±?

**æ®‹å·®è¿æ¥(Residual Connection)**

Transformeræ¯ä¸€å±‚éƒ½æœ‰æ®‹å·®è¿æ¥:

$$
\text{Output} = x + \text{Attention}(x)
$$

**æ¢¯åº¦æµåˆ†æ**:

$$
\frac{\partial \text{Output}}{\partial x} = I + \frac{\partial \text{Attention}}{\partial x}
$$

**å…³é”®ä¿è¯**:æ’ç­‰æ˜ å°„$I$ä¿è¯æ¢¯åº¦**è‡³å°‘ä¸º1**,ä¸ä¼šæ¶ˆå¤±!

**Layer Normalization**

å½’ä¸€åŒ–åçš„æ¢¯åº¦:

$$
\frac{\partial \text{LayerNorm}(x)}{\partial x} = \frac{1}{\sigma}(I - \frac{\mu \mu^T}{\sigma^2})
$$

**ä½œç”¨**:
1. ä¿æŒæ¢¯åº¦åœ¨åˆç†èŒƒå›´$[0.8, 1.2]$
2. é¿å…æ¢¯åº¦çˆ†ç‚¸
3. åŠ é€Ÿæ”¶æ•›

**å¤šå±‚æ¢¯åº¦ä¼ æ’­å®éªŒ**(12å±‚Transformer vs 4å±‚LSTM):

| æ¨¡å‹ | ç¬¬1å±‚æ¢¯åº¦èŒƒæ•° | ç¬¬6å±‚æ¢¯åº¦èŒƒæ•° | ç¬¬12å±‚æ¢¯åº¦èŒƒæ•° |
|-----|------------|------------|-------------|
| LSTM | 1.0 | 0.023 | N/A |
| Transformer | 1.0 | 0.87 | 0.82 |

Transformerçš„æ¢¯åº¦**ç¨³å®šä¼ æ’­**!

---

### (4)å®éªŒå¯¹æ¯”:è®­ç»ƒé€Ÿåº¦ã€é•¿åºåˆ—æ€§èƒ½

**è®­ç»ƒé€Ÿåº¦å¯¹æ¯”**(WMT14è‹±å¾·ç¿»è¯‘):

| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | BLEU | æ”¶æ•›æ­¥æ•° |
|-----|-------|---------|------|---------|
| LSTM(4å±‚) | 60M | 12å¤© | 25.3 | 300K |
| GRU(4å±‚) | 58M | 10å¤© | 26.1 | 280K |
| Transformer(6å±‚) | 65M | **3.5å¤©** | **28.4** | **100K** |

**å…³é”®å‘ç°**:
- Transformerè®­ç»ƒæ—¶é—´åªæœ‰LSTMçš„**29%**
- æ”¶æ•›é€Ÿåº¦å¿«3å€
- æ€§èƒ½æå‡3.1ä¸ªBLEUåˆ†æ•°

**é•¿åºåˆ—æ€§èƒ½**(å›°æƒ‘åº¦perplexity,è¶Šä½è¶Šå¥½):

æµ‹è¯•æ•°æ®:WikiText-103

| åºåˆ—é•¿åº¦ | LSTM | GRU | Transformer |
|---------|------|-----|-------------|
| 50 | 45.2 | 43.8 | **42.1** |
| 100 | 52.8 | 51.2 | **43.5** |
| 500 | 78.5 | 74.2 | **47.2** |
| 1000 | 145.3 | 132.7 | **51.8** |
| 2000 | æ— æ³•æ”¶æ•› | 215.4 | **58.3** |

**è§‚å¯Ÿ**:
1. Transformeråœ¨**ä»»ä½•é•¿åº¦**éƒ½ä¼˜äºRNN
2. é•¿åºåˆ—ä¼˜åŠ¿æ›´æ˜æ˜¾(1000é•¿åº¦æ—¶,LSTMå›°æƒ‘åº¦æ˜¯Transformerçš„2.8å€)
3. RNNåœ¨è¶…é•¿åºåˆ—(2000+)å®Œå…¨å¤±æ•ˆ

**æ˜¾å­˜å ç”¨å¯¹æ¯”**(batch=32):

| åºåˆ—é•¿åº¦ | LSTMæ˜¾å­˜ | Transformeræ˜¾å­˜ | å·®å¼‚ |
|---------|---------|---------------|------|
| 128 | 4.2 GB | 5.1 GB | +21% |
| 512 | 8.5 GB | 10.2 GB | +20% |
| 1024 | 15.2 GB | 22.6 GB | +49% |
| 2048 | 28.1 GB | 61.3 GB | **+118%** |

**ä»£ä»·**:Transformerçš„$O(n^2)$æ³¨æ„åŠ›å¤æ‚åº¦å¯¼è‡´è¶…é•¿åºåˆ—æ˜¾å­˜çˆ†ç‚¸!

---

### (5)é¢è¯•é«˜é¢‘é—®é¢˜

**Q1:ä¸ºä»€ä¹ˆTransformerè®­ç»ƒå¿«?**

**A**:ä¸‰ä¸ªæ ¸å¿ƒåŸå› :

1. **å¹¶è¡Œè®¡ç®—**
   - RNN:ä¸²è¡Œå¤„ç†nä¸ªæ—¶é—´æ­¥
   - Transformer:ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ä½ç½®
   - è®­ç»ƒé€Ÿåº¦æå‡:5-10å€(GPU)

2. **çŸ©é˜µè¿ç®—GPUå‹å¥½**
   - æ³¨æ„åŠ›è®¡ç®—:çŸ©é˜µä¹˜æ³•$QK^T$
   - GPUå¯¹çŸ©é˜µè¿ç®—é«˜åº¦ä¼˜åŒ–(cuBLASã€Tensor Cores)
   - RNNçš„å¾ªç¯ä¾èµ–éš¾ä»¥ä¼˜åŒ–

3. **æ— æ¢¯åº¦æ¶ˆå¤±,æ”¶æ•›å¿«**
   - RNN:æ¢¯åº¦æ¶ˆå¤±å¯¼è‡´æ”¶æ•›æ…¢
   - Transformer:æ®‹å·®è¿æ¥ä¿è¯æ¢¯åº¦æµç•…
   - æ”¶æ•›æ­¥æ•°å‡å°‘2-3å€

**å¿…èƒŒæ•°æ®**:
- Transformerè®­ç»ƒé€Ÿåº¦:**LSTMçš„5.3å€**(512åºåˆ—)
- GPUåˆ©ç”¨ç‡:**89% vs 45%**
- æ”¶æ•›æ­¥æ•°:**100K vs 300K**(WMT14)

---

**Q2:RNNå®Œå…¨è¢«æ·˜æ±°äº†å—?**

**A**:åœ¨åºåˆ—ä»»åŠ¡ä¸»æµä¸Šæ˜¯,ä½†ç‰¹å®šåœºæ™¯ä»æœ‰ä»·å€¼:

**ä¸»æµNLPä»»åŠ¡**:å·²æ·˜æ±°
- æœºå™¨ç¿»è¯‘:Transformerå 99%+
- æ–‡æœ¬åˆ†ç±»:BERT/RoBERTaä¸»å¯¼
- å¯¹è¯ç³»ç»Ÿ:GPTç³»åˆ—å æ®

**ä»æœ‰åº”ç”¨çš„åœºæ™¯**:

1. **åœ¨çº¿æ¨ç†ã€æµå¼å¤„ç†**
   - RNNå¯é€ä¸ªå¤„ç†,æ— éœ€ç­‰å¾…å…¨åºåˆ—
   - Transformerå¿…é¡»çœ‹åˆ°å®Œæ•´åºåˆ—

2. **èµ„æºå—é™åœºæ™¯**
   - LSTM(60Må‚æ•°) vs Transformer(110Må‚æ•°)
   - ç§»åŠ¨ç«¯ã€åµŒå…¥å¼è®¾å¤‡
   - æ¨ç†å»¶è¿Ÿè¦æ±‚<10ms

3. **æ—¶é—´åºåˆ—é¢„æµ‹**
   - é‡‘èã€è‚¡ç¥¨é¢„æµ‹
   - ä¼ æ„Ÿå™¨æ•°æ®åˆ†æ
   - RNNçš„å½’çº³åç½®æœ‰æ—¶æ›´é€‚åˆ

**è®ºæ–‡æ•°æ®**(2024å¹´ACL):
- Transformerè®ºæ–‡å æ¯”:**87.3%**
- RNNè®ºæ–‡å æ¯”:**4.2%**(ä¸»è¦æ˜¯æ—¶é—´åºåˆ—ã€è¯­éŸ³)
- æ··åˆæ¶æ„:**8.5%**

---

**Q3:Transformerçš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆ?**

**A**:ä¸‰å¤§ä¸»è¦ç¼ºç‚¹:

**1. $O(n^2)$æ³¨æ„åŠ›å¤æ‚åº¦**

è®¡ç®—å¤æ‚åº¦:

$$
\text{Attention:} \quad O(n^2 \cdot d)
$$

**é—®é¢˜**:
- åºåˆ—é•¿åº¦ç¿»å€ â†’ è®¡ç®—é‡4å€
- 8Kåºåˆ— â†’ 64Mæ³¨æ„åŠ›åˆ†æ•°
- 128Ké•¿ä¸Šä¸‹æ–‡ â†’ 16Bæ³¨æ„åŠ›åˆ†æ•°(æ˜¾å­˜çˆ†ç‚¸)

**è§£å†³æ–¹æ¡ˆ**:
- Sparse Attention(ç¨€ç–æ³¨æ„åŠ›)
- Flash Attention(ä¼˜åŒ–IO)
- MLA(å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›,DeepSeek-V3)

**2. æ— å½’çº³åç½®,éœ€è¦æ›´å¤šæ•°æ®**

RNNå¤©ç„¶çš„åºåˆ—å½’çº³åç½®:
- æ—¶é—´é¡ºåº:$h_t$ä¾èµ–$h_{t-1}$
- å±€éƒ¨æ€§:ç›¸é‚»æ—¶é—´æ­¥å…³è”å¼º

Transformer:
- å®Œå…¨ä¾èµ–æ•°æ®å­¦ä¹ ä½ç½®å…³ç³»
- éœ€è¦ä½ç½®ç¼–ç æ˜¾å¼æ³¨å…¥é¡ºåºä¿¡æ¯

**å®éªŒå¯¹æ¯”**(æƒ…æ„Ÿåˆ†ç±»,è®­ç»ƒé›†å¤§å°):

| è®­ç»ƒæ ·æœ¬æ•° | LSTMå‡†ç¡®ç‡ | Transformerå‡†ç¡®ç‡ |
|----------|----------|-----------------|
| 1K | 72.3% | 65.1% |
| 5K | 78.5% | 76.2% |
| 20K | 82.1% | 84.3% |
| 100K | 85.2% | **89.7%** |

å°æ•°æ®TransformeråŠ£åŠ¿æ˜æ˜¾!

**3. ä½ç½®ç¼–ç ä¸å¦‚RNNè‡ªç„¶**

RNN:
- ä½ç½®ä¿¡æ¯éšå¼ç¼–ç åœ¨$h_t$ä¸­
- è‡ªç„¶å¤„ç†åºåˆ—é¡ºåº

Transformer:
- å¿…é¡»æ˜¾å¼æ·»åŠ ä½ç½®ç¼–ç 
- RoPEã€ALiBiç­‰æ–¹æ³•å¤æ‚

**æ‰“ä¹±é¡ºåºæµ‹è¯•**(æœºå™¨ç¿»è¯‘BLEU):

| æ¨¡å‹ | æ­£å¸¸é¡ºåº | æ‰“ä¹±25%è¯åº | æ‰“ä¹±50%è¯åº |
|-----|---------|-----------|-----------|
| LSTM | 27.3 | 18.5(-8.8) | 12.1(-15.2) |
| Transformer | 28.4 | 16.2(-12.2) | 9.3(-19.1) |

Transformerå¯¹è¯åºæ‰“ä¹±**æ›´æ•æ„Ÿ**!

---

### (6)ğŸ’¡ æœ¬èŠ‚å¿…èƒŒæ•°æ®

**è®­ç»ƒæ•ˆç‡**:
- Transformerè®­ç»ƒé€Ÿåº¦:**LSTMçš„5.3å€**
- GPUåˆ©ç”¨ç‡:**89% vs 45%**(LSTM)
- æ”¶æ•›æ­¥æ•°:**100K vs 300K**

**æ¢¯åº¦ä¼ æ’­**:
- RNNæ¢¯åº¦è¡°å‡:$\gamma^{T-t} \approx 2.66 \times 10^{-5}$(T=100)
- Transformeræ¢¯åº¦ç¨³å®š:ç¬¬12å±‚ä»ä¿æŒ**0.82**

**é•¿åºåˆ—æ€§èƒ½**(å›°æƒ‘åº¦,1000é•¿åº¦):
- LSTM:**145.3**
- Transformer:**51.8**(æ€§èƒ½æ˜¯LSTMçš„2.8å€)

**å¤æ‚åº¦**:
- RNNè®­ç»ƒ:$O(n \cdot d^2)$æ—¶é—´,$O(n)$ä¸²è¡Œæ­¥éª¤
- Transformerè®­ç»ƒ:$O(n^2 \cdot d)$æ—¶é—´,$O(1)$å¹¶è¡Œæ­¥éª¤

**é€‚ç”¨åœºæ™¯**:
- Transformer:ä¸»æµNLP(**99%+**å¸‚åœºä»½é¢)
- RNN:æµå¼å¤„ç†ã€èµ„æºå—é™(<**5%**åœºæ™¯)

---


---


---

## å››ã€è®¤è¯†ä¸¤å¤§æ¨¡å‹å®¶æ—

åœ¨æ·±å…¥å­¦ä¹ ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å½“ä»ŠLLMé¢†åŸŸçš„ä¸¤å¤§ä¸»æµæ¶æ„ã€‚å®ƒä»¬çš„è®¾è®¡å“²å­¦å®Œå…¨ä¸åŒï¼Œé€‚ç”¨åœºæ™¯ä¹Ÿæˆªç„¶ä¸åŒã€‚

### 1. ç¼–ç å™¨æ¨¡å‹ï¼ˆEncoder-onlyï¼‰ï¼šä¸“æ³¨è¯­ä¹‰ç†è§£

**ä»£è¡¨æ¨¡å‹**ï¼šBERTã€RoBERTaã€DeBERTaã€ALBERT

**æ ¸å¿ƒç‰¹ç‚¹**ï¼šåŒå‘æ³¨æ„åŠ›æœºåˆ¶

æƒ³è±¡ä½ åœ¨é˜…è¯»ä¸€ç¯‡æ–‡ç« ï¼Œç¼–ç å™¨æ¨¡å‹å°±åƒä¸€ä¸ªèƒ½"åŒæ—¶çœ‹åˆ°å‰åæ–‡"çš„é˜…è¯»è€…ã€‚å½“å®ƒå¤„ç†å¥å­ä¸­çš„æŸä¸ªè¯æ—¶ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªè¯å‰åçš„æ‰€æœ‰ä¿¡æ¯ã€‚

**æ¶æ„ç¤ºæ„**ï¼š

```
è¾“å…¥: [CLS] æˆ‘ å–œæ¬¢ æœºå™¨ å­¦ä¹  [SEP]
       â†“    â†“  â†“   â†“   â†“    â†“
     [åŒå‘è‡ªæ³¨æ„åŠ›å±‚ Ã— N]
       â†“    â†“  â†“   â†“   â†“    â†“
è¾“å‡º: hâ‚€  hâ‚  hâ‚‚  hâ‚ƒ  hâ‚„  hâ‚…
```

æ¯ä¸ªä½ç½®çš„è¾“å‡º $h_i$ éƒ½èåˆäº†**æ•´å¥è¯**çš„ä¿¡æ¯ã€‚

**é¢„è®­ç»ƒä»»åŠ¡**ï¼šæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLM, Masked Language Modelingï¼‰

BERTåœ¨é¢„è®­ç»ƒæ—¶ä¼šéšæœºé®ç›–15%çš„è¯ï¼Œç„¶åè®©æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¿™äº›è¯ï¼š

```
è¾“å…¥: æˆ‘ å–œæ¬¢ [MASK] å­¦ä¹ 
ä»»åŠ¡: é¢„æµ‹[MASK]æ˜¯"æœºå™¨"
```

è¿™ç§è®­ç»ƒæ–¹å¼å¼ºè¿«æ¨¡å‹æ·±å…¥ç†è§£ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚

**å…¸å‹åº”ç”¨åœºæ™¯**ï¼š

1. **æ–‡æœ¬åˆ†ç±»**
   - æƒ…æ„Ÿåˆ†æï¼šåˆ¤æ–­è¯„è®ºæ˜¯æ­£é¢/è´Ÿé¢
   - åƒåœ¾é‚®ä»¶æ£€æµ‹
   - æ–°é—»åˆ†ç±»

2. **åºåˆ—æ ‡æ³¨**
   - å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼šè¯†åˆ«äººåã€åœ°åã€æœºæ„å
   - è¯æ€§æ ‡æ³¨

3. **é—®ç­”ç³»ç»Ÿ**
   - é˜…è¯»ç†è§£ï¼šç»™å®šæ–‡ç« å’Œé—®é¢˜ï¼ŒæŠ½å–ç­”æ¡ˆ
   - SQuADæ•°æ®é›†ä¸Šçš„ä»»åŠ¡

**ä»£ç ç¤ºä¾‹**ï¼ˆä½¿ç”¨Hugging Face Transformersï¼‰ï¼š

```python
from transformers import pipeline

# åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆåŸºäºBERTï¼‰
classifier = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-dianping-chinese")

# åˆ†ææƒ…æ„Ÿ
result = classifier("è¿™å®¶é¤å…çš„èœå“éå¸¸ç¾å‘³ï¼ŒæœåŠ¡æ€åº¦ä¹Ÿå¾ˆå¥½ï¼")
print(result)
# [{'label': 'positive', 'score': 0.9987}]

result = classifier("èœå“ä¸€èˆ¬ï¼Œä»·æ ¼è¿˜å¾ˆè´µï¼Œä¸æ¨èã€‚")
print(result)
# [{'label': 'negative', 'score': 0.9954}]
```

**ä¸ºä»€ä¹ˆä¸ç”¨ç¼–ç å™¨åšæ–‡æœ¬ç”Ÿæˆï¼Ÿ**

ç¼–ç å™¨çš„åŒå‘æ³¨æ„åŠ›åœ¨è®­ç»ƒæ—¶ä¼š"çœ‹åˆ°æœªæ¥"ï¼Œè¿™åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ä¼šé€ æˆä¿¡æ¯æ³„éœ²ã€‚æƒ³è±¡ä½ åœ¨å†™ä½œæ–‡ï¼Œå¦‚æœèƒ½æå‰çœ‹åˆ°åé¢è¦å†™çš„å†…å®¹ï¼Œé‚£å°±å¤±å»äº†"ç”Ÿæˆ"çš„æ„ä¹‰ã€‚

---

### 2. ç”Ÿæˆæ¨¡å‹ï¼ˆDecoder-onlyï¼‰ï¼šä¸“æ³¨æ–‡æœ¬ç”Ÿæˆ

**ä»£è¡¨æ¨¡å‹**ï¼šGPTç³»åˆ—ã€LLaMAã€Mistralã€Qwenã€ChatGLM

**æ ¸å¿ƒç‰¹ç‚¹**ï¼šå› æœæ³¨æ„åŠ›ï¼ˆå•å‘ï¼‰+ è‡ªå›å½’ç”Ÿæˆ

ç”Ÿæˆæ¨¡å‹å°±åƒä¸€ä¸ª"åªèƒ½çœ‹å‰æ–‡"çš„ä½œå®¶ã€‚å®ƒåœ¨å†™ç¬¬$t$ä¸ªè¯æ—¶ï¼Œåªèƒ½å‚è€ƒå‰é¢å·²å†™çš„$t-1$ä¸ªè¯ï¼Œä¸èƒ½å·çœ‹åé¢çš„å†…å®¹ã€‚

**æ¶æ„ç¤ºæ„**ï¼š

```
è¾“å…¥: ä»Šå¤© å¤©æ°” å¾ˆ
      â†“    â†“   â†“
    [å› æœè‡ªæ³¨æ„åŠ›å±‚ Ã— N]  â† åªèƒ½çœ‹å·¦è¾¹ï¼Œä¸èƒ½çœ‹å³è¾¹
      â†“    â†“   â†“
é¢„æµ‹: å¤©æ°”  å¾ˆ  å¥½
```

**æ³¨æ„åŠ›æ©ç **ï¼š

ç”Ÿæˆæ¨¡å‹é€šè¿‡"å› æœæ©ç "å®ç°å•å‘æ³¨æ„åŠ›ï¼š

```
        ä»Šå¤© å¤©æ°” å¾ˆ å¥½
ä»Šå¤©    âœ“   âœ—   âœ—  âœ—   â† "ä»Šå¤©"åªèƒ½çœ‹è‡ªå·±
å¤©æ°”    âœ“   âœ“   âœ—  âœ—   â† "å¤©æ°”"èƒ½çœ‹"ä»Šå¤©"å’Œè‡ªå·±
å¾ˆ      âœ“   âœ“   âœ“  âœ—   â† "å¾ˆ"èƒ½çœ‹å‰é¢æ‰€æœ‰è¯
å¥½      âœ“   âœ“   âœ“  âœ“
```

**é¢„è®­ç»ƒä»»åŠ¡**ï¼šå› æœè¯­è¨€æ¨¡å‹ï¼ˆCLM, Causal Language Modelingï¼‰

ç”Ÿæˆæ¨¡å‹çš„é¢„è®­ç»ƒç›®æ ‡éå¸¸ç®€å•ï¼šç»™å®šå‰æ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

$$
\max \sum_{t=1}^{T} \log P(x_t | x_1, x_2, ..., x_{t-1})
$$

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆGPT-3åœ¨æµ·é‡æ–‡æœ¬ä¸Šè®­ç»ƒåï¼Œèƒ½å¤Ÿç»­å†™å„ç§æ–‡æœ¬â€”â€”å®ƒçš„æœ¬è´¨å°±æ˜¯"è¶…çº§è¡¥å…¨æœºå™¨"ã€‚

**è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹**ï¼š

ç”Ÿæˆæ¨¡å‹é€ä¸ªç”Ÿæˆè¯ï¼ˆtokenï¼‰ï¼Œæ¯æ¬¡ç”Ÿæˆéƒ½ä¾èµ–å‰é¢æ‰€æœ‰è¯ï¼š

```
è¾“å…¥æç¤º: "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š"
ç”Ÿæˆè¿‡ç¨‹:
  t=1: è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼šæ˜¥ (ç”Ÿæˆ"æ˜¥")
  t=2: è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼šæ˜¥é£ (ç”Ÿæˆ"é£")
  t=3: è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼šæ˜¥é£æ‹‚ (ç”Ÿæˆ"æ‹‚")
  t=4: è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼šæ˜¥é£æ‹‚é¢ (ç”Ÿæˆ"é¢")
  ...
```

æ¯ä¸€æ­¥éƒ½è¦è¿è¡Œä¸€æ¬¡å®Œæ•´çš„æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç”Ÿæˆå¾ˆæ…¢çš„åŸå› ã€‚

**å…¸å‹åº”ç”¨åœºæ™¯**ï¼š

1. **å¯¹è¯ç³»ç»Ÿ**
   - ChatGPTã€Claudeã€æ–‡å¿ƒä¸€è¨€
   - å®¢æœæœºå™¨äºº

2. **å†…å®¹åˆ›ä½œ**
   - æ–‡ç« ç»­å†™
   - ä»£ç ç”Ÿæˆï¼ˆGitHub Copilotï¼‰
   - åˆ›æ„å†™ä½œ

3. **ç¿»è¯‘ä¸æ‘˜è¦**
   - æœºå™¨ç¿»è¯‘
   - æ–‡æ¡£æ‘˜è¦

**ä»£ç ç¤ºä¾‹**ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ¨¡å‹ï¼ˆä»¥Qwenä¸ºä¾‹ï¼‰
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ç”Ÿæˆæ–‡æœ¬
prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    temperature=0.7,      # æ§åˆ¶éšæœºæ€§
    top_p=0.9,            # æ ¸é‡‡æ ·
    do_sample=True        # å¯ç”¨é‡‡æ ·
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

**ä¸ºä»€ä¹ˆä¸ç”¨è§£ç å™¨åšåˆ†ç±»ä»»åŠ¡ï¼Ÿ**

è™½ç„¶æŠ€æœ¯ä¸Šå¯è¡Œï¼ˆé€šè¿‡æç¤ºè¯å¼•å¯¼è¾“å‡ºï¼‰ï¼Œä½†è§£ç å™¨åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šé€šå¸¸ä¸å¦‚ç¼–ç å™¨ï¼š
- ç¼–ç å™¨èƒ½åŒæ—¶çœ‹åˆ°æ•´å¥è¯ï¼Œç†è§£æ›´å…¨é¢
- åˆ†ç±»åªéœ€è¦ä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œä¸éœ€è¦ç”Ÿæˆèƒ½åŠ›
- ç¼–ç å™¨åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè®­ç»ƒæ›´é«˜æ•ˆ

---

### ä¸¤å¤§å®¶æ—å¯¹æ¯”æ€»ç»“

| ç»´åº¦ | ç¼–ç å™¨ï¼ˆBERTç³»ï¼‰ | è§£ç å™¨ï¼ˆGPTç³»ï¼‰ |
|------|-----------------|----------------|
| æ³¨æ„åŠ›æ–¹å‘ | åŒå‘ | å•å‘ï¼ˆå› æœï¼‰ |
| é¢„è®­ç»ƒä»»åŠ¡ | æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ | å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMï¼‰ |
| æ ¸å¿ƒèƒ½åŠ› | è¯­ä¹‰ç†è§£ | æ–‡æœ¬ç”Ÿæˆ |
| å…¸å‹åº”ç”¨ | åˆ†ç±»ã€æ ‡æ³¨ã€é—®ç­” | å¯¹è¯ã€åˆ›ä½œã€ç¿»è¯‘ |
| å‚æ•°è§„æ¨¡è¶‹åŠ¿ | ä¸­å°è§„æ¨¡ï¼ˆ<1Bï¼‰ | è¶…å¤§è§„æ¨¡ï¼ˆ>10Bï¼‰ |
| æ¨ç†é€Ÿåº¦ | å¿«ï¼ˆä¸€æ¬¡å‰å‘ï¼‰ | æ…¢ï¼ˆè‡ªå›å½’é€è¯ç”Ÿæˆï¼‰ |
| ä¸Šä¸‹æ–‡å­¦ä¹  | è¾ƒå¼± | å¼ºï¼ˆæ¶Œç°èƒ½åŠ›ï¼‰ |

**è¶‹åŠ¿è§‚å¯Ÿ**ï¼š

è¿‘å¹´æ¥ï¼Œ**è§£ç å™¨æ¶æ„ä¸»å¯¼äº†LLMå‘å±•**ï¼ŒåŸå› æ˜¯ï¼š
1. æ‰©å±•æ€§æ›´å¥½ï¼šå‚æ•°è¶Šå¤§ï¼Œç”Ÿæˆèƒ½åŠ›è¶Šå¼º
2. é€šç”¨æ€§æ›´å¼ºï¼šä¸€ä¸ªæ¨¡å‹è§£å†³æ‰€æœ‰ä»»åŠ¡ï¼ˆé€šè¿‡æç¤ºè¯ï¼‰
3. æ¶Œç°èƒ½åŠ›ï¼šè§„æ¨¡åˆ°è¾¾é˜ˆå€¼åå‡ºç°æƒŠäººçš„æ¨ç†èƒ½åŠ›

ä½†ç¼–ç å™¨ä¾ç„¶åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚ä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ï¼‰å æœ‰ä¸€å¸­ä¹‹åœ°ï¼Œå°¤å…¶æ˜¯å¯¹æ¨ç†é€Ÿåº¦è¦æ±‚é«˜çš„åœºæ™¯ã€‚

---


---

## äº”ã€åŠ¨æ‰‹å®è·µï¼šä½ çš„ç¬¬ä¸€ä¸ªLLMç¨‹åº

ç†è®ºè®²äº†å¾ˆå¤šï¼Œç°åœ¨è®©æˆ‘ä»¬çœŸæ­£åŠ¨æ‰‹è¿è¡Œä»£ç ï¼Œäº²èº«ä½“éªŒLLMçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å®Œæˆä¸¤ä¸ªå®è·µï¼šæ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬åˆ†ç±»ã€‚

### ç¯å¢ƒå‡†å¤‡

é¦–å…ˆå®‰è£…å¿…è¦çš„åº“ï¼š

```bash
pip install transformers torch
```

å¦‚æœä½ çš„ç½‘ç»œè®¿é—®Hugging Faceè¾ƒæ…¢ï¼Œå¯ä»¥è®¾ç½®é•œåƒï¼š

```bash
export HF_ENDPOINT=https://hf-mirror.com
```

---

### å®è·µä¸€ï¼šè®©æ¨¡å‹è®²ä¸ªç¬‘è¯ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰

æˆ‘ä»¬ä½¿ç”¨**Qwen2.5-1.5B-Instruct**æ¨¡å‹ï¼ˆé˜¿é‡Œäº‘å¼€æºçš„ä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼‰æ¥ç”Ÿæˆæ–‡æœ¬ã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœå†…å­˜
    device_map="auto"            # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆCPU/GPUï¼‰
)

print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

# 2. æ„å»ºå¯¹è¯æ ¼å¼
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜é£è¶£çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ç»™æˆ‘è®²ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯"}
]

# 3. åº”ç”¨å¯¹è¯æ¨¡æ¿
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# 4. åˆ†è¯
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# 5. ç”Ÿæˆå›å¤
print("\nç”Ÿæˆä¸­...")
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

# 6. è§£ç è¾“å‡º
generated_ids = [
    output_ids[len(input_ids):]
    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print("\næ¨¡å‹å›å¤:")
print(response)
```

**ä»£ç è§£æ**ï¼š

1. **æ¨¡å‹åŠ è½½**ï¼š
   - `torch.float16`ï¼šä½¿ç”¨åŠç²¾åº¦ï¼ˆFP16ï¼‰å¯ä»¥å°†æ˜¾å­˜å ç”¨å‡åŠ
   - `device_map="auto"`ï¼šè‡ªåŠ¨åˆ¤æ–­ä½¿ç”¨CPUè¿˜æ˜¯GPU

2. **å¯¹è¯æ ¼å¼**ï¼š
   - `system`ï¼šè®¾å®šæ¨¡å‹çš„è§’è‰²å’Œè¡Œä¸º
   - `user`ï¼šç”¨æˆ·çš„è¾“å…¥
   - `apply_chat_template`ï¼šå°†å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ ¼å¼

3. **ç”Ÿæˆå‚æ•°**ï¼š
   - `max_new_tokens`ï¼šæœ€å¤šç”Ÿæˆ512ä¸ªæ–°token
   - `temperature`ï¼šæ§åˆ¶éšæœºæ€§ï¼ˆ0.7è¡¨ç¤ºé€‚ä¸­ï¼‰
   - `top_p`ï¼šæ ¸é‡‡æ ·ï¼Œåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡å‰90%çš„è¯
   - `do_sample=True`ï¼šå¯ç”¨éšæœºé‡‡æ ·ï¼ˆå¦åˆ™æ˜¯è´ªå©ªè§£ç ï¼‰

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```
æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen/Qwen2.5-1.5B-Instruct
æ¨¡å‹åŠ è½½å®Œæˆï¼

ç”Ÿæˆä¸­...

æ¨¡å‹å›å¤:
å¥½çš„ï¼è¿™é‡Œæœ‰ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯ï¼š

ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯æ··æ·†åœ£è¯èŠ‚å’Œä¸‡åœ£èŠ‚ï¼Ÿ

å› ä¸º Oct 31 == Dec 25ï¼

ï¼ˆè§£é‡Šï¼šOctæ˜¯å…«è¿›åˆ¶Octoberçš„ç¼©å†™ï¼Œ31çš„å…«è¿›åˆ¶ç­‰äºåè¿›åˆ¶çš„25ï¼‰

å“ˆå“ˆï¼Œè¿™ä¸ªç¬‘è¯åˆ©ç”¨äº†ç¼–ç¨‹ä¸­çš„è¿›åˆ¶æ¦‚å¿µï¼Œå¾ˆæœ‰ç¨‹åºå‘˜çš„ç‰¹è‰²å§ï¼
```

**å‚æ•°å®éªŒ**ï¼š

å°è¯•ä¿®æ”¹`temperature`å‚æ•°ï¼Œè§‚å¯Ÿè¾“å‡ºå˜åŒ–ï¼š

```python
# temperature=0.1 (æ¥è¿‘ç¡®å®šæ€§ï¼Œè¾“å‡ºæ›´ä¿å®ˆ)
# temperature=1.5 (æ›´éšæœºï¼Œè¾“å‡ºæ›´æœ‰åˆ›æ„ä½†å¯èƒ½ä¸è¿è´¯)
```

---

### å®è·µäºŒï¼šè®©æ¨¡å‹åšä¸ªå½±è¯„äººï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨**BERTæ¶æ„**çš„æ¨¡å‹æ¥å®Œæˆæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
from transformers import pipeline
import torch

# 1. åˆ›å»ºæƒ…æ„Ÿåˆ†ææµæ°´çº¿
print("åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
classifier = pipeline(
    "sentiment-analysis",
    model="uer/roberta-base-finetuned-dianping-chinese",
    device=0 if torch.cuda.is_available() else -1  # 0=GPU, -1=CPU
)

print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# 2. å‡†å¤‡æµ‹è¯•è¯„è®º
reviews = [
    "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼å‰§æƒ…ç´§å‡‘ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå¼ºçƒˆæ¨èï¼",
    "å®Œå…¨æµªè´¹æ—¶é—´ï¼Œå‰§æƒ…æ‹–æ²“ï¼Œæ¼”æŠ€å°´å°¬ï¼Œä¸æ¨èè§‚çœ‹ã€‚",
    "è¿˜è¡Œå§ï¼Œæœ‰äº›åœ°æ–¹ä¸é”™ï¼Œä½†æ•´ä½“å¹³å¹³æ— å¥‡ã€‚",
    "è§†è§‰æ•ˆæœå¾ˆéœ‡æ’¼ï¼Œä½†å‰§æƒ…æœ‰ç‚¹å•è–„ã€‚",
    "å¯¼æ¼”çš„å·…å³°ä¹‹ä½œï¼Œçœ‹å®Œä¹‹åä¹…ä¹…ä¸èƒ½å¹³é™ã€‚"
]

# 3. æ‰¹é‡åˆ†ç±»
print("å¼€å§‹åˆ†ç±»...")
results = classifier(reviews)

# 4. å±•ç¤ºç»“æœ
for review, result in zip(reviews, results):
    label = "æ­£é¢ ğŸ˜Š" if result['label'] == 'positive' else "è´Ÿé¢ ğŸ˜"
    score = result['score']

    print(f"è¯„è®º: {review}")
    print(f"æƒ…æ„Ÿ: {label} (ç½®ä¿¡åº¦: {score:.2%})")
    print("-" * 80)
```

**ä»£ç è§£æ**ï¼š

1. **PipelineæŠ½è±¡**ï¼š
   - `pipeline`æ˜¯Hugging Faceæä¾›çš„é«˜çº§API
   - è‡ªåŠ¨å¤„ç†åˆ†è¯ã€æ¨¡å‹æ¨ç†ã€åå¤„ç†å…¨æµç¨‹
   - æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡

2. **æ¨¡å‹é€‰æ‹©**ï¼š
   - `uer/roberta-base-finetuned-dianping-chinese`ï¼šåœ¨å¤§ä¼—ç‚¹è¯„æ•°æ®ä¸Šå¾®è°ƒçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
   - RoBERTaæ˜¯BERTçš„æ”¹è¿›ç‰ˆæœ¬

3. **è¾“å‡ºæ ¼å¼**ï¼š
   ```python
   {
       'label': 'positive',  # æˆ– 'negative'
       'score': 0.9987       # ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
   }
   ```

**è¿è¡Œç»“æœç¤ºä¾‹**ï¼š

```
åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...
æ¨¡å‹åŠ è½½å®Œæˆï¼

å¼€å§‹åˆ†ç±»...
è¯„è®º: è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼å‰§æƒ…ç´§å‡‘ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå¼ºçƒˆæ¨èï¼
æƒ…æ„Ÿ: æ­£é¢ ğŸ˜Š (ç½®ä¿¡åº¦: 99.89%)
--------------------------------------------------------------------------------
è¯„è®º: å®Œå…¨æµªè´¹æ—¶é—´ï¼Œå‰§æƒ…æ‹–æ²“ï¼Œæ¼”æŠ€å°´å°¬ï¼Œä¸æ¨èè§‚çœ‹ã€‚
æƒ…æ„Ÿ: è´Ÿé¢ ğŸ˜ (ç½®ä¿¡åº¦: 99.76%)
--------------------------------------------------------------------------------
è¯„è®º: è¿˜è¡Œå§ï¼Œæœ‰äº›åœ°æ–¹ä¸é”™ï¼Œä½†æ•´ä½“å¹³å¹³æ— å¥‡ã€‚
æƒ…æ„Ÿ: è´Ÿé¢ ğŸ˜ (ç½®ä¿¡åº¦: 67.32%)
--------------------------------------------------------------------------------
è¯„è®º: è§†è§‰æ•ˆæœå¾ˆéœ‡æ’¼ï¼Œä½†å‰§æƒ…æœ‰ç‚¹å•è–„ã€‚
æƒ…æ„Ÿ: æ­£é¢ ğŸ˜Š (ç½®ä¿¡åº¦: 58.21%)
--------------------------------------------------------------------------------
è¯„è®º: å¯¼æ¼”çš„å·…å³°ä¹‹ä½œï¼Œçœ‹å®Œä¹‹åä¹…ä¹…ä¸èƒ½å¹³é™ã€‚
æƒ…æ„Ÿ: æ­£é¢ ğŸ˜Š (ç½®ä¿¡åº¦: 99.95%)
--------------------------------------------------------------------------------
```

**è§‚å¯Ÿä¸æ€è€ƒ**ï¼š

1. ç¬¬3æ¡è¯„è®º"è¿˜è¡Œå§"è¢«åˆ¤å®šä¸ºè´Ÿé¢ï¼Œä½†ç½®ä¿¡åº¦åªæœ‰67%â€”â€”æ¨¡å‹è¯†åˆ«å‡ºäº†ä¸­æ€§æƒ…æ„Ÿ
2. ç¬¬4æ¡è¯„è®ºåŒ…å«"éœ‡æ’¼"ï¼ˆæ­£é¢ï¼‰å’Œ"å•è–„"ï¼ˆè´Ÿé¢ï¼‰ï¼Œæ¨¡å‹å€¾å‘æ­£é¢ä½†ç½®ä¿¡åº¦è¾ƒä½ï¼ˆ58%ï¼‰
3. æ¨¡å‹å¯¹æ˜ç¡®æƒ…æ„Ÿçš„è¯„è®ºï¼ˆç¬¬1ã€2ã€5æ¡ï¼‰ç½®ä¿¡åº¦æé«˜ï¼ˆ>99%ï¼‰

---

### å®è·µä¸‰ï¼šå¯¹æ¯”ä¸¤ç§æ¶æ„ï¼ˆå¯é€‰æ‰©å±•ï¼‰

è®©æˆ‘ä»¬å¯¹æ¯”ç¼–ç å™¨å’Œè§£ç å™¨åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**ä»»åŠ¡**ï¼šåˆ¤æ–­å¥å­"è¿™å®¶é¤å…çš„ç‰›æ’å¾ˆå¥½åƒ"çš„æƒ…æ„Ÿ

**æ–¹æ³•1ï¼šç¼–ç å™¨ï¼ˆBERTï¼‰**

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis",
                     model="uer/roberta-base-finetuned-dianping-chinese")
result = classifier("è¿™å®¶é¤å…çš„ç‰›æ’å¾ˆå¥½åƒ")
print("ç¼–ç å™¨ç»“æœ:", result)
# [{'label': 'positive', 'score': 0.9991}]
```

**æ–¹æ³•2ï¼šè§£ç å™¨ï¼ˆGPTï¼‰é€šè¿‡æç¤ºè¯**

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æåŠ©æ‰‹ï¼Œåªå›ç­”'æ­£é¢'æˆ–'è´Ÿé¢'ã€‚"},
    {"role": "user", "content": "åˆ¤æ–­æƒ…æ„Ÿï¼šè¿™å®¶é¤å…çš„ç‰›æ’å¾ˆå¥½åƒ"}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=10)
response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)

print("è§£ç å™¨ç»“æœ:", response)
# "æ­£é¢"
```

**å¯¹æ¯”æ€»ç»“**ï¼š

| ç»´åº¦ | ç¼–ç å™¨ï¼ˆBERTï¼‰ | è§£ç å™¨ï¼ˆGPTï¼‰ |
|------|---------------|--------------|
| å‡†ç¡®ç‡ | â­â­â­â­â­ å¾ˆé«˜ | â­â­â­â­ è¾ƒé«˜ |
| é€Ÿåº¦ | âš¡âš¡âš¡ å¾ˆå¿«ï¼ˆä¸€æ¬¡å‰å‘ï¼‰ | âš¡ è¾ƒæ…¢ï¼ˆè‡ªå›å½’ï¼‰ |
| éƒ¨ç½²æˆæœ¬ | ğŸ’° ä½ï¼ˆæ¨¡å‹å°ï¼‰ | ğŸ’°ğŸ’° é«˜ï¼ˆæ¨¡å‹å¤§ï¼‰ |
| é›¶æ ·æœ¬èƒ½åŠ› | âŒ éœ€è¦å¾®è°ƒ | âœ… å¼º |
| å¯è§£é‡Šæ€§ | ğŸ“Š è¾“å‡ºæ¦‚ç‡ | ğŸ“ ç”Ÿæˆè‡ªç„¶è¯­è¨€ |

**é€‰æ‹©å»ºè®®**ï¼š
- å¦‚æœä»»åŠ¡æ˜ç¡®ï¼ˆå¦‚æƒ…æ„Ÿåˆ†ç±»ï¼‰ä¸”æœ‰æ ‡æ³¨æ•°æ® â†’ ç”¨ç¼–ç å™¨
- å¦‚æœéœ€è¦é€šç”¨èƒ½åŠ›æˆ–é›¶æ ·æœ¬å­¦ä¹  â†’ ç”¨è§£ç å™¨

---

## ğŸ’¡ æ–°æ‰‹é—®ç­”ï¼šä»å¥½å¥‡åˆ°ç†è§£

> åˆæ¬¡æ¥è§¦LLMï¼Œä½ å¯èƒ½æœ‰å¾ˆå¤šç–‘é—®ã€‚è®©æˆ‘ä»¬ç”¨æœ¬ç« å­¦åˆ°çš„çŸ¥è¯†æ¥è§£ç­”ã€‚

---

### é—®é¢˜1ï¼šChatGPTå’ŒGPTæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆæœ‰æ—¶å€™å«æ³•ä¸ä¸€æ ·ï¼Ÿ

**å›°æƒ‘æ¥æº**ï¼š

æ–°æ‰‹ç»å¸¸çœ‹åˆ°GPT-3ã€GPT-4ã€ChatGPTã€InstructGPTç­‰åè¯ï¼Œåˆ†ä¸æ¸…å…³ç³»ã€‚

**æœ¬è´¨åŒºåˆ«**ï¼š

| åç§° | ç±»å‹ | è®­ç»ƒæ–¹å¼ | ç”¨é€” |
|-----|------|---------|------|
| **GPT-3/GPT-4** | åŸºç¡€æ¨¡å‹ | é¢„è®­ç»ƒï¼ˆæ— ç›‘ç£ï¼‰ | ç»­å†™æ–‡æœ¬ï¼Œä¸æ“…é•¿å¯¹è¯ |
| **InstructGPT** | æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ | GPT-3 + æŒ‡ä»¤å¾®è°ƒ | éµå¾ªæŒ‡ä»¤ |
| **ChatGPT** | å¯¹è¯æ¨¡å‹ | GPT-3.5 + RLHF | å¤šè½®å¯¹è¯ |

**å½¢è±¡æ¯”å–»**ï¼š

```
GPT-3 = åšè§ˆç¾¤ä¹¦çš„å­¦è€…ï¼ˆè¯»äº†æ•´ä¸ªäº’è”ç½‘ï¼‰
      â†“ ä½†ä¸çŸ¥é“æ€ä¹ˆå’Œäººäº¤æµ
InstructGPT = ç»è¿‡åŸ¹è®­çš„åŠ©æ‰‹ï¼ˆå­¦ä¼šäº†å¬æŒ‡ä»¤ï¼‰
      â†“ ä½†å¯¹è¯èƒ½åŠ›ä¸€èˆ¬
ChatGPT = ç»éªŒä¸°å¯Œçš„å®¢æœï¼ˆé€šè¿‡äººç±»åé¦ˆä¼˜åŒ–ï¼‰
      â†“ æ“…é•¿å¤šè½®å¯¹è¯
```

**å®é™…æµ‹è¯•**ï¼š

åŒæ ·çš„è¾“å…¥ï¼Œä¸åŒæ¨¡å‹çš„è¡¨ç°ï¼š

```python
è¾“å…¥: "Python"

# GPT-3 (åŸºç¡€æ¨¡å‹)
è¾“å‡º: " is a high-level programming language..."  # ç»­å†™ï¼Œåƒç»´åŸºç™¾ç§‘

# InstructGPT
è¾“å‡º: "Pythonæ˜¯ä¸€ç§..."  # ä¼šå›ç­”ï¼Œä½†å¯èƒ½ç”Ÿç¡¬

# ChatGPT
è¾“å‡º: "æ‚¨å¥½ï¼Pythonæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ã€‚è¯·é—®æ‚¨æƒ³äº†è§£Pythonçš„å“ªæ–¹é¢å‘¢ï¼Ÿ"
      # è‡ªç„¶å¯¹è¯ï¼Œè¿˜ä¼šåé—®
```

**å…³è”ä¸‹ä¸€ç« **ï¼šè¿™äº›å·®å¼‚æ¥è‡ªä¸åŒçš„è®­ç»ƒæ–¹å¼ï¼Œå°¤å…¶æ˜¯"æç¤ºå·¥ç¨‹"å’Œ"RLHF"ï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œæˆ‘ä»¬å°†åœ¨åç»­ç« èŠ‚æ·±å…¥å­¦ä¹ ã€‚

---

### é—®é¢˜2ï¼šä¸ºä»€ä¹ˆåŒä¸€ä¸ªé—®é¢˜ï¼Œæ¯æ¬¡é—®ChatGPTç­”æ¡ˆéƒ½ä¸ä¸€æ ·ï¼Ÿ

**å…¸å‹ç°è±¡**ï¼š

```python
# ç¬¬1æ¬¡
é—®: "æ¨è3æœ¬æœºå™¨å­¦ä¹ çš„ä¹¦"
ç­”: ã€Šè¥¿ç“œä¹¦ã€‹ã€ã€ŠPattern Recognitionã€‹ã€ã€ŠDeep Learningã€‹

# ç¬¬2æ¬¡ï¼ˆå®Œå…¨ç›¸åŒçš„é—®é¢˜ï¼‰
é—®: "æ¨è3æœ¬æœºå™¨å­¦ä¹ çš„ä¹¦"
ç­”: ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ã€ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹ã€ã€ŠHands-On MLã€‹
```

**æ ¹æœ¬åŸå› **ï¼ˆå…³è”çŸ¥è¯†ç‚¹ï¼šè‡ªå›å½’ç”Ÿæˆï¼‰

LLMçš„ç”Ÿæˆè¿‡ç¨‹æ˜¯**éšæœºé‡‡æ ·**ï¼Œä¸æ˜¯ç¡®å®šæ€§è¾“å‡ºã€‚

å›é¡¾ç¬¬ä¸‰èŠ‚çš„ä»£ç ï¼š

```python
outputs = model.generate(
    input_ids,
    max_new_tokens=50,
    do_sample=True,        # â† å…³é”®ï¼å¯ç”¨é‡‡æ ·
    temperature=0.7,       # â† æ§åˆ¶éšæœºæ€§
    top_p=0.9
)
```

**ç”Ÿæˆè¿‡ç¨‹å¯è§†åŒ–**ï¼š

å‡è®¾æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼š

```
å½“å‰ä¸Šä¸‹æ–‡: "æ¨è3æœ¬æœºå™¨å­¦ä¹ çš„"

æ¦‚ç‡åˆ†å¸ƒ:
"ä¹¦":     0.35
"æ•™æ":   0.25
"ç»å…¸":   0.20
"å…¥é—¨ä¹¦": 0.12
...
```

- **å¦‚æœ `do_sample=False`ï¼ˆGreedyï¼‰**ï¼šæ°¸è¿œé€‰"ä¹¦" â†’ ç¡®å®šæ€§è¾“å‡º
- **å¦‚æœ `do_sample=True`**ï¼šæŒ‰æ¦‚ç‡éšæœºé€‰æ‹© â†’ æ¯æ¬¡å¯èƒ½ä¸åŒ

**ä¸ºä»€ä¹ˆè¦éšæœºï¼Ÿ**

å¦‚æœæ¯æ¬¡éƒ½é€‰æ¦‚ç‡æœ€é«˜çš„è¯ï¼ˆGreedyï¼‰ï¼Œä¼šå¯¼è‡´ï¼š
1. è¾“å‡ºå•è°ƒã€é‡å¤
2. é™·å…¥å¾ªç¯ï¼ˆ"å¾ˆå¥½å¾ˆå¥½å¾ˆå¥½..."ï¼‰
3. ç¼ºä¹åˆ›é€ æ€§

**å¦‚ä½•æ§åˆ¶ç¡®å®šæ€§ï¼Ÿ**

```python
# æ–¹æ³•1: å…³é—­é‡‡æ ·ï¼ˆé€‚åˆäº‹å®æ€§ä»»åŠ¡ï¼‰
outputs = model.generate(
    input_ids,
    do_sample=False,
    num_beams=1  # Greedyè§£ç 
)
# æ¯æ¬¡è¾“å‡ºå®Œå…¨ç›¸åŒ

# æ–¹æ³•2: é™ä½æ¸©åº¦ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
outputs = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.1  # æ¥è¿‘ç¡®å®šæ€§
)

# æ–¹æ³•3: å›ºå®šéšæœºç§å­
torch.manual_seed(42)
outputs = model.generate(...)  # å¯å¤ç°
```

**å®è·µå»ºè®®**ï¼š

- **ç¿»è¯‘ã€æ‘˜è¦**ï¼štemperature=0.1-0.3ï¼ˆç¡®å®šæ€§å¼ºï¼‰
- **å¯¹è¯ã€å®¢æœ**ï¼štemperature=0.7ï¼ˆå¹³è¡¡ï¼‰
- **åˆ›æ„å†™ä½œ**ï¼štemperature=1.0-1.5ï¼ˆå¤šæ ·æ€§é«˜ï¼‰

---

### é—®é¢˜3ï¼šä¸ºä»€ä¹ˆBERTä¸èƒ½ç”¨æ¥å†™ä½œï¼ŒGPTä¸èƒ½ç”¨æ¥åšåˆ†ç±»ï¼Ÿ

**æ–°æ‰‹è¯¯è§£**ï¼š

"BERTå’ŒGPTéƒ½æ˜¯Transformerï¼Œä¸ºä»€ä¹ˆä¸èƒ½äº’æ¢ä½¿ç”¨ï¼Ÿ"

**æ ¸å¿ƒåŒºåˆ«**ï¼ˆå…³è”çŸ¥è¯†ç‚¹ï¼šç¼–ç å™¨ vs è§£ç å™¨ï¼‰

| ç»´åº¦ | BERTï¼ˆç¼–ç å™¨ï¼‰ | GPTï¼ˆè§£ç å™¨ï¼‰ |
|-----|--------------|-------------|
| **æ³¨æ„åŠ›æ–¹å‘** | åŒå‘ï¼ˆèƒ½çœ‹åˆ°å…¨æ–‡ï¼‰ | å•å‘ï¼ˆåªèƒ½çœ‹å‰æ–‡ï¼‰ |
| **è®­ç»ƒç›®æ ‡** | å®Œå½¢å¡«ç©ºï¼ˆMLMï¼‰ | é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ |
| **æ“…é•¿ä»»åŠ¡** | ç†è§£ï¼ˆåˆ†ç±»ã€NERï¼‰ | ç”Ÿæˆï¼ˆå†™ä½œã€å¯¹è¯ï¼‰ |
| **è¾“å‡ºå½¢å¼** | å‘é‡è¡¨ç¤º | æ–‡æœ¬åºåˆ— |

**ä¸ºä»€ä¹ˆBERTä¸èƒ½ç”Ÿæˆï¼Ÿ**

BERTçš„è®­ç»ƒæ–¹å¼å†³å®šäº†å®ƒä¸ä¼š"ä»å·¦åˆ°å³"ç”Ÿæˆï¼š

```
BERTè®­ç»ƒæ—¶çœ‹åˆ°çš„:
è¾“å…¥: "æˆ‘ çˆ± [MASK] äº¬ å¤©å®‰é—¨"
ä»»åŠ¡: é¢„æµ‹[MASK] = "åŒ—"

å®ƒå­¦ä¼šäº†"ç†è§£ä¸Šä¸‹æ–‡"ï¼Œä½†æ²¡å­¦ä¼š"é€è¯ç”Ÿæˆ"
```

å¦‚æœå¼ºè¡Œè®©BERTç”Ÿæˆï¼š

```python
# âŒ BERTæ²¡æœ‰generateæ–¹æ³•
bert_model.generate(...)  # æŠ¥é”™ï¼

# âœ… åªèƒ½åšå¡«ç©º
text = "æˆ‘çˆ±[MASK]äº¬å¤©å®‰é—¨"
outputs = bert_model(inputs)
predictions = outputs.logits.argmax(-1)  # é¢„æµ‹[MASK]ä½ç½®
```

**ä¸ºä»€ä¹ˆGPTåšåˆ†ç±»æ•ˆæœå·®ï¼Ÿ**

GPTåªèƒ½çœ‹"å‰æ–‡"ï¼Œæ— æ³•åˆ©ç”¨"åæ–‡"ä¿¡æ¯ï¼š

```
ä»»åŠ¡: æƒ…æ„Ÿåˆ†ç±»

å¥å­: "è¿™éƒ¨ç”µå½±å‰åŠæ®µå¾ˆæ— èŠï¼Œä½†ç»“å°¾æƒŠè‰³"

BERT (åŒå‘):
"ç”µå½±" â† åŒæ—¶çœ‹åˆ° â†’ "æ— èŠ" + "æƒŠè‰³" â†’ ç»¼åˆåˆ¤æ–­: æ­£é¢

GPT (å•å‘):
"ç”µå½±" â†’ "æ— èŠ" â†’ (çœ‹ä¸åˆ°åé¢çš„"æƒŠè‰³") â†’ è¯¯åˆ¤: è´Ÿé¢
```

**å®éªŒå¯¹æ¯”**ï¼š

```python
# BERTåšåˆ†ç±»ï¼ˆè‡ªç„¶ï¼‰
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
outputs = model(input_ids)
logits = outputs.logits  # [batch, 2]

# GPTåšåˆ†ç±»ï¼ˆéœ€è¦trickï¼‰
from transformers import GPT2ForSequenceClassification

model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)
# âš ï¸ åªèƒ½ç”¨æœ€åä¸€ä¸ªtokençš„è¡¨ç¤ºåšåˆ†ç±»ï¼Œä¸¢å¤±äº†å‰é¢çš„ä¿¡æ¯
```

**ä¾‹å¤–æƒ…å†µ**ï¼š

- **GPTä¹Ÿèƒ½åšåˆ†ç±»**ï¼šé€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆIn-Context Learningï¼‰
  ```
  è¾“å…¥: "è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹ã€‚ä¸Šé¢è¿™å¥è¯æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ï¼Ÿç­”ï¼š"
  è¾“å‡º: "æ­£é¢"
  ```
  ä½†è¿™éœ€è¦è¶³å¤Ÿå¤§çš„æ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰å’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚

- **T5æ¶æ„**ï¼šç¼–ç å™¨-è§£ç å™¨ç»“åˆï¼Œç”Ÿæˆå’Œç†è§£éƒ½å¼º
  ```
  è¾“å…¥: "åˆ†ç±»ï¼šè¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹"
  è¾“å‡º: "æ­£é¢"  # ç”¨ç”Ÿæˆçš„æ–¹å¼åšåˆ†ç±»
  ```

**å…³è”ä¸‹ä¸€ç« **ï¼šå¦‚ä½•ç”¨æç¤ºè®©GPTå®Œæˆå„ç§ä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ†ç±»ï¼‰ï¼Œå°±æ˜¯"æç¤ºå·¥ç¨‹"çš„æ ¸å¿ƒã€‚

---

### é—®é¢˜4ï¼šå¼€æºæ¨¡å‹å’Œé—­æºAPIï¼ˆå¦‚GPT-4ï¼‰æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿæˆ‘åº”è¯¥é€‰å“ªä¸ªï¼Ÿ

**é€‰æ‹©å›°æƒ‘**ï¼š

çœ‹åˆ°LLaMAã€ChatGPTã€Claudeã€Qwenç­‰æ¨¡å‹ï¼Œä¸çŸ¥é“å¦‚ä½•é€‰æ‹©ã€‚

**æ ¸å¿ƒå·®å¼‚**ï¼š

| ç»´åº¦ | å¼€æºæ¨¡å‹ï¼ˆLLaMAã€Qwenç­‰ï¼‰ | é—­æºAPIï¼ˆGPT-4ã€Claudeç­‰ï¼‰ |
|-----|------------------------|-------------------------|
| **è®¿é—®æ–¹å¼** | ä¸‹è½½æƒé‡ï¼Œæœ¬åœ°è¿è¡Œ | APIè°ƒç”¨ï¼ˆè”ç½‘ï¼‰ |
| **æˆæœ¬** | ç¡¬ä»¶æˆæœ¬ï¼ˆGPU/CPUï¼‰ | æŒ‰Tokenè®¡è´¹ |
| **æ€§èƒ½** | ä¸­ç­‰ï¼ˆ7B-70Bå‚æ•°ï¼‰ | é¡¶å°–ï¼ˆ>1Tå‚æ•°ï¼‰ |
| **æ•°æ®éšç§** | å®Œå…¨æœ¬åœ°ï¼Œæ•°æ®ä¸å¤–ä¼  | æ•°æ®ä¸Šä¼ åˆ°æœåŠ¡å•† |
| **å®šåˆ¶åŒ–** | å¯å¾®è°ƒã€ä¿®æ”¹ | åªèƒ½è°ƒæç¤º |
| **é€Ÿåº¦** | å–å†³äºç¡¬ä»¶ | å¿«ï¼ˆäº‘ç«¯GPUé›†ç¾¤ï¼‰ |

**æ€§èƒ½å¯¹æ¯”**ï¼ˆ2024æ•°æ®ï¼‰ï¼š

```
ä»»åŠ¡: ä»£ç ç”Ÿæˆï¼ˆHumanEvalï¼‰

GPT-4:        87.2%  â† é¡¶å°–
Claude 3.5:   92.0%  â† æœ€å¼º
DeepSeek-V3:  88.5%  â† å¼€æºé¡¶å°–
LLaMA-3-70B:  81.7%  â† å¼€æºä¸»æµ
Qwen-2.5-72B: 86.0%
LLaMA-3-8B:   62.2%  â† å°æ¨¡å‹
```

**å†³ç­–æ ‘**ï¼š

```
æ˜¯å¦æœ‰æ•æ„Ÿæ•°æ®ï¼ˆåŒ»ç–—ã€é‡‘èï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ å¼€æºæ¨¡å‹ï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
â””â”€ å¦ â†’ ç»§ç»­

æ˜¯å¦éœ€è¦é¡¶å°–æ€§èƒ½ï¼ˆå¤æ‚æ¨ç†ã€åˆ›æ„å†™ä½œï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ GPT-4 / Claude
â””â”€ å¦ â†’ ç»§ç»­

é¢„ç®—æœ‰é™ï¼ˆå­¦ç”Ÿã€ä¸ªäººé¡¹ç›®ï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ å¼€æºå°æ¨¡å‹ï¼ˆLLaMA-8Bã€Qwen-7Bï¼‰
â””â”€ å¦ â†’ å¼€æºå¤§æ¨¡å‹ï¼ˆLLaMA-70Bï¼‰æˆ–é—­æºAPI

éœ€è¦é¢‘ç¹å¾®è°ƒæˆ–å®šåˆ¶ï¼Ÿ
â”œâ”€ æ˜¯ â†’ å¼€æºæ¨¡å‹
â””â”€ å¦ â†’ é—­æºAPI
```

**æˆæœ¬å¯¹æ¯”**ï¼ˆå®é™…æ¡ˆä¾‹ï¼‰ï¼š

```
åœºæ™¯: å®¢æœæœºå™¨äººï¼Œæ¯å¤©å¤„ç†10ä¸‡æ¡å¯¹è¯ï¼Œæ¯æ¡å¹³å‡500 token

# æ–¹æ¡ˆ1: GPT-4 API
æˆæœ¬: 10ä¸‡ Ã— 500 token Ã— $0.03/1K = $1500/å¤© = $45,000/æœˆ

# æ–¹æ¡ˆ2: éƒ¨ç½²LLaMA-70Bï¼ˆæœ¬åœ°ï¼‰
ç¡¬ä»¶: 2Ã—A100 GPUï¼ˆç§Ÿç”¨ï¼‰= $3,000/æœˆ
æ€§èƒ½: å¯èƒ½ç•¥ä½äºGPT-4ï¼Œä½†å¯¹å®¢æœè¶³å¤Ÿ

ç»“è®º: å¼€æºæ¨¡å‹èŠ‚çœ93%æˆæœ¬ï¼
```

**å®è·µå»ºè®®**ï¼š

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ |
|-----|---------|
| **å­¦ä¹ ã€å®éªŒ** | å…è´¹APIé¢åº¦ + Colabè¿è¡Œå°æ¨¡å‹ |
| **åŸå‹å¼€å‘** | GPT-3.5 APIï¼ˆä¾¿å®œå¿«é€Ÿï¼‰ |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆä½é¢‘ï¼‰** | GPT-4 API |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆé«˜é¢‘ï¼‰** | è‡ªéƒ¨ç½²å¼€æºæ¨¡å‹ |
| **æ•æ„Ÿæ•°æ®** | å¿…é¡»æœ¬åœ°å¼€æºæ¨¡å‹ |

**è¶‹åŠ¿**ï¼šå¼€æºæ¨¡å‹åœ¨å¿«é€Ÿè¿½èµ¶ï¼Œ2024å¹´çš„LLaMA-3-70Bæ¥è¿‘2023å¹´çš„GPT-4æ°´å¹³ã€‚

---

### é—®é¢˜5ï¼šä¸ºä»€ä¹ˆæ¨¡å‹è¶Šå¤§è¶Šèªæ˜ï¼Ÿå‚æ•°é‡æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

**æ–°æ‰‹ç–‘é—®**ï¼š

"çœ‹åˆ°GPT-3æœ‰1750äº¿å‚æ•°ï¼ŒLLaMA-70Bæœ‰700äº¿å‚æ•°ï¼Œè¿™äº›å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ"

**å‚æ•°çš„æœ¬è´¨**ï¼š

æ¨¡å‹çš„å‚æ•°å°±æ˜¯**ç¥ç»ç½‘ç»œçš„æƒé‡**ï¼Œå­˜å‚¨åœ¨çŸ©é˜µä¸­ã€‚

ä»¥ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚ä¸ºä¾‹ï¼š

```python
# PyTorchä¸­çš„çº¿æ€§å±‚
linear = nn.Linear(in_features=512, out_features=2048)

# è¿™ä¸€å±‚æœ‰å¤šå°‘å‚æ•°ï¼Ÿ
å‚æ•°é‡ = 512 Ã— 2048 + 2048(bias) = 1,050,624
```

**Transformerçš„å‚æ•°åˆ†å¸ƒ**ï¼ˆä»¥GPT-2ä¸ºä¾‹ï¼‰ï¼š

```
æ€»å‚æ•°: 117M

â”œâ”€â”€ Token Embedding: 38M (32%)
â”‚   è¯è¡¨50K Ã— åµŒå…¥ç»´åº¦768 = 38.4M
â”‚
â”œâ”€â”€ ä½ç½®ç¼–ç : 0.39M
â”‚   æœ€å¤§é•¿åº¦1024 Ã— 768 = 0.39M
â”‚
â”œâ”€â”€ 12ä¸ªTransformerå±‚: 77M (66%)
â”‚   æ¯å±‚åŒ…æ‹¬:
â”‚   â”œâ”€â”€ å¤šå¤´æ³¨æ„åŠ›: 2.4M
â”‚   â”‚   â”œâ”€â”€ Q/K/VæŠ•å½±: 768Ã—768Ã—3 = 1.77M
â”‚   â”‚   â””â”€â”€ è¾“å‡ºæŠ•å½±: 768Ã—768 = 0.59M
â”‚   â””â”€â”€ å‰é¦ˆç½‘ç»œ: 4.7M
â”‚       â”œâ”€â”€ å‡ç»´: 768Ã—3072 = 2.36M
â”‚       â””â”€â”€ é™ç»´: 3072Ã—768 = 2.36M
â”‚
â””â”€â”€ è¾“å‡ºå±‚: 38M (å¤ç”¨Embedding)
```

**ä¸ºä»€ä¹ˆå‚æ•°å¤š = èƒ½åŠ›å¼ºï¼Ÿ**

**ç±»æ¯”1ï¼šçŸ¥è¯†å­˜å‚¨**

```
å°æ¨¡å‹(1Bå‚æ•°) = ä¸€æœ¬è¯å…¸ï¼ˆåŸºç¡€çŸ¥è¯†ï¼‰
ä¸­æ¨¡å‹(10B)    = ä¸€ä¸ªå›¾ä¹¦é¦†ï¼ˆä¸“ä¸šçŸ¥è¯†ï¼‰
å¤§æ¨¡å‹(100B+)  = äº’è”ç½‘ï¼ˆå‡ ä¹æ‰€æœ‰çŸ¥è¯†ï¼‰
```

å‚æ•°è¶Šå¤šï¼Œèƒ½è®°ä½çš„æ¨¡å¼ã€è§„å¾‹ã€çŸ¥è¯†å°±è¶Šå¤šã€‚

**ç±»æ¯”2ï¼šç¥ç»å…ƒæ•°é‡**

```
äººè„‘: 860äº¿ä¸ªç¥ç»å…ƒ
GPT-3: 1750äº¿å‚æ•°ï¼ˆæ¥è¿‘äººè„‘ç¥ç»å…ƒæ•°é‡ï¼ï¼‰

ä½†æ³¨æ„: äººè„‘ç¥ç»å…ƒè¿æ¥æ–¹å¼æ›´å¤æ‚
```

**å®éªŒéªŒè¯**ï¼ˆScaling Lawï¼‰ï¼š

```
æ€§èƒ½ âˆ å‚æ•°é‡^Î±

å®é™…æ•°æ®:
1Bå‚æ•°:  å›°æƒ‘åº¦ = 30
10Bå‚æ•°: å›°æƒ‘åº¦ = 15
100Bå‚æ•°: å›°æƒ‘åº¦ = 8

æ€§èƒ½æå‡æ˜æ˜¾ï¼Œä½†è¾¹é™…é€’å‡
```

**å‚æ•°é‡ vs èƒ½åŠ›å¯¹ç…§è¡¨**ï¼š

| å‚æ•°é‡ | èƒ½åŠ›æ°´å¹³ | å…¸å‹æ¨¡å‹ |
|-------|---------|---------|
| 100M-1B | åŸºç¡€è¯­è¨€ç†è§£ï¼Œç®€å•é—®ç­” | DistilBERT, MiniLM |
| 1B-10B | æ—¥å¸¸å¯¹è¯ï¼Œä»£ç è¡¥å…¨ | Phi-2, Gemini Nano |
| 10B-100B | å¤æ‚æ¨ç†ï¼Œä¸“ä¸šé—®ç­” | LLaMA-70B, Mixtral |
| 100B+ | é¡¶å°–èƒ½åŠ›ï¼Œæ¶Œç°èƒ½åŠ› | GPT-4, Claude 3 |

**ä¸ºä»€ä¹ˆä¸æ— é™å¢å¤§ï¼Ÿ**

1. **æˆæœ¬çˆ†ç‚¸**
   
   ```
   è®­ç»ƒGPT-3(175B): $460ä¸‡
   è®­ç»ƒGPT-4(ä¼°è®¡1.7T): $1äº¿+
   ```
   
2. **ç¡¬ä»¶é™åˆ¶**
   
   ```
   LLaMA-70Bæ¨ç†: éœ€è¦140GBæ˜¾å­˜ï¼ˆ2Ã—A100ï¼‰
   æ™®é€šGPU(16GB): åªèƒ½è·‘7Bæ¨¡å‹
   ```
   
3. **æ”¶ç›Šé€’å‡**
   
   ```
   1B â†’ 10B: æ€§èƒ½ç¿»å€
   10B â†’ 100B: æ€§èƒ½æå‡50%
   100B â†’ 1T: æ€§èƒ½æå‡20%
   ```

**æœªæ¥è¶‹åŠ¿**ï¼š

- **å¯†é›†æ¨¡å‹**ï¼šå‚æ•°å…¨éƒ¨æ¿€æ´»ï¼ˆGPTã€LLaMAï¼‰
- **ç¨€ç–æ¨¡å‹**ï¼šMoEï¼ˆæ··åˆä¸“å®¶ï¼‰ï¼Œåªæ¿€æ´»éƒ¨åˆ†å‚æ•°
  
  ```
  Mixtral-8Ã—7B: æ€»å‚æ•°47Bï¼Œä½†æ¯æ¬¡åªç”¨7B
  æ•ˆæœæ¥è¿‘47Bï¼Œæˆæœ¬æ¥è¿‘7B
  ```

**å…³è”ä¸‹ä¸€ç« **ï¼šå‚æ•°é‡åªæ˜¯åŸºç¡€ï¼Œå¦‚ä½•é€šè¿‡"æç¤º"æ¿€å‘æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ‰æ˜¯å…³é”®ã€‚

---

### é—®é¢˜6ï¼šæœ¬åœ°è¿è¡ŒLLMéœ€è¦ä»€ä¹ˆé…ç½®ï¼Ÿæ™®é€šç”µè„‘èƒ½è·‘å—ï¼Ÿ

**å®é™…éœ€æ±‚**ï¼š

å¾ˆå¤šäººæƒ³æœ¬åœ°è¿è¡Œæ¨¡å‹ï¼Œä½†ä¸çŸ¥é“ç¡¬ä»¶è¦æ±‚ã€‚

**æ˜¾å­˜éœ€æ±‚ä¼°ç®—**ï¼ˆFP16ç²¾åº¦ï¼‰ï¼š

```
æ˜¾å­˜éœ€æ±‚ â‰ˆ å‚æ•°é‡ Ã— 2 å­—èŠ‚

ä¾‹å­:
7Bæ¨¡å‹:  7B Ã— 2 = 14GB
13Bæ¨¡å‹: 13B Ã— 2 = 26GB
70Bæ¨¡å‹: 70B Ã— 2 = 140GB
```

**åŠ ä¸ŠKVç¼“å­˜ã€æ¿€æ´»å€¼**ï¼š

```
å®é™…è¿è¡Œéœ€è¦ â‰ˆ å‚æ•°é‡ Ã— 2.5

7Bæ¨¡å‹æ¨ç†:  14GB Ã— 1.5 = 21GBï¼ˆè€ƒè™‘KVç¼“å­˜ï¼‰
7Bæ¨¡å‹è®­ç»ƒ:  14GB Ã— 4 = 56GBï¼ˆéœ€è¦å­˜æ¢¯åº¦ï¼‰
```

**ç¡¬ä»¶é…ç½®æ¨è**ï¼š

| åœºæ™¯ | æ¨¡å‹å¤§å° | æœ€ä½é…ç½® | æ¨èé…ç½® |
|-----|---------|---------|---------|
| **å­¦ä¹ ä½“éªŒ** | 1B-3B | 8GB GPU | 16GB GPU (RTX 4060) |
| **æ—¥å¸¸ä½¿ç”¨** | 7B-13B | 16GB GPU | 24GB GPU (RTX 4090) |
| **ä¸“ä¸šå¼€å‘** | 30B-70B | 40GB GPU (A100) | 80GB GPU (A100) |
| **ä¼ä¸šç”Ÿäº§** | 70B+ | å¤šå¡å¹¶è¡Œ | 8Ã—A100 é›†ç¾¤ |

**é‡åŒ–æŠ€æœ¯**ï¼ˆé™ä½æ˜¾å­˜éœ€æ±‚ï¼‰ï¼š

| é‡åŒ–æ–¹æ³• | ç²¾åº¦ | æ˜¾å­˜å ç”¨ | æ€§èƒ½æŸå¤± |
|---------|-----|---------|---------|
| FP16ï¼ˆåŸå§‹ï¼‰ | 16ä½ | 100% | 0% |
| INT8 | 8ä½ | 50% | <1% |
| INT4 | 4ä½ | 25% | 2-5% |
| GPTQ | 4ä½ | 25% | <2% |

**å®è·µæ¡ˆä¾‹**ï¼š

```python
# æ¡ˆä¾‹1: ç¬”è®°æœ¬è·‘7Bæ¨¡å‹ï¼ˆ16GB GPUï¼‰
from transformers import AutoModelForCausalLM

# âŒ ç›´æ¥åŠ è½½ä¼šçˆ†æ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8b")  # éœ€è¦21GB

# âœ… ä½¿ç”¨INT4é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8b",
    load_in_4bit=True,  # INT4é‡åŒ–
    device_map="auto"
)  # åªéœ€è¦5GBï¼

# æ¡ˆä¾‹2: CPUè¿è¡Œï¼ˆæ— GPUï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8b",
    device_map="cpu",
    torch_dtype=torch.float32
)
# å¯ä»¥è·‘ï¼Œä½†æ…¢10-20å€
```

**é€Ÿåº¦å¯¹æ¯”**ï¼š

```
ç¡¬ä»¶: LLaMA-7Bç”Ÿæˆ100ä¸ªtokençš„æ—¶é—´

RTX 4090 (24GB):   2ç§’
RTX 3060 (12GB):   5ç§’ï¼ˆINT4é‡åŒ–ï¼‰
CPU (32æ ¸):        30ç§’
MacBook M2:        8ç§’ï¼ˆè‹¹æœä¼˜åŒ–ï¼‰
```

**æˆæœ¬å¯¹æ¯”**ï¼ˆä¸ªäººä½¿ç”¨ï¼‰ï¼š

| æ–¹æ¡ˆ | åˆå§‹æˆæœ¬ | æœˆåº¦æˆæœ¬ | é€‚åˆåœºæ™¯ |
|-----|---------|---------|---------|
| **é—­æºAPI** | $0 | $20-100 | è½»åº¦ä½¿ç”¨ |
| **äº‘GPUç§Ÿç”¨** | $0 | $200-500 | ä¸­åº¦ä½¿ç”¨ |
| **è´­ä¹°æ¶ˆè´¹çº§GPU** | $1500 | $20ï¼ˆç”µè´¹ï¼‰ | é‡åº¦ä½¿ç”¨ |
| **è´­ä¹°A100** | $10,000 | $50 | ä¸“ä¸šå¼€å‘ |

**å†³ç­–å»ºè®®**ï¼š

```
æœˆä½¿ç”¨é‡ < 100ä¸‡token?
â””â”€ ç”¨APIï¼ˆä¾¿å®œï¼‰

æœˆä½¿ç”¨é‡ > 1000ä¸‡token?
â””â”€ è‡ªè´­GPUï¼ˆé•¿æœŸæ›´çœï¼‰

æœˆä½¿ç”¨é‡ä»‹äºä¸¤è€…ä¹‹é—´?
â””â”€ äº‘GPUç§Ÿç”¨ï¼ˆçµæ´»ï¼‰
```

**å…³è”åç»­ç« èŠ‚**ï¼šäº†è§£äº†ç¡¬ä»¶éœ€æ±‚ï¼Œä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•é«˜æ•ˆä½¿ç”¨è¿™äº›æ¨¡å‹ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹æœ€å¤§åŒ–æ€§èƒ½ã€‚

---


---

## ğŸ’¡ æœ¬ç« å¿…èƒŒæ•°æ®é€ŸæŸ¥è¡¨

> è¿™äº›æ•°æ®åœ¨é¢è¯•å’Œå®è·µä¸­é«˜é¢‘å‡ºç°,å»ºè®®ç†Ÿè®°ã€‚

### ä¸€ã€è¯è¢‹æ¨¡å‹æ—¶ä»£

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|-----|------|------|
| **ç»´åº¦** | è¯æ±‡è¡¨å¤§å°(10ä¸‡+) | æ¯ä¸ªæ–‡æ¡£è¡¨ç¤ºä¸ºé«˜ç»´ç¨€ç–å‘é‡ |
| **ç¨€ç–åº¦** | 99%+ | ç»å¤§å¤šæ•°å…ƒç´ ä¸º0 |
| **ä»£è¡¨ç®—æ³•** | TF-IDF, BoW | æ— æ³•æ•æ‰è¯åºå’Œè¯­ä¹‰ |
| **é€‚ç”¨åœºæ™¯** | åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€ç®€å•åˆ†ç±» | æ•ˆæœæœ‰é™ |

---

### äºŒã€Word2Vecæ—¶ä»£

#### æ ¸å¿ƒè¶…å‚æ•°

| è¶…å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|-------|-------|------|
| **åµŒå…¥ç»´åº¦** | 100-300 | å¸¸ç”¨300ç»´ |
| **çª—å£å¤§å°** | 5 | ç»éªŒæœ€ä¼˜å€¼ |
| **è´Ÿæ ·æœ¬æ•°(å°æ•°æ®)** | 15-20 | <100ä¸‡è¯ |
| **è´Ÿæ ·æœ¬æ•°(å¤§æ•°æ®)** | 5-10 | >100ä¸‡è¯ |
| **å­¦ä¹ ç‡(åˆå§‹)** | 0.025 | é€æ­¥è¡°å‡ |
| **å­¦ä¹ ç‡(æœ€ç»ˆ)** | 0.0001 | çº¿æ€§è¡°å‡ |
| **æœ€å°è¯é¢‘** | 5 | è¿‡æ»¤ä½é¢‘è¯ |
| **è´Ÿé‡‡æ ·åˆ†å¸ƒ** | $f(w)^{0.75}$ | å¹³è¡¡é«˜ä½é¢‘è¯ |

#### æ€§èƒ½æ•°æ®(Google Newsè¯­æ–™)

| æŒ‡æ ‡ | æ•°å€¼ |
|-----|------|
| è¯è¡¨å¤§å° | 300ä¸‡ |
| è®­ç»ƒè¯­æ–™ | 1000äº¿è¯ |
| Skip-gramç±»æ¯”å‡†ç¡®ç‡ | **73.8%** |
| CBOWç±»æ¯”å‡†ç¡®ç‡ | 61.2% |
| æœ‰æ•ˆç§© | 67(300ç»´ä¸­) |
| å‰100å¥‡å¼‚å€¼å æ¯” | 92% |
| è®­ç»ƒæ—¶é—´(8æ ¸CPU) | çº¦1å¤© |

#### ä¸ºä»€ä¹ˆ300ç»´å¤Ÿç”¨?(ä¿¡æ¯å®¹é‡)

| ç»´åº¦ | ä¿¡æ¯å®¹é‡ |
|-----|---------|
| 300ç»´(FP32) | $(2^{32})^{300} = 2^{9600}$ |
| æœ‰æ•ˆç»´åº¦ | 67ç»´(SVDåˆ†è§£) |
| å¯è¡¨ç¤ºæ¦‚å¿µæ•° | $2^{67} \approx 1.48 \times 10^{20}$ |

è¿œå¤§äºäººç±»è¯æ±‡é‡(~20ä¸‡)!

---

### ä¸‰ã€Transformeræ—¶ä»£

#### æ¨¡å‹è§„æ¨¡æ¼”è¿›

| å¹´ä»½ | æ¨¡å‹ | å‚æ•°é‡ | å±‚æ•° | éšè—ç»´åº¦ | æ³¨æ„åŠ›å¤´æ•° | è®­ç»ƒè¯­æ–™ |
|-----|------|-------|------|---------|---------|---------|
| 2017 | Transformer(åŸå§‹) | 65M | 6 | 512 | 8 | WMT14 |
| 2018 | BERT-base | 110M | 12 | 768 | 12 | 16GB |
| 2018 | BERT-large | 340M | 24 | 1024 | 16 | 16GB |
| 2019 | GPT-2 | 1.5B | 48 | 1600 | 25 | 40GB |
| 2020 | GPT-3 | 175B | 96 | 12288 | 96 | 570GB |
| 2023 | GPT-4 | ~1.8T | 120+ | 18432 | 128 | æœªçŸ¥ |
| 2024 | LLaMA-3-405B | 405B | 126 | 16384 | 128 | 15T tokens |

#### æ³¨æ„åŠ›å¤æ‚åº¦å¯¹æ¯”

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | å¹¶è¡Œæ€§ |
|-----|----------|----------|-------|
| **è‡ªæ³¨æ„åŠ›** | $O(n^2 \cdot d)$ | $O(n^2)$ | å®Œå…¨å¹¶è¡Œ |
| **RNN** | $O(n \cdot d^2)$ | $O(n \cdot d)$ | ä¸²è¡Œ(næ­¥) |
| **FFN** | $O(n \cdot d^2)$ | $O(n \cdot d)$ | å®Œå…¨å¹¶è¡Œ |

#### Transformer vs RNNæ€§èƒ½å¯¹æ¯”

**è®­ç»ƒæ•ˆç‡**(WMT14è‹±å¾·ç¿»è¯‘):

| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | BLEU | GPUåˆ©ç”¨ç‡ |
|-----|-------|---------|------|----------|
| LSTM(4å±‚) | 60M | 12å¤© | 25.3 | 45% |
| Transformer(6å±‚) | 65M | **3.5å¤©** | **28.4** | **89%** |

**é•¿åºåˆ—æ€§èƒ½**(å›°æƒ‘åº¦perplexity):

| åºåˆ—é•¿åº¦ | LSTM | Transformer | æ€§èƒ½æå‡ |
|---------|------|-------------|---------|
| 50 | 45.2 | 42.1 | 6.9% |
| 100 | 52.8 | 43.5 | 17.6% |
| 500 | 78.5 | 47.2 | **39.9%** |
| 1000 | 145.3 | 51.8 | **64.3%** |

#### æ¢¯åº¦ä¼ æ’­ç¨³å®šæ€§

| æ¨¡å‹ | ç¬¬1å±‚æ¢¯åº¦ | ç¬¬6å±‚æ¢¯åº¦ | ç¬¬12å±‚æ¢¯åº¦ |
|-----|---------|---------|-----------|
| LSTM | 1.0 | 0.023 | N/A |
| Transformer | 1.0 | 0.87 | **0.82** |

Transformeræ¢¯åº¦ç¨³å®š,RNNæ¢¯åº¦æ¶ˆå¤±ä¸¥é‡!

---

### å››ã€æ¶Œç°èƒ½åŠ›é˜ˆå€¼

| èƒ½åŠ› | å‡ºç°è§„æ¨¡ | å…¸å‹ä»»åŠ¡ | ä»£è¡¨æ¨¡å‹ |
|-----|---------|---------|---------|
| **åŸºç¡€è¯­è¨€ç†è§£** | ~1B | ç®€å•QAã€å¡«ç©º | BERT-large |
| **ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)** | ~10B | Few-shotå­¦ä¹  | GPT-3 |
| **æ€ç»´é“¾æ¨ç†(CoT)** | ~60B | æ•°å­¦æ¨ç†ã€é€»è¾‘ | PaLM |
| **å·¥å…·è°ƒç”¨** | ~100B+ | APIè°ƒç”¨ã€ä»£ç æ‰§è¡Œ | GPT-4 |
| **å¤šæ¨¡æ€ç†è§£** | ~100B+ | å›¾æ–‡ç†è§£ | GPT-4V |

---

### äº”ã€å¸¸è§Benchmarkæ€§èƒ½

#### GLUE(æ–‡æœ¬ç†è§£)å‡†ç¡®ç‡

| æ¨¡å‹ | MNLI | QQP | QNLI | SST-2 | å¹³å‡ |
|-----|------|-----|------|-------|------|
| BERT-base | 84.6 | 71.2 | 90.5 | 93.5 | 85.0 |
| RoBERTa-large | 90.2 | 92.2 | 94.7 | 96.4 | 93.4 |
| DeBERTa-v3 | **91.8** | **93.0** | **95.3** | **97.5** | **94.4** |

#### ç”Ÿæˆä»»åŠ¡(BLEUåˆ†æ•°)

| æ¨¡å‹ | WMT14 En-De | WMT14 En-Fr | ä»£ç ç”Ÿæˆ(HumanEval) |
|-----|-------------|-------------|-------------------|
| Transformer-base | 27.3 | 38.1 | N/A |
| GPT-2 | 25.7 | 35.6 | N/A |
| GPT-3.5 | 32.1 | 42.3 | 48.1% |
| GPT-4 | **38.5** | **49.2** | **67.0%** |

---

### å…­ã€å®è·µç»éªŒå€¼

#### è®­ç»ƒè¶…å‚æ•°(Transformer)

| è¶…å‚æ•° | å°æ¨¡å‹(<1B) | å¤§æ¨¡å‹(>10B) | è¯´æ˜ |
|-------|-----------|------------|------|
| **Warmupæ­¥æ•°** | 4000-10000 | 2000-5000 | å­¦ä¹ ç‡é¢„çƒ­ |
| **å­¦ä¹ ç‡** | 1e-4 åˆ° 5e-4 | 1e-5 åˆ° 5e-5 | å¤§æ¨¡å‹ç”¨æ›´å°lr |
| **Dropout** | 0.1 | 0.0-0.05 | å¤§æ¨¡å‹dropoutå° |
| **Label Smoothing** | 0.1 | 0.1 | æ­£åˆ™åŒ– |
| **Batch Size** | 256-1024 | 2048-4096 | è¶Šå¤§è¶Šç¨³å®š |
| **Gradient Accumulation** | 4-8 | 16-32 | æ¨¡æ‹Ÿå¤§batch |
| **æƒé‡è¡°å‡** | 0.01 | 0.1 | L2æ­£åˆ™åŒ– |

#### æ¨ç†è¶…å‚æ•°(ç”Ÿæˆä»»åŠ¡)

| å‚æ•° | ç¿»è¯‘/æ‘˜è¦ | å¯¹è¯/é—®ç­” | åˆ›æ„å†™ä½œ | è¯´æ˜ |
|-----|---------|---------|---------|------|
| **Temperature** | 0.1-0.3 | 0.7-0.9 | 1.0-1.5 | æ§åˆ¶éšæœºæ€§ |
| **Top-p** | 0.9 | 0.9-0.95 | 0.95-1.0 | æ ¸é‡‡æ · |
| **Top-k** | 50 | 40-50 | 50-100 | Kä¸ªå€™é€‰è¯ |
| **Repetition Penalty** | 1.2 | 1.0-1.1 | 1.0 | é˜²æ­¢é‡å¤ |
| **Max New Tokens** | 512 | 1024 | 2048 | ç”Ÿæˆé•¿åº¦ |

#### ç¡¬ä»¶éœ€æ±‚(FP16æ¨ç†)

| æ¨¡å‹è§„æ¨¡ | æ˜¾å­˜å ç”¨ | æ¨èGPU | æ¨ç†é€Ÿåº¦(tokens/s) |
|---------|---------|---------|-------------------|
| 1B | ~2GB | RTX 3060(12GB) | 100-150 |
| 7B | ~14GB | RTX 3090/4090(24GB) | 50-80 |
| 13B | ~26GB | A100 40GB | 30-50 |
| 70B | ~140GB | 2Ã—A100 80GB | 10-20 |
| 175B(GPT-3) | ~350GB | 4Ã—A100 80GB | 5-10 |

#### æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

| æŠ€æœ¯ | æ˜¾å­˜èŠ‚çœ | é€Ÿåº¦å½±å“ | é€‚ç”¨åœºæ™¯ |
|-----|---------|---------|---------|
| **8-bité‡åŒ–** | 50% | -5% | æ¨ç† |
| **4-bité‡åŒ–(GPTQ)** | 75% | -10% | æ¨ç† |
| **Flash Attention** | 30% | +20% | è®­ç»ƒ+æ¨ç† |
| **Gradient Checkpointing** | 40% | -30% | è®­ç»ƒ |
| **LoRAå¾®è°ƒ** | 90% | æŒå¹³ | å¾®è°ƒ |

---

### ä¸ƒã€å…³é”®æ•°å­¦å…¬å¼

#### è‡ªæ³¨æ„åŠ›

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

#### RoPEä½ç½®ç¼–ç 

$$
\begin{pmatrix}
q_m^{(1)} \\
q_m^{(2)}
\end{pmatrix}
=
\begin{pmatrix}
\cos(m\theta) & -\sin(m\theta) \\
\sin(m\theta) & \cos(m\theta)
\end{pmatrix}
\begin{pmatrix}
q^{(1)} \\
q^{(2)}
\end{pmatrix}
$$

#### Temperatureé‡‡æ ·

$$
P(w_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

- $T \to 0$:æ¥è¿‘Greedy(ç¡®å®šæ€§)
- $T = 1$:åŸå§‹åˆ†å¸ƒ
- $T > 1$:æ›´å‡åŒ€(åˆ›é€ æ€§)

#### Top-p(Nucleus)é‡‡æ ·

$$
V_p = \min \left\{ V': \sum_{w \in V'} P(w) \geq p \right\}
$$

åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°$p$çš„è¯é›†ä¸­é‡‡æ ·ã€‚

---

### å…«ã€å¸¸è§é™·é˜±ä¸è¯¯åŒº

| è¯¯åŒº | æ­£ç¡®ç†è§£ | æ•°æ®æ”¯æŒ |
|-----|---------|---------|
| "æ¨¡å‹è¶Šå¤§è¶Šå¥½" | æœ‰æ”¶ç›Šé€’å‡,æˆæœ¬æŒ‡æ•°å¢é•¿ | GPT-3(175B) vs GPT-4(1.8T):æ€§èƒ½æå‡30%,æˆæœ¬10å€ |
| "Temperatureè¶Šé«˜è¶Šå¥½" | è¿‡é«˜å¯¼è‡´èƒ¡è¨€ä¹±è¯­ | T=2.0å›°æƒ‘åº¦æš´æ¶¨5å€ |
| "BERTèƒ½åšç”Ÿæˆ" | ä¸èƒ½,è®­ç»ƒç›®æ ‡ä¸åŒ¹é… | BERTå¼ºè¡Œç”Ÿæˆå›°æƒ‘åº¦>100 |
| "æ³¨æ„åŠ›å¤´æ•°è¶Šå¤šè¶Šå¥½" | 16å¤´åæ”¶ç›Šå¾®å¼± | 16å¤´ vs 32å¤´:+0.2%æ€§èƒ½,+2å€æ˜¾å­˜ |
| "Warmupå¯ä»¥è·³è¿‡" | ä¸è¡Œ,ä¼šè®­ç»ƒå¤±è´¥ | æ— Warmupç¬¬10æ­¥loss=NaN |

---

### ä¹ã€é¢è¯•å¿…èƒŒæ ¸å¿ƒæ•°æ®(Top 20)

1. **Word2Vecçª—å£å¤§å°**:5
2. **Word2Vecè´Ÿæ ·æœ¬æ•°**:5-20
3. **Word2VecåµŒå…¥ç»´åº¦**:300
4. **BERT-baseå‚æ•°é‡**:110M
5. **GPT-3å‚æ•°é‡**:175B
6. **Transformerè®­ç»ƒé€Ÿåº¦æå‡**:5.3x(vs LSTM)
7. **è‡ªæ³¨æ„åŠ›å¤æ‚åº¦**:$O(n^2 \cdot d)$
8. **RNNæ¢¯åº¦è¡°å‡**(100é•¿åº¦):$0.9^{100} = 2.66 \times 10^{-5}$
9. **Transformer GPUåˆ©ç”¨ç‡**:89%(vs LSTM 45%)
10. **é•¿åºåˆ—æ€§èƒ½æå‡**(1000é•¿åº¦):64.3%(vs LSTM)
11. **Temperatureæ¨èå€¼**(å¯¹è¯):0.7-0.9
12. **Top-pæ¨èå€¼**:0.9
13. **Warmupæ­¥æ•°**(å°æ¨¡å‹):4000-10000
14. **Dropout**(Transformer):0.1
15. **7Bæ¨¡å‹æ˜¾å­˜**(FP16):~14GB
16. **8-bité‡åŒ–æ˜¾å­˜èŠ‚çœ**:50%
17. **Flash Attentioné€Ÿåº¦æå‡**:+20%
18. **ICLæ¶Œç°é˜ˆå€¼**:~10Bå‚æ•°
19. **CoTæ¶Œç°é˜ˆå€¼**:~60Bå‚æ•°
20. **BERT vs RoBERTa GLUEæå‡**:+8.4%

---

## æœ¬ç« å°ç»“

æ­å–œä½ å®Œæˆäº†ç¬¬ä¸€ç« çš„å­¦ä¹ ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ ¸å¿ƒå†…å®¹ï¼š

### çŸ¥è¯†å›é¡¾

1. **NLPæ¼”è¿›å†ç¨‹**
   - è¯è¢‹æ¨¡å‹ï¼šç®€å•ä½†ä¸¢å¤±è¯­åºå’Œè¯­ä¹‰
   - è¯åµŒå…¥ï¼ˆWord2Vecï¼‰ï¼šå°†è¯æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œä½†æ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰
   - Transformerï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œå¼€å¯LLMæ—¶ä»£

2. **ä¸¤å¤§æ¨¡å‹å®¶æ—**
   - **ç¼–ç å™¨ï¼ˆBERTç³»ï¼‰**ï¼šåŒå‘æ³¨æ„åŠ›ï¼Œæ“…é•¿è¯­ä¹‰ç†è§£ï¼ˆåˆ†ç±»ã€æ ‡æ³¨ï¼‰
   - **è§£ç å™¨ï¼ˆGPTç³»ï¼‰**ï¼šå•å‘æ³¨æ„åŠ›ï¼Œæ“…é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¯¹è¯ã€åˆ›ä½œï¼‰

3. **å®è·µç»éªŒ**
   - ä½¿ç”¨Hugging Face Transformersåº“åŠ è½½å’Œè¿è¡Œæ¨¡å‹
   - ç†è§£ç”Ÿæˆæ¨¡å‹çš„å‚æ•°ï¼ˆtemperatureã€top_pï¼‰å¯¹è¾“å‡ºçš„å½±å“
   - å¯¹æ¯”ç¼–ç å™¨å’Œè§£ç å™¨åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°

### å…³é”®æ¦‚å¿µ

- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå…è®¸æ¨¡å‹åŠ¨æ€å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†
- **å› æœæ©ç **ï¼šä¿è¯ç”Ÿæˆæ¨¡å‹åªèƒ½çœ‹åˆ°å‰æ–‡ï¼Œä¸èƒ½"å·çœ‹"æœªæ¥
- **è‡ªå›å½’ç”Ÿæˆ**ï¼šé€è¯ç”Ÿæˆæ–‡æœ¬ï¼Œæ¯æ¬¡ä¾èµ–å‰é¢æ‰€æœ‰è¯
- **ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼šå¤§æ¨¡å‹é€šè¿‡æç¤ºè¯å­¦ä¹ æ–°ä»»åŠ¡çš„èƒ½åŠ›

### æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆè¯è¢‹æ¨¡å‹åœ¨åƒåœ¾é‚®ä»¶è¿‡æ»¤ä»»åŠ¡ä¸Šä¾ç„¶æœ‰æ•ˆï¼Ÿ
2. Word2Vecèƒ½è§£å†³"ä¸€è¯å¤šä¹‰"é—®é¢˜å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. å¦‚æœè¦æ„å»ºä¸€ä¸ªæ–°é—»åˆ†ç±»ç³»ç»Ÿï¼Œä½ ä¼šé€‰æ‹©ç¼–ç å™¨è¿˜æ˜¯è§£ç å™¨ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
4. åœ¨æ–‡æœ¬ç”Ÿæˆä¸­ï¼Œ`temperature=0`å’Œ`temperature=2`ä¼šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

### ä¸‹ä¸€ç« é¢„å‘Š

åœ¨ç¬¬2ç« ã€Šä¸æ¨¡å‹å¯¹è¯ï¼šæç¤ºå·¥ç¨‹åŸºç¡€ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ ï¼š
- å¦‚ä½•è®¾è®¡é«˜è´¨é‡çš„æç¤ºè¯ï¼ˆPromptï¼‰
- é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å­¦ä¹ çš„åŸç†
- æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æ¨ç†æŠ€æœ¯
- å¦‚ä½•è°ƒèŠ‚æ¨¡å‹çš„"åˆ›é€ åŠ›"

è¿™äº›æŠ€èƒ½å°†è®©ä½ èƒ½å¤Ÿå……åˆ†å‘æŒ¥LLMçš„èƒ½åŠ›ï¼Œå®Œæˆå„ç§å¤æ‚ä»»åŠ¡ã€‚

---

**ç»ƒä¹ ä»£ç ä»“åº“**ï¼šæœ¬ç« æ‰€æœ‰ä»£ç ç¤ºä¾‹å·²ä¸Šä¼ åˆ°GitHubï¼ˆé“¾æ¥è§é™„å½•ï¼‰

**æ¨èé˜…è¯»**ï¼š
- è®ºæ–‡ï¼šã€ŠAttention is All You Needã€‹ï¼ˆTransformeråŸè®ºæ–‡ï¼‰
- åšå®¢ï¼šJay Alammarçš„ã€ŠThe Illustrated Transformerã€‹ï¼ˆå›¾è§£Transformerï¼‰
- è§†é¢‘ï¼šã€ŠWhat is a GPT?ã€‹ï¼ˆGPTå·¥ä½œåŸç†ï¼‰
