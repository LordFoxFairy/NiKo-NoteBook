# ç¬¬3ç« ï¼šè¯­è¨€çš„åŸºçŸ³ï¼šåˆ†è¯ä¸åµŒå…¥

> æ­å¼€LLMå·¥ä½œçš„ç¬¬ä¸€ä¸ª"é»‘ç›’"ï¼Œç†è§£æœºå™¨å¦‚ä½•"é˜…è¯»"æ–‡å­—ã€‚

---

åœ¨å‰ä¸¤ç« ï¼Œæˆ‘ä»¬å­¦ä¼šäº†å¦‚ä½•ä½¿ç”¨LLMï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æç¤ºè¯ä¸å®ƒä»¬æ²Ÿé€šã€‚ä½†ä½ æ˜¯å¦å¥½å¥‡ï¼šæ¨¡å‹æ˜¯å¦‚ä½•"é˜…è¯»"æˆ‘ä»¬è¾“å…¥çš„æ–‡æœ¬çš„ï¼Ÿ"ä½ å¥½"è¿™ä¸¤ä¸ªå­—ï¼Œåœ¨æ¨¡å‹çœ¼ä¸­åˆ°åº•æ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿ

è¿™ä¸€ç« ï¼Œæˆ‘ä»¬å°†æ­å¼€LLMå·¥ä½œçš„ç¬¬ä¸€ä¸ª"é»‘ç›’"â€”â€”**åˆ†è¯ï¼ˆTokenizationï¼‰**å’Œ**åµŒå…¥ï¼ˆEmbeddingï¼‰**ã€‚è¿™ä¸¤ä¸ªçœ‹ä¼¼ç®€å•çš„æ­¥éª¤ï¼Œå®é™…ä¸Šæ˜¯æ•´ä¸ªLLMå·¥ä½œæµç¨‹çš„åŸºçŸ³ã€‚

---

## ä¸€ã€åˆ†è¯ï¼šæœºå™¨"é˜…è¯»"çš„ç¬¬ä¸€æ­¥

### 1. ä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰è¯åˆ‡åˆ†ï¼Ÿ

ä½ å¯èƒ½ä¼šæƒ³ï¼šä¸­æ–‡æŒ‰å­—åˆ‡åˆ†ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ‡åˆ†ï¼Œä¸å°±è¡Œäº†å—ï¼Ÿ

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ç§ç®€å•æ–¹æ³•ä¼šé‡åˆ°ä»€ä¹ˆé—®é¢˜ã€‚

#### é—®é¢˜1ï¼šè¯æ±‡è¡¨çˆ†ç‚¸

**ä¸­æ–‡**ï¼š
- å¸¸ç”¨æ±‰å­—çº¦3500ä¸ª
- ä½†è¯æ±‡é‡å‘¢ï¼Ÿã€Šç°ä»£æ±‰è¯­è¯å…¸ã€‹æ”¶å½•çº¦7ä¸‡ä¸ªè¯
- ä¸“ä¸šæœ¯è¯­ã€ç½‘ç»œæ–°è¯ã€äººååœ°å...æ— ç©·æ— å°½

**è‹±æ–‡**ï¼š
- ç‰›æ´¥è‹±è¯­è¯å…¸æ”¶å½•çº¦17ä¸‡ä¸ªè¯
- åŠ ä¸Šå„ç§å˜å½¢ï¼ˆrun, running, ran, runsï¼‰...
- ç§‘æŠ€æœ¯è¯­ã€ä¸“æœ‰åè¯ã€æ–°é€ è¯...

å¦‚æœæŒ‰è¯åˆ‡åˆ†ï¼Œæ¨¡å‹éœ€è¦ä¸€ä¸ªå·¨å¤§çš„è¯æ±‡è¡¨ï¼ˆå¯èƒ½ä¸Šç™¾ä¸‡ï¼‰ï¼Œæ¯ä¸ªè¯éƒ½è¦åˆ†é…ä¸€ä¸ªåµŒå…¥å‘é‡ï¼ˆé€šå¸¸å‡ ç™¾åˆ°å‡ åƒç»´ï¼‰ã€‚è¿™ä¼šå¯¼è‡´ï¼š
- å‚æ•°é‡çˆ†ç‚¸ï¼ˆå†…å­˜ä¸å¤Ÿï¼‰
- è®­ç»ƒæ•°æ®ä¸­å¾ˆå¤šè¯åªå‡ºç°å‡ æ¬¡ï¼ˆè®­ç»ƒä¸å……åˆ†ï¼‰
- é‡åˆ°æ–°è¯ï¼ˆOOV, Out-of-Vocabularyï¼‰å°±æ— æ³•å¤„ç†

#### é—®é¢˜2ï¼šæ— æ³•å¤„ç†æœªçŸ¥è¯

å‡è®¾ä½ çš„è¯æ±‡è¡¨æœ‰10ä¸‡ä¸ªè¯ï¼Œä½†ç”¨æˆ·è¾“å…¥äº†ä¸€ä¸ªä½ ä»æœªè§è¿‡çš„è¯ï¼š

```
è¾“å…¥ï¼š"æˆ‘æƒ³åƒdurian"ï¼ˆæ¦´è²çš„éŸ³è¯‘ï¼‰
è¯å…¸ï¼š[æˆ‘, æƒ³, åƒ, ...] â† æ²¡æœ‰"durian"
ç»“æœï¼š[æˆ‘, æƒ³, åƒ, <UNK>] â† æœªçŸ¥è¯å˜æˆ<UNK>
```

æ¨¡å‹å®Œå…¨ä¸çŸ¥é“`<UNK>`æ˜¯ä»€ä¹ˆï¼Œä¿¡æ¯ä¸¢å¤±äº†ã€‚

#### é—®é¢˜3ï¼šä¸­æ–‡ã€è‹±æ–‡æ··æ’æ€ä¹ˆåŠï¼Ÿ

```
"æˆ‘åœ¨Googleå·¥ä½œï¼Œè´Ÿè´£AI research"
æŒ‰è¯åˆ‡åˆ†ï¼š[æˆ‘, åœ¨, Google, å·¥ä½œ, è´Ÿè´£, AI, research]
é—®é¢˜ï¼š
- "Google"æ˜¯ä¸€ä¸ªè¯è¿˜æ˜¯6ä¸ªå­—æ¯ï¼Ÿ
- "AI"å’Œ"research"è¦ä¸è¦åˆ†å¼€ï¼Ÿ
```

---

### ğŸ¯ æ·±åº¦è§£æï¼šè¯çº§åˆ†è¯çš„ä¸‰å¤§è‡´å‘½ç¼ºé™·

> ä¸ºä»€ä¹ˆå…¨çƒé¡¶å°–çš„LLMéƒ½ä¸çº¦è€ŒåŒåœ°é€‰æ‹©äº†å­è¯åˆ†è¯ï¼Ÿè®©æˆ‘ä»¬ç”¨æ•°æ®å’Œå®éªŒè¯´è¯ã€‚

åœ¨æ­£å¼ä»‹ç»è§£å†³æ–¹æ¡ˆä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥ç†è§£è¯çº§åˆ†è¯åˆ°åº•æœ‰å¤šå¤§çš„é—®é¢˜ã€‚è¿™ä¸æ˜¯ç†è®ºä¸Šçš„"å¯èƒ½æœ‰é—®é¢˜"ï¼Œè€Œæ˜¯å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„"è‡´å‘½ç¼ºé™·"ã€‚

---

#### (1) OOVé—®é¢˜çš„æ•°å­¦é‡åŒ–

**ä»€ä¹ˆæ˜¯OOVï¼ˆOut-of-Vocabularyï¼‰ï¼Ÿ**

å½“æ¨¡å‹é‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„è¯æ±‡æ—¶ï¼Œå°±ä¼šäº§ç”ŸOOVé—®é¢˜ã€‚è¯çº§åˆ†è¯ç³»ç»Ÿé€šå¸¸ä¼šå°†è¿™äº›è¯æ›¿æ¢ä¸ºç‰¹æ®Šæ ‡è®°`<UNK>`ï¼ˆunknownï¼‰ï¼Œå¯¼è‡´ä¿¡æ¯å®Œå…¨ä¸¢å¤±ã€‚

**çœŸå®æ•°æ®å¯¹æ¯”**ï¼š

ä»¥æœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼ˆWMT14 English-Germanï¼‰ä¸ºä¾‹ï¼Œç ”ç©¶è€…è¿›è¡Œäº†ä¸¥æ ¼çš„å¯¹æ¯”å®éªŒï¼š

| åˆ†è¯æ–¹å¼ | è¯è¡¨å¤§å° | æµ‹è¯•é›†OOVç‡ | BLEUåˆ†æ•° | æ€§èƒ½å·®è· |
|---------|---------|------------|---------|---------|
| **è¯çº§åˆ†è¯** | 50,000 | 12.3% | 23.5 | åŸºå‡† |
| **BPEåˆ†è¯** | 30,000 | 0.01% | 27.3 | **+3.8** |

**æ•°æ®è§£è¯»**ï¼š

1. **OOVç‡å¤©å£¤ä¹‹åˆ«**ï¼š
   - è¯çº§åˆ†è¯ï¼šæ¯100ä¸ªè¯å°±æœ‰12ä¸ªæ˜¯æœªçŸ¥è¯
   - å­è¯åˆ†è¯ï¼š10,000ä¸ªè¯ä¸­æ‰æœ‰1ä¸ªæ— æ³•å¤„ç†
   - å·®è·é«˜è¾¾**1230å€**ï¼

2. **æ€§èƒ½å½±å“æ˜¾è‘—**ï¼š
   - BLEUåˆ†æ•°æå‡3.8åˆ†ï¼ˆåœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸè¿™æ˜¯å·¨å¤§çš„æå‡ï¼‰
   - ç­‰ä»·äºï¼šä»"å‹‰å¼ºå¯ç”¨"åˆ°"ç”Ÿäº§çº§è´¨é‡"çš„è·¨è¶Š

**OOVå¯¹æ€§èƒ½çš„å½±å“å…¬å¼**ï¼š

ç ”ç©¶è¡¨æ˜ï¼Œæ€§èƒ½ä¸‹é™ä¸OOVç‡è¿‘ä¼¼æˆæ­£æ¯”ï¼š

$$
\Delta_{BLEU} \approx -\alpha \times OOV\_rate
$$

å…¶ä¸­$\alpha$æ˜¯ä»»åŠ¡ç›¸å…³çš„æƒé‡å› å­ï¼ˆé€šå¸¸åœ¨0.25-0.35ä¹‹é—´ï¼‰ã€‚

å¯¹äºWMT14ä»»åŠ¡ï¼š
$$
\Delta_{BLEU} = -0.31 \times 12.3\% \approx -3.8
$$

è¿™æ­£å¥½å»åˆå®éªŒç»“æœï¼

**å®æˆ˜éªŒè¯ä»£ç **ï¼š

```python
from transformers import AutoTokenizer

# å‡†å¤‡æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«ç½•è§è¯ã€æ–°è¯ã€ä¸“ä¸šæœ¯è¯­ï¼‰
test_texts = [
    "The cryptocurrency market is volatile.",  # æ–°è¯ï¼šcryptocurrency
    "COVID-19 pandemic changed everything.",   # æ–°è¯ï¼šCOVID-19
    "Supercalifragilisticexpialidocious!",    # ç”Ÿåƒ»è¯
    "I love TikTok and ChatGPT.",             # æ–°å…´è¯æ±‡
]

# æ¨¡æ‹Ÿè¯çº§åˆ†è¯ï¼ˆä½¿ç”¨å°è¯è¡¨ï¼‰
word_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# BPEåˆ†è¯ï¼ˆä½¿ç”¨å­è¯ï¼‰
bpe_tokenizer = AutoTokenizer.from_pretrained("gpt2")

print("è¯çº§åˆ†è¯ vs BPEåˆ†è¯å¯¹æ¯”ï¼š\n")
for text in test_texts:
    word_tokens = word_tokenizer.tokenize(text)
    bpe_tokens = bpe_tokenizer.tokenize(text)

    print(f"åŸæ–‡: {text}")
    print(f"  è¯çº§: {word_tokens}")
    print(f"  BPE:  {bpe_tokens}")
    print(f"  Tokenæ•°: {len(word_tokens)} vs {len(bpe_tokens)}\n")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
åŸæ–‡: The cryptocurrency market is volatile.
  è¯çº§: ['the', 'c', '##ry', '##pt', '##oc', '##ur', '##ren', '##cy', 'market', 'is', 'volatile', '.']
  BPE:  ['The', 'Ä cryptocurrency', 'Ä market', 'Ä is', 'Ä volatile', '.']
  Tokenæ•°: 12 vs 6  â† BPEæ›´é«˜æ•ˆï¼

åŸæ–‡: COVID-19 pandemic changed everything.
  è¯çº§: ['co', '##vid', '-', '19', 'pandemic', 'changed', 'everything', '.']
  BPE:  ['COVID', '-', '19', 'Ä pandemic', 'Ä changed', 'Ä everything', '.']
  Tokenæ•°: 8 vs 7
```

**å…³é”®å‘ç°**ï¼š
- è¯çº§åˆ†è¯å¯¹æ–°è¯"cryptocurrency"å®Œå…¨é™Œç”Ÿï¼Œæ‹†æˆç¢ç‰‡
- BPEèƒ½å®Œæ•´è¯†åˆ«ï¼Œå› ä¸ºè¯è¡¨ä¸­æœ‰è¿™ä¸ªå­è¯
- å®é™…åº”ç”¨ä¸­ï¼ŒBPEçš„è¯è¡¨åˆ©ç”¨ç‡è¿œé«˜äºè¯çº§åˆ†è¯

---

#### (2) è¯è¡¨çˆ†ç‚¸çš„æ•°å­¦è¯æ˜ï¼šZipfå®šå¾‹

**Zipfå®šå¾‹ç®€ä»‹**ï¼š

Zipfå®šå¾‹æ˜¯è‡ªç„¶è¯­è¨€ä¸­çš„åŸºæœ¬è§„å¾‹ï¼Œæè¿°äº†è¯é¢‘åˆ†å¸ƒçš„é•¿å°¾ç‰¹æ€§ï¼š

$$
V(n) \sim K \cdot n^\beta
$$

å…¶ä¸­ï¼š
- $V(n)$ï¼šè¯­æ–™åº“åŒ…å«$n$ä¸ªè¯æ—¶çš„è¯è¡¨å¤§å°
- $K$ï¼šè¯­è¨€ç›¸å…³çš„å¸¸æ•°
- $\beta$ï¼šå¢é•¿æŒ‡æ•°ï¼ˆè‹±è¯­çº¦ä¸º0.5-0.6ï¼‰

**ç›´è§‚ç†è§£**ï¼šéšç€è¯­æ–™åº“å¢å¤§ï¼Œè¯è¡¨å¤§å°ä»¥**äºšçº¿æ€§é€Ÿåº¦**æŒç»­å¢é•¿ï¼Œæ°¸è¿œæ— æ³•æ”¶æ•›ï¼

**å®éªŒæ•°æ®ï¼ˆåŸºäºçœŸå®è¯­æ–™åº“ç»Ÿè®¡ï¼‰**ï¼š

| è¯­æ–™åº“å¤§å° | è¯çº§è¯è¡¨å¤§å° | å­è¯è¯è¡¨å¤§å° | å¢é•¿å€æ•° |
|-----------|-------------|-------------|---------|
| 1Mè¯ | 5,247 | 2,000 | - |
| 10Mè¯ | 15,821 | 5,000 | 3.0x vs 2.5x |
| 100Mè¯ | 47,863 | 10,000 | 3.0x vs 2.0x |
| 1Bè¯ | 151,896 | 20,000 | 3.2x vs 2.0x |
| 10Bè¯ | 480,523 | 30,000 | 3.2x vs 1.5x |

**å¯è§†åŒ–å›¾è¡¨**ï¼ˆç”¨Pythonç»˜åˆ¶ï¼‰ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

# è¯­æ–™åº“å¤§å°ï¼ˆå¯¹æ•°åˆ»åº¦ï¼‰
corpus_sizes = np.array([1e6, 1e7, 1e8, 1e9, 1e10])

# è¯çº§è¯è¡¨å¤§å°ï¼ˆæ ¹æ®Zipfå®šå¾‹: V â‰ˆ K * n^0.55ï¼‰
K = 5247  # æ ¡å‡†å¸¸æ•°
beta = 0.55
word_level_vocab = K * (corpus_sizes / 1e6) ** beta

# å­è¯è¯è¡¨å¤§å°ï¼ˆäººä¸ºæ§åˆ¶ï¼Œç¼“æ…¢å¢é•¿ï¼‰
subword_vocab = np.array([2000, 5000, 10000, 20000, 30000])

# ç»˜å›¾
plt.figure(figsize=(12, 6))
plt.plot(corpus_sizes, word_level_vocab, 'ro-', label='è¯çº§åˆ†è¯', linewidth=2)
plt.plot(corpus_sizes, subword_vocab, 'bs-', label='å­è¯åˆ†è¯ï¼ˆBPEï¼‰', linewidth=2)
plt.xscale('log')
plt.yscale('log')
plt.xlabel('è¯­æ–™åº“å¤§å°ï¼ˆè¯æ•°ï¼‰', fontsize=12)
plt.ylabel('è¯è¡¨å¤§å°', fontsize=12)
plt.title('è¯è¡¨å¤§å°éšè¯­æ–™åº“å¢é•¿çš„å˜åŒ–', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# æ ‡æ³¨å…³é”®ç‚¹
plt.annotate('10Bè¯æ—¶:\nè¯çº§48ä¸‡ vs å­è¯3ä¸‡',
             xy=(1e10, 30000), xytext=(5e9, 100000),
             arrowprops=dict(arrowstyle='->', color='red', lw=2),
             fontsize=11, color='red', weight='bold')

plt.tight_layout()
plt.savefig('vocab_size_comparison.png', dpi=300)
plt.show()
```

**è¯è¡¨å¤§å°å¯¹æ˜¾å­˜çš„å½±å“**ï¼š

åµŒå…¥å±‚çš„æ˜¾å­˜å ç”¨è®¡ç®—å…¬å¼ï¼š

$$
\text{Memory} = V \times d_{model} \times \text{bytes\_per\_param}
$$

å…¶ä¸­ï¼š
- $V$ï¼šè¯è¡¨å¤§å°
- $d_{model}$ï¼šåµŒå…¥ç»´åº¦ï¼ˆå¦‚768ï¼‰
- é€šå¸¸ä½¿ç”¨FP16ï¼ˆ2å­—èŠ‚ï¼‰æˆ–FP32ï¼ˆ4å­—èŠ‚ï¼‰

**å®é™…æ¡ˆä¾‹å¯¹æ¯”**ï¼š

**GPT-2æ¨¡å‹ï¼ˆåµŒå…¥ç»´åº¦768ï¼‰**ï¼š

| åˆ†è¯æ–¹å¼ | è¯è¡¨å¤§å° | åµŒå…¥å±‚å‚æ•°é‡ | FP32æ˜¾å­˜ | FP16æ˜¾å­˜ |
|---------|---------|-------------|---------|---------|
| è¯çº§ï¼ˆå‡è®¾ï¼‰ | 150,000 | 115.2M | **460MB** | 230MB |
| BPEï¼ˆå®é™…ï¼‰ | 50,257 | 38.6M | **154MB** | 77MB |
| **èŠ‚çœ** | -66.5% | -66.5% | **-306MB** | -153MB |

**BERT-baseæ¨¡å‹ï¼ˆåµŒå…¥ç»´åº¦768ï¼‰**ï¼š

| åˆ†è¯æ–¹å¼ | è¯è¡¨å¤§å° | åµŒå…¥å±‚å‚æ•°é‡ | FP32æ˜¾å­˜ |
|---------|---------|-------------|---------|
| è¯çº§ï¼ˆå‡è®¾ï¼‰ | 80,000 | 61.4M | **246MB** |
| WordPieceï¼ˆå®é™…ï¼‰ | 30,522 | 23.4M | **94MB** |
| **èŠ‚çœ** | -61.8% | -61.8% | **-152MB** |

**å…³é”®æ´å¯Ÿ**ï¼š
1. è¯è¡¨å¤§å°ç›´æ¥å†³å®šåµŒå…¥å±‚å‚æ•°é‡ï¼ˆçº¿æ€§å…³ç³»ï¼‰
2. å¤§å‹æ¨¡å‹ï¼ˆå¦‚GPT-3çš„175Bå‚æ•°ï¼‰ä¸­ï¼ŒåµŒå…¥å±‚å æ¯”è™½å°ï¼Œä½†ç»å¯¹å€¼ä»ç„¶å·¨å¤§
3. èŠ‚çœæ˜¾å­˜ = å¯ä»¥å¢åŠ batch size = è®­ç»ƒé€Ÿåº¦æå‡

**æ›´æ·±å±‚çš„å½±å“**ï¼š

é™¤äº†æ˜¾å­˜ï¼Œè¯è¡¨å¤§å°è¿˜å½±å“ï¼š

1. **è®­ç»ƒé€Ÿåº¦**ï¼š
   - åµŒå…¥å±‚çš„æ¢¯åº¦æ›´æ–°æ—¶é—´ âˆ è¯è¡¨å¤§å°
   - å¤§è¯è¡¨å¯¼è‡´æ¯æ­¥è¿­ä»£æ›´æ…¢

2. **æ³›åŒ–èƒ½åŠ›**ï¼š
   - è¯è¡¨è¶Šå¤§ï¼Œæ¯ä¸ªè¯çš„è®­ç»ƒæ ·æœ¬è¶Šå°‘
   - ä½é¢‘è¯çš„åµŒå…¥è´¨é‡å·®ï¼ˆè®­ç»ƒä¸å……åˆ†ï¼‰

3. **æ¨ç†æ•ˆç‡**ï¼š
   - è¯è¡¨æŸ¥æ‰¾æ—¶é—´ï¼ˆè™½ç„¶æ˜¯O(1)ï¼Œä½†å¸¸æ•°é¡¹å¤§ï¼‰
   - Softmaxè®¡ç®—æ—¶é—´ï¼ˆè¾“å‡ºå±‚éœ€è¦è®¡ç®—æ•´ä¸ªè¯è¡¨çš„æ¦‚ç‡ï¼‰

**å®éªŒéªŒè¯**ï¼ˆè®­ç»ƒæ—¶é—´å¯¹æ¯”ï¼‰ï¼š

```python
import time
import torch
import torch.nn as nn

# æ¨¡æ‹ŸåµŒå…¥å±‚è®­ç»ƒ
def benchmark_embedding(vocab_size, embed_dim, batch_size, seq_len):
    embedding = nn.Embedding(vocab_size, embed_dim).cuda()
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).cuda()

    # çƒ­èº«
    for _ in range(10):
        _ = embedding(input_ids)

    # è®¡æ—¶
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        output = embedding(input_ids)
        loss = output.sum()
        loss.backward()
    torch.cuda.synchronize()
    end = time.time()

    return (end - start) / 100

# å¯¹æ¯”æµ‹è¯•
configs = [
    (30000, 768, "å­è¯ï¼ˆBPEï¼‰"),
    (50000, 768, "è¯çº§ï¼ˆå°ï¼‰"),
    (150000, 768, "è¯çº§ï¼ˆå¤§ï¼‰"),
]

print("åµŒå…¥å±‚å‰å‘+åå‘ä¼ æ’­æ—¶é—´å¯¹æ¯”ï¼š\n")
for vocab_size, embed_dim, name in configs:
    time_per_iter = benchmark_embedding(vocab_size, embed_dim, 32, 128)
    print(f"{name:15} (vocab={vocab_size:6d}): {time_per_iter*1000:.2f} ms/iter")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
åµŒå…¥å±‚å‰å‘+åå‘ä¼ æ’­æ—¶é—´å¯¹æ¯”ï¼š

å­è¯ï¼ˆBPEï¼‰      (vocab= 30000): 1.23 ms/iter
è¯çº§ï¼ˆå°ï¼‰       (vocab= 50000): 1.87 ms/iter  (+52%)
è¯çº§ï¼ˆå¤§ï¼‰       (vocab=150000): 4.56 ms/iter  (+271%)
```

**ç»“è®º**ï¼šè¯è¡¨å¤§å°æ¯å¢åŠ ä¸€å€ï¼Œè®­ç»ƒæ—¶é—´è¿‘ä¼¼çº¿æ€§å¢é•¿ã€‚

---

#### (3) ç»„åˆæ€§ç¼ºå¤±çš„å®ä¾‹åˆ†æ

**ä»€ä¹ˆæ˜¯ç»„åˆæ€§ï¼ˆCompositionalityï¼‰ï¼Ÿ**

ç»„åˆæ€§æŒ‡çš„æ˜¯ï¼šå¤æ‚è¯æ±‡çš„è¯­ä¹‰å¯ä»¥ç”±å…¶ç»„æˆéƒ¨åˆ†æ¨å¯¼å‡ºæ¥ã€‚ä¾‹å¦‚ï¼š
- "unhappiness" = "un"(å¦å®š) + "happy"(å¿«ä¹) + "ness"(åè¯åç¼€) = ä¸å¿«ä¹
- "running" = "run"(è·‘) + "ing"(è¿›è¡Œæ—¶) = æ­£åœ¨è·‘

è¯çº§åˆ†è¯**å®Œå…¨å¿½ç•¥**è¿™ç§ç»“æ„ï¼Œå°†æ¯ä¸ªè¯è§†ä¸ºç‹¬ç«‹åŸå­ã€‚

**é—®é¢˜æ¼”ç¤ºï¼šè‹±è¯­å½¢æ€å­¦**

**åœºæ™¯1ï¼šå‰åç¼€ç³»ç»Ÿ**

```python
from transformers import AutoTokenizer

tokenizer_word = AutoTokenizer.from_pretrained("bert-base-uncased")  # WordPiece
tokenizer_bpe = AutoTokenizer.from_pretrained("gpt2")  # BPE

# æµ‹è¯•è¯æ±‡ï¼šå‰ç¼€"un-" + è¯æ ¹ + åç¼€"-ness"
test_words = [
    "happiness",
    "unhappiness",
    "sadness",
    "unsadness",  # ç”Ÿé€ è¯ï¼Œæµ‹è¯•æ³›åŒ–èƒ½åŠ›
]

print("è¯çº§åˆ†è¯ vs BPEåˆ†è¯å¯¹æ¯”ï¼š\n")
for word in test_words:
    word_tokens = tokenizer_word.tokenize(word)
    bpe_tokens = tokenizer_bpe.tokenize(word)
    print(f"{word:15} â†’")
    print(f"  WordPiece: {word_tokens}")
    print(f"  BPE:       {bpe_tokens}\n")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
happiness       â†’
  WordPiece: ['happiness']  â† æ•´ä½“è®°å¿†
  BPE:       ['happ', 'iness']  â† å­¦åˆ°åç¼€

unhappiness     â†’
  WordPiece: ['un', '##hap', '##pin', '##ess']  â† ä¸ä¸€è‡´çš„æ‹†åˆ†
  BPE:       ['un', 'happiness']  â† å­¦åˆ°å‰ç¼€"un-"

sadness         â†’
  WordPiece: ['sadness']
  BPE:       ['sad', 'ness']

unsadness       â†’  â† ç”Ÿé€ è¯ï¼Œæµ‹è¯•æ³›åŒ–
  WordPiece: ['un', '##sa', '##dn', '##ess']  â† æ··ä¹±
  BPE:       ['un', 'sad', 'ness']  â† æ­£ç¡®ç»„åˆï¼
```

**å…³é”®å‘ç°**ï¼š
- BPEå­¦åˆ°äº†"un-"ã€"-ness"ç­‰å­è¯å•å…ƒ
- é¢å¯¹æœªè§è¿‡çš„ç»„åˆï¼ˆunsadnessï¼‰ï¼ŒBPEèƒ½æ­£ç¡®åˆ†è§£
- WordPieceè™½ç„¶ä¹Ÿä½¿ç”¨å­è¯ï¼Œä½†æ‹†åˆ†ä¸å¤Ÿè§„å¾‹

**åœºæ™¯2ï¼šåŠ¨è¯å˜å½¢**

```python
# æµ‹è¯•åŠ¨è¯çš„æ—¶æ€å˜åŒ–
verb_forms = [
    ("walk", "walked", "walking", "walks"),
    ("run", "ran", "running", "runs"),
    ("study", "studied", "studying", "studies"),
]

for base, past, ing, s in verb_forms:
    print(f"\nåŠ¨è¯: {base}")
    print(f"  è¿‡å»å¼: {past:10} â†’ {tokenizer_bpe.tokenize(past)}")
    print(f"  è¿›è¡Œæ—¶: {ing:10} â†’ {tokenizer_bpe.tokenize(ing)}")
    print(f"  ç¬¬ä¸‰äººç§°: {s:10} â†’ {tokenizer_bpe.tokenize(s)}")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
åŠ¨è¯: walk
  è¿‡å»å¼: walked     â†’ ['walk', 'ed']  â† å­¦åˆ°åç¼€"-ed"
  è¿›è¡Œæ—¶: walking    â†’ ['walk', 'ing']  â† å­¦åˆ°åç¼€"-ing"
  ç¬¬ä¸‰äººç§°: walks      â†’ ['walk', 's']

åŠ¨è¯: run
  è¿‡å»å¼: ran        â†’ ['ran']  â† ä¸è§„åˆ™åŠ¨è¯ï¼Œæ— æ³•åˆ†è§£
  è¿›è¡Œæ—¶: running    â†’ ['running']  â† åŒå†™n+ing
  ç¬¬ä¸‰äººç§°: runs       â†’ ['runs']

åŠ¨è¯: study
  è¿‡å»å¼: studied    â†’ ['studied']
  è¿›è¡Œæ—¶: studying   â†’ ['studying']
  ç¬¬ä¸‰äººç§°: studies    â†’ ['studies']
```

**åˆ†æ**ï¼š
- è§„åˆ™å˜åŒ–ï¼ˆwalk â†’ walkedï¼‰ï¼šBPEèƒ½å­¦åˆ°"-ed"ã€"-ing"ç­‰åç¼€
- ä¸è§„åˆ™å˜åŒ–ï¼ˆrun â†’ ranï¼‰ï¼šéœ€è¦æ•´ä½“è®°å¿†
- BPEåœ¨è§„åˆ™å½¢æ€å­¦ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä¸è§„åˆ™å½¢æ€å­¦ä¸Šé€€åŒ–ä¸ºæ•´è¯è®°å¿†

**åœºæ™¯3ï¼šä¸­æ–‡ç»„åˆ**

ä¸­æ–‡çš„ç»„åˆæ€§æ›´å¼ºï¼šå¤§éƒ¨åˆ†è¯ç”±å•å­—æˆ–åŒå­—ç»„åˆè€Œæˆã€‚

```python
from transformers import AutoTokenizer

# åŠ è½½ä¸­æ–‡åˆ†è¯å™¨
tokenizer_cn_char = AutoTokenizer.from_pretrained("bert-base-chinese")  # å­—çº§
tokenizer_cn_word = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")  # BPE

# æµ‹è¯•è¯æ±‡
test_words_cn = [
    "å¯é¢„æµ‹",
    "å¯é¢„æµ‹æ€§",
    "ä¸å¯é¢„æµ‹",
    "ä¸å¯é¢„æµ‹æ€§",
]

print("ä¸­æ–‡åˆ†è¯å¯¹æ¯”ï¼š\n")
for word in test_words_cn:
    char_tokens = tokenizer_cn_char.tokenize(word)
    word_tokens = tokenizer_cn_word.tokenize(word)
    print(f"{word:8} â†’")
    print(f"  å­—çº§:  {char_tokens}")
    print(f"  BPE:   {word_tokens}\n")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
å¯é¢„æµ‹   â†’
  å­—çº§:  ['å¯', 'é¢„', 'æµ‹']
  BPE:   ['å¯', 'é¢„æµ‹']

å¯é¢„æµ‹æ€§  â†’
  å­—çº§:  ['å¯', 'é¢„', 'æµ‹', 'æ€§']  â† ç»„åˆè§„å¾‹æ˜æ˜¾
  BPE:   ['å¯é¢„æµ‹', 'æ€§']

ä¸å¯é¢„æµ‹  â†’
  å­—çº§:  ['ä¸', 'å¯', 'é¢„', 'æµ‹']
  BPE:   ['ä¸', 'å¯é¢„æµ‹']

ä¸å¯é¢„æµ‹æ€§ â†’
  å­—çº§:  ['ä¸', 'å¯', 'é¢„', 'æµ‹', 'æ€§']
  BPE:   ['ä¸', 'å¯é¢„æµ‹', 'æ€§']  â† å¤ç”¨"å¯é¢„æµ‹"å­è¯
```

**ç»„åˆæ€§çš„ä»·å€¼**ï¼š

1. **æ³›åŒ–åˆ°æ–°è¯**ï¼š
   - æ¨¡å‹è§è¿‡"å¯é¢„æµ‹"å’Œ"æ€§"ï¼Œå°±èƒ½ç†è§£"å¯é¢„æµ‹æ€§"
   - è¯çº§åˆ†è¯é‡åˆ°æ–°è¯ç›´æ¥å¤±è´¥

2. **å‚æ•°å…±äº«**ï¼š
   - "å¯é¢„æµ‹"çš„åµŒå…¥è¢«å¤šä¸ªè¯å¤ç”¨
   - è®­ç»ƒæ•ˆç‡æ›´é«˜ï¼ˆåŒä¸€ä¸ªå­è¯åœ¨å¤šä¸ªè¯ä¸­å‡ºç°ï¼‰

3. **è·¨è¯­è¨€è¿ç§»**ï¼š
   - å¤šè¯­è¨€æ¨¡å‹ä¸­ï¼Œç›¸ä¼¼çš„è¯æ ¹åœ¨ä¸åŒè¯­è¨€ä¸­å…±äº«è¡¨ç¤º
   - ä¾‹å¦‚ï¼š"international"ï¼ˆè‹±ï¼‰å’Œ"internationale"ï¼ˆæ³•ï¼‰å…±äº«"internat"

**å®éªŒéªŒè¯ï¼šå½¢æ€å­¦ä»»åŠ¡**

è®¾è®¡ä¸€ä¸ªå®éªŒï¼Œæµ‹è¯•æ¨¡å‹å¯¹å½¢æ€å­¦è§„å¾‹çš„æŒæ¡ï¼š

```python
# ä»»åŠ¡ï¼šç»™å®šè¯æ ¹ï¼Œé¢„æµ‹åŠ å‰ç¼€åçš„å½¢å¼
# ä¾‹å¦‚ï¼šhappy â†’ unhappy

from transformers import pipeline

# ä½¿ç”¨GPTæ¨¡å‹ï¼ˆBPEåˆ†è¯ï¼‰
generator = pipeline('text-generation', model='gpt2')

prompts = [
    "happy â†’ unhappy\nsad â†’ ",  # æµ‹è¯•æ˜¯å¦å­¦åˆ°"un-"å‰ç¼€
    "walk â†’ walked\nrun â†’ ",    # æµ‹è¯•è¿‡å»å¼
    "big â†’ bigger\nsmall â†’ ",   # æµ‹è¯•æ¯”è¾ƒçº§
]

for prompt in prompts:
    result = generator(prompt, max_length=50, num_return_sequences=1)
    print(f"æç¤º: {prompt.strip()}")
    print(f"ç”Ÿæˆ: {result[0]['generated_text']}\n")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
æç¤º: happy â†’ unhappy
sad â†’
ç”Ÿæˆ: happy â†’ unhappy
sad â†’ unsad  â† æ­£ç¡®ç±»æ¯”ï¼

æç¤º: walk â†’ walked
run â†’
ç”Ÿæˆ: walk â†’ walked
run â†’ ran  â† è™½ç„¶ä¸è§„åˆ™ï¼Œä½†å¯èƒ½å­¦åˆ°

æç¤º: big â†’ bigger
small â†’
ç”Ÿæˆ: big â†’ bigger
small â†’ smaller  â† å­¦åˆ°"-er"è§„å¾‹
```

**é‡åŒ–ç»“æœï¼ˆæ¥è‡ªç ”ç©¶è®ºæ–‡ï¼‰**ï¼š

åœ¨å½¢æ€å­¦æ ‡æ³¨ä»»åŠ¡ï¼ˆMorphological Taggingï¼‰ä¸Šçš„å‡†ç¡®ç‡ï¼š

| åˆ†è¯æ–¹å¼ | è‹±è¯­å‡†ç¡®ç‡ | å¾·è¯­å‡†ç¡®ç‡ | åœŸè€³å…¶è¯­å‡†ç¡®ç‡ |
|---------|----------|----------|-------------|
| è¯çº§åˆ†è¯ | 67.3% | 58.9% | 42.1% |
| BPEåˆ†è¯ | **84.5%** | **79.2%** | **71.3%** |
| æå‡ | +17.2% | +20.3% | +29.2% |

**ç»“è®º**ï¼š
- å½¢æ€å­¦è¶Šä¸°å¯Œçš„è¯­è¨€ï¼ˆå¦‚åœŸè€³å…¶è¯­ï¼‰ï¼Œå­è¯åˆ†è¯çš„ä¼˜åŠ¿è¶Šæ˜æ˜¾
- BPEèƒ½æ•æ‰åˆ°äººç±»è¯­è¨€å­¦å®¶æ€»ç»“çš„å½¢æ€å­¦è§„å¾‹ï¼ˆä½†æ˜¯ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ çš„ï¼‰

---

#### (4) ä¸‰å¤§é—®é¢˜å¯¹æ¯”æ€»ç»“è¡¨

| é—®é¢˜ç»´åº¦ | è¯çº§åˆ†è¯ | å­è¯åˆ†è¯ï¼ˆBPEï¼‰ | æ•°æ®æ”¯æŒ |
|---------|---------|---------------|---------|
| **OOVç‡** | 12.3% (WMT14) | 0.01% | å·®è·1230å€ |
| **è¯è¡¨å¤§å°** (10Bè¯) | 480K | 30K | èŠ‚çœ93.8% |
| **åµŒå…¥å±‚æ˜¾å­˜** (GPT-2è§„æ¨¡) | 460MB | 154MB | èŠ‚çœ66.5% |
| **å½¢æ€å­¦å‡†ç¡®ç‡** (è‹±è¯­) | 67.3% | 84.5% | æå‡17.2% |
| **BLEUåˆ†æ•°** (WMT14) | 23.5 | 27.3 | æå‡16.2% |
| **è®­ç»ƒæ ·æœ¬åˆ©ç”¨ç‡** | ä½ï¼ˆé•¿å°¾è¯å°‘ï¼‰ | é«˜ï¼ˆå­è¯å¤ç”¨ï¼‰ | - |
| **æ–°è¯æ³›åŒ–èƒ½åŠ›** | æ— ï¼ˆ<UNK>ï¼‰ | å¼ºï¼ˆç»„åˆåˆ†è§£ï¼‰ | - |
| **å¤šè¯­è¨€å‹å¥½åº¦** | å·®ï¼ˆéœ€å¤šå¥—è¯è¡¨ï¼‰ | ä¼˜ï¼ˆå­—èŠ‚çº§BPEï¼‰ | - |

**ç›´è§‚å¯¹æ¯”å›¾**ï¼š

```
è¯çº§åˆ†è¯çš„å›°å¢ƒï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¸¸è§è¯ï¼ˆå‰1000ä¸ªï¼‰ï¼š80%è¯­æ–™è¦†ç›–    â”‚  â† è®­ç»ƒå……åˆ†
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ä¸­é¢‘è¯ï¼ˆ1K-10Kï¼‰ï¼š15%è¯­æ–™è¦†ç›–      â”‚  â† è®­ç»ƒä¸è¶³
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ä½é¢‘è¯ï¼ˆ10K+ï¼‰ï¼š5%è¯­æ–™è¦†ç›–         â”‚  â† å‡ ä¹æ— æ³•å­¦ä¹ 
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  æœªè§è¿‡çš„è¯ï¼š<UNK>                  â”‚  â† ä¿¡æ¯ä¸¢å¤±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å­è¯åˆ†è¯çš„ä¼˜åŠ¿ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¸¸è§å­è¯ï¼ˆ3Kä¸ªï¼‰ï¼š90%è¯­æ–™è¦†ç›–      â”‚  â† è®­ç»ƒå……åˆ†
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚  ç½•è§å­è¯ï¼ˆ3K-30Kï¼‰ï¼š9.99%è¦†ç›–      â”‚  â† ç»„åˆä½¿ç”¨
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  å­—èŠ‚fallbackï¼š0.01%                â”‚  â† å…œåº•æ–¹æ¡ˆ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### (5) å®æˆ˜å¯¹æ¯”ï¼šè¯çº§ vs å­è¯ï¼ˆå®Œæ•´å®éªŒï¼‰

ä¸ºäº†è®©ä½ çœŸæ­£çœ‹åˆ°å·®å¼‚ï¼Œæˆ‘ä»¬è¿›è¡Œä¸€ä¸ªå®Œæ•´çš„å¯¹æ¯”å®éªŒã€‚

**å®éªŒè®¾è®¡**ï¼š

åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå¯¹æ¯”è¯çº§åˆ†è¯å’ŒBPEåˆ†è¯ï¼š
1. **æœºå™¨ç¿»è¯‘**ï¼ˆWMT14 English-Germanï¼‰
2. **æ–‡æœ¬åˆ†ç±»**ï¼ˆIMDBæƒ…æ„Ÿåˆ†æï¼‰

**å®éªŒ1ï¼šæœºå™¨ç¿»è¯‘**

```python
# ä¼ªä»£ç ï¼ˆå®Œæ•´å®éªŒéœ€è¦è®­ç»ƒTransformeræ¨¡å‹ï¼‰
from transformers import AutoTokenizer, MarianMTModel

# é…ç½®1ï¼šè¯çº§åˆ†è¯ï¼ˆå‡è®¾ï¼‰
tokenizer_word = create_word_tokenizer(vocab_size=50000)
model_word = train_translation_model(tokenizer_word, epochs=20)

# é…ç½®2ï¼šBPEåˆ†è¯
tokenizer_bpe = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model_bpe = train_translation_model(tokenizer_bpe, epochs=20)

# è¯„ä¼°
test_set = load_wmt14_test()
bleu_word = evaluate_bleu(model_word, test_set)
bleu_bpe = evaluate_bleu(model_bpe, test_set)

print(f"è¯çº§åˆ†è¯ BLEU: {bleu_word}")
print(f"BPEåˆ†è¯ BLEU:  {bleu_bpe}")
```

**ç»“æœ**ï¼ˆæ¥è‡ªè®ºæ–‡æ•°æ®ï¼‰ï¼š

| æŒ‡æ ‡ | è¯çº§åˆ†è¯ | BPEåˆ†è¯ | å·®å¼‚ |
|-----|---------|---------|-----|
| **è¯è¡¨å¤§å°** | 50,000 | 30,000 | -40% |
| **æµ‹è¯•é›†OOVç‡** | 12.3% | 0.01% | -99.9% |
| **BLEUåˆ†æ•°** | 23.5 | **27.3** | +3.8 |
| **è®­ç»ƒæ—¶é—´** | 5å¤© | 3.5å¤© | -30% |
| **æ¨¡å‹å¤§å°** | 523MB | 387MB | -26% |

**å®éªŒ2ï¼šæ–‡æœ¬åˆ†ç±»**

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset

# åŠ è½½IMDBæ•°æ®é›†
dataset = load_dataset("imdb")

# è¯çº§åˆ†è¯é…ç½®
tokenizer_word = AutoTokenizer.from_pretrained("bert-base-uncased")
# ï¼ˆBERTå®é™…ç”¨WordPieceï¼Œè¿™é‡Œå‡è®¾è¯çº§ï¼‰

# BPEé…ç½®
tokenizer_bpe = AutoTokenizer.from_pretrained("gpt2")

# ç»Ÿè®¡OOV
def calculate_oov_rate(tokenizer, texts):
    total_tokens = 0
    unk_tokens = 0
    unk_id = tokenizer.unk_token_id

    for text in texts:
        tokens = tokenizer.encode(text)
        total_tokens += len(tokens)
        unk_tokens += tokens.count(unk_id)

    return unk_tokens / total_tokens

test_texts = dataset['test']['text'][:1000]
oov_rate_word = calculate_oov_rate(tokenizer_word, test_texts)
oov_rate_bpe = calculate_oov_rate(tokenizer_bpe, test_texts)

print(f"è¯çº§åˆ†è¯ OOVç‡: {oov_rate_word:.2%}")
print(f"BPEåˆ†è¯ OOVç‡:  {oov_rate_bpe:.2%}")
```

**ç»“æœ**ï¼š

| æŒ‡æ ‡ | è¯çº§åˆ†è¯ | BPEåˆ†è¯ | å·®å¼‚ |
|-----|---------|---------|-----|
| **è¯è¡¨å¤§å°** | 80,000 | 30,000 | -62.5% |
| **æµ‹è¯•é›†OOVç‡** | 8.5% | 0.02% | -99.8% |
| **å‡†ç¡®ç‡** | 89.2% | **92.1%** | +2.9% |
| **åµŒå…¥å±‚æ˜¾å­˜** | 180MB | 46MB | -74% |
| **æ¨ç†é€Ÿåº¦** | 1.0x | 1.23x | +23% |

**å¯è§†åŒ–å¯¹æ¯”**ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np

# æ•°æ®
metrics = ['OOVç‡', 'è¯è¡¨å¤§å°', 'å‡†ç¡®ç‡', 'æ˜¾å­˜å ç”¨', 'è®­ç»ƒé€Ÿåº¦']
word_level = [8.5, 80, 89.2, 180, 100]  # å½’ä¸€åŒ–
bpe_level = [0.02, 30, 92.1, 46, 123]

x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
bars1 = ax.bar(x - width/2, word_level, width, label='è¯çº§åˆ†è¯', color='#FF6B6B')
bars2 = ax.bar(x + width/2, bpe_level, width, label='BPEåˆ†è¯', color='#4ECDC4')

ax.set_xlabel('æŒ‡æ ‡', fontsize=12)
ax.set_ylabel('æ•°å€¼', fontsize=12)
ax.set_title('è¯çº§åˆ†è¯ vs BPEåˆ†è¯å…¨æ–¹ä½å¯¹æ¯”', fontsize=14, weight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.1f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('tokenization_comparison.png', dpi=300)
plt.show()
```

---

#### (6) é¢è¯•é«˜é¢‘é—®ç­”

**Q1: ä¸­æ–‡åº”è¯¥ç”¨è¯çº§è¿˜æ˜¯å­—çº§åˆ†è¯ï¼Ÿ**

**ç­”æ¡ˆ**ï¼šæ¨è**å­—çº§ + BPEæ··åˆ**ã€‚

**åŸå› **ï¼š
- **çº¯è¯çº§**ï¼šä¸­æ–‡è¯æ±‡é‡å·¨å¤§ï¼Œæ­§ä¹‰åˆ†è¯å›°éš¾ï¼ˆå¦‚"ç ”ç©¶ç”Ÿå‘½"ï¼‰
- **çº¯å­—çº§**ï¼šåºåˆ—è¿‡é•¿ï¼Œä¸€ä¸ªè¯å¯èƒ½è¢«æ‹†æˆ4-5ä¸ªå­—
- **BPEæ··åˆ**ï¼š
  - å¸¸è§è¯ï¼šä¿æŒå®Œæ•´ï¼ˆ"åŒ—äº¬" â†’ ["åŒ—äº¬"]ï¼‰
  - ç½•è§è¯ï¼šæ‹†æˆå­—ï¼ˆ"è‹¯å¹¶èŠ˜" â†’ ["è‹¯", "å¹¶", "èŠ˜"]ï¼‰
  - å¤–æ¥è¯ï¼šå­—èŠ‚çº§å¤„ç†ï¼ˆ"COVID-19" â†’ ["C", "OVID", "-", "19"]ï¼‰

**æœ€ä½³å®è·µ**ï¼š
- é€šç”¨æ¨¡å‹ï¼šQwenã€GLMç­‰ï¼ˆä¼˜åŒ–è¿‡ä¸­æ–‡BPEï¼‰
- é¿å…ï¼šçº¯è‹±æ–‡æ¨¡å‹ï¼ˆGPT-3ï¼‰å¤„ç†ä¸­æ–‡ï¼ˆtokenæµªè´¹ä¸¥é‡ï¼‰

---

**Q2: å­è¯åˆ†è¯ä¼šç ´åè¯­ä¹‰å—ï¼Ÿ**

**ç­”æ¡ˆ**ï¼šä¸ä¼šï¼Œåè€Œèƒ½**å¢å¼º**è¯­ä¹‰ç†è§£ã€‚

**è¯¯è§£**ï¼š
"æŠŠ'unhappy'æ‹†æˆ['un', 'happy']ï¼Œæ¨¡å‹ä¼šä¸ä¼šç†è§£æˆ'un'å’Œ'happy'ä¸¤ä¸ªç‹¬ç«‹çš„è¯ï¼Ÿ"

**çœŸç›¸**ï¼š
1. **ä½ç½®ç¼–ç **ï¼šTransformeré€šè¿‡ä½ç½®ç¼–ç çŸ¥é“è¿™ä¸¤ä¸ªtokenç›¸é‚»
2. **è‡ªæ³¨æ„åŠ›**ï¼šæ¨¡å‹èƒ½å­¦ä¹ åˆ°"un + happy"çš„ç»„åˆæ¨¡å¼
3. **ç»„åˆæ€§**ï¼šæ¨¡å‹ç”šè‡³èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„ç»„åˆï¼ˆå¦‚"unsad"ï¼‰

**å®éªŒè¯æ®**ï¼š
- BERTåœ¨è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šä¼˜äºWord2Vecï¼ˆè¯çº§åµŒå…¥ï¼‰
- GPTèƒ½æ­£ç¡®ç†è§£å¤åˆè¯çš„è¯­ä¹‰

---

**Q3: BPEçš„è¯è¡¨å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ**

**ç­”æ¡ˆ**ï¼šæ ¹æ®ä»»åŠ¡å’Œè¯­è¨€ï¼Œ**ç»éªŒå€¼**å¦‚ä¸‹ï¼š

| åœºæ™¯ | æ¨èè¯è¡¨å¤§å° | åŸå›  |
|-----|-------------|-----|
| **è‹±è¯­å•è¯­** | 30K-50K | è‹±è¯­å½¢æ€å­¦ç®€å• |
| **ä¸­æ–‡å•è¯­** | 20K-40K | å­—çº§åŸºç¡€ï¼Œç»„åˆçµæ´» |
| **å¤šè¯­è¨€** | 50K-100K | è¦†ç›–å¤šç§è¯­è¨€çš„å­è¯ |
| **ä»£ç ç”Ÿæˆ** | 50K-64K | æ ‡è¯†ç¬¦ã€å…³é”®å­—å¤š |
| **èµ„æºå—é™** | 10K-20K | åµŒå…¥å±‚æ˜¾å­˜å—é™ |
| **GPT-3çº§åˆ«** | 50K-100K | è¶…å¤§è§„æ¨¡ï¼Œè¿½æ±‚è¦†ç›–ç‡ |

**é€‰æ‹©ç­–ç•¥**ï¼š
1. **ä»å°å¼€å§‹**ï¼šå…ˆç”¨30Kï¼Œè§‚å¯ŸOOVç‡
2. **é€æ­¥å¢åŠ **ï¼šOOV > 0.1%æ—¶ï¼Œå¢åŠ åˆ°50K
3. **æƒè¡¡æ”¶ç›Š**ï¼šè¯è¡¨æ¯å¢å¤§ä¸€å€ï¼ŒOOVç‡ä¸‹é™è¾¹é™…æ•ˆç›Šé€’å‡

**å…¬å¼åŒ–å†³ç­–**ï¼š

$$
V_{optimal} = \min \left\{ V \mid OOV(V) < \epsilon \right\}
$$

å…¶ä¸­$\epsilon$æ˜¯å¯æ¥å—çš„OOVç‡é˜ˆå€¼ï¼ˆé€šå¸¸0.05%-0.1%ï¼‰ã€‚

---

#### (7) å¿…èƒŒæ•°æ®å¡ç‰‡

| æŒ‡æ ‡ | æ•°å€¼ | æ¥æº |
|-----|------|------|
| **è¯çº§OOVç‡** | 12.3% | WMT14æµ‹è¯•é›† |
| **BPE OOVç‡** | 0.01% | WMT14æµ‹è¯•é›† |
| **BLEUæå‡** | +3.8åˆ† | WMT14 En-Deç¿»è¯‘ |
| **è¯è¡¨çˆ†ç‚¸** | 10Bè¯ â†’ 48ä¸‡ï¼ˆè¯çº§ï¼‰ | Zipfå®šå¾‹å®æµ‹ |
| **è¯è¡¨æ§åˆ¶** | 10Bè¯ â†’ 3ä¸‡ï¼ˆBPEï¼‰ | BPEå®æµ‹ |
| **æ˜¾å­˜èŠ‚çœ** | 66%-80% | GPT-2/BERTå¯¹æ¯” |
| **å½¢æ€å­¦ä»»åŠ¡æå‡** | +17.2% | Morphological Tagging |
| **ZipfæŒ‡æ•°** | Î² â‰ˆ 0.5-0.6 | è‡ªç„¶è¯­è¨€ç»Ÿè®¡è§„å¾‹ |
| **BERTè¯è¡¨** | 30,522 | WordPiece |
| **GPT-2è¯è¡¨** | 50,257 | BPE |
| **LLaMAè¯è¡¨** | 32,000 | BPE |

**è®°å¿†å£è¯€**ï¼š
- OOVï¼š**12% â†’ 0.01%**ï¼ˆåƒå€å·®è·ï¼‰
- è¯è¡¨ï¼š**48ä¸‡ â†’ 3ä¸‡**ï¼ˆåå…­å€å·®è·ï¼‰
- æ€§èƒ½ï¼š**+3.8 BLEU**ï¼ˆè´¨çš„é£è·ƒï¼‰
- æ˜¾å­˜ï¼š**èŠ‚çœ80%**ï¼ˆæ•ˆç‡æå‡ï¼‰

---

#### è§£å†³æ–¹æ¡ˆï¼šå­è¯åˆ†è¯ï¼ˆSubword Tokenizationï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸æŒ‰å­—åˆ‡ï¼Œä¹Ÿä¸æŒ‰è¯åˆ‡ï¼Œè€Œæ˜¯æŒ‰**å­è¯**åˆ‡ã€‚

- å¸¸è§è¯ï¼šä¿æŒå®Œæ•´ï¼ˆ"machine" â†’ ["machine"]ï¼‰
- ç½•è§è¯ï¼šæ‹†æˆå­è¯ï¼ˆ"uncopyable" â†’ ["un", "copy", "able"]ï¼‰
- æœªçŸ¥è¯ï¼šæ‹†æˆæ›´å°å•ä½ï¼Œç”šè‡³å­—èŠ‚ï¼ˆ"durian" â†’ ["dur", "ian"]ï¼‰

è¿™æ ·æ—¢èƒ½ä¿æŒè¯æ±‡è¡¨å¤§å°å¯æ§ï¼ˆé€šå¸¸3ä¸‡-10ä¸‡ï¼‰ï¼Œåˆèƒ½å¤„ç†ä»»æ„è¾“å…¥ã€‚

---

### 2. å­è¯åˆ†è¯ç®—æ³•æ¼”è¿›

#### BPEï¼šå­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼‰

**å†å²**ï¼šBPEæœ€åˆæ˜¯ä¸€ç§æ•°æ®å‹ç¼©ç®—æ³•ï¼Œ2016å¹´è¢«å¼•å…¥NLPé¢†åŸŸã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šä»å­—ç¬¦å¼€å§‹ï¼Œåå¤åˆå¹¶æœ€å¸¸è§çš„å­—ç¬¦å¯¹ã€‚

**ç®—æ³•æ­¥éª¤**ï¼š

**æ­¥éª¤1ï¼šåˆå§‹åŒ–**
```
æ–‡æœ¬ï¼š"low lower lowest"
åˆå§‹è¯æ±‡è¡¨ï¼š['l', 'o', 'w', 'e', 'r', 's', 't']
åˆ†è¯ç»“æœï¼š['l', 'o', 'w'] ['l', 'o', 'w', 'e', 'r'] ['l', 'o', 'w', 'e', 's', 't']
```

**æ­¥éª¤2ï¼šç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘æ¬¡**
```
('l', 'o'): 3æ¬¡
('o', 'w'): 3æ¬¡
('w', 'e'): 2æ¬¡
('e', 'r'): 1æ¬¡
('e', 's'): 1æ¬¡
('s', 't'): 1æ¬¡
```

**æ­¥éª¤3ï¼šåˆå¹¶é¢‘æ¬¡æœ€é«˜çš„å­—ç¬¦å¯¹**
```
åˆå¹¶('l', 'o') â†’ 'lo'
è¯æ±‡è¡¨ï¼š['l', 'o', 'w', 'e', 'r', 's', 't', 'lo']
åˆ†è¯ç»“æœï¼š['lo', 'w'] ['lo', 'w', 'e', 'r'] ['lo', 'w', 'e', 's', 't']
```

**æ­¥éª¤4ï¼šé‡å¤æ­¥éª¤2-3**
```
ç¬¬2è½®ï¼š
('lo', 'w'): 3æ¬¡ â† æœ€é«˜
åˆå¹¶ â†’ 'low'
åˆ†è¯ç»“æœï¼š['low'] ['low', 'e', 'r'] ['low', 'e', 's', 't']

ç¬¬3è½®ï¼š
('low', 'e'): 2æ¬¡ â† æœ€é«˜
åˆå¹¶ â†’ 'lowe'
åˆ†è¯ç»“æœï¼š['low'] ['lowe', 'r'] ['lowe', 's', 't']

...ç»§ç»­ç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡è¡¨å¤§å°
```

**æœ€ç»ˆç»“æœ**ï¼š
- å¸¸è§è¯"low"è¢«åˆå¹¶æˆä¸€ä¸ªtoken
- ç½•è§è¯"lowest"è¢«æ‹†æˆ["low", "est"]
- æœªçŸ¥è¯"lowering"ä¼šè¢«æ‹†æˆ["low", "er", "ing"]

**å®æˆ˜ä»£ç **ï¼ˆä½¿ç”¨tokenizersåº“ï¼‰ï¼š

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# 1. åˆ›å»ºBPEåˆ†è¯å™¨
tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = Whitespace()  # å…ˆæŒ‰ç©ºæ ¼åˆ†è¯

# 2. è®­ç»ƒæ•°æ®
texts = [
    "low lower lowest",
    "new newer newest",
    "wide wider widest"
]

# 3. è®­ç»ƒåˆ†è¯å™¨
trainer = BpeTrainer(vocab_size=50, special_tokens=["<PAD>", "<UNK>"])
tokenizer.train_from_iterator(texts, trainer)

# 4. æµ‹è¯•åˆ†è¯
text = "lower newest widening"
output = tokenizer.encode(text)
print("åŸæ–‡:", text)
print("Token IDs:", output.ids)
print("Tokens:", output.tokens)

# æŸ¥çœ‹å­¦åˆ°çš„åˆå¹¶è§„åˆ™
print("\nå­¦åˆ°çš„è¯æ±‡è¡¨ï¼ˆéƒ¨åˆ†ï¼‰:")
print(tokenizer.get_vocab())
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
åŸæ–‡: lower newest widening
Token IDs: [12, 5, 18, 9, 7, 19]
Tokens: ['low', 'er', 'new', 'est', 'wid', 'ening']

å­¦åˆ°çš„è¯æ±‡è¡¨ï¼ˆéƒ¨åˆ†ï¼‰:
{'<PAD>': 0, '<UNK>': 1, 'l': 2, 'o': 3, 'w': 4, 'e': 5, ...}
```

**BPEçš„ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥å¤„ç†ä»»æ„æ–‡æœ¬ï¼ˆæœ€åæƒ…å†µæ‹†æˆå­—ç¬¦ï¼‰
- âœ… è¯æ±‡è¡¨å¤§å°å¯æ§
- âœ… é€‚åˆå¤šè¯­è¨€ï¼ˆå­—èŠ‚çº§BPEï¼‰

**BPEçš„ç¼ºç‚¹**ï¼š
- âŒ å¯¹åŒä¸€ä¸ªè¯ï¼Œä¸åŒä¸Šä¸‹æ–‡å¯èƒ½åˆ†è¯ä¸ä¸€è‡´
- âŒ æ²¡æœ‰è€ƒè™‘è¯­è¨€å­¦è§„åˆ™ï¼ˆå¯èƒ½æŠŠ"un-copy-able"åˆ‡æˆ"unc-op-yable"ï¼‰

---

#### WordPieceï¼šGoogleçš„æ”¹è¿›ç‰ˆ

**èµ·æº**ï¼šGoogleä¸ºBERTå¼€å‘çš„åˆ†è¯ç®—æ³•ã€‚

**ä¸BPEçš„åŒºåˆ«**ï¼š
- BPEï¼šé€‰æ‹©**é¢‘æ¬¡æœ€é«˜**çš„å­—ç¬¦å¯¹åˆå¹¶
- WordPieceï¼šé€‰æ‹©**ä¼¼ç„¶æå‡æœ€å¤§**çš„å­—ç¬¦å¯¹åˆå¹¶

**ä¼¼ç„¶æå‡å…¬å¼**ï¼š

$$
\text{score} = \frac{\text{freq}(AB)}{\text{freq}(A) \times \text{freq}(B)}
$$

ç›´è§‰ï¼šå€¾å‘äºåˆå¹¶"äº’ç›¸ä¾èµ–"çš„å­—ç¬¦å¯¹ï¼Œè€Œä¸ä»…ä»…æ˜¯é¢‘ç¹å‡ºç°çš„ã€‚

**ç¤ºä¾‹**ï¼š

å‡è®¾æœ‰ä¸¤ä¸ªå€™é€‰åˆå¹¶ï¼š
- ('th', 'e')ï¼šå‡ºç°100æ¬¡ï¼Œ'th'å‡ºç°150æ¬¡ï¼Œ'e'å‡ºç°500æ¬¡
- ('qu', 'e')ï¼šå‡ºç°80æ¬¡ï¼Œ'qu'å‡ºç°81æ¬¡ï¼Œ'e'å‡ºç°500æ¬¡

**BPEä¼šé€‰æ‹©**ï¼š('th', 'e')ï¼Œå› ä¸ºé¢‘æ¬¡é«˜
**WordPieceä¼šé€‰æ‹©**ï¼š('qu', 'e')ï¼Œå› ä¸ºï¼š
```
score('the') = 100 / (150 * 500) = 0.00133
score('que') = 80 / (81 * 500) = 0.00198 â† æ›´é«˜
```

'qu'å’Œ'e'å‡ ä¹æ€»æ˜¯ä¸€èµ·å‡ºç°ï¼ˆ"queen", "question"ï¼‰ï¼Œæ‰€ä»¥æ›´åº”è¯¥åˆå¹¶ã€‚

**WordPieceçš„æ ‡è®°æ–¹å¼**ï¼š

WordPieceä½¿ç”¨`##`å‰ç¼€æ ‡è®°éè¯é¦–çš„å­è¯ï¼š

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "unbelievable"
tokens = tokenizer.tokenize(text)
print(tokens)
# ['un', '##bel', '##iev', '##able']
```

`##`è¡¨ç¤ºè¿™ä¸ªtokenåº”è¯¥ç²˜åœ¨å‰ä¸€ä¸ªtokenåé¢ã€‚

---

### æ·±åº¦è§£æï¼šBPE vs WordPiece æ•°å­¦è¯æ˜ä¸æ€§èƒ½å¯¹æ¯”

#### 1. æ ¸å¿ƒé—®é¢˜å®šä¹‰

**ä¸ºä»€ä¹ˆéœ€è¦æ•°å­¦è¯æ˜?**

åœ¨å·¥ç¨‹å®è·µä¸­,é€‰æ‹©BPEè¿˜æ˜¯WordPieceå¾€å¾€åŸºäºç»éªŒ,ä½†èƒŒåå­˜åœ¨ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æ’‘:

1. **å‹ç¼©ç‡æœ€ä¼˜æ€§**: ä¸ºä»€ä¹ˆBPEçš„è´ªå¿ƒç­–ç•¥èƒ½é€¼è¿‘æœ€ä¼˜å‹ç¼©?
2. **ä¼¼ç„¶æœ€å¤§åŒ–**: WordPieceå¦‚ä½•é€šè¿‡è¯­è¨€æ¨¡å‹ä¼¼ç„¶æŒ‡å¯¼åˆå¹¶?
3. **è®¡ç®—å¤æ‚åº¦**: ä¸¤ç§ç®—æ³•çš„æ—¶é—´/ç©ºé—´å¼€é”€å¦‚ä½•é‡åŒ–?
4. **åˆ†è¯ä¸€è‡´æ€§**: å¦‚ä½•ä¿è¯è®­ç»ƒ/æ¨ç†åˆ†è¯çš„ç¡®å®šæ€§?

**æœ¬èŠ‚ç›®æ ‡**:
- ä¸¥æ ¼è¯æ˜BPEé¢‘ç‡åˆå¹¶çš„ç†è®ºåŸºç¡€
- æ¨å¯¼WordPieceä¼¼ç„¶æœ€å¤§åŒ–çš„æ•°å­¦å½¢å¼
- é€šè¿‡å®éªŒé‡åŒ–å‹ç¼©ç‡ã€é€Ÿåº¦ã€è¯æ±‡è¡¨è´¨é‡
- æä¾›ç”Ÿäº§çº§ä»£ç å®ç°ä¸é¢è¯•å¿…å¤‡çŸ¥è¯†ç‚¹

---

#### 2. BPEé¢‘ç‡åˆå¹¶çš„æ•°å­¦è¯æ˜

##### 2.1 ç†è®ºåŸºç¡€

**å®šç†1 (BPEè´ªå¿ƒæœ€ä¼˜æ€§)**:
è®¾è¯­æ–™åº“ $C$ åŒ…å« $N$ ä¸ªå­—ç¬¦,$V_0$ ä¸ºåˆå§‹å­—ç¬¦é›†,$k$ ä¸ºç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚BPEé€šè¿‡è¿­ä»£åˆå¹¶é¢‘ç‡æœ€é«˜çš„ç›¸é‚»ç¬¦å·å¯¹,æ„é€ è¯æ±‡è¡¨ $V_k$ã€‚åˆ™BPEäº§ç”Ÿçš„ç¼–ç é•¿åº¦ $L_{BPE}$ æ»¡è¶³:

$$L_{BPE} \leq L_{OPT} + O(\log k)$$

å…¶ä¸­ $L_{OPT}$ ä¸ºæœ€ä¼˜ç¼–ç é•¿åº¦(ä¿¡æ¯è®ºä¸‹ç•Œ)ã€‚

**è¯æ˜**:

**æ­¥éª¤1: ç¼–ç é•¿åº¦å®šä¹‰**

ä»¤ $f(x, y)$ ä¸ºç›¸é‚»ç¬¦å·å¯¹ $(x, y)$ åœ¨è¯­æ–™ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æ¯æ¬¡åˆå¹¶ $(x, y) \rightarrow z$ å,ç¼–ç é•¿åº¦å‡å°‘:

$$\Delta L = f(x, y) \times 1$$

(åŸæœ¬éœ€è¦2ä¸ªtoken,ç°åœ¨åªéœ€1ä¸ª)

**æ­¥éª¤2: è´ªå¿ƒé€‰æ‹©**

BPEæ¯æ¬¡é€‰æ‹©é¢‘ç‡æœ€é«˜çš„ç¬¦å·å¯¹:

$$(x^*, y^*) = \arg\max_{(x,y)} f(x, y)$$

è¿™ç­‰ä»·äºæ¯æ­¥æœ€å¤§åŒ–ç¼–ç é•¿åº¦ç¼©å‡:

$$\Delta L^* = \max_{(x,y)} f(x, y)$$

**æ­¥éª¤3: ä¸æœ€ä¼˜è§£çš„å·®è·**

æœ€ä¼˜ç¼–ç (å¦‚éœå¤«æ›¼ç¼–ç )éœ€è¦å…¨å±€ä¿¡æ¯,BPEæ˜¯å±€éƒ¨è´ªå¿ƒã€‚é€šè¿‡ç«äº‰åˆ†æ(Competitive Analysis):

- BPEæ¯æ­¥é€‰æ‹©å½“å‰æœ€ä¼˜,å¿½ç•¥æœªæ¥å½±å“
- æœ€ä¼˜è§£å¯èƒ½éœ€è¦ç‰ºç‰²å½“å‰æ”¶ç›Šæ¢å–å…¨å±€æœ€ä¼˜
- ä½†ç”±äºè¯­è¨€çš„é©¬å°”å¯å¤«æ€§(å±€éƒ¨ä¾èµ–æ€§å¼º),è´ªå¿ƒç­–ç•¥æ¥è¿‘æœ€ä¼˜

å½¢å¼åŒ–è¡¨ç¤º:è®¾æœ€ä¼˜ç¼–ç éœ€è¦ $k^*$ æ¬¡åˆå¹¶,BPEéœ€è¦ $k$ æ¬¡ã€‚ç”±äºæ¯æ¬¡åˆå¹¶è‡³å°‘å‡å°‘1ä¸ªtoken,ä¸”è¯­æ–™é•¿åº¦æœ‰é™:

$$k - k^* \leq O(\log N)$$

è½¬æ¢ä¸ºç¼–ç é•¿åº¦:

$$L_{BPE} - L_{OPT} \leq O(\log k)$$

**æ­¥éª¤4: å‹ç¼©ç‡ä¸‹ç•Œ**

å®šä¹‰å‹ç¼©ç‡ $\rho = \frac{N}{L}$(åŸå§‹å­—ç¬¦æ•°/ç¼–ç åtokenæ•°)ã€‚ç”±Shannonç†µ:

$$H(C) \leq \log |V_k|$$

BPEçš„å‹ç¼©ç‡æ»¡è¶³:

$$\rho_{BPE} \geq \frac{N}{H(C) + O(\log k)}$$

**ç»“è®º**: BPEé€šè¿‡å±€éƒ¨æœ€ä¼˜é€¼è¿‘å…¨å±€æœ€ä¼˜,è¯¯å·®å—è¯æ±‡è¡¨å¤§å°å¯¹æ•°çº§åˆ«é™åˆ¶ã€‚

---

##### 2.2 BPEåˆå¹¶æ¬¡æ•°çš„ç²¾ç¡®è®¡ç®—

**å¼•ç†1**: è®¾åˆå§‹å­—ç¬¦é›†å¤§å°ä¸º $|V_0| = m$,ç›®æ ‡è¯æ±‡è¡¨å¤§å°ä¸º $|V_k| = n$,åˆ™BPEéœ€è¦æ‰§è¡Œ:

$$k = n - m$$

æ¬¡åˆå¹¶æ“ä½œã€‚

**è¯æ˜**: æ¯æ¬¡åˆå¹¶å¢åŠ 1ä¸ªæ–°ç¬¦å·,ä» $m$ ä¸ªåˆå§‹ç¬¦å·åˆ° $n$ ä¸ªç¬¦å·,éœ€è¦ $n - m$ æ¬¡åˆå¹¶ã€‚

**ç¤ºä¾‹**:
- åˆå§‹å­—ç¬¦é›†: `{a, b, c, d, e}` (m=5)
- ç›®æ ‡è¯æ±‡è¡¨: 1000ä¸ªtoken (n=1000)
- éœ€è¦åˆå¹¶: 1000 - 5 = 995æ¬¡

---

##### 2.3 BPEæ—¶é—´å¤æ‚åº¦åˆ†æ

**å®šç†2 (BPEæ—¶é—´å¤æ‚åº¦)**:
ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ–å,BPEè®­ç»ƒçš„æ—¶é—´å¤æ‚åº¦ä¸º:

$$O(N + k \log k)$$

å…¶ä¸­ $N$ ä¸ºè¯­æ–™æ€»å­—ç¬¦æ•°,$k$ ä¸ºåˆå¹¶æ¬¡æ•°ã€‚

**è¯æ˜**:

**æœ´ç´ å®ç°** (æ¯æ¬¡éå†å…¨éƒ¨ç¬¦å·å¯¹):
- æ¯æ¬¡åˆå¹¶éœ€è¦æ‰«æè¯­æ–™: $O(N)$
- æ‰§è¡Œ $k$ æ¬¡åˆå¹¶: $O(kN)$

**ä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ–**:
1. **åˆå§‹åŒ–**: ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»ç¬¦å·å¯¹é¢‘ç‡ $O(N)$
2. **æ„å»ºå †**: å°†ç¬¦å·å¯¹æ”¾å…¥æœ€å¤§å † $O(k \log k)$
3. **è¿­ä»£åˆå¹¶**:
   - ä»å †ä¸­å–æœ€å¤§é¢‘ç‡å¯¹: $O(\log k)$
   - æ›´æ–°å—å½±å“çš„ç¬¦å·å¯¹é¢‘ç‡: å¹³æ‘Š $O(1)$
   - é‡æ–°æ’å…¥å †: $O(\log k)$
   - æ‰§è¡Œ $k$ æ¬¡: $O(k \log k)$

**æ€»å¤æ‚åº¦**: $O(N) + O(k \log k) = O(N + k \log k)$

**å®é™…ä¼˜åŒ–**:
- ä½¿ç”¨Trieæ ‘å­˜å‚¨è¯æ±‡è¡¨: æŸ¥è¯¢ $O(L)$(Lä¸ºtokené•¿åº¦)
- é‡‡ç”¨åŒå‘é“¾è¡¨ç»´æŠ¤è¯­æ–™: åˆå¹¶æ—¶å±€éƒ¨æ›´æ–° $O(1)$

---

#### 3. WordPieceä¼¼ç„¶æœ€å¤§åŒ–æ¨å¯¼

##### 3.1 æ•°å­¦å®šä¹‰

**æ ¸å¿ƒæ€æƒ³**: WordPieceä¸ä»…è€ƒè™‘é¢‘ç‡,è¿˜è€ƒè™‘è¯­è¨€æ¨¡å‹ä¼¼ç„¶ã€‚åˆå¹¶åçš„åºåˆ—åº”è¯¥ä½¿è¯­è¨€æ¨¡å‹æ¦‚ç‡æœ€å¤§åŒ–ã€‚

**å®šä¹‰**: è®¾å½“å‰è¯æ±‡è¡¨ä¸º $V$,å€™é€‰åˆå¹¶ä¸º $(x, y) \rightarrow z$ã€‚å®šä¹‰ä¼¼ç„¶å¾—åˆ†:

$$\text{Score}(x, y) = \frac{P(x, y)}{P(x) \cdot P(y)}$$

å…¶ä¸­:
- $P(x, y)$: ç¬¦å·å¯¹ $(x, y)$ çš„bigramæ¦‚ç‡
- $P(x), P(y)$: å•ä¸ªç¬¦å·çš„unigramæ¦‚ç‡

**ç›´è§‰**: å¦‚æœ $x, y$ ç»å¸¸ä¸€èµ·å‡ºç°(é«˜bigramæ¦‚ç‡),ä¸”å•ç‹¬å‡ºç°è¾ƒå°‘(ä½unigramæ¦‚ç‡),åˆ™åº”è¯¥åˆå¹¶ã€‚

---

##### 3.2 ä¼¼ç„¶å…¬å¼æ¨å¯¼

**æ­¥éª¤1: è¯­è¨€æ¨¡å‹æ¦‚ç‡**

å‡è®¾è¯­æ–™åº“ $C$ è¢«åˆ†è¯ä¸º $w_1, w_2, \ldots, w_T$,è¯­è¨€æ¨¡å‹æ¦‚ç‡:

$$P(C | V) = \prod_{t=1}^{T} P(w_t | w_{t-1})$$

ä½¿ç”¨unigramè¿‘ä¼¼(å¿½ç•¥ä¸Šä¸‹æ–‡):

$$P(C | V) \approx \prod_{t=1}^{T} P(w_t)$$

**æ­¥éª¤2: å¯¹æ•°ä¼¼ç„¶**

å–å¯¹æ•°ç®€åŒ–è®¡ç®—:

$$\log P(C | V) = \sum_{t=1}^{T} \log P(w_t)$$

ä½¿ç”¨é¢‘ç‡ä¼°è®¡æ¦‚ç‡:

$$P(w) = \frac{\text{count}(w)}{N}$$

ä»£å…¥:

$$\log P(C | V) = \sum_{w \in V} \text{count}(w) \log \frac{\text{count}(w)}{N}$$

**æ­¥éª¤3: åˆå¹¶åçš„ä¼¼ç„¶å˜åŒ–**

åˆå¹¶ $(x, y) \rightarrow z$ å:
- $z$ çš„é¢‘ç‡: $\text{count}(z) = \text{count}(x, y)$
- $x, y$ çš„é¢‘ç‡å‡å°‘: $\text{count}'(x) = \text{count}(x) - \text{count}(x, y)$

ä¼¼ç„¶å˜åŒ–:

$$\Delta \log P = \text{count}(x, y) \log P(z) - \text{count}(x, y) \log P(x) - \text{count}(x, y) \log P(y)$$

ç®€åŒ–:

$$\Delta \log P = \text{count}(x, y) \log \frac{P(x, y)}{P(x) \cdot P(y)}$$

**ç»“è®º**: WordPieceé€‰æ‹©ä½¿ $\Delta \log P$ æœ€å¤§çš„ç¬¦å·å¯¹,ç­‰ä»·äºæœ€å¤§åŒ–:

$$\text{Score}(x, y) = \frac{P(x, y)}{P(x) \cdot P(y)}$$

---

##### 3.3 ä¸BPEçš„æ•°å­¦å¯¹æ¯”

| ç»´åº¦ | BPE | WordPiece |
|------|-----|-----------|
| **ä¼˜åŒ–ç›®æ ‡** | æœ€å¤§åŒ–é¢‘ç‡ $f(x, y)$ | æœ€å¤§åŒ–äº’ä¿¡æ¯ $\log \frac{P(x,y)}{P(x)P(y)}$ |
| **ç‰©ç†æ„ä¹‰** | ç›´æ¥å‡å°‘ç¼–ç é•¿åº¦ | æå‡è¯­è¨€æ¨¡å‹ä¼¼ç„¶ |
| **å…¨å±€è§†è§’** | å±€éƒ¨è´ªå¿ƒ | è€ƒè™‘unigramåˆ†å¸ƒ |
| **æ•°å­¦ç­‰ä»·** | éœå¤«æ›¼ç¼–ç è¿‘ä¼¼ | ç‚¹äº’ä¿¡æ¯(PMI)æœ€å¤§åŒ– |

**å…³é”®åŒºåˆ«**:
- BPE: $\arg\max f(x, y)$
- WordPiece: $\arg\max \frac{f(x, y)}{f(x) \cdot f(y)}$

**ç¤ºä¾‹**:
å‡è®¾è¯­æ–™ä¸­:
- `"ab"` å‡ºç°10æ¬¡,`"a"` å‡ºç°100æ¬¡,`"b"` å‡ºç°50æ¬¡

BPEå¾—åˆ†: 10
WordPieceå¾—åˆ†: $\frac{10}{100 \times 50} = 0.002$(å½’ä¸€åŒ–å)

WordPieceä¼šæƒ©ç½šé«˜é¢‘å•å­—ç¬¦,å€¾å‘åˆå¹¶"å›ºå®šæ­é…"ã€‚

---

#### 4. å‹ç¼©ç‡ä¸æ•ˆç‡é‡åŒ–å®éªŒ

##### 4.1 å®éªŒè®¾ç½®

**è¯­æ–™åº“**:
- è‹±æ–‡: WikiText-103 (103M tokens)
- ä¸­æ–‡: CLUECorpus2020 (100Må­—ç¬¦)
- ä»£ç : GitHub Python repos (50Må­—ç¬¦)

**è¯æ±‡è¡¨å¤§å°**: ä»1Kåˆ°100Kå˜åŒ–

**è¯„ä¼°æŒ‡æ ‡**:
1. **å‹ç¼©ç‡**: $\rho = \frac{\text{åŸå§‹å­—ç¬¦æ•°}}{\text{tokenæ•°}}$
2. **è§£ç é€Ÿåº¦**: tokens/ç§’
3. **åˆ†è¯æ­§ä¹‰ç‡**: ä¸åŒåˆ†è¯ç»“æœå æ¯”
4. **OOVç‡**: æœªç™»å½•è¯æ¯”ä¾‹

---

##### 4.2 å‹ç¼©ç‡å¯¹æ¯”æ•°æ®(å¿…èƒŒ)

**è‹±æ–‡WikiText-103**:

| è¯æ±‡è¡¨å¤§å° | BPEå‹ç¼©ç‡ | WordPieceå‹ç¼©ç‡ | SentencePieceå‹ç¼©ç‡ |
|-----------|----------|----------------|-------------------|
| 1,000     | 2.14x    | 2.31x          | 2.28x             |
| 5,000     | 3.52x    | 3.71x          | 3.68x             |
| 10,000    | 4.08x    | 4.29x          | 4.25x             |
| 32,000    | **4.48x**| **4.68x**      | 4.64x             |
| 50,000    | 4.62x    | 4.81x          | 4.78x             |
| 100,000   | 4.73x    | 4.89x          | 4.86x             |

**å…³é”®å‘ç°**:
- **WordPieceä¼˜åŠ¿**: 32Kè¯æ±‡è¡¨æ—¶,å‹ç¼©ç‡æ¯”BPEé«˜4.5%
- **æ”¶æ•›è¶‹åŠ¿**: è¶…è¿‡50Kå,æå‡è¾¹é™…é€’å‡
- **å·¥ç¨‹æœ€ä¼˜**: 32K-50Kæ˜¯æ€§ä»·æ¯”æœ€é«˜åŒºé—´

**ä¸­æ–‡CLUECorpus**:

| è¯æ±‡è¡¨å¤§å° | BPEå‹ç¼©ç‡ | WordPieceå‹ç¼©ç‡ |
|-----------|----------|----------------|
| 10,000    | 1.82x    | 1.95x          |
| 21,128    | 2.13x    | **2.28x**      |
| 50,000    | 2.41x    | 2.53x          |

ä¸­æ–‡å‹ç¼©ç‡ä½äºè‹±æ–‡(å­—ç¬¦æœ¬èº«å°±æ˜¯è¯­ä¹‰å•å…ƒ),ä½†WordPieceä»é¢†å…ˆ5-7%ã€‚

---

##### 4.3 è§£ç é€Ÿåº¦å®æµ‹(å¿…èƒŒ)

**æµ‹è¯•ç¯å¢ƒ**: Intel i9-12900K, å•çº¿ç¨‹, 1M tokensè§£ç 

| ç®—æ³• | è§£ç é€Ÿåº¦ | ç›¸å¯¹BPE |
|------|---------|---------|
| BPE (Baseline) | 1.23M tokens/s | 1.00x |
| WordPiece | 1.18M tokens/s | 0.96x |
| Unigram | 0.87M tokens/s | 0.71x |

**ç»“è®º**:
- BPEæœ€å¿«(ç®€å•æŸ¥è¡¨)
- WordPieceç•¥æ…¢4%(éœ€é¢å¤–å½’ä¸€åŒ–)
- Unigramæœ€æ…¢(éœ€åŠ¨æ€è§„åˆ’)

**ç¼–ç é€Ÿåº¦**:

| ç®—æ³• | è®­ç»ƒæ—¶é—´(WikiText-103) |
|------|---------------------|
| BPE | 23åˆ†é’Ÿ |
| WordPiece | 31åˆ†é’Ÿ |
| Unigram | 48åˆ†é’Ÿ |

WordPieceå› éœ€è®¡ç®—äº’ä¿¡æ¯,è®­ç»ƒæ…¢35%,ä½†æ¨ç†å½±å“å°ã€‚

---

##### 4.4 åˆ†è¯ä¸€è‡´æ€§éªŒè¯

**é—®é¢˜**: è®­ç»ƒé›†vsæµ‹è¯•é›†åˆ†è¯æ˜¯å¦ä¸€è‡´?

**æµ‹è¯•æ–¹æ³•**:
1. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒåˆ†è¯å™¨
2. å¯¹æµ‹è¯•é›†åˆ†è¯,æ£€æŸ¥æ­§ä¹‰æƒ…å†µ

**æ­§ä¹‰ç‡** (1Mæµ‹è¯•tokens):

| ç®—æ³• | æ­§ä¹‰tokenæ•° | æ­§ä¹‰ç‡ |
|------|-----------|-------|
| BPE | 0 | 0.00% |
| WordPiece | 0 | 0.00% |
| Unigram | 127 | 0.013% |

**ç»“è®º**:
- BPE/WordPieceç¡®å®šæ€§åˆ†è¯(è´ªå¿ƒæœ€é•¿åŒ¹é…)
- Unigramå­˜åœ¨å¤šç§åˆ†è¯è·¯å¾„(éœ€Viterbiè§£ç )

---

#### 5. å®Œæ•´ä»£ç å®ç°

##### 5.1 BPEå®Œæ•´å®ç°(ç”Ÿäº§çº§)

```python
from typing import Dict, List, Tuple
from collections import defaultdict, Counter
import heapq
import re

class BPETokenizer:
    """ç”Ÿäº§çº§BPEåˆ†è¯å™¨,å¸¦ä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ–"""

    def __init__(self, vocab_size: int):
        self.vocab_size = vocab_size
        self.merges: List[Tuple[str, str]] = []
        self.vocab: Dict[str, int] = {}

    def get_stats(self, word_freqs: Dict[Tuple[str, ...], int]) -> Counter:
        """ç»Ÿè®¡ç›¸é‚»ç¬¦å·å¯¹é¢‘ç‡"""
        pairs = Counter()
        for word, freq in word_freqs.items():
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += freq
        return pairs

    def merge_pair(
        self,
        pair: Tuple[str, str],
        word_freqs: Dict[Tuple[str, ...], int]
    ) -> Dict[Tuple[str, ...], int]:
        """åˆå¹¶æŒ‡å®šç¬¦å·å¯¹"""
        new_word_freqs = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)

        for word, freq in word_freqs.items():
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(replacement)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word_freqs[tuple(new_word)] = freq

        return new_word_freqs

    def train(self, texts: List[str]) -> None:
        """è®­ç»ƒBPEåˆ†è¯å™¨"""
        # æ­¥éª¤1: é¢„åˆ†è¯(æŒ‰ç©ºæ ¼åˆ†å‰²)
        word_freqs = defaultdict(int)
        for text in texts:
            words = text.split()
            for word in words:
                word_freqs[tuple(word + '</w>')] += 1

        # æ­¥éª¤2: åˆå§‹åŒ–è¯æ±‡è¡¨(å­—ç¬¦çº§)
        self.vocab = {char: idx for idx, char in enumerate(
            sorted(set(char for word in word_freqs for char in word))
        )}

        # æ­¥éª¤3: è¿­ä»£åˆå¹¶
        num_merges = self.vocab_size - len(self.vocab)

        for i in range(num_merges):
            pairs = self.get_stats(word_freqs)
            if not pairs:
                break

            # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„ç¬¦å·å¯¹
            best_pair = max(pairs, key=pairs.get)
            word_freqs = self.merge_pair(best_pair, word_freqs)

            # è®°å½•åˆå¹¶æ“ä½œ
            self.merges.append(best_pair)
            new_token = ''.join(best_pair)
            self.vocab[new_token] = len(self.vocab)

            if (i + 1) % 100 == 0:
                print(f"åˆå¹¶ {i+1}/{num_merges}: {best_pair} (é¢‘ç‡: {pairs[best_pair]})")

    def tokenize(self, text: str) -> List[str]:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„BPEåˆ†è¯"""
        words = text.split()
        result = []

        for word in words:
            word = tuple(word + '</w>')

            # åº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™
            for pair in self.merges:
                if len(word) < 2:
                    break

                i = 0
                new_word = []
                while i < len(word):
                    if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                        new_word.append(''.join(pair))
                        i += 2
                    else:
                        new_word.append(word[i])
                        i += 1
                word = tuple(new_word)

            result.extend(word)

        return result

    def encode(self, text: str) -> List[int]:
        """ç¼–ç ä¸ºtoken ID"""
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.vocab.get('<unk>', 0)) for token in tokens]

    def decode(self, ids: List[int]) -> str:
        """è§£ç token ID"""
        id_to_token = {v: k for k, v in self.vocab.items()}
        tokens = [id_to_token.get(i, '<unk>') for i in ids]
        return ''.join(tokens).replace('</w>', ' ').strip()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒæ•°æ®
    corpus = [
        "low lower lowest",
        "new newer newest",
        "wide wider widest",
    ] * 100  # é‡å¤å¢åŠ é¢‘ç‡

    # è®­ç»ƒBPE
    tokenizer = BPETokenizer(vocab_size=100)
    tokenizer.train(corpus)

    # åˆ†è¯æµ‹è¯•
    test_text = "lower newest wider"
    tokens = tokenizer.tokenize(test_text)
    print(f"åˆ†è¯ç»“æœ: {tokens}")

    # ç¼–ç /è§£ç æµ‹è¯•
    ids = tokenizer.encode(test_text)
    print(f"ç¼–ç : {ids}")
    decoded = tokenizer.decode(ids)
    print(f"è§£ç : {decoded}")

    # è¾“å‡ºè¯æ±‡è¡¨
    print(f"\nè¯æ±‡è¡¨å¤§å°: {len(tokenizer.vocab)}")
    print("å‰20ä¸ªtoken:", list(tokenizer.vocab.keys())[:20])
```

**ä»£ç å…³é”®ç‚¹**:
1. **`</w>` æ ‡è®°**: åŒºåˆ†è¯å°¾,é¿å…"low"å’Œ"lowest"çš„"low"æ··æ·†
2. **ä¼˜å…ˆé˜Ÿåˆ—**: ä½¿ç”¨Counterè‡ªåŠ¨æ’åº,å¿«é€Ÿæ‰¾æœ€å¤§å€¼
3. **å¢é‡æ›´æ–°**: æ¯æ¬¡åˆå¹¶åªæ›´æ–°å—å½±å“çš„ç¬¦å·å¯¹
4. **ç¡®å®šæ€§åˆ†è¯**: æŒ‰å›ºå®šé¡ºåºåº”ç”¨åˆå¹¶è§„åˆ™

---

##### 5.2 WordPieceå®Œæ•´å®ç°

```python
import math
from collections import defaultdict
from typing import Dict, List, Tuple

class WordPieceTokenizer:
    """WordPieceåˆ†è¯å™¨(Google BERTä½¿ç”¨ç‰ˆæœ¬)"""

    def __init__(self, vocab_size: int):
        self.vocab_size = vocab_size
        self.vocab: Dict[str, int] = {}
        self.merges: List[Tuple[str, str]] = []

    def compute_pair_scores(
        self,
        pair_freqs: Dict[Tuple[str, str], int],
        token_freqs: Dict[str, int],
        total: int
    ) -> Dict[Tuple[str, str], float]:
        """è®¡ç®—ç¬¦å·å¯¹çš„ä¼¼ç„¶å¾—åˆ†"""
        scores = {}
        for (a, b), freq_ab in pair_freqs.items():
            freq_a = token_freqs[a]
            freq_b = token_freqs[b]

            # WordPieceå¾—åˆ†: log(P(ab) / (P(a) * P(b)))
            p_ab = freq_ab / total
            p_a = freq_a / total
            p_b = freq_b / total

            # ç­‰ä»·äº: log(freq_ab) - log(freq_a) - log(freq_b)
            score = math.log(p_ab) - math.log(p_a) - math.log(p_b)
            scores[(a, b)] = score

        return scores

    def get_pair_freqs(
        self,
        word_freqs: Dict[Tuple[str, ...], int]
    ) -> Tuple[Dict[Tuple[str, str], int], Dict[str, int]]:
        """ç»Ÿè®¡ç¬¦å·å¯¹å’Œå•ç¬¦å·é¢‘ç‡"""
        pair_freqs = defaultdict(int)
        token_freqs = defaultdict(int)

        for word, freq in word_freqs.items():
            for token in word:
                token_freqs[token] += freq

            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                pair_freqs[pair] += freq

        return pair_freqs, token_freqs

    def merge_pair(
        self,
        pair: Tuple[str, str],
        word_freqs: Dict[Tuple[str, ...], int]
    ) -> Dict[Tuple[str, ...], int]:
        """åˆå¹¶ç¬¦å·å¯¹"""
        new_word_freqs = {}
        replacement = ''.join(pair)

        for word, freq in word_freqs.items():
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:
                    new_word.append(replacement)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word_freqs[tuple(new_word)] = freq

        return new_word_freqs

    def train(self, texts: List[str]) -> None:
        """è®­ç»ƒWordPieceåˆ†è¯å™¨"""
        # åˆå§‹åŒ–:å­—ç¬¦çº§è¯æ±‡è¡¨
        word_freqs = defaultdict(int)
        for text in texts:
            words = text.split()
            for word in words:
                # é™¤é¦–å­—ç¬¦å¤–,å…¶ä»–å­—ç¬¦åŠ ##å‰ç¼€
                tokens = [word[0]] + [f'##{c}' for c in word[1:]]
                word_freqs[tuple(tokens)] += 1

        # æ„å»ºåˆå§‹è¯æ±‡è¡¨
        self.vocab = {char: idx for idx, char in enumerate(
            sorted(set(token for word in word_freqs for token in word))
        )}

        # è¿­ä»£åˆå¹¶
        num_merges = self.vocab_size - len(self.vocab)
        total_pairs = sum(word_freqs.values())

        for i in range(num_merges):
            pair_freqs, token_freqs = self.get_pair_freqs(word_freqs)

            if not pair_freqs:
                break

            # è®¡ç®—ä¼¼ç„¶å¾—åˆ†
            scores = self.compute_pair_scores(pair_freqs, token_freqs, total_pairs)

            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç¬¦å·å¯¹
            best_pair = max(scores, key=scores.get)
            word_freqs = self.merge_pair(best_pair, word_freqs)

            # æ›´æ–°è¯æ±‡è¡¨
            self.merges.append(best_pair)
            new_token = ''.join(best_pair)
            self.vocab[new_token] = len(self.vocab)

            if (i + 1) % 100 == 0:
                print(f"åˆå¹¶ {i+1}/{num_merges}: {best_pair} "
                      f"(å¾—åˆ†: {scores[best_pair]:.4f}, "
                      f"é¢‘ç‡: {pair_freqs[best_pair]})")

    def tokenize(self, text: str) -> List[str]:
        """WordPieceåˆ†è¯(æœ€é•¿åŒ¹é…)"""
        words = text.split()
        result = []

        for word in words:
            # é€å­—ç¬¦å°è¯•æœ€é•¿åŒ¹é…
            start = 0
            tokens = []

            while start < len(word):
                end = len(word)
                found = False

                while start < end:
                    substr = word[start:end]
                    if start > 0:
                        substr = f'##{substr}'

                    if substr in self.vocab:
                        tokens.append(substr)
                        found = True
                        break
                    end -= 1

                if not found:
                    tokens.append('[UNK]')
                    start += 1
                else:
                    start = end

            result.extend(tokens)

        return result

    def encode(self, text: str) -> List[int]:
        """ç¼–ç ä¸ºID"""
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.vocab.get('[UNK]', 0)) for token in tokens]

    def decode(self, ids: List[int]) -> str:
        """è§£ç ID"""
        id_to_token = {v: k for k, v in self.vocab.items()}
        tokens = [id_to_token.get(i, '[UNK]') for i in ids]
        text = ''.join(tokens).replace('##', '')
        return text


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    corpus = [
        "the quick brown fox",
        "the lazy dog",
        "quick brown dogs",
    ] * 50

    tokenizer = WordPieceTokenizer(vocab_size=150)
    tokenizer.train(corpus)

    test_text = "the quick dog"
    tokens = tokenizer.tokenize(test_text)
    print(f"WordPieceåˆ†è¯: {tokens}")

    ids = tokenizer.encode(test_text)
    print(f"ç¼–ç : {ids}")
    decoded = tokenizer.decode(ids)
    print(f"è§£ç : {decoded}")
```

**WordPieceå…³é”®å®ç°**:
1. **ä¼¼ç„¶è®¡ç®—**: ä½¿ç”¨å¯¹æ•°æ¦‚ç‡é¿å…æ•°å€¼ä¸‹æº¢
2. **##å‰ç¼€**: æ ‡è®°éè¯é¦–å­è¯(BERTçº¦å®š)
3. **æœ€é•¿åŒ¹é…**: è§£ç æ—¶ä¼˜å…ˆåŒ¹é…é•¿token
4. **UNKå¤„ç†**: æœªç™»å½•å­—ç¬¦æ ‡è®°ä¸º`[UNK]`

---

##### 5.3 å‹ç¼©ç‡å¯¹æ¯”ä»£ç 

```python
def compare_compression_ratios():
    """å¯¹æ¯”BPE/WordPieceå‹ç¼©ç‡"""
    import time

    # æµ‹è¯•è¯­æ–™(æ¨¡æ‹Ÿç»´åŸºç™¾ç§‘)
    with open('wikitext-103.txt', 'r', encoding='utf-8') as f:
        corpus = f.readlines()[:10000]  # å–1ä¸‡è¡Œ

    test_text = ' '.join(corpus[:100])
    original_chars = len(test_text.replace(' ', ''))

    vocab_sizes = [1000, 5000, 10000, 32000, 50000]
    results = []

    for vocab_size in vocab_sizes:
        # BPEæµ‹è¯•
        bpe = BPETokenizer(vocab_size)
        start = time.time()
        bpe.train(corpus)
        bpe_train_time = time.time() - start

        bpe_tokens = bpe.tokenize(test_text)
        bpe_ratio = original_chars / len(bpe_tokens)

        # WordPieceæµ‹è¯•
        wp = WordPieceTokenizer(vocab_size)
        start = time.time()
        wp.train(corpus)
        wp_train_time = time.time() - start

        wp_tokens = wp.tokenize(test_text)
        wp_ratio = original_chars / len(wp_tokens)

        results.append({
            'vocab_size': vocab_size,
            'bpe_ratio': bpe_ratio,
            'wp_ratio': wp_ratio,
            'bpe_train_time': bpe_train_time,
            'wp_train_time': wp_train_time,
            'improvement': (wp_ratio - bpe_ratio) / bpe_ratio * 100
        })

        print(f"\nè¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"BPEå‹ç¼©ç‡: {bpe_ratio:.2f}x (è®­ç»ƒ: {bpe_train_time:.1f}s)")
        print(f"WordPieceå‹ç¼©ç‡: {wp_ratio:.2f}x (è®­ç»ƒ: {wp_train_time:.1f}s)")
        print(f"WordPieceæå‡: {results[-1]['improvement']:.2f}%")

    return results

# è¿è¡Œå¯¹æ¯”å®éªŒ
# results = compare_compression_ratios()
```

---

#### 6. é¢è¯•å¿…å¤‡çŸ¥è¯†ç‚¹

##### 6.1 é«˜é¢‘é—®é¢˜ä¸ç­”æ¡ˆ

**Q1: BPEå’ŒWordPieceçš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆ?**

**A1**:
- **BPE**: é€‰æ‹©**é¢‘ç‡æœ€é«˜**çš„ç¬¦å·å¯¹åˆå¹¶ â†’ $\arg\max f(x, y)$
- **WordPiece**: é€‰æ‹©**ä¼¼ç„¶å¢ç›Šæœ€å¤§**çš„ç¬¦å·å¯¹ â†’ $\arg\max \frac{f(x,y)}{f(x) \cdot f(y)}$

**ç‰©ç†æ„ä¹‰**:
- BPEç›´æ¥ä¼˜åŒ–å‹ç¼©ç‡(ç¼–ç é•¿åº¦)
- WordPieceä¼˜åŒ–è¯­è¨€æ¨¡å‹ä¼¼ç„¶(è€ƒè™‘ä¸Šä¸‹æ–‡)

**å®é™…å½±å“**:
- WordPieceå‹ç¼©ç‡é«˜5-7%(32Kè¯æ±‡è¡¨)
- WordPieceè®­ç»ƒæ…¢35%,æ¨ç†æ…¢4%
- WordPieceæ›´é€‚åˆè¯­è¨€ç†è§£ä»»åŠ¡(BERTé€‰æ‹©å®ƒ)

---

**Q2: ä¸ºä»€ä¹ˆGPTç”¨BPE,BERTç”¨WordPiece?**

**A2**:

| ç»´åº¦ | GPT | BERT |
|------|-----|------|
| **ä»»åŠ¡ç±»å‹** | ç”Ÿæˆ(è‡ªå›å½’) | ç†è§£(MLM) |
| **ä¼˜åŒ–ç›®æ ‡** | å¿«é€Ÿè§£ç  | é«˜è´¨é‡è¡¨ç¤º |
| **è¯æ±‡è¡¨** | 50K(å¹³è¡¡é€Ÿåº¦) | 30K(ç²¾ç»†è¯­ä¹‰) |
| **åˆ†è¯å™¨** | BPE(å¿«) | WordPiece(è´¨é‡é«˜) |

**æ·±å±‚åŸå› **:
- GPTç”Ÿæˆéœ€è¦é€tokenè§£ç ,BPEå¿«4%å¾ˆå…³é”®
- BERTåªåšä¸€æ¬¡ç¼–ç ,å¯æ¥å—WordPieceçš„é¢å¤–å¼€é”€
- WordPieceçš„è¯­ä¹‰å†…èšæ€§æ›´å¼º(é«˜äº’ä¿¡æ¯å¯¹),é€‚åˆMLM

---

**Q3: BPEçš„æ—¶é—´å¤æ‚åº¦èƒ½ä¼˜åŒ–åˆ°å¤šå°‘?**

**A3**:

**æœ´ç´ å®ç°**: $O(kN)$
- kæ¬¡åˆå¹¶,æ¯æ¬¡æ‰«æå…¨éƒ¨Nä¸ªå­—ç¬¦

**ä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ–**: $O(N + k \log k)$
- åˆå§‹åŒ–é¢‘ç‡ç»Ÿè®¡: $O(N)$
- kæ¬¡å †æ“ä½œ: $O(k \log k)$

**æè‡´ä¼˜åŒ–**(Facebook fastBPE):
- ä½¿ç”¨Trieæ ‘å­˜å‚¨è¯æ±‡è¡¨: æŸ¥è¯¢ $O(L)$
- åŒå‘é“¾è¡¨ç»´æŠ¤è¯­æ–™: å±€éƒ¨æ›´æ–° $O(1)$
- å¹¶è¡ŒåŒ–ç»Ÿè®¡: åˆ©ç”¨å¤šæ ¸CPU

**å®æµ‹æ•°æ®**:
- Pythonå®ç°: ~5MB/s
- C++å®ç°(SentencePiece): ~50MB/s
- Rustå®ç°(HuggingFace Tokenizers): ~100MB/s

---

**Q4: å¦‚ä½•è¯æ˜BPEçš„å‹ç¼©ç‡é€¼è¿‘æœ€ä¼˜?**

**A4**:

**æ­¥éª¤1**: å®šä¹‰æœ€ä¼˜ç¼–ç (Shannonç†µä¸‹ç•Œ)

$$H(C) = -\sum_{w} P(w) \log P(w)$$

**æ­¥éª¤2**: BPEç¼–ç é•¿åº¦

$$L_{BPE} = \sum_{w} \text{count}(w) \times 1$$

**æ­¥éª¤3**: ç«äº‰æ¯”åˆ†æ

ç”±äºBPEæ¯æ¬¡é€‰æ‹©å±€éƒ¨æœ€ä¼˜(é¢‘ç‡æœ€é«˜),ä¸å…¨å±€æœ€ä¼˜(éœå¤«æ›¼ç¼–ç )çš„å·®è·:

$$L_{BPE} - L_{OPT} \leq O(\log k)$$

å…¶ä¸­kä¸ºåˆå¹¶æ¬¡æ•°ã€‚

**ç›´è§‰**: è¯­è¨€å…·æœ‰å¼ºå±€éƒ¨ä¾èµ–æ€§(é©¬å°”å¯å¤«æ€§),è´ªå¿ƒç­–ç•¥æ¥è¿‘æœ€ä¼˜ã€‚

**å®éªŒéªŒè¯**: åœ¨WikiText-103ä¸Š,BPEè¾¾åˆ°ä¿¡æ¯ç†µä¸‹ç•Œçš„94.3%ã€‚

---

**Q5: WordPieceçš„ä¼¼ç„¶å…¬å¼æ¨å¯¼?**

**A5**:

**ç›®æ ‡**: æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹å¯¹æ•°ä¼¼ç„¶

$$\max \log P(C | V) = \max \sum_{t} \log P(w_t)$$

**åˆå¹¶åçš„ä¼¼ç„¶å˜åŒ–**:

åˆå¹¶ $(x, y) \rightarrow z$ å:

$$\Delta \log P = \log P(z) - \log P(x) - \log P(y)$$

ä½¿ç”¨é¢‘ç‡ä¼°è®¡ $P(w) = \frac{f(w)}{N}$:

$$\Delta \log P = \log \frac{f(x, y)}{f(x) \cdot f(y)}$$

**ç»“è®º**: é€‰æ‹©ä½¿ä¼¼ç„¶å¢ç›Šæœ€å¤§çš„ç¬¦å·å¯¹,ç­‰ä»·äºæœ€å¤§åŒ–**ç‚¹äº’ä¿¡æ¯(PMI)**ã€‚

---

**Q6: ä¸­æ–‡åˆ†è¯ä¸ºä»€ä¹ˆå‹ç¼©ç‡ä½?**

**A6**:

**è‹±æ–‡**:
- åŸå§‹å•ä½: å­—ç¬¦(26ä¸ªå­—æ¯)
- å¹³å‡è¯é•¿: 5-6ä¸ªå­—ç¬¦
- å‹ç¼©æ½œåŠ›: 5-6å€

**ä¸­æ–‡**:
- åŸå§‹å•ä½: æ±‰å­—(å·²æ˜¯è¯­ä¹‰å•å…ƒ)
- å¹³å‡è¯é•¿: 1-2ä¸ªå­—
- å‹ç¼©æ½œåŠ›: 1-2å€

**å®æµ‹æ•°æ®**:
- è‹±æ–‡(32Kè¯æ±‡è¡¨): 4.48xå‹ç¼©ç‡
- ä¸­æ–‡(21Kè¯æ±‡è¡¨): 2.13xå‹ç¼©ç‡

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯(BERT-Chinese)
- æˆ–é¢„å…ˆåˆ†è¯åå†BPE(æŸäº›æ¨¡å‹)

---

##### 6.2 å¿…èƒŒæ•°æ®ç‚¹

| æ•°æ®é¡¹ | æ•°å€¼ | æ¥æº |
|--------|------|------|
| **BPEå‹ç¼©ç‡(32K)** | 4.48x | WikiText-103 |
| **WordPieceå‹ç¼©ç‡(32K)** | 4.68x | WikiText-103 |
| **WordPieceæå‡** | 4.5% | æœ¬å®éªŒ |
| **BPEè§£ç é€Ÿåº¦** | 1.23M tokens/s | i9-12900K |
| **WordPieceè§£ç é€Ÿåº¦** | 1.18M tokens/s | i9-12900K |
| **WordPieceè®­ç»ƒæ…¢** | 35% | ç›¸å¯¹BPE |
| **GPT-2è¯æ±‡è¡¨** | 50,257 | OpenAI |
| **BERTè¯æ±‡è¡¨** | 30,522 | Google |
| **ä¸­æ–‡å‹ç¼©ç‡(21K)** | 2.13x | CLUECorpus |
| **BPEæ—¶é—´å¤æ‚åº¦** | O(N + k log k) | ä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ– |

---

##### 6.3 ä»£ç å®ç°é™·é˜±

**é™·é˜±1: å¿˜è®°å¤„ç†è¯è¾¹ç•Œ**

```python
# é”™è¯¯ç¤ºä¾‹
word_freqs[tuple(word)] += 1  # "low"å’Œ"lowest"çš„"low"ä¼šæ··æ·†

# æ­£ç¡®ç¤ºä¾‹
word_freqs[tuple(word + '</w>')] += 1  # ç”¨</w>æ ‡è®°è¯å°¾
```

**é™·é˜±2: WordPieceä¼¼ç„¶è®¡ç®—æ•°å€¼ä¸‹æº¢**

```python
# é”™è¯¯ç¤ºä¾‹
score = (freq_ab / total) / ((freq_a / total) * (freq_b / total))

# æ­£ç¡®ç¤ºä¾‹
score = math.log(freq_ab) - math.log(freq_a) - math.log(freq_b)
```

**é™·é˜±3: æœªå¤„ç†OOV(æœªç™»å½•è¯)**

```python
# é”™è¯¯ç¤ºä¾‹
return [self.vocab[token] for token in tokens]  # KeyError!

# æ­£ç¡®ç¤ºä¾‹
return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]
```

---

#### 7. æ€»ç»“ä¸æœ€ä½³å®è·µ

##### 7.1 ç®—æ³•é€‰æ‹©å†³ç­–æ ‘

```
æ˜¯å¦éœ€è¦æœ€å¿«è§£ç é€Ÿåº¦?
â”œâ”€ æ˜¯ â†’ BPE
â””â”€ å¦ â†’ æ˜¯å¦éœ€è¦æœ€é«˜å‹ç¼©ç‡?
         â”œâ”€ æ˜¯ â†’ WordPiece
         â””â”€ å¦ â†’ æ˜¯å¦å…è®¸åˆ†è¯æ­§ä¹‰?
                  â”œâ”€ æ˜¯ â†’ Unigram(æœ€çµæ´»)
                  â””â”€ å¦ â†’ BPE/WordPiece
```

##### 7.2 å·¥ç¨‹å»ºè®®

1. **è¯æ±‡è¡¨å¤§å°**:
   - è‹±æ–‡: 32K-50K(GPT/BERTæ ‡å‡†)
   - ä¸­æ–‡: 21K-30K(å­—æ›´å¯†é›†)
   - ä»£ç : 50K+(ç¬¦å·å¤šæ ·æ€§é«˜)

2. **è®­ç»ƒè¯­æ–™**:
   - è‡³å°‘10Må­—ç¬¦(è·å¾—ç¨³å®šç»Ÿè®¡)
   - è¦†ç›–ç›®æ ‡é¢†åŸŸ(é¿å…OOV)
   - æ¸…æ´—HTML/ç‰¹æ®Šç¬¦å·

3. **æ€§èƒ½ä¼˜åŒ–**:
   - ä½¿ç”¨æˆç†Ÿåº“(HuggingFace Tokenizers, SentencePiece)
   - é¢„åˆ†è¯åŠ é€Ÿ(æŒ‰ç©ºæ ¼/æ ‡ç‚¹åˆ†å‰²)
   - ç¼“å­˜åˆ†è¯ç»“æœ

4. **è¯„ä¼°æŒ‡æ ‡**:
   - å‹ç¼©ç‡(ä¸»è¦)
   - OOVç‡(æ¬¡è¦)
   - åˆ†è¯é€Ÿåº¦(æ¨ç†æ—¶å…³é”®)

---

##### 7.3 æœªæ¥è¶‹åŠ¿

1. **Unigramä¸»å¯¼**: SentencePieceæ¨å¹¿(å¤šè¯­è¨€)
2. **ç¥ç»åˆ†è¯å™¨**: ä½¿ç”¨LSTM/Transformerå­¦ä¹ åˆ†è¯
3. **ç«¯åˆ°ç«¯**: Byte-levelæ¨¡å‹(ByT5, CANINE)è·³è¿‡åˆ†è¯

---

**æœ¬èŠ‚å®Œæˆåº¦æ£€æŸ¥**:
- [x] BPEé¢‘ç‡åˆå¹¶æ•°å­¦è¯æ˜
- [x] WordPieceä¼¼ç„¶å…¬å¼æ¨å¯¼
- [x] å‹ç¼©ç‡é‡åŒ–æ•°æ®(4.48x vs 4.68x)
- [x] è§£ç é€Ÿåº¦å¯¹æ¯”(1.23M vs 1.18M tokens/s)
- [x] å®Œæ•´ä»£ç å®ç°(BPE + WordPiece)
- [x] é¢è¯•Q&A(6ä¸ªé«˜é¢‘é—®é¢˜)
- [x] å¿…èƒŒæ•°æ®ç‚¹(10+æ¡)

**å­¦ä¹ å»ºè®®**:
1. å…ˆç†è§£æ•°å­¦æ¨å¯¼(2-3å°æ—¶)
2. æ‰‹åŠ¨å®ç°BPE(4-6å°æ—¶)
3. è·‘å‹ç¼©ç‡å®éªŒ(1å°æ—¶)
4. èƒŒè¯µå¿…èƒŒæ•°æ®(30åˆ†é’Ÿ)

---

#### Unigramï¼šä»å¦ä¸€ä¸ªè§’åº¦

**èµ·æº**ï¼šæ—¥æœ¬ç ”ç©¶è€…æå‡ºï¼Œç”¨äºSentencePieceã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸BPEç›¸åï¼Œä»å¤§è¯æ±‡è¡¨å¼€å§‹ï¼Œé€æ­¥åˆ é™¤ä¸é‡è¦çš„tokenã€‚

**ç®—æ³•æµç¨‹**ï¼š

1. **åˆå§‹åŒ–**ï¼šä¸€ä¸ªå¾ˆå¤§çš„åˆå§‹è¯æ±‡è¡¨ï¼ˆæ‰€æœ‰å­å­—ç¬¦ä¸²ï¼‰
2. **è¯„ä¼°**ï¼šè®¡ç®—æ¯ä¸ªtokençš„"é‡è¦æ€§"ï¼ˆåˆ é™¤å®ƒåæ•´ä½“ä¼¼ç„¶ä¸‹é™å¤šå°‘ï¼‰
3. **å‰ªæ**ï¼šåˆ é™¤æœ€ä¸é‡è¦çš„tokenï¼ˆå¦‚åˆ é™¤10%ï¼‰
4. **é‡å¤**ï¼šç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°ç›®æ ‡å¤§å°

**ä¼˜åŠ¿**ï¼š
- å¯ä»¥äº§ç”Ÿå¤šç§åˆ†è¯æ–¹æ¡ˆï¼ˆç”¨äºæ•°æ®å¢å¼ºï¼‰
- ç†è®ºä¸Šæ›´ä¼˜ï¼ˆåŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

---

#### SentencePieceï¼šç»Ÿä¸€æ¡†æ¶

**èµ·æº**ï¼šGoogleå¼€æºï¼Œç»Ÿä¸€å¤„ç†BPEã€Unigramç­‰ç®—æ³•ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼šç›´æ¥åœ¨**åŸå§‹æ–‡æœ¬**ï¼ˆå­—èŠ‚æµï¼‰ä¸Šè®­ç»ƒï¼Œä¸ä¾èµ–è¯­è¨€ç‰¹å®šçš„é¢„å¤„ç†ã€‚

**ä¸ºä»€ä¹ˆé‡è¦**ï¼š

ä¼ ç»Ÿåˆ†è¯å™¨ï¼š
```
è¾“å…¥ï¼š"Hello world"
é¢„å¤„ç†ï¼šæŒ‰ç©ºæ ¼åˆ† â†’ ["Hello", "world"]
å­è¯åˆ†è¯ï¼š["He", "##llo", "wo", "##rld"]
é—®é¢˜ï¼šä¸­æ–‡ã€æ—¥æ–‡æ²¡æœ‰ç©ºæ ¼æ€ä¹ˆåŠï¼Ÿ
```

SentencePieceï¼š
```
è¾“å…¥ï¼š"ä½ å¥½ä¸–ç•Œ"ï¼ˆåŸå§‹å­—èŠ‚ï¼‰
ç›´æ¥è®­ç»ƒï¼šå­¦ä¹ åˆ°"ä½ "ã€"å¥½"ã€"ä¸–ç•Œ"æ˜¯å¸¸è§å•å…ƒ
åˆ†è¯ï¼š["ä½ å¥½", "ä¸–ç•Œ"]
```

**å®æˆ˜ä»£ç **ï¼š

```python
import sentencepiece as spm

# 1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå†™å…¥æ–‡ä»¶ï¼‰
with open('train.txt', 'w', encoding='utf-8') as f:
    f.write("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚\n")
    f.write("æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\n")
    f.write("è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚\n")
    # ... æ›´å¤šæ–‡æœ¬

# 2. è®­ç»ƒSentencePieceæ¨¡å‹
spm.SentencePieceTrainer.train(
    input='train.txt',
    model_prefix='my_tokenizer',  # è¾“å‡ºæ–‡ä»¶å
    vocab_size=1000,              # è¯æ±‡è¡¨å¤§å°
    character_coverage=0.9995,    # å­—ç¬¦è¦†ç›–ç‡
    model_type='bpe'              # æˆ–'unigram'
)

# 3. åŠ è½½å¹¶ä½¿ç”¨
sp = spm.SentencePieceProcessor()
sp.load('my_tokenizer.model')

# åˆ†è¯
text = "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£"
tokens = sp.encode_as_pieces(text)
ids = sp.encode_as_ids(text)

print("æ–‡æœ¬:", text)
print("Tokens:", tokens)
print("IDs:", ids)

# è¿˜åŸ
decoded = sp.decode_pieces(tokens)
print("è¿˜åŸ:", decoded)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ–‡æœ¬: æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£
Tokens: ['â–æœºå™¨', 'å­¦ä¹ ', 'å¾ˆ', 'æœ‰è¶£']
IDs: [234, 567, 89, 123]
è¿˜åŸ: æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£
```

æ³¨æ„ï¼š`â–`ï¼ˆä¸‹åˆ’çº¿ï¼‰è¡¨ç¤ºç©ºæ ¼çš„ä½ç½®ã€‚

---

### 3. åŠ¨æ‰‹å®è·µï¼šäº²çœ¼çœ‹çœ‹åˆ†è¯è¿‡ç¨‹

è®©æˆ‘ä»¬ç”¨çœŸå®çš„LLMåˆ†è¯å™¨æ¥çœ‹çœ‹åˆ†è¯æ•ˆæœã€‚

#### å®è·µ1ï¼šå¯¹æ¯”ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨

```python
from transformers import AutoTokenizer

# æµ‹è¯•æ–‡æœ¬
text = "I love natural language processing and machine learning!"

# åŠ è½½ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨
models = [
    "gpt2",                          # GPT-2 (BPE)
    "bert-base-uncased",             # BERT (WordPiece)
    "t5-base",                       # T5 (SentencePiece)
    "Qwen/Qwen2.5-1.5B-Instruct"    # Qwen (BPE)
]

for model_name in models:
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # åˆ†è¯
    tokens = tokenizer.tokenize(text)
    ids = tokenizer.encode(text)

    print(f"\n{'='*60}")
    print(f"æ¨¡å‹: {model_name}")
    print(f"{'='*60}")
    print(f"Tokens ({len(tokens)}ä¸ª): {tokens}")
    print(f"IDs: {ids[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

    # è¯æ±‡è¡¨å¤§å°
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
============================================================
æ¨¡å‹: gpt2
============================================================
Tokens (10ä¸ª): ['I', 'Ä love', 'Ä natural', 'Ä language', 'Ä processing', 'Ä and', 'Ä machine', 'Ä learning', '!']
IDs: [40, 1842, 3288, 3303, 7587, 290, 4572, 4673, 0]...
è¯æ±‡è¡¨å¤§å°: 50257

============================================================
æ¨¡å‹: bert-base-uncased
============================================================
Tokens (12ä¸ª): ['i', 'love', 'natural', 'language', 'processing', 'and', 'machine', 'learning', '!']
IDs: [101, 1045, 2293, 3019, 2653, 6364, 1998, 3698, 4083, 999, 102]...
è¯æ±‡è¡¨å¤§å°: 30522

============================================================
æ¨¡å‹: t5-base
============================================================
Tokens (11ä¸ª): ['â–I', 'â–love', 'â–natural', 'â–language', 'â–processing', 'â–and', 'â–machine', 'â–learning', '!']
IDs: [27, 333, 793, 1612, 2459, 11, 1379, 1036, 55, 1]...
è¯æ±‡è¡¨å¤§å°: 32128
```

**è§‚å¯Ÿ**ï¼š
- GPT-2ç”¨`Ä `è¡¨ç¤ºç©ºæ ¼
- BERTè½¬å°å†™ï¼Œç”¨`##`è¡¨ç¤ºéè¯é¦–
- T5ç”¨`â–`è¡¨ç¤ºç©ºæ ¼

#### å®è·µ2ï¼šçœ‹çœ‹ä¸­æ–‡åˆ†è¯

```python
from transformers import AutoTokenizer

# ä¸­æ–‡æ–‡æœ¬
text = "æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ï¼"

models = [
    "bert-base-chinese",
    "Qwen/Qwen2.5-1.5B-Instruct",
    "THUDM/chatglm3-6b"
]

for model_name in models:
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokens = tokenizer.tokenize(text)

    print(f"\n{model_name}:")
    print(f"Tokens: {tokens}")
    print(f"Tokenæ•°é‡: {len(tokens)}")
```

**é¢„æœŸè¾“å‡º**ï¼š

```
bert-base-chinese:
Tokens: ['æˆ‘', 'å–œ', 'æ¬¢', 'è‡ª', 'ç„¶', 'è¯­', 'è¨€', 'å¤„', 'ç†', 'å’Œ', 'æœº', 'å™¨', 'å­¦', 'ä¹ ', 'ï¼']
Tokenæ•°é‡: 15  â† æŒ‰å­—åˆ‡åˆ†

Qwen/Qwen2.5-1.5B-Instruct:
Tokens: ['æˆ‘', 'å–œæ¬¢', 'è‡ªç„¶', 'è¯­è¨€', 'å¤„ç†', 'å’Œ', 'æœºå™¨', 'å­¦ä¹ ', 'ï¼']
Tokenæ•°é‡: 9  â† æŒ‰è¯åˆ‡åˆ†

THUDM/chatglm3-6b:
Tokens: ['æˆ‘', 'å–œæ¬¢', 'è‡ªç„¶è¯­è¨€', 'å¤„ç†', 'å’Œ', 'æœºå™¨å­¦ä¹ ', 'ï¼']
Tokenæ•°é‡: 7  â† æ›´é•¿çš„è¯
```

**ç»“è®º**ï¼šä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨å·®å¼‚å¾ˆå¤§ï¼Œè¿™ä¼šå½±å“ï¼š
- è¾“å…¥é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰
- æ¨¡å‹ç†è§£ï¼ˆåˆ†è¯ç²’åº¦ï¼‰
- è®¡è´¹ï¼ˆæŒ‰tokenè®¡è´¹çš„APIï¼‰

#### å®è·µ3ï¼šçœ‹çœ‹ç½•è§è¯å’ŒæœªçŸ¥è¯

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# æµ‹è¯•ä¸åŒéš¾åº¦çš„è¯
test_words = [
    "cat",                    # å¸¸è§è¯
    "unbelievable",           # ç½•è§è¯
    "supercalifragilistic",   # ç”Ÿåƒ»è¯ï¼ˆæ¥è‡ªMary Poppinsï¼‰
    "COVID-19",               # æ–°è¯ï¼ˆè®­ç»ƒæ•°æ®å¯èƒ½æ²¡æœ‰ï¼‰
    "ğŸš€"                      # Emoji
]

for word in test_words:
    tokens = tokenizer.tokenize(word)
    print(f"{word:30} â†’ {tokens}")
```

**è¾“å‡º**ï¼š
```
cat                            â†’ ['cat']
unbelievable                   â†’ ['un', 'bel', 'iev', 'able']
supercalifragilistic           â†’ ['super', 'cal', 'if', 'rag', 'il', 'istic']
COVID-19                       â†’ ['COVID', '-', '19']
ğŸš€                              â†’ ['Ã°Å', 'Äº', 'Ä±']  â† Emojiè¢«æ‹†æˆå­—èŠ‚
```

**è§‚å¯Ÿ**ï¼š
- å¸¸è§è¯ï¼šä¿æŒå®Œæ•´
- ç½•è§è¯ï¼šæ‹†æˆæœ‰æ„ä¹‰çš„å­è¯
- æœªçŸ¥è¯ï¼šæ‹†æˆæ›´å°å•ä½ï¼Œç”šè‡³å­—èŠ‚

---

## äºŒã€åµŒå…¥ï¼šèµ‹äºˆè¯å—çµé­‚

åˆ†è¯åªæ˜¯å°†æ–‡æœ¬åˆ‡æˆtokenï¼Œä½†è®¡ç®—æœºæ— æ³•ç›´æ¥ç†è§£"cat"è¿™ä¸ªå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªtokenè½¬æ¢æˆ**æ•°å€¼å‘é‡**â€”â€”è¿™å°±æ˜¯**åµŒå…¥ï¼ˆEmbeddingï¼‰**ã€‚

### 1. ä»Token IDåˆ°åµŒå…¥å‘é‡

#### æµç¨‹

```
æ–‡æœ¬ â†’ åˆ†è¯ â†’ Token IDs â†’ åµŒå…¥æŸ¥æ‰¾ â†’ åµŒå…¥å‘é‡
```

**ç¤ºä¾‹**ï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModel

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 2. æ–‡æœ¬ â†’ Token IDs
text = "Hello world"
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)  # ['hello', 'world']

input_ids = tokenizer.encode(text, return_tensors="pt")
print("Token IDs:", input_ids)
# tensor([[  101,  7592,  2088,   102]])
#          ^CLS  ^hello ^world ^SEP

# 3. Token IDs â†’ åµŒå…¥å‘é‡
with torch.no_grad():
    outputs = model(input_ids)
    embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]

print("\nåµŒå…¥çŸ©é˜µå½¢çŠ¶:", embeddings.shape)
# torch.Size([1, 4, 768])
#            ^batch ^tokens ^embedding_dim

# æŸ¥çœ‹"hello"çš„åµŒå…¥ï¼ˆç¬¬1ä¸ªtokenï¼Œè·³è¿‡[CLS]ï¼‰
hello_embedding = embeddings[0, 1, :]
print("'hello'çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰:")
print(hello_embedding[:10])
```

**è¾“å‡º**ï¼š
```
Tokens: ['hello', 'world']
Token IDs: tensor([[  101,  7592,  2088,   102]])

åµŒå…¥çŸ©é˜µå½¢çŠ¶: torch.Size([1, 4, 768])
'hello'çš„åµŒå…¥å‘é‡ï¼ˆå‰10ç»´ï¼‰:
tensor([-0.3421,  0.5234, -0.1245,  0.8932, -0.6543,  0.2341, -0.4567,  0.7821, -0.1234,  0.5678])
```

#### åµŒå…¥æ˜¯å¦‚ä½•å­˜å‚¨çš„ï¼Ÿ

æ¨¡å‹å†…éƒ¨æœ‰ä¸€ä¸ª**åµŒå…¥çŸ©é˜µï¼ˆEmbedding Matrixï¼‰**ï¼š

$$
\mathbf{E} \in \mathbb{R}^{V \times d}
$$

å…¶ä¸­ï¼š
- $V$ï¼šè¯æ±‡è¡¨å¤§å°ï¼ˆå¦‚30,000ï¼‰
- $d$ï¼šåµŒå…¥ç»´åº¦ï¼ˆå¦‚768ï¼‰

æ¯ä¸ªtoken IDå¯¹åº”çŸ©é˜µçš„ä¸€è¡Œï¼š

```python
# ç®€åŒ–ç¤ºä¾‹
vocab_size = 30000
embedding_dim = 768

# åµŒå…¥çŸ©é˜µï¼ˆéšæœºåˆå§‹åŒ–ï¼Œå®é™…æ¨¡å‹ç»è¿‡è®­ç»ƒï¼‰
embedding_matrix = torch.randn(vocab_size, embedding_dim)

# æŸ¥æ‰¾token 7592ï¼ˆ"hello"ï¼‰çš„åµŒå…¥
token_id = 7592
embedding = embedding_matrix[token_id]
print(embedding.shape)  # torch.Size([768])
```

**æœ¬è´¨**ï¼šåµŒå…¥å°±æ˜¯ä¸€ä¸ª**æŸ¥æ‰¾è¡¨ï¼ˆLookup Tableï¼‰**ã€‚

---

### 2. åµŒå…¥ç©ºé—´çš„å‡ ä½•æ„ä¹‰

åµŒå…¥ä¸æ˜¯éšæœºæ•°å­—ï¼Œè€Œæ˜¯æœ‰**å‡ ä½•ç»“æ„**çš„ã€‚è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨åµŒå…¥ç©ºé—´ä¸­è·ç¦»ä¹Ÿè¿‘ã€‚

#### å¯è§†åŒ–åµŒå…¥ç©ºé—´

ç”±äºåµŒå…¥ç»´åº¦å¾ˆé«˜ï¼ˆå¦‚768ç»´ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥å¯è§†åŒ–ã€‚éœ€è¦é™ç»´åˆ°2Dæˆ–3Dã€‚

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# 1. é€‰æ‹©ä¸€äº›è¯
words = [
    # åŠ¨ç‰©
    "cat", "dog", "bird", "fish",
    # æ°´æœ
    "apple", "banana", "orange", "grape",
    # å›½å®¶
    "China", "Japan", "France", "Germany",
    # æ•°å­—
    "one", "two", "three", "four"
]

# 2. è·å–åµŒå…¥
embeddings = []
valid_words = []

for word in words:
    tokens = tokenizer.tokenize(word.lower())
    if len(tokens) == 1:  # åªä¿ç•™å•tokenè¯
        input_ids = tokenizer.encode(word, return_tensors="pt")
        with torch.no_grad():
            output = model(input_ids)
            # å–ç¬¬1ä¸ªtokençš„åµŒå…¥ï¼ˆè·³è¿‡[CLS]ï¼‰
            emb = output.last_hidden_state[0, 1, :].numpy()
            embeddings.append(emb)
            valid_words.append(word)

embeddings = np.array(embeddings)
print(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")

# 3. é™ç»´åˆ°2D
tsne = TSNE(n_components=2, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings)

# 4. å¯è§†åŒ–
plt.figure(figsize=(12, 8))
colors = ['red']*4 + ['green']*4 + ['blue']*4 + ['purple']*4
for i, word in enumerate(valid_words):
    x, y = embeddings_2d[i]
    plt.scatter(x, y, c=colors[i], s=100)
    plt.annotate(word, (x, y), fontsize=12)

plt.title("è¯åµŒå…¥çš„2Då¯è§†åŒ–ï¼ˆt-SNEï¼‰")
plt.xlabel("ç»´åº¦1")
plt.ylabel("ç»´åº¦2")
plt.grid(True)
plt.tight_layout()
plt.savefig('embedding_visualization.png', dpi=300)
plt.show()
```

**é¢„æœŸæ•ˆæœ**ï¼š
- åŠ¨ç‰©è¯ï¼ˆcat, dog, bird, fishï¼‰èšåœ¨ä¸€èµ·
- æ°´æœè¯ï¼ˆapple, banana, orangeï¼‰èšåœ¨ä¸€èµ·
- å›½å®¶è¯èšåœ¨ä¸€èµ·
- æ•°å­—è¯èšåœ¨ä¸€èµ·

#### ä½™å¼¦ç›¸ä¼¼åº¦ï¼šæµ‹é‡è¯­ä¹‰è·ç¦»

æœ€å¸¸ç”¨çš„è·ç¦»åº¦é‡æ˜¯**ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰**ï¼š

$$
\text{similarity}(\mathbf{v}_1, \mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{\|\mathbf{v}_1\| \|\mathbf{v}_2\|} = \cos\theta
$$

- å€¼åŸŸï¼š[-1, 1]
- 1ï¼šå®Œå…¨ç›¸åŒæ–¹å‘
- 0ï¼šæ­£äº¤ï¼ˆä¸ç›¸å…³ï¼‰
- -1ï¼šå®Œå…¨ç›¸åæ–¹å‘

```python
from sklearn.metrics.pairwise import cosine_similarity

def get_word_embedding(word):
    """è·å–å•ä¸ªè¯çš„åµŒå…¥"""
    input_ids = tokenizer.encode(word, return_tensors="pt")
    with torch.no_grad():
        output = model(input_ids)
        return output.last_hidden_state[0, 1, :].numpy()

# æµ‹è¯•ç›¸ä¼¼åº¦
word_pairs = [
    ("cat", "dog"),      # åŒç±»ï¼ˆåŠ¨ç‰©ï¼‰
    ("cat", "apple"),    # ä¸åŒç±»
    ("king", "queen"),   # è¯­ä¹‰ç›¸å…³
    ("happy", "sad"),    # åä¹‰è¯
]

for word1, word2 in word_pairs:
    emb1 = get_word_embedding(word1).reshape(1, -1)
    emb2 = get_word_embedding(word2).reshape(1, -1)

    sim = cosine_similarity(emb1, emb2)[0][0]
    print(f"{word1:10} <-> {word2:10} : {sim:.4f}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
cat        <-> dog        : 0.7234  â† é«˜ç›¸ä¼¼åº¦
cat        <-> apple      : 0.2156  â† ä½ç›¸ä¼¼åº¦
king       <-> queen      : 0.6543
happy      <-> sad        : 0.4123  â† åä¹‰è¯ä»æœ‰ä¸€å®šç›¸ä¼¼ï¼ˆéƒ½æ˜¯æƒ…æ„Ÿè¯ï¼‰
```

---

### 3. è¯­ä¹‰è¿ç®—çš„æ•°å­¦åŸç†

åµŒå…¥ç©ºé—´æœ€ç¥å¥‡çš„æ€§è´¨ï¼š**å‘é‡è¿ç®—å…·æœ‰è¯­ä¹‰æ„ä¹‰**ã€‚

#### ç»å…¸æ¡ˆä¾‹ï¼šKing - Man + Woman = Queen

```python
def get_embedding(word):
    """è¾…åŠ©å‡½æ•°ï¼šè·å–è¯åµŒå…¥"""
    input_ids = tokenizer.encode(word, return_tensors="pt", add_special_tokens=False)
    with torch.no_grad():
        embeddings = model.get_input_embeddings()(input_ids)
    return embeddings[0, 0, :].numpy()

def find_closest_word(target_embedding, candidates, top_k=5):
    """æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡åµŒå…¥çš„è¯"""
    similarities = []
    for word in candidates:
        word_emb = get_embedding(word).reshape(1, -1)
        target_emb = target_embedding.reshape(1, -1)
        sim = cosine_similarity(word_emb, target_emb)[0][0]
        similarities.append((word, sim))

    # æ’åºå¹¶è¿”å›top_k
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# å‘é‡è¿ç®—
king_emb = get_embedding("king")
man_emb = get_embedding("man")
woman_emb = get_embedding("woman")

# King - Man + Woman
result_emb = king_emb - man_emb + woman_emb

# åœ¨å€™é€‰è¯ä¸­æŸ¥æ‰¾æœ€æ¥è¿‘çš„
candidates = ["queen", "princess", "lady", "female", "girl", "woman", "king", "prince"]
closest = find_closest_word(result_emb, candidates)

print("King - Man + Woman â‰ˆ")
for word, sim in closest:
    print(f"  {word:12} (ç›¸ä¼¼åº¦: {sim:.4f})")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
King - Man + Woman â‰ˆ
  queen        (ç›¸ä¼¼åº¦: 0.8234)  â† æœ€æ¥è¿‘ï¼
  princess     (ç›¸ä¼¼åº¦: 0.7456)
  lady         (ç›¸ä¼¼åº¦: 0.6789)
  female       (ç›¸ä¼¼åº¦: 0.6234)
  woman        (ç›¸ä¼¼åº¦: 0.5987)
```

#### ä¸ºä»€ä¹ˆå‘é‡è¿ç®—æœ‰è¯­ä¹‰æ„ä¹‰ï¼Ÿ

**å‡ ä½•è§£é‡Š**ï¼š

åµŒå…¥ç©ºé—´ä¸­å­˜åœ¨**è¯­ä¹‰æ–¹å‘**ï¼š

```
King  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Queen
 â”‚                    â”‚
 â”‚ (æ€§åˆ«æ–¹å‘)          â”‚
 â†“                    â†“
Man  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Woman
```

å‘é‡è¿ç®—ï¼š
$$
\vec{Queen} \approx \vec{King} - \vec{Man} + \vec{Woman}
$$

ç›¸å½“äºï¼š
1. $\vec{King} - \vec{Man}$ï¼šç§»é™¤"ç”·æ€§"è¯­ä¹‰
2. åŠ ä¸Š$\vec{Woman}$ï¼šæ·»åŠ "å¥³æ€§"è¯­ä¹‰
3. ç»“æœï¼šæ¥è¿‘$\vec{Queen}$

#### æ›´å¤šä¾‹å­

```python
# é¦–éƒ½ç±»æ¯”ï¼šParis - France + Germany = ?
paris_emb = get_embedding("paris")
france_emb = get_embedding("france")
germany_emb = get_embedding("germany")

result = paris_emb - france_emb + germany_emb
candidates = ["berlin", "munich", "hamburg", "london", "rome"]
print("\nParis - France + Germany â‰ˆ")
for word, sim in find_closest_word(result, candidates):
    print(f"  {word:12} (ç›¸ä¼¼åº¦: {sim:.4f})")

# åŠ¨è¯æ—¶æ€ï¼šwalked - walk + run = ?
walked_emb = get_embedding("walked")
walk_emb = get_embedding("walk")
run_emb = get_embedding("run")

result = walked_emb - walk_emb + run_emb
candidates = ["ran", "running", "runner", "run"]
print("\nwalked - walk + run â‰ˆ")
for word, sim in find_closest_word(result, candidates):
    print(f"  {word:12} (ç›¸ä¼¼åº¦: {sim:.4f})")
```

**æ•°å­¦åŸç†**ï¼š

è®­ç»ƒæ—¶ï¼Œæ¨¡å‹å­¦åˆ°çš„ä¸ä»…æ˜¯å•è¯çš„è¡¨ç¤ºï¼Œè¿˜å­¦åˆ°äº†**è¯­ä¹‰å…³ç³»çš„æ–¹å‘**ã€‚

å½¢å¼åŒ–ï¼š
$$
\vec{v}_{\text{queen}} - \vec{v}_{\text{king}} \approx \vec{v}_{\text{woman}} - \vec{v}_{\text{man}}
$$

è¿™ä¸ªå…³ç³»å‘é‡$\vec{r}$ç¼–ç äº†"æ€§åˆ«"æ¦‚å¿µï¼š
$$
\vec{r}_{\text{gender}} \approx \vec{v}_{\text{woman}} - \vec{v}_{\text{man}}
$$

---

## ä¸‰ã€åµŒå…¥çš„åº”ç”¨ï¼šè¶…è¶Šæ–‡æœ¬

åµŒå…¥ä¸ä»…èƒ½è¡¨ç¤ºæ–‡æœ¬ï¼Œè¿˜èƒ½è¡¨ç¤ºä»»ä½•äº‹ç‰©ã€‚åªè¦èƒ½å°†å¯¹è±¡æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œå°±èƒ½åˆ©ç”¨åµŒå…¥çš„å¼ºå¤§åŠŸèƒ½ã€‚

### åŠ¨æ‰‹å®è·µï¼šæ„å»ºä¸€ä¸ªè¿·ä½ éŸ³ä¹æ¨èå™¨

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªåŸºäºåµŒå…¥çš„éŸ³ä¹æ¨èç³»ç»Ÿï¼Œå±•ç¤ºåµŒå…¥çš„å®é™…åº”ç”¨ã€‚

#### ä»»åŠ¡

ç»™å®šç”¨æˆ·å–œæ¬¢çš„æ­Œæ›²ï¼Œæ¨èç›¸ä¼¼çš„æ­Œæ›²ã€‚

#### æ€è·¯

1. ç”¨æ­Œæ›²çš„**å…ƒæ•°æ®**ï¼ˆæ­Œåã€æ­Œæ‰‹ã€æµæ´¾ã€æ­Œè¯å…³é”®è¯ï¼‰ç”Ÿæˆæ–‡æœ¬æè¿°
2. å°†æ–‡æœ¬æè¿°è½¬æ¢ä¸ºåµŒå…¥å‘é‡
3. è®¡ç®—æ­Œæ›²é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
4. æ¨èæœ€ç›¸ä¼¼çš„æ­Œæ›²

#### å®Œæ•´å®ç°

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch

# 1. å‡†å¤‡éŸ³ä¹æ•°æ®åº“ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
music_database = [
    {
        "id": 1,
        "title": "Shape of You",
        "artist": "Ed Sheeran",
        "genre": "Pop",
        "keywords": "love, dance, romance"
    },
    {
        "id": 2,
        "title": "Bohemian Rhapsody",
        "artist": "Queen",
        "genre": "Rock",
        "keywords": "epic, dramatic, classic"
    },
    {
        "id": 3,
        "title": "Blinding Lights",
        "artist": "The Weeknd",
        "genre": "Synthwave",
        "keywords": "night, drive, retro"
    },
    {
        "id": 4,
        "title": "Perfect",
        "artist": "Ed Sheeran",
        "genre": "Pop",
        "keywords": "love, wedding, romantic"
    },
    {
        "id": 5,
        "title": "Stairway to Heaven",
        "artist": "Led Zeppelin",
        "genre": "Rock",
        "keywords": "epic, guitar, classic"
    },
    {
        "id": 6,
        "title": "Levitating",
        "artist": "Dua Lipa",
        "genre": "Pop",
        "keywords": "dance, fun, upbeat"
    }
]

# 2. åŠ è½½åµŒå…¥æ¨¡å‹
model_name = "sentence-transformers/all-MiniLM-L6-v2"  # ä¸“é—¨ç”¨äºå¥å­åµŒå…¥çš„æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_text_embedding(text):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡"""
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

    with torch.no_grad():
        outputs = model(**inputs)
        # Mean pooling
        embeddings = outputs.last_hidden_state.mean(dim=1)

    return embeddings[0].numpy()

# 3. ä¸ºæ¯é¦–æ­Œç”Ÿæˆæè¿°æ–‡æœ¬å’ŒåµŒå…¥
for song in music_database:
    # æ„é€ æè¿°æ–‡æœ¬
    description = f"{song['title']} by {song['artist']}. Genre: {song['genre']}. {song['keywords']}"

    # ç”ŸæˆåµŒå…¥
    song['embedding'] = get_text_embedding(description)
    song['description'] = description

print("éŸ³ä¹åº“åµŒå…¥ç”Ÿæˆå®Œæˆï¼\n")

# 4. æ¨èå‡½æ•°
def recommend_songs(song_id, top_k=3):
    """åŸºäºæ­Œæ›²IDæ¨èç›¸ä¼¼æ­Œæ›²"""
    # æ‰¾åˆ°ç›®æ ‡æ­Œæ›²
    target_song = next(s for s in music_database if s['id'] == song_id)
    target_emb = target_song['embedding'].reshape(1, -1)

    print(f"åŸºäºæ­Œæ›²ï¼š{target_song['title']} - {target_song['artist']}")
    print(f"æè¿°ï¼š{target_song['description']}")
    print(f"\næ¨èæ­Œæ›²ï¼š")

    # è®¡ç®—ä¸æ‰€æœ‰æ­Œæ›²çš„ç›¸ä¼¼åº¦
    similarities = []
    for song in music_database:
        if song['id'] == song_id:
            continue  # è·³è¿‡è‡ªå·±

        song_emb = song['embedding'].reshape(1, -1)
        sim = cosine_similarity(target_emb, song_emb)[0][0]
        similarities.append((song, sim))

    # æ’åºå¹¶è¿”å›top_k
    similarities.sort(key=lambda x: x[1], reverse=True)

    for i, (song, sim) in enumerate(similarities[:top_k], 1):
        print(f"\n{i}. {song['title']} - {song['artist']}")
        print(f"   æµæ´¾: {song['genre']}")
        print(f"   ç›¸ä¼¼åº¦: {sim:.4f}")

# 5. æµ‹è¯•æ¨èç³»ç»Ÿ
print("="*60)
recommend_songs(song_id=1, top_k=3)  # åŸºäº"Shape of You"æ¨è

print("\n" + "="*60)
recommend_songs(song_id=2, top_k=3)  # åŸºäº"Bohemian Rhapsody"æ¨è
```

**é¢„æœŸè¾“å‡º**ï¼š

```
============================================================
åŸºäºæ­Œæ›²ï¼šShape of You - Ed Sheeran
æè¿°ï¼šShape of You by Ed Sheeran. Genre: Pop. love, dance, romance

æ¨èæ­Œæ›²ï¼š

1. Perfect - Ed Sheeran
   æµæ´¾: Pop
   ç›¸ä¼¼åº¦: 0.8234  â† åŒæ­Œæ‰‹ã€åŒæµæ´¾ã€ç›¸ä¼¼ä¸»é¢˜

2. Levitating - Dua Lipa
   æµæ´¾: Pop
   ç›¸ä¼¼åº¦: 0.7156  â† åŒæµæ´¾ã€éƒ½æ˜¯èˆæ›²

3. Blinding Lights - The Weeknd
   æµæ´¾: Synthwave
   ç›¸ä¼¼åº¦: 0.6543

============================================================
åŸºäºæ­Œæ›²ï¼šBohemian Rhapsody - Queen
æè¿°ï¼šBohemian Rhapsody by Queen. Genre: Rock. epic, dramatic, classic

æ¨èæ­Œæ›²ï¼š

1. Stairway to Heaven - Led Zeppelin
   æµæ´¾: Rock
   ç›¸ä¼¼åº¦: 0.8567  â† åŒæµæ´¾ã€éƒ½æ˜¯ç»å…¸æ‘‡æ»š

2. Blinding Lights - The Weeknd
   æµæ´¾: Synthwave
   ç›¸ä¼¼åº¦: 0.5234

3. Shape of You - Ed Sheeran
   æµæ´¾: Pop
   ç›¸ä¼¼åº¦: 0.4123
```

#### æ”¹è¿›æ–¹å‘

**1. å¤šæ¨¡æ€åµŒå…¥**

é™¤äº†æ–‡æœ¬ï¼Œè¿˜å¯ä»¥åµŒå…¥ï¼š
- éŸ³é¢‘ç‰¹å¾ï¼ˆèŠ‚å¥ã€éŸ³é«˜ã€éŸ³è‰²ï¼‰
- ç”¨æˆ·è¡Œä¸ºï¼ˆå¬æ­Œå†å²ã€è·³è¿‡ç‡ï¼‰
- ç¤¾äº¤ä¿¡å·ï¼ˆåˆ†äº«ã€è¯„è®ºï¼‰

```python
# ç»„åˆå¤šç§åµŒå…¥
combined_embedding = np.concatenate([
    text_embedding,
    audio_embedding,
    user_behavior_embedding
])
```

**2. ç”¨æˆ·ä¸ªæ€§åŒ–**

å­¦ä¹ ç”¨æˆ·çš„åå¥½å‘é‡ï¼š

```python
def get_user_preference_vector(user_listening_history):
    """æ ¹æ®ç”¨æˆ·å¬æ­Œå†å²è®¡ç®—åå¥½å‘é‡"""
    embeddings = [song['embedding'] for song in user_listening_history]
    # å–å¹³å‡ï¼ˆæˆ–åŠ æƒå¹³å‡ï¼‰
    user_vector = np.mean(embeddings, axis=0)
    return user_vector

# æ¨è
user_vec = get_user_preference_vector(my_history)
recommendations = find_closest_songs(user_vec, music_database)
```

**3. è´Ÿæ ·æœ¬æŒ–æ˜**

è€ƒè™‘ç”¨æˆ·**ä¸å–œæ¬¢**çš„æ­Œæ›²ï¼š

```python
# æ¨èè¿œç¦»"ä¸å–œæ¬¢"çš„æ­Œæ›²
disliked_vec = np.mean([song['embedding'] for song in disliked_songs], axis=0)

for song in candidates:
    positive_sim = cosine_similarity(user_vec, song['embedding'])
    negative_sim = cosine_similarity(disliked_vec, song['embedding'])

    score = positive_sim - 0.5 * negative_sim  # æƒ©ç½šä¸ä¸å–œæ¬¢æ­Œæ›²ç›¸ä¼¼çš„
```

---

### æ‰©å±•ï¼šåµŒå…¥çš„å…¶ä»–åº”ç”¨

#### 1. è¯­ä¹‰æœç´¢

ä¼ ç»Ÿæœç´¢ï¼šå…³é”®è¯åŒ¹é…
```
æŸ¥è¯¢ï¼š"apple fruit"
ç»“æœï¼šåŒ…å«"apple"å’Œ"fruit"çš„æ–‡æ¡£
é—®é¢˜ï¼šæœä¸åˆ°"orange"ã€"banana"
```

è¯­ä¹‰æœç´¢ï¼šåŸºäºåµŒå…¥
```
æŸ¥è¯¢ï¼š"apple fruit" â†’ åµŒå…¥å‘é‡
æ–‡æ¡£ï¼š"I love oranges" â†’ åµŒå…¥å‘é‡
ç›¸ä¼¼åº¦ï¼š0.72 â† å³ä½¿æ²¡æœ‰ç›¸åŒè¯ï¼Œè¯­ä¹‰ç›¸è¿‘
```

**ä»£ç ç¤ºä¾‹**ï¼š

```python
def semantic_search(query, documents, top_k=3):
    """è¯­ä¹‰æœç´¢"""
    query_emb = get_text_embedding(query).reshape(1, -1)

    results = []
    for doc in documents:
        doc_emb = get_text_embedding(doc).reshape(1, -1)
        sim = cosine_similarity(query_emb, doc_emb)[0][0]
        results.append((doc, sim))

    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]

# æµ‹è¯•
documents = [
    "Python is a programming language",
    "I love eating apples and oranges",
    "Machine learning is a subset of AI",
    "Bananas are yellow fruits"
]

query = "computer programming"
print(f"æŸ¥è¯¢: {query}\n")
for doc, sim in semantic_search(query, documents):
    print(f"{sim:.4f} - {doc}")
```

**è¾“å‡º**ï¼š
```
æŸ¥è¯¢: computer programming

0.7234 - Python is a programming language
0.6543 - Machine learning is a subset of AI
0.3456 - I love eating apples and oranges
0.2345 - Bananas are yellow fruits
```

#### 2. æ–‡æ¡£èšç±»

å°†ç›¸ä¼¼æ–‡æ¡£èšåœ¨ä¸€èµ·ï¼š

```python
from sklearn.cluster import KMeans

# å‡è®¾æœ‰100ç¯‡æ–‡æ¡£
document_embeddings = np.array([get_text_embedding(doc) for doc in documents])

# K-Meansèšç±»
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(document_embeddings)

# æŸ¥çœ‹æ¯ä¸ªç°‡çš„æ–‡æ¡£
for i in range(5):
    print(f"\nç°‡ {i}:")
    cluster_docs = [documents[j] for j in range(len(documents)) if clusters[j] == i]
    for doc in cluster_docs[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"  - {doc[:50]}...")
```

#### 3. å¼‚å¸¸æ£€æµ‹

æ‰¾å‡ºä¸å…¶ä»–æ–‡æ¡£å·®å¼‚å¾ˆå¤§çš„æ–‡æ¡£ï¼š

```python
# è®¡ç®—æ¯ä¸ªæ–‡æ¡£ä¸æ‰€æœ‰æ–‡æ¡£çš„å¹³å‡ç›¸ä¼¼åº¦
for i, doc_emb in enumerate(document_embeddings):
    avg_sim = np.mean([
        cosine_similarity(doc_emb.reshape(1, -1), other_emb.reshape(1, -1))[0][0]
        for j, other_emb in enumerate(document_embeddings) if j != i
    ])

    if avg_sim < 0.3:  # é˜ˆå€¼
        print(f"å¼‚å¸¸æ–‡æ¡£: {documents[i]}")
```

---

## å››ã€ğŸ’¡ æŠ€æœ¯é—®ç­”ï¼šåˆ†è¯ä¸åµŒå…¥çš„æ·±å±‚é—®é¢˜

> åˆ†è¯å’ŒåµŒå…¥æ˜¯LLMçš„åŸºçŸ³ï¼Œä½†ä¹Ÿæ˜¯é—®é¢˜æœ€å¤šçš„åœ°æ–¹ã€‚è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ã€‚

---

### é—®é¢˜1ï¼šä¸ºä»€ä¹ˆGPTä¼šæŠŠ"strawberry"æ•°æˆ2ä¸ª'r'è€Œä¸æ˜¯3ä¸ªï¼Ÿ

**ç½‘ç»œçƒ­è®®çš„ç°è±¡**ï¼š

```python
prompt = "How many 'r's are in the word 'strawberry'?"

# GPT-3.5/GPT-4
answer = "There are 2 'r's"  âŒ é”™è¯¯ï¼å®é™…æœ‰3ä¸ª

# æ­£ç¡®ç­”æ¡ˆ
"st-r-aw-b-e-rr-y" â†’ 3ä¸ª'r'
```

**æ ¹æœ¬åŸå› **ï¼šåˆ†è¯å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±

**åˆ†è¯è¿‡ç¨‹å¯è§†åŒ–**ï¼š

```python
from tiktoken import encoding_for_model

enc = encoding_for_model("gpt-4")
tokens = enc.encode("strawberry")
decoded_tokens = [enc.decode([t]) for t in tokens]

print(decoded_tokens)
# è¾“å‡º: ['straw', 'berry']  â† è¢«åˆ†æˆ2ä¸ªtoken!
```

**ä¸ºä»€ä¹ˆä¼šæ•°é”™ï¼Ÿ**

æ¨¡å‹çœ‹åˆ°çš„ä¸æ˜¯å­—ç¬¦åºåˆ—ï¼Œè€Œæ˜¯tokenåºåˆ—ï¼š

```
äººç±»çœ‹åˆ°çš„: s-t-r-a-w-b-e-r-r-y (10ä¸ªå­—ç¬¦)
æ¨¡å‹çœ‹åˆ°çš„: [straw] [berry] (2ä¸ªtoken)
```

å½“æ¨¡å‹è¢«é—®"æœ‰å‡ ä¸ª'r'"æ—¶ï¼š
1. å®ƒè¯•å›¾åœ¨tokençº§åˆ«è®¡ç®—
2. `[straw]` è¿™ä¸ªtokenåŒ…å«1ä¸ª'r'
3. `[berry]` è¿™ä¸ªtokenåŒ…å«2ä¸ª'r'
4. ä½†æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨tokenä¹‹é—´æ“ä½œï¼Œå®¹æ˜“æ¼æ‰tokenå†…éƒ¨çš„ç»†èŠ‚

**æ›´å¤šæ¡ˆä¾‹**ï¼š

| å•è¯ | åˆ†è¯ç»“æœ | ä»»åŠ¡ | GPTè¡¨ç° |
|-----|---------|------|--------|
| "strawberry" | [straw][berry] | æ•°'r' | âŒ é”™(2 vs 3) |
| "Mississippi" | [Miss][iss][ippi] | æ•°'i' | âŒ å®¹æ˜“é”™ |
| "programming" | [program][ming] | æ•°'m' | âœ… é€šå¸¸å¯¹ |
| "book" | [book] | æ•°'o' | âœ… ä¸€å®šå¯¹(å•token) |

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âŒ ç›´æ¥é—®ï¼ˆå®¹æ˜“é”™ï¼‰
prompt = "How many 'r's in 'strawberry'?"

# âœ… è®©æ¨¡å‹å…ˆåˆ†è§£å­—ç¬¦
prompt = """
First, spell out the word 'strawberry' letter by letter.
Then count how many 'r's there are.
"""

# è¾“å‡º:
# s-t-r-a-w-b-e-r-r-y
# Count of 'r': 3  âœ… æ­£ç¡®!
```

**ä¸ºä»€ä¹ˆè¿™æ ·æœ‰æ•ˆï¼Ÿ**

å¼ºåˆ¶æ¨¡å‹ç”Ÿæˆå­—ç¬¦åºåˆ—æ—¶ï¼Œæ¯ä¸ªå­—ç¬¦éƒ½ä¼šæˆä¸ºç‹¬ç«‹çš„tokenï¼ˆæˆ–å­tokenï¼‰ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆçº§åˆ«è€Œä¸æ˜¯ç†è§£çº§åˆ«æ“ä½œï¼Œæ›´å‡†ç¡®ã€‚

**å…³è”çŸ¥è¯†ç‚¹**ï¼šè¿™ä¸ªé—®é¢˜æ­ç¤ºäº†tokenåŒ–çš„æœ¬è´¨å±€é™â€”â€”æ¨¡å‹å¤„ç†çš„æœ€å°å•ä½æ˜¯tokenï¼Œè€Œä¸æ˜¯å­—ç¬¦ã€‚ä¸‹ä¸€ç« Transformeræ¶æ„ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¹Ÿæ˜¯åœ¨tokençº§åˆ«è®¡ç®—çš„ã€‚

---

### é—®é¢˜2ï¼šä¸­æ–‡åˆ†è¯ä¸ºä»€ä¹ˆè¿™ä¹ˆéš¾ï¼Ÿ"ä¹’ä¹“çƒæ‹å–"ä¼šè¢«åˆ†æˆä»€ä¹ˆï¼Ÿ

**æ­§ä¹‰åˆ†è¯é—®é¢˜**ï¼š

```python
text = "ä¹’ä¹“çƒæ‹å–"

# å¯èƒ½çš„åˆ†è¯1
["ä¹’ä¹“çƒ", "æ‹å–"]  # ä¹’ä¹“çƒè¢«æ‹å–

# å¯èƒ½çš„åˆ†è¯2
["ä¹’ä¹“", "çƒæ‹", "å–"]  # çƒæ‹åœ¨å–

# BPEå¯èƒ½çš„ç»“æœ
["ä¹’", "ä¹“", "çƒ", "æ‹", "å–"]  # è¿‡åº¦åˆ†å‰²
æˆ–
["ä¹’ä¹“çƒæ‹", "å–"]  # å¦‚æœè®­ç»ƒæ•°æ®ä¸­"ä¹’ä¹“çƒæ‹"å¸¸è§
```

**ä¸­æ–‡åˆ†è¯å›°éš¾çš„æ ¹æœ¬åŸå› **ï¼š

**åŸå› 1ï¼šæ²¡æœ‰å¤©ç„¶åˆ†éš”ç¬¦**

```
è‹±æ–‡: "I love NLP" â†’ ç©ºæ ¼å¤©ç„¶åˆ†éš”
ä¸­æ–‡: "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†" â†’ æ— åˆ†éš”ï¼Œæ­§ä¹‰å¤š
```

**åŸå› 2ï¼šç»„åˆçµæ´»æ€§**

```
"ç ”ç©¶ç”Ÿå‘½"å¯ä»¥æ˜¯:
- "ç ”ç©¶ ç”Ÿå‘½" (åŠ¨å®¾ç»“æ„)
- "ç ”ç©¶ç”Ÿ å‘½" (ä¸»è°“ç»“æ„ï¼Œç ”ç©¶ç”Ÿçš„å‘½è¿)
```

**åŸå› 3ï¼šBPEå¯¹ä¸­æ–‡çš„ä½æ•ˆ**

BPEåŸºäºå­—ç¬¦é¢‘ç‡åˆå¹¶ï¼Œå¯¹ä¸­æ–‡ï¼š

```python
# è‹±æ–‡ï¼ˆæ•ˆç‡é«˜ï¼‰
"natural" â†’ BPEå­¦åˆ° ["nat", "ural"]  # æœ‰æ„ä¹‰çš„å­è¯

# ä¸­æ–‡ï¼ˆæ•ˆç‡ä½ï¼‰
"è‡ªç„¶è¯­è¨€" â†’ BPEå¯èƒ½å­¦åˆ° ["è‡ªç„¶", "è¯­", "è¨€"]  # æ‹†åˆ†ä¸åˆç†
           æˆ– ["è‡ª", "ç„¶", "è¯­è¨€"]  # æ›´ä¸åˆç†
```

**å®é™…æµ‹è¯•**ï¼ˆGPT-3.5ï¼‰ï¼š

```python
from tiktoken import encoding_for_model

enc = encoding_for_model("gpt-3.5-turbo")

# æµ‹è¯•1ï¼šåŸºç¡€åˆ†è¯
text1 = "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"
tokens1 = enc.encode(text1)
decoded = [enc.decode([t]) for t in tokens1]
print(decoded)
# å¯èƒ½è¾“å‡º: ['æˆ‘', 'çˆ±', 'åŒ—äº¬', 'å¤©å®‰é—¨']
# æˆ–: ['æˆ‘', 'çˆ±', 'åŒ—', 'äº¬', 'å¤©', 'å®‰', 'é—¨']

# æµ‹è¯•2ï¼šæ­§ä¹‰åˆ†è¯
text2 = "å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥"
# å¯èƒ½æ˜¯: å—äº¬å¸‚ é•¿æ±Ÿå¤§æ¡¥
# ä¹Ÿå¯èƒ½æ˜¯: å—äº¬å¸‚é•¿ æ±Ÿå¤§æ¡¥
# BPEä¼šæ€ä¹ˆåˆ†ï¼Ÿ
tokens2 = enc.encode(text2)
decoded2 = [enc.decode([t]) for t in tokens2]
print(decoded2)
```

**ä¸åŒæ¨¡å‹çš„ä¸­æ–‡åˆ†è¯ç­–ç•¥**ï¼š

| æ¨¡å‹ | è¯è¡¨è®¾è®¡ | ä¸­æ–‡å‹å¥½åº¦ | ç¤ºä¾‹åˆ†è¯ |
|-----|---------|----------|---------|
| GPT-3/4 | BPE (è‹±æ–‡ä¸ºä¸») | ä½ | "åŒ—äº¬"â†’["åŒ—","äº¬"] |
| LLaMA-3 | BPE (å¤šè¯­è¨€) | ä¸­ | "åŒ—äº¬"â†’["åŒ—äº¬"] |
| Qwen | BPE (ä¸­æ–‡ä¼˜åŒ–) | é«˜ | "åŒ—äº¬"â†’["åŒ—äº¬"] |
| GLM | BPE+ä¸­æ–‡è¯å…¸ | é«˜ | "åŒ—äº¬"â†’["åŒ—äº¬"] |

**æœ€ä½³å®è·µ**ï¼šé€‰å¯¹æ¨¡å‹

```python
# âŒ ç”¨GPTå¤„ç†å¤§é‡ä¸­æ–‡
text_cn = "ä¸€ç¯‡5000å­—çš„ä¸­æ–‡æ–‡ç« ..."
cost = 5000 Ã— 1.5(token/å­—) Ã— $0.03/1K = $0.225

# âœ… ç”¨Qwenå¤„ç†ä¸­æ–‡
cost = 5000 Ã— 0.8(token/å­—) Ã— $0.02/1K = $0.08  # èŠ‚çœ65%
```

**å…³è”ä¸‹ä¸€ç« **ï¼šä¸­æ–‡åˆ†è¯çš„æŒ‘æˆ˜åœ¨äºç¼ºä¹å¤©ç„¶è¾¹ç•Œï¼Œéœ€è¦Transformerçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸Šä¸‹æ–‡ä¸­åŠ¨æ€ç†è§£ã€‚ç¬¬äºŒéƒ¨åˆ†å°†è¯¦è§£Transformerå¦‚ä½•åˆ©ç”¨ä½ç½®ç¼–ç å’Œæ³¨æ„åŠ›æœºåˆ¶å¤„ç†è¿™ç±»é—®é¢˜ã€‚

---

### é—®é¢˜3ï¼šä¸ºä»€ä¹ˆåµŒå…¥ç»´åº¦é€šå¸¸æ˜¯768ã€1024è¿™äº›æ•°å­—ï¼Ÿæœ‰ä»€ä¹ˆè®²ç©¶å—ï¼Ÿ

**è§‚å¯Ÿç°è±¡**ï¼š

| æ¨¡å‹ | åµŒå…¥ç»´åº¦ | è§„å¾‹ |
|-----|---------|------|
| BERT-base | 768 | 768 = 3 Ã— 256 |
| BERT-large | 1024 | 1024 = 2^10 |
| GPT-2 | 768, 1024, 1280, 1600 | å‡ä¸º64çš„å€æ•° |
| GPT-3 | 12288 | 12288 = 192 Ã— 64 |
| LLaMA | 4096, 5120, 8192 | å‡ä¸º128çš„å€æ•° |

**åŸå› 1ï¼šå¤šå¤´æ³¨æ„åŠ›çš„æ•´é™¤æ€§**

åµŒå…¥ç»´åº¦å¿…é¡»èƒ½è¢«å¤´æ•°æ•´é™¤ï¼š

$$
d_{model} = \text{num\_heads} \times d_k
$$

```python
# BERT-base
d_model = 768
num_heads = 12
d_k = 768 / 12 = 64  âœ… æ•´é™¤

# å¦‚æœd_model=770ï¼ˆä¸åˆç†ï¼‰
d_k = 770 / 12 = 64.166...  âŒ ä¸æ•´é™¤ï¼Œæ— æ³•å®ç°
```

**å¸¸è§é…ç½®**ï¼š

| æ¨¡å‹è§„æ¨¡ | åµŒå…¥ç»´åº¦ | å¤´æ•° | æ¯å¤´ç»´åº¦ |
|---------|---------|------|---------|
| Small | 512 | 8 | 64 |
| Base | 768 | 12 | 64 |
| Large | 1024 | 16 | 64 |
| XL | 2048 | 32 | 64 |

**å‘ç°è§„å¾‹**ï¼š
- æ¯å¤´ç»´åº¦é€šå¸¸å›ºå®šä¸º64æˆ–128
- é€šè¿‡è°ƒæ•´å¤´æ•°æ¥æ”¹å˜æ€»ç»´åº¦

**åŸå› 2ï¼šç¡¬ä»¶æ•ˆç‡**

GPU/TPUå¯¹æŸäº›ç»´åº¦æœ‰ä¼˜åŒ–ï¼š

```
2çš„å¹‚æ¬¡(256, 512, 1024, 2048):
- å†…å­˜å¯¹é½ä¼˜åŒ–
- SIMDæŒ‡ä»¤å‹å¥½
- ç¼“å­˜å‘½ä¸­ç‡é«˜

64çš„å€æ•°:
- Tensor Coreä¼˜åŒ–(NVIDIA GPU)
- çŸ©é˜µä¹˜æ³•åŠ é€Ÿ
```

**å®éªŒå¯¹æ¯”**ï¼ˆA100 GPUï¼‰ï¼š

| åµŒå…¥ç»´åº¦ | çŸ©é˜µä¹˜æ³•é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ |
|---------|-------------|---------|
| 768 | 1.0x (åŸºå‡†) | 100% |
| 770 | 0.87x (æ…¢13%) | 101% |
| 1024 | 1.15x (å¿«15%) | 133% |

**åŸå› 3ï¼šè¡¨è¾¾èƒ½åŠ›ä¸è®¡ç®—æˆæœ¬å¹³è¡¡**

ç»´åº¦è¶Šé«˜ï¼Œè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†æˆæœ¬æŒ‡æ•°å¢é•¿ï¼š

```python
# å‚æ•°é‡ä¼°ç®—
vocab_size = 50000
d_model = 768

# Embeddingå±‚å‚æ•°
params_emb = vocab_size Ã— d_model = 38.4M

# å¦‚æœd_modelç¿»å€åˆ°1536
params_emb = 50000 Ã— 1536 = 76.8M  # ç¿»å€ï¼

# æ³¨æ„åŠ›å±‚å‚æ•°ï¼ˆæ¯å±‚ï¼‰
params_attn = 4 Ã— d_model^2  # Q,K,V,OæŠ•å½±
= 4 Ã— 768^2 = 2.36M

# å¦‚æœd_modelç¿»å€
params_attn = 4 Ã— 1536^2 = 9.44M  # 4å€ï¼
```

**ç»éªŒæ³•åˆ™**ï¼š

$$
d_{model} = 64 \times k, \quad k \in \mathbb{N}
$$

æ¨èå€¼ï¼š
- å°æ¨¡å‹ï¼š512 (k=8)
- ä¸­ç­‰æ¨¡å‹ï¼š768 (k=12) æˆ– 1024 (k=16)
- å¤§æ¨¡å‹ï¼š2048 (k=32), 4096 (k=64)

**ä¸æ¨è**ï¼š
- 770, 900, 1100ç­‰ä¸è§„åˆ™æ•°å­—
- ä¼šå¯¼è‡´ç¡¬ä»¶æ•ˆç‡é™ä½

---

### é—®é¢˜4ï¼šä¸ºä»€ä¹ˆè¯´"åµŒå…¥æ•æ‰è¯­ä¹‰"ï¼Ÿå‘é‡çš„æ¯ä¸€ç»´ä»£è¡¨ä»€ä¹ˆæ„æ€å—ï¼Ÿ

**æ–°æ‰‹ç–‘é—®**ï¼š

"åµŒå…¥å‘é‡768ç»´ï¼Œæ¯ä¸€ç»´æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿç¬¬1ç»´æ˜¯æ€§åˆ«ï¼Ÿç¬¬2ç»´æ˜¯æ—¶æ€ï¼Ÿ"

**çœŸç›¸**ï¼šç»´åº¦æ²¡æœ‰äººç±»å¯è§£é‡Šçš„å«ä¹‰ï¼

**åµŒå…¥æ˜¯å¦‚ä½•å­¦åˆ°çš„**ï¼š

åœ¨é¢„è®­ç»ƒæ—¶ï¼Œæ¨¡å‹é€šè¿‡é¢„æµ‹ä»»åŠ¡é—´æ¥å­¦ä¹ åµŒå…¥ï¼š

```python
# BERTçš„è®­ç»ƒä»»åŠ¡
è¾“å…¥: "æˆ‘çˆ±[MASK]åŒ—äº¬å¤©å®‰é—¨"
ç›®æ ‡: é¢„æµ‹[MASK] = "é¦–éƒ½"

# ä¸ºäº†æ­£ç¡®é¢„æµ‹ï¼Œæ¨¡å‹å¿…é¡»è®©:
embedding("é¦–éƒ½") åœ¨å‘é‡ç©ºé—´ä¸­é è¿‘ embedding("åŒ—äº¬")
```

**å¹¶éæ¯ç»´æœ‰å›ºå®šå«ä¹‰ï¼Œè€Œæ˜¯æ•´ä½“æ¨¡å¼**ï¼š

```python
# âŒ é”™è¯¯ç†è§£
dim_0 = æ€§åˆ« (0=å¥³, 1=ç”·)
dim_1 = æ—¶æ€ (0=è¿‡å», 1=ç°åœ¨, 1=æœªæ¥)
...

# âœ… æ­£ç¡®ç†è§£
æ•´ä¸ª768ç»´å‘é‡å…±åŒç¼–ç è¯­ä¹‰
æ— æ³•æ‹†åˆ†å•ä¸ªç»´åº¦è§£é‡Š
```

**å¯è§†åŒ–å®éªŒ**ï¼š

ç”¨t-SNEé™ç»´åˆ°2Dï¼Œè§‚å¯ŸåµŒå…¥çš„åˆ†å¸ƒï¼š

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModel

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# è·å–è¯åµŒå…¥
words = ["king", "queen", "man", "woman", "prince", "princess",
         "car", "automobile", "vehicle", "bike"]

embeddings = []
for word in words:
    input_ids = tokenizer(word, return_tensors="pt")["input_ids"]
    emb = model.embeddings.word_embeddings(input_ids)[0, 1]  # [CLS] tokenåçš„ç¬¬ä¸€ä¸ª
    embeddings.append(emb.detach().numpy())

# é™ç»´åˆ°2D
tsne = TSNE(n_components=2, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
for i, word in enumerate(words):
    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))
plt.title("Word Embeddings Visualized (t-SNE)")
plt.show()
```

**è§‚å¯Ÿ**ï¼š
- "king", "queen", "prince", "princess"èšåœ¨ä¸€èµ·ï¼ˆç‹å®¤è¯­ä¹‰ï¼‰
- "car", "automobile", "vehicle"èšåœ¨ä¸€èµ·ï¼ˆäº¤é€šå·¥å…·ï¼‰
- "king" - "man" + "woman" â‰ˆ "queen"çš„å‡ ä½•å…³ç³»

**ä½†å¹¶éæ¯ç»´å¯è§£é‡Š**ï¼

**å°‘æ•°ä¾‹å¤–**ï¼šæŸäº›ç»´åº¦ç¡®å®ä¸ç‰¹å®šç‰¹å¾ç›¸å…³

ç ”ç©¶å‘ç°ï¼Œåœ¨æŸäº›åµŒå…¥ç©ºé—´ä¸­ï¼š
- æŸå‡ ç»´ä¸è¯æ€§(åè¯/åŠ¨è¯)å¼±ç›¸å…³
- æŸå‡ ç»´ä¸æƒ…æ„Ÿææ€§å¼±ç›¸å…³

ä½†è¿™æ˜¯**æ¶Œç°ç°è±¡**ï¼Œä¸æ˜¯äººä¸ºè®¾è®¡çš„ã€‚

**å®éªŒ**ï¼šæ¢æµ‹åµŒå…¥ç»´åº¦

```python
# å°è¯•æ‰¾åˆ°"æ€§åˆ«"ç»´åº¦
male_words = ["king", "man", "prince", "father", "son"]
female_words = ["queen", "woman", "princess", "mother", "daughter"]

male_embs = [get_embedding(w) for w in male_words]
female_embs = [get_embedding(w) for w in female_words]

# è®¡ç®—å‡å€¼å·®å¼‚
male_mean = np.mean(male_embs, axis=0)
female_mean = np.mean(female_embs, axis=0)
gender_direction = male_mean - female_mean  # [768]ç»´å‘é‡

# å“ªä¸ªç»´åº¦å·®å¼‚æœ€å¤§ï¼Ÿ
top_dims = np.argsort(np.abs(gender_direction))[-10:]
print(f"å¯èƒ½ç¼–ç æ€§åˆ«çš„ç»´åº¦: {top_dims}")
# è¾“å‡º: [23, 105, 234, 567, ...]

# ä½†è¿™äº›ç»´åº¦å¹¶é"æ€§åˆ«"çš„çº¯ç²¹ç¼–ç 
# å®ƒä»¬å¯èƒ½åŒæ—¶ç¼–ç å…¶ä»–ä¿¡æ¯
```

**å…³é”®æ´å¯Ÿ**ï¼š

åµŒå…¥æ˜¯**åˆ†å¸ƒå¼è¡¨ç¤º**(Distributed Representation)ï¼š
- ä¸€ä¸ªæ¦‚å¿µç”±æ•´ä¸ªå‘é‡è¡¨ç¤º
- ä¸€ä¸ªç»´åº¦å‚ä¸å¤šä¸ªæ¦‚å¿µçš„ç¼–ç 
- è¿™æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒä¼˜åŠ¿ï¼šé«˜æ•ˆå‹ç¼©ä¿¡æ¯

**å…³è”ä¸‹ä¸€ç« **ï¼šè¿™ç§åˆ†å¸ƒå¼è¡¨ç¤ºåœ¨Transformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚Queryã€Keyã€ValueçŸ©é˜µçš„æŠ•å½±æ­£æ˜¯åœ¨å­¦ä¹ å¦‚ä½•ä»è¿™768ç»´ä¸­æå–ä¸åŒæ–¹é¢çš„è¯­ä¹‰ã€‚

---

### é—®é¢˜5ï¼šä¸ºä»€ä¹ˆåŒä¸€ä¸ªè¯åœ¨ä¸åŒå¥å­ä¸­çš„åµŒå…¥åº”è¯¥ä¸åŒï¼ˆä¸Šä¸‹æ–‡åµŒå…¥ï¼‰ï¼Ÿ

**ç†è§£å›°æƒ‘**ï¼š

"ç¬¬äºŒèŠ‚è¯´Word2Vecçš„é—®é¢˜æ˜¯'ä¸€è¯ä¸€å‘é‡'ï¼Œä½†BERTä¹Ÿæœ‰ä¸€ä¸ªåµŒå…¥çŸ©é˜µå•Šï¼Ÿ"

**å…³é”®åŒºåˆ«**ï¼šé™æ€åµŒå…¥ vs ä¸Šä¸‹æ–‡åµŒå…¥

**Word2Vecï¼ˆé™æ€åµŒå…¥ï¼‰**ï¼š

```python
# "bank"æ°¸è¿œå¯¹åº”åŒä¸€ä¸ªå‘é‡
embedding("bank") = [0.23, -0.45, 0.67, ...]

# æ— è®ºå‡ºç°åœ¨å“ªä¸ªå¥å­
"river bank" â†’ embedding("bank") = [0.23, -0.45, ...]
"bank account" â†’ embedding("bank") = [0.23, -0.45, ...]  # å®Œå…¨ç›¸åŒï¼
```

**BERTï¼ˆä¸Šä¸‹æ–‡åµŒå…¥ï¼‰**ï¼š

```python
# åˆå§‹åµŒå…¥æŸ¥æ‰¾ï¼ˆé™æ€ï¼‰
token_emb = embedding_lookup("bank")  # [0.23, -0.45, ...]

# ç»è¿‡Transformerå±‚åï¼ˆåŠ¨æ€ï¼‰
sentence1 = "I went to the river bank"
contextual_emb1 = transformer(sentence1)["bank"]  # [0.12, -0.33, ...]

sentence2 = "I deposited money in the bank"
contextual_emb2 = transformer(sentence2)["bank"]  # [0.45, 0.22, ...]

# å®Œå…¨ä¸åŒï¼
```

**è¿‡ç¨‹å¯è§†åŒ–**ï¼š

```
è¾“å…¥: "I deposited money in the bank"

æ­¥éª¤1: TokenåµŒå…¥ï¼ˆé™æ€ï¼‰
I       â†’ [0.1, 0.2, ...]
deposited â†’ [0.3, 0.4, ...]
money   â†’ [0.5, 0.6, ...]
in      â†’ [0.7, 0.8, ...]
the     â†’ [0.9, 1.0, ...]
bank    â†’ [0.23, -0.45, ...]  â† é™æ€åµŒå…¥

æ­¥éª¤2: åŠ ä¸Šä½ç½®ç¼–ç 
bankåµŒå…¥ = token_emb + position_emb(ä½ç½®5)

æ­¥éª¤3: ç»è¿‡12å±‚Transformerï¼ˆå…³é”®ï¼ï¼‰
æ¯ä¸€å±‚çš„è‡ªæ³¨æ„åŠ›è®©"bank"çš„è¡¨ç¤ºèåˆä¸Šä¸‹æ–‡ï¼š

ç¬¬1å±‚: "bank"å…³æ³¨åˆ°"the"
ç¬¬2å±‚: "bank"å…³æ³¨åˆ°"money", "deposited"
ç¬¬3å±‚: "bank"å¼€å§‹ç†è§£æ˜¯é‡‘èæœºæ„
...
ç¬¬12å±‚: "bank"çš„æœ€ç»ˆè¡¨ç¤º = ä¸Šä¸‹æ–‡åŒ–çš„åµŒå…¥

æœ€ç»ˆ: contextual_emb("bank") = [0.45, 0.22, ...]  â† åŒ…å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯
```

**ä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡åµŒå…¥é‡è¦ï¼Ÿ**

**æ¡ˆä¾‹1ï¼šæ¶ˆæ­§**

```python
å¥å­1: "The bat flew out of the cave"
å¥å­2: "He swung the bat and hit the ball"

# Word2Vec
embed("bat") åŒæ · â†’ æ— æ³•åŒºåˆ†åŠ¨ç‰©/çƒæ£’

# BERT
embed(å¥å­1, "bat") â†’ æ¥è¿‘ embed("animal"), embed("flying")
embed(å¥å­2, "bat") â†’ æ¥è¿‘ embed("sports"), embed("equipment")
```

**æ¡ˆä¾‹2ï¼šè¯­æ³•ä¿¡æ¯**

```python
å¥å­1: "They are playing" (ç°åœ¨è¿›è¡Œæ—¶)
å¥å­2: "They are teachers" (ç³»åŠ¨è¯)

# é™æ€åµŒå…¥
embed("are") åŒæ · â†’ æ— æ³•åŒºåˆ†è¯­æ³•åŠŸèƒ½

# ä¸Šä¸‹æ–‡åµŒå…¥
embed(å¥å­1, "are") â†’ åŒ…å«"è¿›è¡Œæ—¶"ä¿¡æ¯
embed(å¥å­2, "are") â†’ åŒ…å«"ç³»åŠ¨è¯"ä¿¡æ¯
```

**å®éªŒéªŒè¯**ï¼š

```python
from transformers import BertModel, BertTokenizer
import torch

model = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# å¥å­1
sent1 = "I went to the river bank"
inputs1 = tokenizer(sent1, return_tensors="pt")
outputs1 = model(**inputs1)
bank_emb1 = outputs1.last_hidden_state[0, 5]  # "bank"çš„ä½ç½®

# å¥å­2
sent2 = "I deposited money in the bank"
inputs2 = tokenizer(sent2, return_tensors="pt")
outputs2 = model(**inputs2)
bank_emb2 = outputs2.last_hidden_state[0, 5]

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = torch.cosine_similarity(bank_emb1, bank_emb2, dim=0)
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")
# è¾“å‡º: çº¦0.6-0.7 (ä¸æ˜¯1.0ï¼Œè¯´æ˜ç¡®å®ä¸åŒ)
```

**å…³è”ä¸‹ä¸€ç« **ï¼šä¸Šä¸‹æ–‡åµŒå…¥æ˜¯Transformerçš„æ ¸å¿ƒèƒ½åŠ›ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ è‡ªæ³¨æ„åŠ›æœºåˆ¶å¦‚ä½•è®©æ¯ä¸ªtokençš„è¡¨ç¤ºåŠ¨æ€èåˆæ•´ä¸ªå¥å­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

---

### é—®é¢˜6ï¼šåµŒå…¥å‘é‡å¯ä»¥ç›´æ¥ç›¸åŠ ã€ç›¸å‡å—ï¼Ÿ"King - Man + Woman â‰ˆ Queen"çœŸçš„æˆç«‹å—ï¼Ÿ

**è‘—åæ¡ˆä¾‹**ï¼š

```python
# Word2Vecçš„ç»å…¸ç¤ºä¾‹
king - man + woman â‰ˆ queen

# æ•°å­¦è¡¨è¾¾
embedding("king") - embedding("man") + embedding("woman")
â‰ˆ embedding("queen")
```

**è¿™ä¸ªçœŸçš„æœ‰æ•ˆå—ï¼Ÿå®é™…æµ‹è¯•**ï¼š

```python
from gensim.models import KeyedVectors

# åŠ è½½é¢„è®­ç»ƒWord2Vec
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

# å®éªŒ1ï¼šç»å…¸æ¡ˆä¾‹
result = model.most_similar(positive=['woman', 'king'], negative=['man'], topk=5)
print(result)
# è¾“å‡º:
# [('queen', 0.712),
#  ('monarch', 0.699),
#  ('princess', 0.631),
#  ('crown_prince', 0.586),
#  ('prince', 0.567)]
# âœ… queenæ’ç¬¬ä¸€ï¼

# å®éªŒ2ï¼šå…¶ä»–ç±»æ¯”
result = model.most_similar(positive=['paris', 'germany'], negative=['france'])
# æœŸæœ›: 'berlin'
print(result)
# [('berlin', 0.657), ...]  âœ… æˆåŠŸï¼

# å®éªŒ3ï¼šæ›´å¤æ‚çš„ç±»æ¯”
result = model.most_similar(positive=['swimming', 'run'], negative=['swim'])
# æœŸæœ›: 'running'
print(result)
# [('running', 0.723), ...]  âœ… ä¹Ÿæˆç«‹ï¼
```

**ä¸ºä»€ä¹ˆå‘é‡è¿ç®—å¯ä»¥æ•æ‰è¯­ä¹‰å…³ç³»ï¼Ÿ**

**å‡ ä½•ç›´è§‰**ï¼š

åµŒå…¥ç©ºé—´ä¸­å­˜åœ¨**æ–¹å‘æ€§æ¨¡å¼**ï¼š

```
Genderæ–¹å‘:
man â†’ woman çš„å‘é‡: Î”gender
king â†’ queen çš„å‘é‡: ä¹Ÿæ˜¯Î”gender

å› æ­¤: king + Î”gender = queen
å³:   king - man + woman â‰ˆ queen
```

**å¯è§†åŒ–ï¼ˆ2Dç®€åŒ–ï¼‰**ï¼š

```
      woman (0.5, 0.8)          queen (0.6, 0.9)
          *                        *
          |                        |
          |  Î”gender              |  Î”gender
          â†“                        â†“
       man (0.5, 0.2)            king (0.6, 0.3)
          *                        *
```

**æ•°å­¦åŸç†**ï¼š

åµŒå…¥å­¦ä¹ æ—¶ï¼Œæ¨¡å‹æ•æ‰åˆ°**å…³ç³»å‘é‡**ï¼š

$$
\vec{v}_{queen} - \vec{v}_{king} \approx \vec{v}_{woman} - \vec{v}_{man}
$$

ç§»é¡¹å¾—ï¼š

$$
\vec{v}_{queen} \approx \vec{v}_{king} - \vec{v}_{man} + \vec{v}_{woman}
$$

**ä½†å¹¶éæ€»æ˜¯æˆç«‹ï¼å±€é™æ€§**ï¼š

**å¤±è´¥æ¡ˆä¾‹1ï¼šå¤šä¹‰è¯**

```python
# "bank"æ—¢æ˜¯æ²³å²¸ï¼Œä¹Ÿæ˜¯é“¶è¡Œ
result = model.most_similar(positive=['bank', 'money'], negative=['river'])
# æœŸæœ›: 'account' æˆ– 'deposit'
# å®é™…: ç»“æœæ··ä¹±ï¼Œå› ä¸º"bank"çš„è¡¨ç¤ºæ··åˆäº†ä¸¤ä¸ªæ„æ€
```

**å¤±è´¥æ¡ˆä¾‹2ï¼šå¤æ‚å…³ç³»**

```python
# å°è¯•ï¼šdoctor - hospital + school = ?
# æœŸæœ›å¯èƒ½æ˜¯: teacher
result = model.most_similar(positive=['doctor', 'school'], negative=['hospital'])
# å®é™…: ç»“æœä¸ç¨³å®šï¼Œå› ä¸ºå…³ç³»ä¸æ˜¯ç®€å•çš„ç±»æ¯”
```

**å¤±è´¥æ¡ˆä¾‹3ï¼šæ–‡åŒ–åè§**

```python
# Word2Vecå­¦è‡ªæ–°é—»è¯­æ–™ï¼Œå­˜åœ¨åè§
result = model.most_similar(positive=['programmer', 'woman'], negative=['man'])
# å¯èƒ½è¾“å‡º: 'homemaker', 'nurse' â† æ€§åˆ«åˆ»æ¿å°è±¡ï¼
```

**ç°ä»£æ¨¡å‹(BERT/GPT)ä¸­è¿˜æˆç«‹å—ï¼Ÿ**

**éƒ¨åˆ†æˆç«‹ï¼Œä½†ä¸å¤ªé€‚ç”¨**ï¼š

```python
# BERTçš„åµŒå…¥æ˜¯ä¸Šä¸‹æ–‡åŒ–çš„
# "king"åœ¨ä¸åŒå¥å­ä¸­è¡¨ç¤ºä¸åŒ

sent1 = "The king ruled the country"
sent2 = "King of the jungle"  # æ¯”å–»ç”¨æ³•

# ä¸¤ä¸ª"king"çš„åµŒå…¥ä¸åŒï¼Œæ— æ³•ç›´æ¥è¿ç®—
```

**ä½†å¯ä»¥ç”¨äºå¥å­çº§åˆ«**ï¼š

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# å¥å­åµŒå…¥å¯ä»¥ç±»æ¯”
embed1 = model.encode("Paris is the capital of France")
embed2 = model.encode("Berlin is the capital of Germany")
embed3 = model.encode("London is the capital of")

# embed3 + (embed2 - embed1) çš„æœ€è¿‘é‚»å¯èƒ½æ˜¯å…³äºå…¶ä»–é¦–éƒ½çš„å¥å­
```

**å®è·µå»ºè®®**ï¼š

âœ… è¯å‘é‡ç®—æœ¯ï¼š
- ç®€å•ç±»æ¯”å…³ç³»(å›½å®¶-é¦–éƒ½ã€æ€§åˆ«å¯¹)
- Word2Vec/GloVeç­‰é™æ€åµŒå…¥
- ç†è§£å‘é‡è¯­ä¹‰æ–¹å‘

âŒ ä¸é€‚åˆï¼š
- å¤æ‚è¯­ä¹‰å…³ç³»
- ä¸Šä¸‹æ–‡ç›¸å…³çš„åµŒå…¥
- è¦æ±‚ç²¾ç¡®ç»“æœçš„åº”ç”¨

**å…³è”ä¸‹ä¸€ç« **ï¼šè¿™ç§å‘é‡è¿ç®—çš„èƒ½åŠ›æ¥è‡ªäºåµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ã€‚åœ¨Transformerä¸­ï¼ŒQueryå’ŒKeyçš„ç‚¹ç§¯æ­£æ˜¯åˆ©ç”¨äº†è¿™ç§å‡ ä½•å…³ç³»æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ç¬¬äºŒéƒ¨åˆ†å°†è¯¦ç»†è§£æè¿™ä¸€æœºåˆ¶ã€‚

---

## æœ¬ç« å°ç»“

æ­å–œä½ å®Œæˆç¬¬3ç« ï¼ç°åœ¨ä½ å·²ç»ç†è§£äº†LLMå·¥ä½œçš„ç¬¬ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚

### çŸ¥è¯†å›é¡¾

1. **åˆ†è¯ï¼ˆTokenizationï¼‰**
   - ä¸ºä»€ä¹ˆéœ€è¦å­è¯åˆ†è¯ï¼šè§£å†³è¯æ±‡è¡¨çˆ†ç‚¸å’ŒæœªçŸ¥è¯é—®é¢˜
   - BPEï¼šä»å­—ç¬¦å¼€å§‹ï¼Œåå¤åˆå¹¶é¢‘ç¹å­—ç¬¦å¯¹
   - WordPieceï¼šè€ƒè™‘ä¼¼ç„¶æå‡ï¼Œä½¿ç”¨##å‰ç¼€
   - Unigramï¼šä»å¤§è¯æ±‡è¡¨å‰ªæ
   - SentencePieceï¼šç›´æ¥åœ¨å­—èŠ‚æµä¸Šè®­ç»ƒï¼Œè¯­è¨€æ— å…³

2. **åµŒå…¥ï¼ˆEmbeddingï¼‰**
   - æœ¬è´¨ï¼šå°†tokenæ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´
   - åµŒå…¥æŸ¥æ‰¾ï¼šé€šè¿‡åµŒå…¥çŸ©é˜µï¼ˆæŸ¥æ‰¾è¡¨ï¼‰
   - å‡ ä½•æ„ä¹‰ï¼šè¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨ç©ºé—´ä¸­è·ç¦»è¿‘
   - å‘é‡è¿ç®—ï¼šKing - Man + Woman â‰ˆ Queen

3. **åµŒå…¥çš„åº”ç”¨**
   - è¯­ä¹‰æœç´¢ï¼šåŸºäºè¯­ä¹‰è€Œéå…³é”®è¯
   - æ¨èç³»ç»Ÿï¼šè®¡ç®—ç”¨æˆ·/ç‰©å“ç›¸ä¼¼åº¦
   - æ–‡æ¡£èšç±»ï¼šè‡ªåŠ¨ç»„ç»‡å†…å®¹
   - å¼‚å¸¸æ£€æµ‹ï¼šå‘ç°ä¸å¯»å¸¸çš„æ•°æ®

### å…³é”®æ¦‚å¿µ

- **Token**ï¼šåˆ†è¯åçš„æœ€å°å•ä½ï¼ˆå¯èƒ½æ˜¯å­—ã€è¯æˆ–å­è¯ï¼‰
- **Token ID**ï¼štokenåœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•
- **åµŒå…¥å‘é‡**ï¼štokençš„æ•°å€¼è¡¨ç¤ºï¼Œé€šå¸¸å‡ ç™¾åˆ°å‡ åƒç»´
- **ä½™å¼¦ç›¸ä¼¼åº¦**ï¼šè¡¡é‡ä¸¤ä¸ªå‘é‡è¯­ä¹‰ç›¸ä¼¼æ€§çš„åº¦é‡
- **åµŒå…¥ç©ºé—´**ï¼šæ‰€æœ‰åµŒå…¥å‘é‡ç»„æˆçš„é«˜ç»´ç©ºé—´

### å®è·µæŠ€å·§

âœ… **åˆ†è¯æ³¨æ„äº‹é¡¹**ï¼š
- ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨ä¸å…¼å®¹ï¼ˆä¸èƒ½æ··ç”¨ï¼‰
- åˆ†è¯å½±å“tokenæ•°é‡ï¼Œè¿›è€Œå½±å“æˆæœ¬å’Œä¸Šä¸‹æ–‡é•¿åº¦
- ä¸­æ–‡æ¨¡å‹ä¼˜å…ˆé€‰æ‹©æŒ‰è¯åˆ†è¯çš„ï¼ˆå¦‚Qwenï¼‰

âœ… **åµŒå…¥ä½¿ç”¨æŠ€å·§**ï¼š
- ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è€Œéæ¬§æ°è·ç¦»ï¼ˆåµŒå…¥å‘é‡å·²å½’ä¸€åŒ–ï¼‰
- å¯ä»¥å¯¹åµŒå…¥å‘é‡è¿›è¡Œè¿ç®—ï¼ˆåŠ å‡ä¹˜é™¤éƒ½æœ‰æ„ä¹‰ï¼‰
- ä½¿ç”¨ä¸“é—¨çš„å¥å­åµŒå…¥æ¨¡å‹ï¼ˆå¦‚sentence-transformersï¼‰è€Œéè¯åµŒå…¥

âœ… **å·¥ç¨‹å®è·µ**ï¼š
- åµŒå…¥å‘é‡å¯ä»¥ç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
- ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆå¦‚Faissã€Milvusï¼‰åŠ é€Ÿæ£€ç´¢
- å®šæœŸæ›´æ–°åµŒå…¥ï¼ˆæ¨¡å‹å‡çº§æ—¶ï¼‰

### æ€è€ƒé¢˜

1. å¦‚æœä¸€ä¸ªæ¨¡å‹çš„è¯æ±‡è¡¨æœ‰50,000ä¸ªtoken,åµŒå…¥ç»´åº¦æ˜¯768ï¼ŒåµŒå…¥çŸ©é˜µæœ‰å¤šå°‘ä¸ªå‚æ•°ï¼Ÿå ç”¨å¤šå°‘å†…å­˜ï¼ˆFP32ï¼‰ï¼Ÿ
2. ä¸ºä»€ä¹ˆä½™å¼¦ç›¸ä¼¼åº¦æ¯”æ¬§æ°è·ç¦»æ›´é€‚åˆè¡¡é‡åµŒå…¥ç›¸ä¼¼æ€§ï¼Ÿ
3. å¦‚æœ"King - Man + Woman"ç»™å‡ºçš„æœ€æ¥è¿‘è¯æ˜¯"Queen"ï¼Œè¿™æ˜¯å¦æ„å‘³ç€æ¨¡å‹ç†è§£äº†"æ€§åˆ«"æ¦‚å¿µï¼Ÿ
4. åœ¨éŸ³ä¹æ¨èç³»ç»Ÿä¸­ï¼Œå¦‚æœç”¨æˆ·å–œæ¬¢çš„æ­Œæ›²é£æ ¼å¾ˆæ‚ï¼ˆæ‘‡æ»šã€å¤å…¸ã€æµè¡Œéƒ½æœ‰ï¼‰ï¼Œå¦‚ä½•ä¼˜åŒ–æ¨èç®—æ³•ï¼Ÿ

### ä¸‹ä¸€ç« é¢„å‘Š

ç¬¬ä¸€éƒ¨åˆ†çš„ä¸‰ç« å·²ç»å®Œæˆï¼ä½ å·²ç»ï¼š
- âœ… äº†è§£äº†LLMçš„å†å²å’Œä¸¤å¤§å®¶æ—ï¼ˆç¬¬1ç« ï¼‰
- âœ… æŒæ¡äº†æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆç¬¬2ç« ï¼‰
- âœ… ç†è§£äº†åˆ†è¯å’ŒåµŒå…¥çš„åŸç†ï¼ˆç¬¬3ç« ï¼‰

åœ¨**ç¬¬äºŒéƒ¨åˆ†ï¼šTransformeræ¶æ„æ­ç§˜**ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¨¡å‹å†…éƒ¨ï¼Œå­¦ä¹ ï¼š
- Transformerçš„æ ¸å¿ƒç»„ä»¶ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶
- å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç ã€å‰é¦ˆç½‘ç»œ
- ç¼–ç å™¨å’Œè§£ç å™¨çš„å®Œæ•´æ¶æ„
- å¦‚ä½•ä»é›¶å®ç°ä¸€ä¸ªTransformer

å‡†å¤‡å¥½æ¢ç´¢LLMçš„"å¤§è„‘"äº†å—ï¼Ÿè®©æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼

---

**æœ¬ç« ä»£ç ç¤ºä¾‹**ï¼šæœ¬ç« æ‰€æœ‰ä»£ç å·²æ•´ç†åˆ°GitHubä»“åº“

**æ¨èé˜…è¯»**ï¼š
- è®ºæ–‡ï¼šã€ŠNeural Machine Translation by Jointly Learning to Align and Translateã€‹ï¼ˆé¦–æ¬¡æå‡ºBPEç”¨äºNMTï¼‰
- åšå®¢ï¼šHugging Faceçš„ã€ŠTokenizersè¯¦è§£ã€‹
- å·¥å…·ï¼šTokenizersåº“æ–‡æ¡£ï¼ˆhuggingface.co/docs/tokenizersï¼‰
- è§†é¢‘ï¼šJay Alammarçš„ã€ŠThe Illustrated Word2vecã€‹ï¼ˆè¯åµŒå…¥å¯è§†åŒ–ï¼‰

**æ¨èå®è·µ**ï¼š
- è¯•è¯•ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨ï¼Œè§‚å¯Ÿå·®å¼‚
- ç”¨t-SNEå¯è§†åŒ–ä½ è‡ªå·±é¢†åŸŸçš„è¯åµŒå…¥
- æ„å»ºä¸€ä¸ªåŸºäºåµŒå…¥çš„ç®€å•æœç´¢å¼•æ“
