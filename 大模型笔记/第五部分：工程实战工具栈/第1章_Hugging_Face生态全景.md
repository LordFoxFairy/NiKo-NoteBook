# ç¬¬1ç« ï¼šHugging Faceç”Ÿæ€å…¨æ™¯

> æŒæ¡æœ€æµè¡Œçš„LLMå¼€å‘ç”Ÿæ€ç³»ç»Ÿã€‚

## æœ¬ç« å¯¼è¯»

Hugging Faceå·²æˆä¸ºLLMå¼€å‘çš„äº‹å®æ ‡å‡†ç”Ÿæ€ã€‚æ— è®ºæ˜¯æ¨¡å‹åŠ è½½ã€æ•°æ®å¤„ç†ï¼Œè¿˜æ˜¯è®­ç»ƒå¾®è°ƒï¼ŒHugging Faceæä¾›äº†ä¸€æ•´å¥—å¼€ç®±å³ç”¨çš„å·¥å…·é“¾ã€‚æœ¬ç« å°†ç³»ç»Ÿä»‹ç»ï¼š

**æ ¸å¿ƒå†…å®¹**ï¼š
- Transformersåº“ï¼ˆæ¨¡å‹åŠ è½½ã€Pipelineã€è‡ªå®šä¹‰ï¼‰
- Datasetsåº“ï¼ˆæ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€æµå¼å¤„ç†ï¼‰
- Trainer APIï¼ˆè®­ç»ƒå¾ªç¯ã€å›è°ƒå‡½æ•°ã€æ—¥å¿—ï¼‰
- Hubç”Ÿæ€ï¼ˆæ¨¡å‹åˆ†äº«ã€ç‰ˆæœ¬ç®¡ç†ã€Spaceséƒ¨ç½²ï¼‰
- PEFTä¸TRLåº“ï¼ˆé«˜æ•ˆå¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ï¼‰

**å­¦ä¹ ç›®æ ‡**ï¼š
- æŒæ¡Transformersæ ¸å¿ƒAPI
- èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†
- ä½¿ç”¨Trainerå®ç°è®­ç»ƒæµç¨‹
- å‘å¸ƒæ¨¡å‹åˆ°Hugging Face Hub
- åº”ç”¨PEFTè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

---

## ä¸€ã€Transformersåº“æ ¸å¿ƒç”¨æ³•

### 1. æ¨¡å‹åŠ è½½ä¸é…ç½®

#### ï¼ˆ1ï¼‰åŸºç¡€åŠ è½½æµç¨‹

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    AutoConfig
)
import torch

class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨ï¼ˆæœ€ä½³å®è·µï¼‰"""
    
    @staticmethod
    def load_model_for_inference(
        model_name: str,
        device: str = "cuda",
        torch_dtype: torch.dtype = torch.float16,
        use_flash_attention: bool = True
    ):
        """åŠ è½½æ¨¡å‹ç”¨äºæ¨ç†
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚"meta-llama/Llama-3-8B"ï¼‰
            device: è®¾å¤‡
            torch_dtype: æ•°æ®ç±»å‹ï¼ˆfloat16èŠ‚çœæ˜¾å­˜ï¼‰
            use_flash_attention: ä½¿ç”¨Flash Attention 2
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        # 1. åŠ è½½é…ç½®
        config = AutoConfig.from_pretrained(
            model_name,
            trust_remote_code=True  # ä¿¡ä»»è‡ªå®šä¹‰ä»£ç ï¼ˆå¦‚Qwenï¼‰
        )
        
        # 2. å¯ç”¨Flash Attention 2ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if use_flash_attention and hasattr(config, "_attn_implementation"):
            config._attn_implementation = "flash_attention_2"
        
        # 3. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            use_fast=True  # ä½¿ç”¨Rustå®ç°çš„å¿«é€Ÿåˆ†è¯å™¨
        )
        
        # è®¾ç½®padding tokenï¼ˆæŸäº›æ¨¡å‹ç¼ºå¤±ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4. åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            config=config,
            torch_dtype=torch_dtype,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
            trust_remote_code=True
        )
        
        # 5. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        model.eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device}")
        print(f"   å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B")
        print(f"   æ•°æ®ç±»å‹: {torch_dtype}")
        
        return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹
model, tokenizer = ModelLoader.load_model_for_inference(
    model_name="meta-llama/Llama-3-8B-Instruct",
    torch_dtype=torch.bfloat16,
    use_flash_attention=True
)

# å¿«é€Ÿæ¨ç†æµ‹è¯•
prompt = "What is deep learning?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"å›å¤: {response}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ­£åœ¨åŠ è½½æ¨¡å‹: meta-llama/Llama-3-8B-Instruct
âœ… æ¨¡å‹å·²åŠ è½½åˆ° cuda
   å‚æ•°é‡: 8.03B
   æ•°æ®ç±»å‹: torch.bfloat16

å›å¤: What is deep learning? Deep learning is a subset of machine learning...
```

#### ï¼ˆ2ï¼‰é‡åŒ–åŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```python
from transformers import BitsAndBytesConfig

class QuantizedModelLoader:
    """é‡åŒ–æ¨¡å‹åŠ è½½å™¨"""
    
    @staticmethod
    def load_4bit_model(model_name: str):
        """4-bité‡åŒ–åŠ è½½ï¼ˆNF4ï¼‰
        
        æ˜¾å­˜å ç”¨: ~0.5GB/Bå‚æ•°
        ä¾‹: 70Bæ¨¡å‹ä»…éœ€35GBï¼ˆvs 140GB FP16ï¼‰
        """
        # BitsAndBytesé…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",  # NormalFloat4
            bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–
            bnb_4bit_compute_dtype=torch.bfloat16  # è®¡ç®—ç±»å‹
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        return model, tokenizer
    
    @staticmethod
    def load_8bit_model(model_name: str):
        """8-bité‡åŒ–åŠ è½½ï¼ˆLLM.int8()ï¼‰
        
        æ˜¾å­˜å ç”¨: ~1GB/Bå‚æ•°
        """
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0  # å¼‚å¸¸å€¼é˜ˆå€¼
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto"
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹ï¼šåœ¨å•å¼ A100 40GBä¸ŠåŠ è½½70Bæ¨¡å‹
model_70b, tokenizer_70b = QuantizedModelLoader.load_4bit_model(
    "meta-llama/Llama-3-70B-Instruct"
)

print(f"70Bæ¨¡å‹æ˜¾å­˜å ç”¨: ~{70 * 0.5:.0f}GBï¼ˆ4-bitï¼‰")
```

#### ï¼ˆ3ï¼‰å¤šGPUå¹¶è¡ŒåŠ è½½

```python
from accelerate import infer_auto_device_map, dispatch_model
import torch

class MultiGPULoader:
    """å¤šGPUåŠ è½½å™¨"""
    
    @staticmethod
    def load_with_device_map(
        model_name: str,
        num_gpus: int = 2,
        max_memory_per_gpu: str = "40GiB"
    ):
        """è‡ªå®šä¹‰è®¾å¤‡æ˜ å°„
        
        Args:
            model_name: æ¨¡å‹åç§°
            num_gpus: GPUæ•°é‡
            max_memory_per_gpu: æ¯å¼ GPUæœ€å¤§å†…å­˜
        """
        # 1. åŠ è½½é…ç½®ï¼ˆä¸åŠ è½½æƒé‡ï¼‰
        config = AutoConfig.from_pretrained(model_name)
        
        # 2. åˆ›å»ºç©ºæ¨¡å‹
        with torch.device("meta"):
            model = AutoModelForCausalLM.from_config(config)
        
        # 3. æ¨æ–­è®¾å¤‡æ˜ å°„
        max_memory = {i: max_memory_per_gpu for i in range(num_gpus)}
        max_memory["cpu"] = "100GiB"  # CPUç”¨äºæº¢å‡º
        
        device_map = infer_auto_device_map(
            model,
            max_memory=max_memory,
            no_split_module_classes=["LlamaDecoderLayer"]  # ä¸æ‹†åˆ†çš„æ¨¡å—
        )
        
        # 4. åŠ è½½æƒé‡å¹¶åˆ†å‘
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device_map,
            torch_dtype=torch.float16
        )
        
        # æ‰“å°è®¾å¤‡æ˜ å°„
        print("è®¾å¤‡æ˜ å°„:")
        for name, device in model.hf_device_map.items():
            print(f"  {name:40s} -> {device}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹ï¼šå°†70Bæ¨¡å‹åˆ†é…åˆ°2å¼ A100
model_multi_gpu, tokenizer = MultiGPULoader.load_with_device_map(
    model_name="meta-llama/Llama-3-70B-Instruct",
    num_gpus=2,
    max_memory_per_gpu="40GiB"
)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
è®¾å¤‡æ˜ å°„:
  model.embed_tokens                       -> cuda:0
  model.layers.0                           -> cuda:0
  model.layers.1                           -> cuda:0
  ...
  model.layers.40                          -> cuda:0
  model.layers.41                          -> cuda:1
  ...
  model.layers.79                          -> cuda:1
  model.norm                               -> cuda:1
  lm_head                                  -> cuda:1
```

---

### 2. Pipelineå¿«é€Ÿä¸Šæ‰‹

#### ï¼ˆ1ï¼‰å†…ç½®Pipeline

```python
from transformers import pipeline
import time

class PipelineExamples:
    """Pipelineä½¿ç”¨ç¤ºä¾‹"""
    
    @staticmethod
    def text_generation_example():
        """æ–‡æœ¬ç”ŸæˆPipeline"""
        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¹¶ç¼“å­˜
        generator = pipeline(
            "text-generation",
            model="gpt2",
            device=0  # ä½¿ç”¨GPU 0
        )
        
        # ç”Ÿæˆæ–‡æœ¬
        outputs = generator(
            "Once upon a time",
            max_length=50,
            num_return_sequences=3,  # ç”Ÿæˆ3ä¸ªç‰ˆæœ¬
            temperature=0.8
        )
        
        print("æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:")
        for i, output in enumerate(outputs, 1):
            print(f"\nç‰ˆæœ¬{i}: {output['generated_text']}")
    
    @staticmethod
    def fill_mask_example():
        """å¡«ç©ºPipelineï¼ˆBERTç±»æ¨¡å‹ï¼‰"""
        unmasker = pipeline("fill-mask", model="bert-base-uncased")
        
        outputs = unmasker("The capital of France is [MASK].")
        
        print("\nå¡«ç©ºç¤ºä¾‹:")
        for output in outputs[:3]:
            print(f"  {output['token_str']:10s} (ç½®ä¿¡åº¦: {output['score']:.3f})")
    
    @staticmethod
    def text_classification_example():
        """æ–‡æœ¬åˆ†ç±»Pipeline"""
        classifier = pipeline(
            "text-classification",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
        
        outputs = classifier([
            "I love this product!",
            "This is terrible."
        ])
        
        print("\næƒ…æ„Ÿåˆ†ç±»:")
        for text, output in zip(["æ­£é¢è¯„è®º", "è´Ÿé¢è¯„è®º"], outputs):
            print(f"  {text}: {output['label']} (åˆ†æ•°: {output['score']:.3f})")
    
    @staticmethod
    def question_answering_example():
        """é—®ç­”Pipeline"""
        qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")
        
        context = """
        The Transformer is a deep learning model introduced in 2017, used primarily 
        in the field of natural language processing (NLP). It was proposed in the 
        paper "Attention Is All You Need" by Vaswani et al.
        """
        
        question = "When was the Transformer introduced?"
        
        answer = qa_pipeline(question=question, context=context)
        
        print("\né—®ç­”ç¤ºä¾‹:")
        print(f"  é—®é¢˜: {question}")
        print(f"  ç­”æ¡ˆ: {answer['answer']} (ç½®ä¿¡åº¦: {answer['score']:.3f})")

# è¿è¡Œç¤ºä¾‹
examples = PipelineExamples()
examples.text_generation_example()
examples.fill_mask_example()
examples.text_classification_example()
examples.question_answering_example()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:

ç‰ˆæœ¬1: Once upon a time, there was a young girl who lived in a small village...
ç‰ˆæœ¬2: Once upon a time in a galaxy far, far away...
ç‰ˆæœ¬3: Once upon a time, people believed that the earth was flat...

å¡«ç©ºç¤ºä¾‹:
  paris      (ç½®ä¿¡åº¦: 0.952)
  france     (ç½®ä¿¡åº¦: 0.018)
  london     (ç½®ä¿¡åº¦: 0.012)

æƒ…æ„Ÿåˆ†ç±»:
  æ­£é¢è¯„è®º: POSITIVE (åˆ†æ•°: 0.999)
  è´Ÿé¢è¯„è®º: NEGATIVE (åˆ†æ•°: 0.998)

é—®ç­”ç¤ºä¾‹:
  é—®é¢˜: When was the Transformer introduced?
  ç­”æ¡ˆ: 2017 (ç½®ä¿¡åº¦: 0.987)
```

#### ï¼ˆ2ï¼‰è‡ªå®šä¹‰Pipeline

```python
from transformers import Pipeline
from typing import Dict, List

class CustomSummarizationPipeline(Pipeline):
    """è‡ªå®šä¹‰æ‘˜è¦Pipeline"""
    
    def _sanitize_parameters(self, **kwargs):
        """å‚æ•°é¢„å¤„ç†"""
        preprocess_kwargs = {}
        forward_kwargs = {}
        postprocess_kwargs = {}
        
        if "max_length" in kwargs:
            preprocess_kwargs["max_length"] = kwargs["max_length"]
        if "min_length" in kwargs:
            forward_kwargs["min_length"] = kwargs["min_length"]
        if "summary_length" in kwargs:
            forward_kwargs["max_new_tokens"] = kwargs["summary_length"]
        
        return preprocess_kwargs, forward_kwargs, postprocess_kwargs
    
    def preprocess(self, text: str, max_length: int = 1024):
        """é¢„å¤„ç†ï¼šåˆ†è¯"""
        inputs = self.tokenizer(
            text,
            max_length=max_length,
            truncation=True,
            return_tensors="pt"
        )
        return inputs
    
    def _forward(self, model_inputs, min_length: int = 50, max_new_tokens: int = 150):
        """å‰å‘ä¼ æ’­ï¼šç”Ÿæˆæ‘˜è¦"""
        outputs = self.model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            min_length=min_length,
            do_sample=False,  # ä½¿ç”¨beam search
            num_beams=4
        )
        return outputs
    
    def postprocess(self, model_outputs):
        """åå¤„ç†ï¼šè§£ç """
        summary = self.tokenizer.decode(
            model_outputs[0],
            skip_special_tokens=True
        )
        return {"summary_text": summary}

# æ³¨å†Œè‡ªå®šä¹‰Pipeline
from transformers import AutoModelForSeq2SeqLM
from transformers import pipeline as transformers_pipeline

# ä½¿ç”¨è‡ªå®šä¹‰Pipeline
summarizer = CustomSummarizationPipeline(
    model=AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn"),
    tokenizer=AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
)

article = """
Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to 
the natural intelligence displayed by humans and animals. Leading AI textbooks define 
the field as the study of "intelligent agents": any device that perceives its environment 
and takes actions that maximize its chance of successfully achieving its goals.
"""

summary = summarizer(article, summary_length=50)
print(f"åŸæ–‡é•¿åº¦: {len(article)} å­—ç¬¦")
print(f"æ‘˜è¦: {summary['summary_text']}")
```

---

### 3. è‡ªå®šä¹‰æ¨¡å‹ä¸åˆ†è¯å™¨

#### ï¼ˆ1ï¼‰æ‰©å±•è¯è¡¨

```python
class TokenizerExtender:
    """åˆ†è¯å™¨æ‰©å±•å™¨"""
    
    @staticmethod
    def add_special_tokens(
        tokenizer: AutoTokenizer,
        new_tokens: List[str]
    ):
        """æ·»åŠ ç‰¹æ®ŠToken
        
        ç”¨ä¾‹:
        - æ·»åŠ æ–°çš„ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚<image>ã€<tool_call>ï¼‰
        - æ·»åŠ é¢†åŸŸæœ¯è¯­ï¼ˆæå‡åˆ†è¯æ•ˆç‡ï¼‰
        """
        num_added = tokenizer.add_special_tokens({
            "additional_special_tokens": new_tokens
        })
        
        print(f"æ·»åŠ äº† {num_added} ä¸ªç‰¹æ®Štoken")
        print(f"æ–°è¯è¡¨å¤§å°: {len(tokenizer)}")
        
        return num_added
    
    @staticmethod
    def resize_model_embeddings(
        model: AutoModelForCausalLM,
        tokenizer: AutoTokenizer
    ):
        """è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°
        
        æ³¨æ„: æ·»åŠ tokenåå¿…é¡»è°ƒæ•´æ¨¡å‹
        """
        model.resize_token_embeddings(len(tokenizer))
        
        # æ–°tokençš„åµŒå…¥ä¼šéšæœºåˆå§‹åŒ–ï¼Œéœ€è¦å¾®è°ƒ
        print(f"æ¨¡å‹åµŒå…¥å±‚å·²è°ƒæ•´ä¸º {len(tokenizer)}")

# ä½¿ç”¨ç¤ºä¾‹
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# æ·»åŠ æ–°token
new_tokens = ["<image>", "<video>", "<audio>"]
TokenizerExtender.add_special_tokens(tokenizer, new_tokens)

# è°ƒæ•´æ¨¡å‹
TokenizerExtender.resize_model_embeddings(model, tokenizer)

# æµ‹è¯•æ–°token
text = "This is an image: <image>"
tokens = tokenizer(text, return_tensors="pt")
print(f"Token IDs: {tokens['input_ids']}")
```

#### ï¼ˆ2ï¼‰è‡ªå®šä¹‰æ¨¡å‹é…ç½®

```python
from transformers import PretrainedConfig, PreTrainedModel
import torch.nn as nn

class CustomLMConfig(PretrainedConfig):
    """è‡ªå®šä¹‰è¯­è¨€æ¨¡å‹é…ç½®"""
    model_type = "custom_lm"
    
    def __init__(
        self,
        vocab_size: int = 50257,
        hidden_size: int = 768,
        num_hidden_layers: int = 12,
        num_attention_heads: int = 12,
        intermediate_size: int = 3072,
        max_position_embeddings: int = 1024,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings

class CustomLMModel(PreTrainedModel):
    """è‡ªå®šä¹‰è¯­è¨€æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    config_class = CustomLMConfig
    
    def __init__(self, config: CustomLMConfig):
        super().__init__(config)
        
        # åµŒå…¥å±‚
        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # Transformerå±‚ï¼ˆç®€åŒ–ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_size,
            nhead=config.num_attention_heads,
            dim_feedforward=config.intermediate_size,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=config.num_hidden_layers
        )
        
        # è¾“å‡ºå±‚
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
    
    def forward(self, input_ids, attention_mask=None):
        """å‰å‘ä¼ æ’­"""
        # åµŒå…¥
        hidden_states = self.embeddings(input_ids)
        
        # Transformer
        hidden_states = self.transformer(
            hidden_states,
            src_key_padding_mask=~attention_mask.bool() if attention_mask is not None else None
        )
        
        # è¾“å‡º
        logits = self.lm_head(hidden_states)
        
        return {"logits": logits}

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
config = CustomLMConfig(
    vocab_size=50257,
    hidden_size=512,
    num_hidden_layers=6
)

custom_model = CustomLMModel(config)

print(f"è‡ªå®šä¹‰æ¨¡å‹å‚æ•°é‡: {custom_model.num_parameters() / 1e6:.2f}M")

# ä¿å­˜æ¨¡å‹
custom_model.save_pretrained("./custom_lm")

# åŠ è½½æ¨¡å‹
loaded_model = CustomLMModel.from_pretrained("./custom_lm")
```

---

## äºŒã€Datasetsä¸æ•°æ®å¤„ç†

### 1. æ•°æ®é›†åŠ è½½ä¸é¢„å¤„ç†

#### ï¼ˆ1ï¼‰åŠ è½½Hugging Face Hubæ•°æ®é›†

```python
from datasets import load_dataset, DatasetDict
from typing import Optional

class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨"""
    
    @staticmethod
    def load_text_dataset(
        dataset_name: str,
        split: str = "train",
        streaming: bool = False,
        num_samples: Optional[int] = None
    ):
        """åŠ è½½æ–‡æœ¬æ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            split: æ•°æ®é›†åˆ‡åˆ†ï¼ˆtrain/validation/testï¼‰
            streaming: æµå¼åŠ è½½ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰
            num_samples: é™åˆ¶æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name} ({split})")
        
        dataset = load_dataset(
            dataset_name,
            split=split,
            streaming=streaming
        )
        
        # é™åˆ¶æ ·æœ¬æ•°
        if num_samples and not streaming:
            dataset = dataset.select(range(min(num_samples, len(dataset))))
        
        print(f"âœ… æ•°æ®é›†å·²åŠ è½½")
        if not streaming:
            print(f"   æ ·æœ¬æ•°: {len(dataset)}")
            print(f"   åˆ—å: {dataset.column_names}")
        
        return dataset

# ä½¿ç”¨ç¤ºä¾‹
# 1. æ ‡å‡†åŠ è½½
dataset = DatasetLoader.load_text_dataset(
    "c4",
    split="train",
    num_samples=10000
)

# 2. æµå¼åŠ è½½ï¼ˆå¤„ç†TBçº§æ•°æ®ï¼‰
dataset_stream = DatasetLoader.load_text_dataset(
    "c4",
    split="train",
    streaming=True
)

# æµå¼è¿­ä»£
for i, example in enumerate(dataset_stream):
    if i >= 5:
        break
    print(f"æ ·æœ¬{i}: {example['text'][:100]}...")
```

#### ï¼ˆ2ï¼‰æ•°æ®é¢„å¤„ç†

```python
from datasets import Dataset
from transformers import PreTrainedTokenizer
from typing import Callable

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, tokenizer: PreTrainedTokenizer):
        self.tokenizer = tokenizer
    
    def tokenize_function(self, examples: Dict) -> Dict:
        """åŸºç¡€åˆ†è¯"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length"
        )
    
    def tokenize_qa_pairs(self, examples: Dict) -> Dict:
        """é—®ç­”å¯¹åˆ†è¯"""
        # åˆå¹¶é—®é¢˜å’Œç­”æ¡ˆ
        inputs = [
            f"Question: {q}\nAnswer: {a}"
            for q, a in zip(examples["question"], examples["answer"])
        ]
        
        return self.tokenizer(
            inputs,
            truncation=True,
            max_length=512,
            padding="max_length"
        )
    
    def process_dataset(
        self,
        dataset: Dataset,
        processing_fn: Optional[Callable] = None,
        batched: bool = True,
        num_proc: int = 4
    ) -> Dataset:
        """å¤„ç†æ•°æ®é›†
        
        Args:
            dataset: è¾“å…¥æ•°æ®é›†
            processing_fn: å¤„ç†å‡½æ•°ï¼ˆé»˜è®¤ä½¿ç”¨tokenize_functionï¼‰
            batched: æ‰¹é‡å¤„ç†ï¼ˆæé€Ÿ10-100xï¼‰
            num_proc: å¹¶è¡Œè¿›ç¨‹æ•°
        """
        if processing_fn is None:
            processing_fn = self.tokenize_function
        
        print(f"æ­£åœ¨å¤„ç†æ•°æ®é›†ï¼ˆæ‰¹é‡={batched}, è¿›ç¨‹æ•°={num_proc}ï¼‰...")
        
        processed_dataset = dataset.map(
            processing_fn,
            batched=batched,
            num_proc=num_proc,
            remove_columns=dataset.column_names,  # ç§»é™¤åŸå§‹åˆ—
            desc="Tokenizing"
        )
        
        print(f"âœ… å¤„ç†å®Œæˆ")
        print(f"   å¤„ç†ååˆ—å: {processed_dataset.column_names}")
        
        return processed_dataset

# ä½¿ç”¨ç¤ºä¾‹
tokenizer = AutoTokenizer.from_pretrained("gpt2")
preprocessor = DataPreprocessor(tokenizer)

# åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
from datasets import Dataset

raw_data = {
    "text": [
        "This is the first example.",
        "This is the second example.",
        "And this is the third one."
    ]
}
dataset = Dataset.from_dict(raw_data)

# å¤„ç†æ•°æ®é›†
tokenized_dataset = preprocessor.process_dataset(
    dataset,
    batched=True,
    num_proc=2
)

print(tokenized_dataset[0])
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ­£åœ¨å¤„ç†æ•°æ®é›†ï¼ˆæ‰¹é‡=True, è¿›ç¨‹æ•°=2ï¼‰...
Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 145.23 examples/s]
âœ… å¤„ç†å®Œæˆ
   å¤„ç†ååˆ—å: ['input_ids', 'attention_mask']

{'input_ids': [1212, 318, 262, 717, 1672, 13, ...], 'attention_mask': [1, 1, 1, 1, 1, 1, ...]}
```


#### ï¼ˆ3ï¼‰è¿‡æ»¤ä¸é‡‡æ ·

```python
class DatasetFilter:
    """æ•°æ®é›†è¿‡æ»¤å™¨"""
    
    @staticmethod
    def filter_by_length(
        dataset: Dataset,
        min_length: int = 10,
        max_length: int = 1000,
        text_column: str = "text"
    ) -> Dataset:
        """æŒ‰æ–‡æœ¬é•¿åº¦è¿‡æ»¤"""
        def length_filter(example):
            text_len = len(example[text_column])
            return min_length <= text_len <= max_length
        
        filtered = dataset.filter(length_filter, desc="æŒ‰é•¿åº¦è¿‡æ»¤")
        
        print(f"è¿‡æ»¤å‰: {len(dataset)} æ ·æœ¬")
        print(f"è¿‡æ»¤å: {len(filtered)} æ ·æœ¬")
        
        return filtered
    
    @staticmethod
    def filter_by_custom(
        dataset: Dataset,
        filter_fn: Callable
    ) -> Dataset:
        """è‡ªå®šä¹‰è¿‡æ»¤"""
        return dataset.filter(filter_fn, desc="è‡ªå®šä¹‰è¿‡æ»¤")
    
    @staticmethod
    def random_sample(
        dataset: Dataset,
        num_samples: int,
        seed: int = 42
    ) -> Dataset:
        """éšæœºé‡‡æ ·"""
        shuffled = dataset.shuffle(seed=seed)
        return shuffled.select(range(min(num_samples, len(dataset))))

# ä½¿ç”¨ç¤ºä¾‹
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb", split="train")

# è¿‡æ»¤çŸ­æ–‡æœ¬
filtered_dataset = DatasetFilter.filter_by_length(
    dataset,
    min_length=100,
    max_length=5000,
    text_column="text"
)

# é‡‡æ ·1000æ¡
sampled_dataset = DatasetFilter.random_sample(filtered_dataset, 1000)
```

---

### 2. æ•°æ®æ˜ å°„ä¸æ‰¹å¤„ç†

#### ï¼ˆ1ï¼‰é«˜çº§æ˜ å°„æ“ä½œ

```python
class AdvancedDataMapper:
    """é«˜çº§æ•°æ®æ˜ å°„å™¨"""
    
    @staticmethod
    def create_instruction_dataset(dataset: Dataset) -> Dataset:
        """åˆ›å»ºæŒ‡ä»¤å¾®è°ƒæ ¼å¼æ•°æ®é›†"""
        def format_instruction(example):
            # æ ¼å¼: Instruction -> Response
            instruction = f"""Below is an instruction. Write a response.

### Instruction:
{example['input']}

### Response:
{example['output']}"""
            
            return {"text": instruction}
        
        return dataset.map(
            format_instruction,
            remove_columns=dataset.column_names,
            desc="æ ¼å¼åŒ–æŒ‡ä»¤"
        )
    
    @staticmethod
    def add_length_column(dataset: Dataset) -> Dataset:
        """æ·»åŠ é•¿åº¦åˆ—"""
        def add_length(example):
            example["text_length"] = len(example["text"])
            return example
        
        return dataset.map(add_length, desc="æ·»åŠ é•¿åº¦")
    
    @staticmethod
    def batch_processing(dataset: Dataset, tokenizer) -> Dataset:
        """æ‰¹é‡å¤„ç†ï¼ˆåŠ¨æ€paddingï¼‰"""
        def batch_tokenize(examples):
            # æ‰¹é‡åˆ†è¯ï¼ˆè‡ªåŠ¨paddingåˆ°æ‰¹æ¬¡æœ€å¤§é•¿åº¦ï¼‰
            return tokenizer(
                examples["text"],
                truncation=True,
                max_length=512,
                padding=False  # è®­ç»ƒæ—¶åŠ¨æ€padding
            )
        
        return dataset.map(
            batch_tokenize,
            batched=True,
            batch_size=1000,
            remove_columns=["text"],
            desc="æ‰¹é‡åˆ†è¯"
        )

# ä½¿ç”¨ç¤ºä¾‹
# åˆ›å»ºç¤ºä¾‹æ•°æ®
raw_data = {
    "input": ["Translate to French: Hello", "Summarize: Long text..."],
    "output": ["Bonjour", "Summary..."]
}
dataset = Dataset.from_dict(raw_data)

# æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ ¼å¼
mapper = AdvancedDataMapper()
instruction_dataset = mapper.create_instruction_dataset(dataset)

print(instruction_dataset[0]["text"])
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Below is an instruction. Write a response.

### Instruction:
Translate to French: Hello

### Response:
Bonjour
```

---

### 3. å¤§è§„æ¨¡æ•°æ®æµå¼å¤„ç†

#### ï¼ˆ1ï¼‰æµå¼è¿­ä»£

```python
from datasets import load_dataset
from itertools import islice

class StreamingDataHandler:
    """æµå¼æ•°æ®å¤„ç†å™¨"""
    
    @staticmethod
    def stream_and_process(
        dataset_name: str,
        processing_fn: Callable,
        batch_size: int = 1000,
        max_batches: Optional[int] = None
    ):
        """æµå¼å¤„ç†å¤§æ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            processing_fn: å¤„ç†å‡½æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        """
        # æµå¼åŠ è½½
        dataset = load_dataset(dataset_name, split="train", streaming=True)
        
        batch = []
        num_batches = 0
        
        for example in dataset:
            batch.append(example)
            
            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°
            if len(batch) >= batch_size:
                # å¤„ç†æ‰¹æ¬¡
                processing_fn(batch)
                
                num_batches += 1
                batch = []
                
                # è¾¾åˆ°æœ€å¤§æ‰¹æ¬¡æ•°
                if max_batches and num_batches >= max_batches:
                    break
        
        # å¤„ç†å‰©ä½™æ ·æœ¬
        if batch:
            processing_fn(batch)
    
    @staticmethod
    def interleave_datasets(dataset_names: List[str], probabilities: List[float]):
        """äº¤å‰åŠ è½½å¤šä¸ªæ•°æ®é›†
        
        Args:
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
            probabilities: é‡‡æ ·æ¦‚ç‡
        """
        from datasets import interleave_datasets
        
        # åŠ è½½å¤šä¸ªæ•°æ®é›†ï¼ˆæµå¼ï¼‰
        datasets = [
            load_dataset(name, split="train", streaming=True)
            for name in dataset_names
        ]
        
        # äº¤å‰é‡‡æ ·
        interleaved = interleave_datasets(
            datasets,
            probabilities=probabilities,
            seed=42
        )
        
        return interleaved

# ä½¿ç”¨ç¤ºä¾‹
def process_batch(batch):
    """æ‰¹æ¬¡å¤„ç†å‡½æ•°"""
    print(f"å¤„ç†æ‰¹æ¬¡ï¼Œå¤§å°: {len(batch)}")
    # è¿™é‡Œå¯ä»¥è¿›è¡Œåˆ†è¯ã€ä¿å­˜ç­‰æ“ä½œ

# æµå¼å¤„ç†C4æ•°æ®é›†ï¼ˆ314GBï¼‰
StreamingDataHandler.stream_and_process(
    dataset_name="c4",
    processing_fn=process_batch,
    batch_size=1000,
    max_batches=10  # ä»…å¤„ç†å‰10ä¸ªæ‰¹æ¬¡
)

# äº¤å‰å¤šä¸ªæ•°æ®é›†ï¼ˆå¸¸ç”¨äºé¢„è®­ç»ƒï¼‰
interleaved = StreamingDataHandler.interleave_datasets(
    dataset_names=["c4", "wikipedia", "bookcorpus"],
    probabilities=[0.5, 0.3, 0.2]  # C4å 50%ï¼ŒWikipediaå 30%ï¼ŒBookCorpuså 20%
)

# è¿­ä»£äº¤å‰æ•°æ®
for i, example in enumerate(islice(interleaved, 5)):
    print(f"æ ·æœ¬{i}: {example['text'][:50]}...")
```

---

## ä¸‰ã€Trainerä¸è®­ç»ƒæµç¨‹

### 1. TrainingArgumentsé…ç½®è¯¦è§£

```python
from transformers import TrainingArguments
from dataclasses import dataclass

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ï¼ˆæœ€ä½³å®è·µï¼‰"""
    
    @staticmethod
    def get_default_args(output_dir: str = "./results") -> TrainingArguments:
        """é»˜è®¤è®­ç»ƒå‚æ•°"""
        return TrainingArguments(
            # === è¾“å‡ºä¸æ—¥å¿— ===
            output_dir=output_dir,
            logging_dir=f"{output_dir}/logs",
            logging_steps=100,
            save_steps=500,
            save_total_limit=3,  # ä»…ä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
            
            # === è®­ç»ƒè¶…å‚æ•° ===
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=16,
            gradient_accumulation_steps=4,  # ç­‰æ•ˆbatch_size=32
            learning_rate=5e-5,
            weight_decay=0.01,
            warmup_steps=500,
            
            # === è¯„ä¼° ===
            evaluation_strategy="steps",
            eval_steps=500,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            
            # === æ€§èƒ½ä¼˜åŒ– ===
            fp16=True,  # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆA100ç”¨bf16ï¼‰
            dataloader_num_workers=4,
            
            # === å…¶ä»– ===
            seed=42,
            report_to=["tensorboard"],  # æˆ–"wandb"
        )
    
    @staticmethod
    def get_lora_args(output_dir: str = "./lora_results") -> TrainingArguments:
        """LoRAå¾®è°ƒå‚æ•°"""
        return TrainingArguments(
            output_dir=output_dir,
            
            # LoRAé€šå¸¸ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
            learning_rate=1e-4,
            
            # æ›´å°çš„batch size
            per_device_train_batch_size=4,
            gradient_accumulation_steps=8,
            
            # æ›´å¤šepochï¼ˆLoRAæ”¶æ•›å¿«ï¼‰
            num_train_epochs=5,
            
            # èŠ‚çœæ˜¾å­˜
            fp16=True,
            gradient_checkpointing=True,
            
            # å…¶ä»–
            logging_steps=50,
            save_steps=200,
            evaluation_strategy="steps",
            eval_steps=200,
        )
    
    @staticmethod
    def get_deepspeed_args(output_dir: str = "./ds_results") -> TrainingArguments:
        """DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒå‚æ•°"""
        return TrainingArguments(
            output_dir=output_dir,
            
            # DeepSpeedé…ç½®
            deepspeed="ds_config.json",  # DeepSpeedé…ç½®æ–‡ä»¶
            
            # å¤§batchè®­ç»ƒ
            per_device_train_batch_size=16,
            gradient_accumulation_steps=2,
            
            # å­¦ä¹ ç‡
            learning_rate=2e-5,
            warmup_ratio=0.1,
            
            # ä¿å­˜ç­–ç•¥
            save_strategy="epoch",
            evaluation_strategy="epoch",
            
            # æ—¥å¿—
            logging_steps=10,
            report_to=["wandb"],
        )

# ä½¿ç”¨ç¤ºä¾‹
args = TrainingConfig.get_default_args("./my_model_output")
print(f"æœ‰æ•ˆbatchå¤§å°: {args.per_device_train_batch_size * args.gradient_accumulation_steps}")
```

---

### 2. è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

```python
from transformers import Trainer, TrainerCallback
from torch.utils.data import Dataset
import torch

class CustomTrainer(Trainer):
    """è‡ªå®šä¹‰Trainer"""
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """è‡ªå®šä¹‰æŸå¤±å‡½æ•°"""
        # æ ‡å‡†å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        
        # æå–logitså’Œlabels
        logits = outputs.get("logits")
        labels = inputs.get("labels")
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss_fct = torch.nn.CrossEntropyLoss()
        loss = loss_fct(
            logits.view(-1, logits.size(-1)),
            labels.view(-1)
        )
        
        # å¯ä»¥æ·»åŠ é¢å¤–æŸå¤±é¡¹
        # ä¾‹: L2æ­£åˆ™åŒ–
        # l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
        # loss = loss + 0.01 * l2_reg
        
        return (loss, outputs) if return_outputs else loss
    
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        """è‡ªå®šä¹‰é¢„æµ‹æ­¥éª¤"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰çš„é¢„æµ‹é€»è¾‘
        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)

# è‡ªå®šä¹‰å›è°ƒ
class CustomCallback(TrainerCallback):
    """è‡ªå®šä¹‰è®­ç»ƒå›è°ƒ"""
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ªepochå¼€å§‹æ—¶è°ƒç”¨"""
        print(f"\n{'='*50}")
        print(f"å¼€å§‹ Epoch {state.epoch}")
        print(f"{'='*50}")
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•æ—¥å¿—æ—¶è°ƒç”¨"""
        if logs:
            # å¯ä»¥å‘é€åˆ°è‡ªå®šä¹‰ç›‘æ§ç³»ç»Ÿ
            if "loss" in logs:
                print(f"Step {state.global_step}, Loss: {logs['loss']:.4f}")
    
    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜checkpointæ—¶è°ƒç”¨"""
        print(f"âœ… Checkpointå·²ä¿å­˜: step {state.global_step}")

# ä½¿ç”¨ç¤ºä¾‹
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 1. å‡†å¤‡æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names
)

# 2. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 3. é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingConfig.get_default_args("./gpt2_finetuned")

# 4. åˆ›å»ºTrainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    callbacks=[CustomCallback()]
)

# 5. å¼€å§‹è®­ç»ƒ
# trainer.train()
```

---

### 3. å›è°ƒå‡½æ•°ä¸æ—¥å¿—

```python
from transformers import TrainerCallback, TrainerState, TrainerControl
import wandb

class WandbCallback(TrainerCallback):
    """Weights & Biasesé›†æˆ"""
    
    def on_init_end(self, args, state, control, **kwargs):
        """åˆå§‹åŒ–wandb"""
        wandb.init(
            project="llm-finetuning",
            name=args.run_name,
            config=args.to_dict()
        )
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•åˆ°wandb"""
        if logs:
            wandb.log(logs, step=state.global_step)
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸ"""
        wandb.finish()

class EarlyStoppingCallback(TrainerCallback):
    """æ—©åœå›è°ƒ"""
    
    def __init__(self, patience: int = 3, threshold: float = 0.001):
        self.patience = patience
        self.threshold = threshold
        self.best_metric = None
        self.wait = 0
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """è¯„ä¼°æ—¶æ£€æŸ¥æ˜¯å¦æ—©åœ"""
        if metrics is None:
            return
        
        eval_loss = metrics.get("eval_loss")
        if eval_loss is None:
            return
        
        if self.best_metric is None:
            self.best_metric = eval_loss
        elif eval_loss < self.best_metric - self.threshold:
            # æ€§èƒ½æå‡
            self.best_metric = eval_loss
            self.wait = 0
        else:
            # æ€§èƒ½æœªæå‡
            self.wait += 1
            print(f"æ—©åœè®¡æ•°: {self.wait}/{self.patience}")
            
            if self.wait >= self.patience:
                print(f"âš ï¸ è§¦å‘æ—©åœï¼ˆpatience={self.patience}ï¼‰")
                control.should_training_stop = True

class CheckpointCallback(TrainerCallback):
    """Checkpointç®¡ç†å›è°ƒ"""
    
    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜æ—¶è§¦å‘"""
        checkpoint_path = f"{args.output_dir}/checkpoint-{state.global_step}"
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰ä¿å­˜é€»è¾‘
        # ä¾‹å¦‚: ä¸Šä¼ åˆ°äº‘å­˜å‚¨
        print(f"ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path}")
        
        # ä¿å­˜é¢å¤–ä¿¡æ¯
        import json
        metadata = {
            "global_step": state.global_step,
            "epoch": state.epoch,
            "best_metric": state.best_metric
        }
        
        with open(f"{checkpoint_path}/metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

# ç»„åˆä½¿ç”¨å¤šä¸ªå›è°ƒ
callbacks = [
    EarlyStoppingCallback(patience=3),
    CheckpointCallback(),
    # WandbCallback()  # éœ€è¦å…ˆå®‰è£…wandb
]
```

---

## å››ã€æ¨¡å‹åˆ†äº«ä¸éƒ¨ç½²

### 1. Hubä¸Šä¼ ä¸ç‰ˆæœ¬ç®¡ç†

```python
from huggingface_hub import HfApi, create_repo, upload_folder
import os

class ModelUploader:
    """æ¨¡å‹ä¸Šä¼ å™¨"""
    
    def __init__(self, token: str):
        """
        Args:
            token: Hugging Face tokenï¼ˆä»huggingface.co/settings/tokensè·å–ï¼‰
        """
        self.api = HfApi(token=token)
        self.token = token
    
    def upload_model(
        self,
        model_path: str,
        repo_name: str,
        private: bool = False,
        commit_message: str = "Upload model"
    ):
        """ä¸Šä¼ æ¨¡å‹åˆ°Hub
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            repo_name: ä»“åº“åï¼ˆæ ¼å¼: username/model_nameï¼‰
            private: æ˜¯å¦ç§æœ‰
            commit_message: æäº¤ä¿¡æ¯
        """
        # 1. åˆ›å»ºä»“åº“
        try:
            create_repo(
                repo_id=repo_name,
                token=self.token,
                private=private,
                exist_ok=True
            )
            print(f"âœ… ä»“åº“åˆ›å»ºæˆåŠŸ: {repo_name}")
        except Exception as e:
            print(f"âš ï¸ ä»“åº“å·²å­˜åœ¨æˆ–åˆ›å»ºå¤±è´¥: {e}")
        
        # 2. ä¸Šä¼ æ–‡ä»¶å¤¹
        upload_folder(
            folder_path=model_path,
            repo_id=repo_name,
            token=self.token,
            commit_message=commit_message
        )
        
        print(f"âœ… æ¨¡å‹å·²ä¸Šä¼ : https://huggingface.co/{repo_name}")
    
    def upload_with_readme(
        self,
        model_path: str,
        repo_name: str,
        model_card: str
    ):
        """ä¸Šä¼ æ¨¡å‹åŠREADME"""
        # åˆ›å»ºREADME
        readme_path = os.path.join(model_path, "README.md")
        with open(readme_path, "w") as f:
            f.write(model_card)
        
        # ä¸Šä¼ 
        self.upload_model(model_path, repo_name)

# ä½¿ç”¨ç¤ºä¾‹
# uploader = ModelUploader(token="hf_xxx")

# ä¸Šä¼ æ¨¡å‹
# uploader.upload_model(
#     model_path="./my_finetuned_model",
#     repo_name="username/my-awesome-llm",
#     private=False,
#     commit_message="Initial upload"
# )
```

---

### 2. æ¨¡å‹å¡ç‰‡ç¼–å†™

```python
class ModelCardGenerator:
    """æ¨¡å‹å¡ç‰‡ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_card(
        model_name: str,
        base_model: str,
        task: str,
        dataset: str,
        metrics: Dict[str, float],
        usage_example: str
    ) -> str:
        """ç”Ÿæˆæ¨¡å‹å¡ç‰‡
        
        Args:
            model_name: æ¨¡å‹åç§°
            base_model: åŸºç¡€æ¨¡å‹
            task: ä»»åŠ¡ç±»å‹
            dataset: è®­ç»ƒæ•°æ®é›†
            metrics: è¯„ä¼°æŒ‡æ ‡
            usage_example: ä½¿ç”¨ç¤ºä¾‹ä»£ç 
        """
        card = f"""---
language:
- en
license: apache-2.0
tags:
- {task}
- transformers
datasets:
- {dataset}
metrics:
{chr(10).join(f'- {k}: {v}' for k, v in metrics.items())}
---

# {model_name}

## Model Description

This model is a fine-tuned version of [{base_model}](https://huggingface.co/{base_model}) on the {dataset} dataset.

## Intended Uses & Limitations

This model is intended for {task} tasks.

**Limitations:**
- May generate biased or incorrect outputs
- Not suitable for production use without further validation

## Training Data

The model was trained on {dataset}.

## Training Procedure

### Training Hyperparameters

- Learning rate: 5e-5
- Batch size: 32
- Epochs: 3
- Optimizer: AdamW

### Metrics

{chr(10).join(f'- **{k}**: {v}' for k, v in metrics.items())}

## Usage Example

```python
{usage_example}
```

## Citation

```bibtex
@misc{{{model_name.replace('/', '_').replace('-', '_')},
  author = {{Your Name}},
  title = {{{model_name}}},
  year = {{2026}},
  publisher = {{Hugging Face}},
  url = {{https://huggingface.co/{model_name}}}
}}
```

## Contact

For questions, contact: your.email@example.com
"""
        return card

# ç”Ÿæˆç¤ºä¾‹
card = ModelCardGenerator.generate_card(
    model_name="username/my-sentiment-classifier",
    base_model="distilbert-base-uncased",
    task="text-classification",
    dataset="imdb",
    metrics={"accuracy": 0.93, "f1": 0.92},
    usage_example="""from transformers import pipeline

classifier = pipeline("text-classification", model="username/my-sentiment-classifier")
result = classifier("I love this movie!")
print(result)"""
)

print(card[:500])
```

---

### 3. Spacesåº”ç”¨éƒ¨ç½²

```python
# Gradioåº”ç”¨ç¤ºä¾‹ï¼ˆapp.pyï¼‰
import gradio as gr
from transformers import pipeline

class GradioApp:
    """Gradioåº”ç”¨å°è£…"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.pipeline = pipeline("text-generation", model=model_name)
    
    def generate_text(
        self,
        prompt: str,
        max_length: int = 100,
        temperature: float = 0.7
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        outputs = self.pipeline(
            prompt,
            max_length=max_length,
            temperature=temperature,
            do_sample=True
        )
        return outputs[0]["generated_text"]
    
    def launch(self):
        """å¯åŠ¨Gradioç•Œé¢"""
        interface = gr.Interface(
            fn=self.generate_text,
            inputs=[
                gr.Textbox(label="è¾“å…¥æç¤ºè¯", lines=3),
                gr.Slider(10, 500, value=100, label="æœ€å¤§é•¿åº¦"),
                gr.Slider(0.1, 2.0, value=0.7, label="æ¸©åº¦")
            ],
            outputs=gr.Textbox(label="ç”Ÿæˆæ–‡æœ¬", lines=5),
            title="æ–‡æœ¬ç”ŸæˆDemo",
            description=f"ä½¿ç”¨ {self.model_name} ç”Ÿæˆæ–‡æœ¬"
        )
        
        interface.launch()

# éƒ¨ç½²åˆ°Hugging Face Spaces
# 1. åˆ›å»ºrequirements.txt
requirements_txt = """transformers>=4.35.0
torch>=2.0.0
gradio>=4.0.0
"""

# 2. åˆ›å»ºREADME.md (Spacesé…ç½®)
spaces_readme = """---
title: My Text Generator
emoji: ğŸ¤–
colorFrom: blue
colorTo: green
sdk: gradio
sdk_version: 4.0.0
app_file: app.py
pinned: false
---

# My Text Generation Space

This Space demonstrates text generation using a fine-tuned model.
"""

print("éƒ¨ç½²æ­¥éª¤:")
print("1. åˆ›å»º app.pyï¼ˆä½¿ç”¨ä¸Šé¢çš„GradioAppï¼‰")
print("2. åˆ›å»º requirements.txt")
print("3. åˆ›å»º README.mdï¼ˆåŒ…å«Spacesé…ç½®ï¼‰")
print("4. git pushåˆ°Hugging Face Spaceä»“åº“")
print("5. Spaceä¼šè‡ªåŠ¨æ„å»ºå¹¶è¿è¡Œ")
```

---

## äº”ã€PEFTä¸TRLåº“

### 1. PEFTï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒå®æˆ˜

```python
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from transformers import AutoModelForCausalLM, AutoTokenizer

class PEFTTrainer:
    """PEFTå¾®è°ƒè®­ç»ƒå™¨"""
    
    @staticmethod
    def setup_lora_model(
        base_model_name: str,
        lora_r: int = 8,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1,
        target_modules: Optional[List[str]] = None,
        use_4bit: bool = True
    ):
        """é…ç½®LoRAæ¨¡å‹
        
        Args:
            base_model_name: åŸºç¡€æ¨¡å‹
            lora_r: LoRAç§©
            lora_alpha: LoRAç¼©æ”¾å› å­
            lora_dropout: Dropoutç‡
            target_modules: åº”ç”¨LoRAçš„æ¨¡å—ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨ï¼‰
            use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–
        """
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆé‡åŒ–ï¼‰
        if use_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                quantization_config=bnb_config,
                device_map="auto"
            )
            
            # å‡†å¤‡æ¨¡å‹ç”¨äºkbitè®­ç»ƒ
            model = prepare_model_for_kbit_training(model)
        else:
            model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                device_map="auto",
                torch_dtype=torch.float16
            )
        
        # 2. é…ç½®LoRA
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,  # è‡ªåŠ¨æ£€æµ‹query/valueå±‚
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # 3. åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        
        # 4. æ‰“å°å¯è®­ç»ƒå‚æ•°
        model.print_trainable_parameters()
        
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹
lora_model, tokenizer = PEFTTrainer.setup_lora_model(
    base_model_name="meta-llama/Llama-3-8B",
    lora_r=16,
    lora_alpha=32,
    use_4bit=True
)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
trainable params: 41,943,040 || all params: 8,071,014,400 || trainable%: 0.52%
```

---

### 2. TRLï¼šå¼ºåŒ–å­¦ä¹ ä¸åå¥½ä¼˜åŒ–

```python
from trl import SFTTrainer, DPOTrainer
from datasets import load_dataset

class TRLTrainer:
    """TRLè®­ç»ƒå™¨å°è£…"""
    
    @staticmethod
    def sft_train(
        model,
        tokenizer,
        dataset_name: str = "timdettmers/openassistant-guanaco",
        max_seq_length: int = 512
    ):
        """ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰
        
        Args:
            model: æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            dataset_name: æ•°æ®é›†
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(dataset_name, split="train")
        
        # é…ç½®SFTTrainer
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            train_dataset=dataset,
            dataset_text_field="text",  # åŒ…å«æŒ‡ä»¤çš„å­—æ®µ
            max_seq_length=max_seq_length,
            args=TrainingArguments(
                output_dir="./sft_output",
                per_device_train_batch_size=4,
                gradient_accumulation_steps=4,
                learning_rate=2e-4,
                num_train_epochs=3,
                logging_steps=10,
                save_steps=100,
                fp16=True,
            )
        )
        
        return trainer
    
    @staticmethod
    def dpo_train(
        model,
        tokenizer,
        preference_dataset
    ):
        """ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰
        
       Args:
            model: åŸºç¡€æ¨¡å‹ï¼ˆSFTåï¼‰
            tokenizer: åˆ†è¯å™¨
            preference_dataset: åå¥½æ•°æ®é›†
                æ ¼å¼: {"prompt": "...", "chosen": "...", "rejected": "..."}
        """
        # é…ç½®DPO Trainer
        trainer = DPOTrainer(
            model=model,
            ref_model=None,  # è‡ªåŠ¨åˆ›å»ºå‚è€ƒæ¨¡å‹
            tokenizer=tokenizer,
            train_dataset=preference_dataset,
            beta=0.1,  # DPOæ¸©åº¦å‚æ•°
            args=TrainingArguments(
                output_dir="./dpo_output",
                per_device_train_batch_size=2,
                gradient_accumulation_steps=8,
                learning_rate=5e-5,
                num_train_epochs=3,
                logging_steps=10,
                save_steps=100,
                fp16=True,
            )
        )
        
        return trainer

# DPOæ•°æ®é›†ç¤ºä¾‹
dpo_data = {
    "prompt": [
        "Explain quantum computing",
        "Write a poem about AI"
    ],
    "chosen": [
        "Quantum computing uses quantum bits (qubits) that can exist in superposition...",
        "In circuits deep where electrons flow, Intelligence begins to grow..."
    ],
    "rejected": [
        "Quantum stuff is hard to understand.",
        "AI poem: AI is cool. Very cool. The end."
    ]
}

from datasets import Dataset
dpo_dataset = Dataset.from_dict(dpo_data)

# è®­ç»ƒ
# dpo_trainer = TRLTrainer.dpo_train(lora_model, tokenizer, dpo_dataset)
# dpo_trainer.train()
```


---

## æœ¬ç« å°ç»“

### æ ¸å¿ƒçŸ¥è¯†å›é¡¾

æœ¬ç« ç³»ç»Ÿä»‹ç»äº†Hugging Faceç”Ÿæ€çš„æ ¸å¿ƒç»„ä»¶ï¼Œä»æ¨¡å‹åŠ è½½ã€æ•°æ®å¤„ç†åˆ°è®­ç»ƒå¾®è°ƒã€æ¨¡å‹å‘å¸ƒï¼Œæ„å»ºäº†å®Œæ•´çš„LLMå¼€å‘å·¥ä½œæµã€‚

#### 1. Transformersåº“æ ¸å¿ƒç”¨æ³•

**æ¨¡å‹åŠ è½½æœ€ä½³å®è·µ**ï¼š
```python
# æ ‡å‡†åŠ è½½ï¼ˆæ¨ç†ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="flash_attention_2"
)

# 4-bité‡åŒ–åŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4"
)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config
)
# æ˜¾å­˜å ç”¨: ~0.5GB/Bå‚æ•°
```

**Pipelineå¿«é€Ÿä½¿ç”¨**ï¼š
- `text-generation`: æ–‡æœ¬ç”Ÿæˆ
- `fill-mask`: BERTå¡«ç©º
- `text-classification`: æƒ…æ„Ÿåˆ†ç±»
- `question-answering`: é—®ç­”
- è‡ªå®šä¹‰Pipelineï¼šç»§æ‰¿`Pipeline`ç±»

**æ¨¡å‹å®šåˆ¶**ï¼š
- æ‰©å±•è¯è¡¨ï¼š`tokenizer.add_special_tokens()`
- è°ƒæ•´åµŒå…¥ï¼š`model.resize_token_embeddings()`
- è‡ªå®šä¹‰æ¶æ„ï¼šç»§æ‰¿`PreTrainedModel`

#### 2. Datasetsæ•°æ®å¤„ç†

**åŠ è½½ä¸é¢„å¤„ç†**ï¼š
```python
# æ ‡å‡†åŠ è½½
dataset = load_dataset("imdb", split="train")

# æµå¼åŠ è½½ï¼ˆTBçº§æ•°æ®ï¼‰
dataset = load_dataset("c4", split="train", streaming=True)

# æ‰¹é‡é¢„å¤„ç†ï¼ˆ10-100xæé€Ÿï¼‰
dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=4
)
```

**é«˜çº§æ“ä½œ**ï¼š
- è¿‡æ»¤ï¼š`dataset.filter(lambda x: len(x['text']) > 100)`
- é‡‡æ ·ï¼š`dataset.shuffle().select(range(1000))`
- äº¤å‰æ•°æ®é›†ï¼š`interleave_datasets([ds1, ds2], probabilities=[0.7, 0.3])`

**æµå¼å¤„ç†**ï¼š
```python
# å¤„ç†å¤§æ•°æ®é›†ä¸åŠ è½½åˆ°å†…å­˜
for batch in dataset.iter(batch_size=1000):
    process(batch)
```

#### 3. Trainerè®­ç»ƒæµç¨‹

**TrainingArgumentså…³é”®å‚æ•°**ï¼š
```python
TrainingArguments(
    output_dir="./results",
    
    # è®­ç»ƒè¶…å‚æ•°
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,  # ç­‰æ•ˆbatch=32
    num_train_epochs=3,
    
    # è¯„ä¼°
    evaluation_strategy="steps",
    eval_steps=500,
    load_best_model_at_end=True,
    
    # æ€§èƒ½ä¼˜åŒ–
    fp16=True,  # æ··åˆç²¾åº¦
    gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
    
    # DeepSpeed
    deepspeed="ds_config.json"
)
```

**è‡ªå®šä¹‰è®­ç»ƒ**ï¼š
- è‡ªå®šä¹‰æŸå¤±ï¼šé‡å†™`compute_loss()`
- å›è°ƒå‡½æ•°ï¼š`TrainerCallback`
  - `on_epoch_begin/end`
  - `on_log`
  - `on_save`
- æ—©åœï¼š`EarlyStoppingCallback(patience=3)`

#### 4. æ¨¡å‹åˆ†äº«ä¸éƒ¨ç½²

**ä¸Šä¼ åˆ°Hub**ï¼š
```python
# ä¸Šä¼ æ¨¡å‹
model.push_to_hub("username/model-name")
tokenizer.push_to_hub("username/model-name")

# æˆ–ä½¿ç”¨API
upload_folder(
    folder_path="./model",
    repo_id="username/model-name"
)
```

**æ¨¡å‹å¡ç‰‡è¦ç´ **ï¼š
- æ¨¡å‹æè¿°ä¸ç”¨é€”
- è®­ç»ƒæ•°æ®ä¸è¶…å‚æ•°
- è¯„ä¼°æŒ‡æ ‡
- ä½¿ç”¨ç¤ºä¾‹ä»£ç 
- é™åˆ¶ä¸åè§è¯´æ˜
- å¼•ç”¨ä¿¡æ¯

**Spaceséƒ¨ç½²**ï¼š
```python
import gradio as gr

interface = gr.Interface(
    fn=model_fn,
    inputs=gr.Textbox(),
    outputs=gr.Textbox()
)
interface.launch()
# æ¨é€åˆ°Spaceä»“åº“å³è‡ªåŠ¨éƒ¨ç½²
```

#### 5. PEFTä¸TRLåº“

**LoRAå¾®è°ƒ**ï¼š
```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # LoRAç§©
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable%: 0.52%ï¼ˆä»…è®­ç»ƒ0.52%å‚æ•°ï¼‰
```

**TRLè®­ç»ƒ**ï¼š
- **SFTTrainer**: ç›‘ç£å¼å¾®è°ƒ
  - è‡ªåŠ¨å¤„ç†æŒ‡ä»¤æ ¼å¼
  - æ”¯æŒ`dataset_text_field`
- **DPOTrainer**: ç›´æ¥åå¥½ä¼˜åŒ–
  - éœ€è¦`(prompt, chosen, rejected)`æ ¼å¼
  - `beta`å‚æ•°æ§åˆ¶KLæƒ©ç½š

---

### å…³é”®ä»£ç æ¨¡æ¿

#### å®Œæ•´å¾®è°ƒæµç¨‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1. åŠ è½½é‡åŒ–æ¨¡å‹
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)
model = prepare_model_for_kbit_training(model)

# 2. é…ç½®LoRA
lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, bias="none")
model = get_peft_model(model, lora_config)

# 3. åŠ è½½æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B")
dataset = load_dataset("tatsu-lab/alpaca", split="train")
dataset = dataset.map(lambda x: tokenizer(x["text"], truncation=True), batched=True)

# 4. é…ç½®è®­ç»ƒ
args = TrainingArguments(
    output_dir="./lora_output",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10
)

# 5. è®­ç»ƒ
trainer = Trainer(model=model, args=args, train_dataset=dataset)
trainer.train()

# 6. ä¿å­˜
model.save_pretrained("./lora_model")
```

---

### å®æˆ˜å»ºè®®

#### æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥
```
åœºæ™¯1: 8Bæ¨¡å‹ï¼Œ24GBæ˜¾å­˜ï¼ˆRTX 4090ï¼‰
âœ“ 4-bité‡åŒ– + LoRA (r=16)
âœ“ gradient_checkpointing=True
âœ“ per_device_batch_size=2, gradient_accumulation=8
âœ“ max_seq_length=512

åœºæ™¯2: 70Bæ¨¡å‹ï¼Œå•å¼ A100 40GB
âœ“ 4-bité‡åŒ– + LoRA (r=8)
âœ“ æ˜¾å­˜å ç”¨: ~35GB
âœ“ per_device_batch_size=1, gradient_accumulation=16

åœºæ™¯3: 70Bæ¨¡å‹ï¼Œå¤šå¼ GPU
âœ“ device_map="auto"ï¼ˆè‡ªåŠ¨åˆ†é…å±‚ï¼‰
âœ“ æˆ–DeepSpeed ZeRO-3ï¼ˆå‚æ•°åˆ†ç‰‡ï¼‰
```

---

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### Q1: æ˜¾å­˜æº¢å‡ºï¼ˆCUDA Out of Memoryï¼‰
**A**:
1. é™ä½batch sizeï¼ˆ`per_device_train_batch_size=1`ï¼‰
2. å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼ˆ`gradient_accumulation_steps=16`ï¼‰
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆ`gradient_checkpointing=True`ï¼‰
4. ä½¿ç”¨4-bité‡åŒ–ï¼ˆ`load_in_4bit=True`ï¼‰
5. é™ä½åºåˆ—é•¿åº¦ï¼ˆ`max_seq_length=512`ï¼‰
6. ä½¿ç”¨DeepSpeed ZeRO-3

#### Q2: æ•°æ®é›†åŠ è½½æ…¢
**A**:
- ä½¿ç”¨`num_proc`å¹¶è¡Œå¤„ç†
- å¯ç”¨`streaming=True`ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰
- é¢„å¤„ç†åä¿å­˜ï¼š`dataset.save_to_disk("./processed")`
- ä½¿ç”¨SSDå­˜å‚¨ç¼“å­˜

#### Q3: LoRAè®­ç»ƒåå¦‚ä½•æ¨ç†ï¼Ÿ
**A**:
```python
# æ–¹æ³•1: åŠ è½½PEFTæ¨¡å‹
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("base_model")
model = PeftModel.from_pretrained(base_model, "./lora_weights")

# æ–¹æ³•2: åˆå¹¶LoRAæƒé‡
model = model.merge_and_unload()
model.save_pretrained("./merged_model")
```

---

### å®æˆ˜ç»ƒä¹ 

#### ç»ƒä¹ 1: å¾®è°ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰
**ä»»åŠ¡**ï¼š
1. ä½¿ç”¨`distilbert-base-uncased`
2. åœ¨IMDBæ•°æ®é›†ä¸Šå¾®è°ƒ
3. ä½¿ç”¨Trainer API
4. è¾¾åˆ°å‡†ç¡®ç‡ > 90%

#### ç»ƒä¹ 2: LoRAå¾®è°ƒå¯¹è¯æ¨¡å‹ï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰
**ä»»åŠ¡**ï¼š
1. ä½¿ç”¨`meta-llama/Llama-3-8B`
2. 4-bité‡åŒ– + LoRA (r=16)
3. åœ¨Alpacaæ•°æ®é›†ä¸Šå¾®è°ƒ
4. ä¸Šä¼ åˆ°Hugging Face Hub

#### ç»ƒä¹ 3: æµå¼å¤„ç†å¤§æ•°æ®é›†ï¼ˆéš¾åº¦ï¼šâ­â­â­â­ï¼‰
**ä»»åŠ¡**ï¼š
1. æµå¼åŠ è½½C4æ•°æ®é›†
2. æ‰¹é‡åˆ†è¯ï¼ˆbatch_size=1000ï¼‰
3. ä¿å­˜åˆ°ç£ç›˜ï¼ˆåˆ†å—ï¼‰
4. ç»Ÿè®¡è¯é¢‘ï¼ˆTop 10000ï¼‰

#### ç»ƒä¹ 4: Gradioåº”ç”¨éƒ¨ç½²ï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰
**ä»»åŠ¡**ï¼š
1. åˆ›å»ºæ–‡æœ¬ç”ŸæˆGradioç•Œé¢
2. æ”¯æŒæ¸©åº¦ã€æœ€å¤§é•¿åº¦è°ƒèŠ‚
3. éƒ¨ç½²åˆ°Hugging Face Spaces
4. æ·»åŠ ç¤ºä¾‹è¾“å…¥

---

### ä¸‹ä¸€ç« é¢„å‘Š

æŒæ¡äº†Hugging Faceç”Ÿæ€åï¼Œä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥**DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒ**ï¼Œå­¦ä¹ å¦‚ä½•çªç ´å•å¡é™åˆ¶ï¼Œè®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹ï¼š

- ZeROä¼˜åŒ–å™¨åŸç†ï¼ˆZeRO-1/2/3ï¼‰
- DeepSpeedé…ç½®ä¸ä½¿ç”¨
- CPU/NVMeå¸è½½ï¼ˆZeRO-Offloadï¼‰
- æ— é™è§„æ¨¡è®­ç»ƒï¼ˆZeRO-Infinityï¼‰
- å¤šæœºåˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜

ä»å•å¡åˆ°åƒå¡ï¼Œä»8Båˆ°åƒBï¼ŒDeepSpeedè®©å¤§æ¨¡å‹è®­ç»ƒè§¦æ‰‹å¯åŠï¼

---

**æœ¬ç« å®Œ**

