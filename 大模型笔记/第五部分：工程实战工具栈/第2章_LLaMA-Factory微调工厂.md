# ç¬¬4ç« ï¼šLLaMA-Factoryå¾®è°ƒå·¥å‚

> ä¸€ç«™å¼å¾®è°ƒå¹³å°ï¼Œè®©LLMå®šåˆ¶åŒ–è§¦æ‰‹å¯åŠã€‚

---

## æœ¬ç« å¯¼è¯»

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•**è®­ç»ƒ**å¤§æ¨¡å‹ï¼ˆDeepSpeedï¼‰å’Œå¦‚ä½•**æ¨ç†**å¤§æ¨¡å‹ï¼ˆvLLMï¼‰ã€‚ä½†åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€éœ€è¦åœ¨é€šç”¨æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œ**å¾®è°ƒ**ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸã€‚

ä¼ ç»Ÿå¾®è°ƒæµç¨‹å¤æ‚ç¹çï¼š
- æ•°æ®éœ€è¦æ‰‹åŠ¨è½¬æ¢ä¸ºç‰¹å®šæ ¼å¼
- é…ç½®æ–‡ä»¶éœ€è¦æ·±å…¥ç†è§£Transformersåº“
- ä¸åŒPEFTæ–¹æ³•éœ€è¦åˆ†åˆ«ç¼–å†™ä»£ç 
- è¶…å‚æ•°è°ƒä¼˜éœ€è¦åå¤è¯•éªŒ
- ç¼ºä¹å¯è§†åŒ–ç•Œé¢ï¼Œè°ƒè¯•å›°éš¾

**LLaMA-Factory**åº”è¿è€Œç”Ÿï¼Œä½œä¸ºä¸€ä¸ª**å¼€ç®±å³ç”¨çš„LLMå¾®è°ƒå·¥å…·ç®±**ï¼Œæå¤§ç®€åŒ–äº†å¾®è°ƒæµç¨‹ï¼š

| ç‰¹æ€§ | ä¼ ç»Ÿå¾®è°ƒ | LLaMA-Factory |
|------|---------|---------------|
| **ä¸Šæ‰‹éš¾åº¦** | éœ€æ·±å…¥ç†è§£ä»£ç  | Web UIé›¶ä»£ç  |
| **æ•°æ®å‡†å¤‡** | æ‰‹åŠ¨è½¬æ¢æ ¼å¼ | å†…ç½®100+æ•°æ®é›† |
| **PEFTæ–¹æ³•** | åˆ†åˆ«å®ç° | ä¸€é”®åˆ‡æ¢LoRA/QLoRAç­‰ |
| **è¶…å‚è°ƒä¼˜** | æ‰‹å†™è„šæœ¬ | å¯è§†åŒ–è°ƒæ•´ |
| **æ¨¡å‹æ”¯æŒ** | éœ€é€‚é… | æ”¯æŒ100+ä¸»æµæ¨¡å‹ |
| **éƒ¨ç½²å¯¼å‡º** | æ‰‹åŠ¨åˆå¹¶ | ä¸€é”®å¯¼å‡º |

### æœ¬ç« ä½ å°†å­¦åˆ°ï¼š

1. **LLaMA-Factoryå…¨æ™¯**
   - æ ¸å¿ƒç‰¹æ€§ä¸æ¶æ„
   - æ”¯æŒçš„æ¨¡å‹å’Œæ–¹æ³•
   - å®‰è£…ä¸å¿«é€Ÿä¸Šæ‰‹

2. **Web UIé›¶ä»£ç å¾®è°ƒ**
   - LLaMA Boardç•Œé¢æ“ä½œ
   - æ•°æ®é›†ç®¡ç†
   - è®­ç»ƒç›‘æ§ä¸å¯è§†åŒ–

3. **å‘½ä»¤è¡Œé«˜çº§å¾®è°ƒ**
   - é…ç½®æ–‡ä»¶è¯¦è§£
   - å¤šç§PEFTæ–¹æ³•ï¼ˆLoRA/QLoRA/DoRA/AdaLoRAï¼‰
   - å…¨é‡å¾®è°ƒ vs. å‚æ•°é«˜æ•ˆå¾®è°ƒ

4. **æ•°æ®å·¥ç¨‹**
   - æ•°æ®æ ¼å¼è§„èŒƒ
   - è‡ªå®šä¹‰æ•°æ®é›†
   - æ•°æ®å¢å¼ºæŠ€å·§

5. **ç”Ÿäº§å®æˆ˜**
   - æ¨¡å‹åˆå¹¶ä¸å¯¼å‡º
   - é‡åŒ–ä¸å‹ç¼©
   - vLLMéƒ¨ç½²é›†æˆ

### å‰ç½®çŸ¥è¯†

- å¾®è°ƒåŸºç¡€æ¦‚å¿µï¼ˆç¬¬å››éƒ¨åˆ†ç¬¬1ç« ï¼‰
- Hugging Face Transformersåº“ï¼ˆç¬¬å…­éƒ¨åˆ†ç¬¬1ç« ï¼‰
- åŸºç¡€çš„Pythonå’Œå‘½ä»¤è¡Œæ“ä½œ

### å­¦ä¹ è·¯å¾„

```mermaid
graph LR
    A[å®‰è£…LLaMA-Factory] --> B[Web UIå¿«é€Ÿä½“éªŒ]
    B --> C[æ•°æ®å‡†å¤‡]
    C --> D[é€‰æ‹©PEFTæ–¹æ³•]
    D --> E[å¼€å§‹å¾®è°ƒ]
    E --> F[æ¨¡å‹è¯„ä¼°]
    F --> G[å¯¼å‡ºéƒ¨ç½²]
```

è®©æˆ‘ä»¬å¼€å§‹æ¢ç´¢è¿™ä¸ªå¼ºå¤§çš„å¾®è°ƒå·¥å‚ï¼

---

## ç¬¬ä¸€èŠ‚ï¼šLLaMA-Factoryå…¨æ™¯

> äº†è§£LLaMA-Factoryçš„æ ¸å¿ƒèƒ½åŠ›ä¸ç”Ÿæ€ã€‚

### ä¸€ã€æ ¸å¿ƒç‰¹æ€§

#### 1. ç‰¹æ€§æ¦‚è§ˆ

```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class LLaMAFactoryFeatures:
    """LLaMA-Factoryæ ¸å¿ƒç‰¹æ€§"""
    
    @staticmethod
    def display_features():
        """å±•ç¤ºæ ¸å¿ƒç‰¹æ€§"""
        print("=== LLaMA-Factoryæ ¸å¿ƒç‰¹æ€§ ===\n")
        
        features = {
            "æ¨¡å‹æ”¯æŒ": {
                "æè¿°": "æ”¯æŒ100+ä¸»æµå¼€æºLLM",
                "ç¤ºä¾‹": [
                    "LLaMA/LLaMA-2/LLaMA-3 ç³»åˆ—",
                    "Qwen/Qwen2 ç³»åˆ—",
                    "Mistral/Mixtral ç³»åˆ—",
                    "Baichuan/ChatGLM ç³»åˆ—",
                    "Phi/Gemma ç³»åˆ—"
                ],
                "äº®ç‚¹": "è‡ªåŠ¨é€‚é…ï¼Œæ— éœ€ä¿®æ”¹ä»£ç "
            },
            "å¾®è°ƒæ–¹æ³•": {
                "æè¿°": "æ”¯æŒå…¨é‡ä¸å‚æ•°é«˜æ•ˆå¾®è°ƒ",
                "ç¤ºä¾‹": [
                    "Full Fine-tuningï¼ˆå…¨é‡å¾®è°ƒï¼‰",
                    "LoRAï¼ˆä½ç§©é€‚åº”ï¼‰",
                    "QLoRAï¼ˆé‡åŒ–LoRAï¼‰",
                    "DoRAï¼ˆæƒé‡åˆ†è§£LoRAï¼‰",
                    "AdaLoRAï¼ˆè‡ªé€‚åº”ç§©åˆ†é…ï¼‰",
                    "LoRA+ï¼ˆæ”¹è¿›åˆå§‹åŒ–ï¼‰"
                ],
                "äº®ç‚¹": "ä¸€é”®åˆ‡æ¢ï¼Œé…ç½®ç®€å•"
            },
            "è®­ç»ƒåœºæ™¯": {
                "æè¿°": "è¦†ç›–å¤šç§è®­ç»ƒèŒƒå¼",
                "ç¤ºä¾‹": [
                    "Supervised Fine-Tuningï¼ˆç›‘ç£å¾®è°ƒï¼‰",
                    "Reward Modelingï¼ˆå¥–åŠ±å»ºæ¨¡ï¼‰",
                    "PPO/DPO/ORPOï¼ˆåå¥½å¯¹é½ï¼‰",
                    "Pre-trainingï¼ˆé¢„è®­ç»ƒï¼‰"
                ],
                "äº®ç‚¹": "RLHFå…¨æµç¨‹æ”¯æŒ"
            },
            "æ•°æ®é›†": {
                "æè¿°": "å†…ç½®100+é«˜è´¨é‡æ•°æ®é›†",
                "ç¤ºä¾‹": [
                    "Alpaca/ShareGPTï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰",
                    "BELLE/COIGï¼ˆä¸­æ–‡æŒ‡ä»¤ï¼‰",
                    "HH-RLHFï¼ˆåå¥½æ•°æ®ï¼‰",
                    "è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆè½»æ¾é›†æˆï¼‰"
                ],
                "äº®ç‚¹": "å³å¼€å³ç”¨ï¼Œæ ¼å¼ç»Ÿä¸€"
            },
            "æ˜“ç”¨æ€§": {
                "æè¿°": "é™ä½ä½¿ç”¨é—¨æ§›",
                "ç¤ºä¾‹": [
                    "Web UIï¼ˆLLaMA Boardï¼‰é›¶ä»£ç ",
                    "å‘½ä»¤è¡Œï¼ˆä¸€è¡Œå¯åŠ¨ï¼‰",
                    "Python APIï¼ˆçµæ´»æ§åˆ¶ï¼‰",
                    "é…ç½®æ–‡ä»¶ï¼ˆYAML/JSONï¼‰"
                ],
                "äº®ç‚¹": "å¤šç§ä½¿ç”¨æ–¹å¼ï¼Œçµæ´»é€‰æ‹©"
            },
            "é«˜çº§åŠŸèƒ½": {
                "æè¿°": "ç”Ÿäº§çº§ç‰¹æ€§",
                "ç¤ºä¾‹": [
                    "FlashAttention-2åŠ é€Ÿ",
                    "Unslothä¼˜åŒ–ï¼ˆ2å€åŠ é€Ÿï¼‰",
                    "DeepSpeedé›†æˆï¼ˆåˆ†å¸ƒå¼ï¼‰",
                    "æ¨¡å‹é‡åŒ–ï¼ˆGPTQ/AWQï¼‰",
                    "æ¨¡å‹åˆå¹¶ï¼ˆMerge LoRAï¼‰"
                ],
                "äº®ç‚¹": "æ€§èƒ½ä¸æ˜“ç”¨æ€§å…¼é¡¾"
            }
        }
        
        for category, info in features.items():
            print(f"## {category}")
            print(f"æè¿°: {info['æè¿°']}")
            print(f"äº®ç‚¹: {info['äº®ç‚¹']}")
            print("\næ”¯æŒ:")
            for item in info['ç¤ºä¾‹']:
                print(f"  âœ“ {item}")
            print()

LLaMAFactoryFeatures.display_features()
```

---

#### 2. æ¶æ„è®¾è®¡

```python
from dataclasses import dataclass

@dataclass
class LLaMAFactoryArchitecture:
    """LLaMA-Factoryæ¶æ„"""
    
    @staticmethod
    def explain():
        """è§£é‡Šæ¶æ„"""
        print("=== LLaMA-Factoryæ¶æ„ ===\n")
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç”¨æˆ·ç•Œé¢å±‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Web UI   â”‚  â”‚ CLI      â”‚  â”‚ Python APIâ”‚      â”‚
â”‚  â”‚(LLaMA    â”‚  â”‚(llamafac-â”‚  â”‚(llamafac  â”‚      â”‚
â”‚  â”‚ Board)   â”‚  â”‚ tory-cli)â”‚  â”‚ .train()) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              é…ç½®ç®¡ç†å±‚                          â”‚
â”‚  - æ•°æ®é›†é…ç½® (dataset_info.json)               â”‚
â”‚  - æ¨¡å‹é…ç½® (model args)                        â”‚
â”‚  - è®­ç»ƒé…ç½® (training args)                     â”‚
â”‚  - PEFTé…ç½® (peft args)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ•°æ®å¤„ç†å±‚                          â”‚
â”‚  - æ•°æ®åŠ è½½å™¨ (DataLoader)                      â”‚
â”‚  - æ¨¡æ¿å¼•æ“ (Template)                          â”‚
â”‚  - é¢„å¤„ç†å™¨ (Preprocessor)                      â”‚
â”‚  - æ•°æ®æ•´ç†å™¨ (DataCollator)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è®­ç»ƒæ‰§è¡Œå±‚                          â”‚
â”‚  - Trainerï¼ˆåŸºäºTransformers Trainerï¼‰         â”‚
â”‚  - PEFTæ¨¡å—ï¼ˆLoRA/QLoRAç­‰ï¼‰                     â”‚
â”‚  - ä¼˜åŒ–å™¨ï¼ˆAdamW/AdaFactorï¼‰                    â”‚
â”‚  - å­¦ä¹ ç‡è°ƒåº¦å™¨                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ¨¡å‹ä¸åŠ é€Ÿå±‚                        â”‚
â”‚  - HuggingFace Transformers                     â”‚
â”‚  - PEFTåº“                                       â”‚
â”‚  - FlashAttention-2                             â”‚
â”‚  - DeepSpeed / Accelerate                       â”‚
â”‚  - Unsloth                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("æ ¸å¿ƒè®¾è®¡åŸåˆ™:")
        print("  1. æ¨¡å—åŒ–: æ¯å±‚èŒè´£æ¸…æ™°ï¼Œæ˜“äºæ‰©å±•")
        print("  2. é…ç½®é©±åŠ¨: é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶è¡Œä¸º")
        print("  3. å¼€ç®±å³ç”¨: å†…ç½®å¸¸ç”¨æ•°æ®é›†å’Œæ¨¡æ¿")
        print("  4. å…¼å®¹æ€§: åŸºäºTransformersï¼Œç”Ÿæ€å…¼å®¹")

LLaMAFactoryArchitecture.explain()
```

---

### äºŒã€å®‰è£…ä¸ç¯å¢ƒé…ç½®

#### 1. å¿«é€Ÿå®‰è£…

```bash
# æ–¹å¼1ï¼špipå®‰è£…ï¼ˆæ¨èï¼‰
pip install llamafactory

# æ–¹å¼2ï¼šä»æºç å®‰è£…ï¼ˆå¼€å‘è€…ï¼‰
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .

# å®‰è£…å¯é€‰ä¾èµ–
pip install llamafactory[torch,metrics]  # åŸºç¡€
pip install llamafactory[bitsandbytes]   # é‡åŒ–æ”¯æŒ
pip install llamafactory[vllm]           # vLLMæ¨ç†
pip install llamafactory[deepspeed]      # DeepSpeedåŠ é€Ÿ
pip install llamafactory[all]            # å…¨éƒ¨ä¾èµ–
```

---

#### 2. ç¯å¢ƒæ£€æŸ¥

```python
"""
ç¯å¢ƒæ£€æŸ¥è„šæœ¬
æ£€æŸ¥LLaMA-Factoryè¿è¡Œæ‰€éœ€çš„ä¾èµ–
"""

import subprocess
import sys
from typing import List, Tuple

def check_package(package_name: str, import_name: str = None) -> Tuple[bool, str]:
    """æ£€æŸ¥PythonåŒ…"""
    import_name = import_name or package_name
    try:
        __import__(import_name)
        version = subprocess.check_output(
            [sys.executable, "-m", "pip", "show", package_name],
            stderr=subprocess.DEVNULL
        ).decode()
        
        for line in version.split('\n'):
            if line.startswith('Version:'):
                return True, line.split(':')[1].strip()
        return True, "unknown"
    except:
        return False, None

def check_cuda() -> Tuple[bool, str]:
    """æ£€æŸ¥CUDA"""
    try:
        import torch
        if torch.cuda.is_available():
            return True, torch.version.cuda
        else:
            return False, "CUDAä¸å¯ç”¨"
    except:
        return False, "torchæœªå®‰è£…"

def check_environment():
    """å®Œæ•´ç¯å¢ƒæ£€æŸ¥"""
    print("=== LLaMA-Factoryç¯å¢ƒæ£€æŸ¥ ===\n")
    
    # æ ¸å¿ƒä¾èµ–
    print("æ ¸å¿ƒä¾èµ–:")
    core_packages = [
        ("transformers", "transformers"),
        ("datasets", "datasets"),
        ("peft", "peft"),
        ("accelerate", "accelerate"),
    ]
    
    for package, import_name in core_packages:
        installed, version = check_package(package, import_name)
        status = f"âœ“ {version}" if installed else "âœ— æœªå®‰è£…"
        print(f"  {package}: {status}")
    
    print()
    
    # å¯é€‰ä¾èµ–
    print("å¯é€‰ä¾èµ–:")
    optional_packages = [
        ("deepspeed", "deepspeed"),
        ("bitsandbytes", "bitsandbytes"),
        ("flash-attn", "flash_attn"),
        ("vllm", "vllm"),
    ]
    
    for package, import_name in optional_packages:
        installed, version = check_package(package, import_name)
        status = f"âœ“ {version}" if installed else "â—‹ æœªå®‰è£…ï¼ˆå¯é€‰ï¼‰"
        print(f"  {package}: {status}")
    
    print()
    
    # CUDAæ£€æŸ¥
    print("GPUç¯å¢ƒ:")
    cuda_available, cuda_version = check_cuda()
    if cuda_available:
        import torch
        print(f"  âœ“ CUDA: {cuda_version}")
        print(f"  âœ“ GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"    - GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print(f"  âœ— CUDA: {cuda_version}")
        print("  âš ï¸  å»ºè®®: å®‰è£…GPUç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    print()
    
    # LLaMA-Factory
    print("LLaMA-Factory:")
    installed, version = check_package("llamafactory", "llamafactory")
    if installed:
        print(f"  âœ“ ç‰ˆæœ¬: {version}")
        print(f"  âœ“ å®‰è£…æˆåŠŸ!")
    else:
        print(f"  âœ— æœªå®‰è£…")
        print(f"  æç¤º: pip install llamafactory")

# è¿è¡Œæ£€æŸ¥
check_environment()
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
=== LLaMA-Factoryç¯å¢ƒæ£€æŸ¥ ===

æ ¸å¿ƒä¾èµ–:
  transformers: âœ“ 4.36.2
  datasets: âœ“ 2.16.1
  peft: âœ“ 0.7.1
  accelerate: âœ“ 0.25.0

å¯é€‰ä¾èµ–:
  deepspeed: âœ“ 0.12.6
  bitsandbytes: âœ“ 0.41.3
  flash-attn: âœ“ 2.5.0
  vllm: â—‹ æœªå®‰è£…ï¼ˆå¯é€‰ï¼‰

GPUç¯å¢ƒ:
  âœ“ CUDA: 12.1
  âœ“ GPUæ•°é‡: 1
    - GPU 0: NVIDIA A100-SXM4-80GB

LLaMA-Factory:
  âœ“ ç‰ˆæœ¬: 0.4.0
  âœ“ å®‰è£…æˆåŠŸ!
```

---

### ä¸‰ã€å¿«é€Ÿä¸Šæ‰‹

#### 1. ä¸€è¡Œå‘½ä»¤å¾®è°ƒ

```bash
# ä½¿ç”¨å†…ç½®æ•°æ®é›†å¾®è°ƒLlama-2-7B
llamafactory-cli train \
  --model_name_or_path meta-llama/Llama-2-7b-hf \
  --dataset alpaca_en \
  --template default \
  --finetuning_type lora \
  --output_dir output/llama2-7b-alpaca-lora \
  --per_device_train_batch_size 4 \
  --gradient_accumulation_steps 4 \
  --lr_scheduler_type cosine \
  --logging_steps 10 \
  --save_steps 1000 \
  --learning_rate 5e-5 \
  --num_train_epochs 3 \
  --fp16
```

---

#### 2. Python APIä½¿ç”¨

```python
"""
LLaMA-Factory Python APIç¤ºä¾‹
"""

from llamafactory.train import train_model
from llamafactory.data import DataArguments
from llamafactory.model import ModelArguments
from llamafactory.train import TrainingArguments
from llamafactory.hparams import FinetuningArguments

def train_with_python_api():
    """ä½¿ç”¨Python APIè®­ç»ƒ"""
    
    # æ¨¡å‹å‚æ•°
    model_args = ModelArguments(
        model_name_or_path="meta-llama/Llama-2-7b-hf",
        trust_remote_code=True,
    )
    
    # æ•°æ®å‚æ•°
    data_args = DataArguments(
        dataset="alpaca_en",  # ä½¿ç”¨å†…ç½®æ•°æ®é›†
        template="default",
        cutoff_len=1024,
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="output/llama2-7b-alpaca-lora",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        num_train_epochs=3,
        lr_scheduler_type="cosine",
        logging_steps=10,
        save_steps=1000,
        fp16=True,
    )
    
    # å¾®è°ƒå‚æ•°
    finetuning_args = FinetuningArguments(
        finetuning_type="lora",  # LoRAå¾®è°ƒ
        lora_rank=8,
        lora_alpha=16,
        lora_dropout=0.05,
        lora_target="all",  # å¯¹æ‰€æœ‰linearå±‚åº”ç”¨LoRA
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_model(
        model_args=model_args,
        data_args=data_args,
        training_args=training_args,
        finetuning_args=finetuning_args
    )
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

# æ¼”ç¤ºï¼ˆå®é™…è¿è¡Œéœ€è¦GPUå’Œæ•°æ®ï¼‰
def demonstrate_api():
    """æ¼”ç¤ºAPIç»“æ„"""
    print("=== LLaMA-Factory Python API ===\n")
    
    print("1. æ¨¡å‹å‚æ•° (ModelArguments):")
    print("   - model_name_or_path: æ¨¡å‹è·¯å¾„")
    print("   - quantization_bit: é‡åŒ–ä½æ•°ï¼ˆ4/8ï¼‰")
    print("   - adapter_name_or_path: LoRAé€‚é…å™¨è·¯å¾„")
    print()
    
    print("2. æ•°æ®å‚æ•° (DataArguments):")
    print("   - dataset: æ•°æ®é›†åç§°")
    print("   - template: å¯¹è¯æ¨¡æ¿")
    print("   - cutoff_len: æœ€å¤§åºåˆ—é•¿åº¦")
    print()
    
    print("3. è®­ç»ƒå‚æ•° (TrainingArguments):")
    print("   - output_dir: è¾“å‡ºç›®å½•")
    print("   - learning_rate: å­¦ä¹ ç‡")
    print("   - num_train_epochs: è®­ç»ƒè½®æ•°")
    print()
    
    print("4. å¾®è°ƒå‚æ•° (FinetuningArguments):")
    print("   - finetuning_type: lora/freeze/full")
    print("   - lora_rank: LoRAç§©")
    print("   - lora_target: ç›®æ ‡æ¨¡å—")

demonstrate_api()
```

---

#### 3. æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class SupportedModels:
    """æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    
    @staticmethod
    def display_models():
        """å±•ç¤ºæ”¯æŒçš„æ¨¡å‹"""
        print("=== LLaMA-Factoryæ”¯æŒçš„æ¨¡å‹ ===\n")
        
        models = {
            "LLaMAç³»åˆ—": [
                "LLaMA (7B/13B/33B/65B)",
                "LLaMA-2 (7B/13B/70B)",
                "LLaMA-3 (8B/70B)",
                "Code Llama",
                "Vicuna",
                "Alpaca",
            ],
            "ä¸­æ–‡æ¨¡å‹": [
                "Qwen/Qwen2 (0.5B-72B)",
                "Baichuan/Baichuan2 (7B/13B)",
                "ChatGLM2/ChatGLM3 (6B)",
                "InternLM/InternLM2 (7B/20B)",
                "Yi (6B/34B)",
            ],
            "Mistralç³»åˆ—": [
                "Mistral (7B)",
                "Mixtral (8x7B, 8x22B)",
                "Zephyr",
            ],
            "å°å‚æ•°æ¨¡å‹": [
                "Phi-2/Phi-3 (2.7B-14B)",
                "Gemma (2B/7B)",
                "TinyLlama (1.1B)",
                "StableLM",
            ],
            "å¤šæ¨¡æ€": [
                "LLaVA (7B/13B)",
                "Qwen-VL",
            ]
        }
        
        total_count = 0
        for category, model_list in models.items():
            print(f"## {category}")
            for model in model_list:
                print(f"  âœ“ {model}")
                total_count += 1
            print()
        
        print(f"æ€»è®¡æ”¯æŒ: {total_count}+ æ¨¡å‹")

SupportedModels.display_models()
```

---

## ç¬¬äºŒèŠ‚ï¼šWeb UIé›¶ä»£ç å¾®è°ƒ

> é€šè¿‡LLaMA Boardå¯è§†åŒ–ç•Œé¢ï¼Œé›¶ä»£ç å®Œæˆå¾®è°ƒå…¨æµç¨‹ã€‚

### ä¸€ã€å¯åŠ¨LLaMA Board

#### 1. å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨Web UI
llamafactory-cli webui

# æˆ–æŒ‡å®šç«¯å£å’Œhost
llamafactory-cli webui --host 0.0.0.0 --port 7860

# Dockerå¯åŠ¨ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
docker run -it --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 7860:7860 \
  hiyouga/llama-factory:latest \
  llamafactory-cli webui
```

è®¿é—® `http://localhost:7860` å³å¯æ‰“å¼€ç•Œé¢ã€‚

---

#### 2. ç•Œé¢å¸ƒå±€

```python
from dataclasses import dataclass

@dataclass
class LLaMABoardUI:
    """LLaMA Board UIå¸ƒå±€"""
    
    @staticmethod
    def explain_layout():
        """è§£é‡Šç•Œé¢å¸ƒå±€"""
        print("=== LLaMA Boardç•Œé¢å¸ƒå±€ ===\n")
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLaMA Board - LLaMA-Factory Web UI               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Train] [Evaluate] [Chat] [Export]  â† é¡¶éƒ¨Tab    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  ã€Train Tab - è®­ç»ƒç•Œé¢ã€‘                          â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€ Model Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Model Name: [meta-llama/Llama-2-7b-hf]â”‚       â”‚
â”‚  â”‚ Finetuning Type: [LoRA â–¼]             â”‚       â”‚
â”‚  â”‚ Quantization: [4-bit â–¼]                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€ Dataset Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Dataset: [alpaca_en â–¼]                 â”‚       â”‚
â”‚  â”‚ Template: [default â–¼]                  â”‚       â”‚
â”‚  â”‚ Max Length: [1024]                     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€ Training Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Learning Rate: [5e-5]                  â”‚       â”‚
â”‚  â”‚ Epochs: [3]                            â”‚       â”‚
â”‚  â”‚ Batch Size: [4]                        â”‚       â”‚
â”‚  â”‚ LoRA Rank: [8]                         â”‚       â”‚
â”‚  â”‚ LoRA Alpha: [16]                       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                    â”‚
â”‚  [â–¶ Start Training]  [â¹ Stop]                    â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€ Training Log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Step 10: loss=2.456                    â”‚       â”‚
â”‚  â”‚ Step 20: loss=2.123                    â”‚       â”‚
â”‚  â”‚ ...                                     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€ Loss Curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    ğŸ“Š (å®æ—¶lossæ›²çº¿å›¾)                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("ä¸»è¦TabåŠŸèƒ½:")
        print("  1. Train: æ¨¡å‹è®­ç»ƒ")
        print("  2. Evaluate: æ¨¡å‹è¯„ä¼°")
        print("  3. Chat: å¯¹è¯æµ‹è¯•")
        print("  4. Export: æ¨¡å‹å¯¼å‡º")

LLaMABoardUI.explain_layout()
```

---

### äºŒã€å®Œæ•´å¾®è°ƒæµç¨‹

#### 1. æ­¥éª¤1ï¼šé€‰æ‹©æ¨¡å‹

```python
from dataclasses import dataclass
from typing import List

@dataclass
class ModelSelectionGuide:
    """æ¨¡å‹é€‰æ‹©æŒ‡å—"""
    
    @staticmethod
    def display_guide():
        """æ˜¾ç¤ºé€‰æ‹©æŒ‡å—"""
        print("=== æ¨¡å‹é€‰æ‹©æŒ‡å— ===\n")
        
        print("åœ¨LLaMA Boardä¸­é€‰æ‹©æ¨¡å‹ï¼š")
        print()
        print("1. æœ¬åœ°æ¨¡å‹:")
        print("   è·¯å¾„: /path/to/local/model")
        print("   ç¤ºä¾‹: /home/user/models/llama-2-7b-hf")
        print()
        print("2. HuggingFaceæ¨¡å‹:")
        print("   æ ¼å¼: organization/model-name")
        print("   ç¤ºä¾‹: meta-llama/Llama-2-7b-hf")
        print("   æ³¨æ„: é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½")
        print()
        print("3. é‡åŒ–é€‰é¡¹:")
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é€‰é¡¹       â”‚ å†…å­˜å ç”¨  â”‚ ç²¾åº¦    â”‚  é€‚ç”¨åœºæ™¯  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ None (FP16)  â”‚   é«˜     â”‚  æœ€é«˜   â”‚  å¤§æ˜¾å­˜GPU â”‚
â”‚ 8-bit        â”‚   ä¸­     â”‚  é«˜     â”‚  ä¸­ç­‰GPU   â”‚
â”‚ 4-bit        â”‚   ä½     â”‚  ä¸­     â”‚  å°æ˜¾å­˜GPU â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("æ¨èé…ç½®:")
        scenarios = [
            ("RTX 3090 (24GB)", "Llama-2-7B", "4-bit"),
            ("RTX 4090 (24GB)", "Llama-2-13B", "4-bit"),
            ("A100 (40GB)", "Llama-2-13B", "8-bit æˆ– FP16"),
            ("A100 (80GB)", "Llama-2-70B", "4-bit + LoRA"),
        ]
        
        for gpu, model, quant in scenarios:
            print(f"  {gpu}: {model} + {quant}")

ModelSelectionGuide.display_guide()
```

---

#### 2. æ­¥éª¤2ï¼šé…ç½®æ•°æ®é›†

```python
@dataclass
class DatasetConfiguration:
    """æ•°æ®é›†é…ç½®"""
    
    @staticmethod
    def display_builtin_datasets():
        """æ˜¾ç¤ºå†…ç½®æ•°æ®é›†"""
        print("=== å†…ç½®æ•°æ®é›† ===\n")
        
        datasets = {
            "é€šç”¨æŒ‡ä»¤": [
                ("alpaca_en", "52Kè‹±æ–‡æŒ‡ä»¤", "é€šç”¨"),
                ("alpaca_zh", "52Kä¸­æ–‡æŒ‡ä»¤", "é€šç”¨"),
                ("sharegpt", "90Kå¤šè½®å¯¹è¯", "å¯¹è¯"),
            ],
            "ä¸­æ–‡ä¼˜åŒ–": [
                ("belle_2m", "200ä¸‡ä¸­æ–‡æŒ‡ä»¤", "é€šç”¨"),
                ("belle_school_math", "æ•°å­¦é¢˜", "æ•°å­¦"),
                ("firefly", "115ä¸‡ä¸­æ–‡æŒ‡ä»¤", "é€šç”¨"),
            ],
            "ä»£ç ": [
                ("code_alpaca", "20Kä»£ç æŒ‡ä»¤", "ç¼–ç¨‹"),
                ("codeup", "ä»£ç é¢˜ç›®", "ç®—æ³•"),
            ],
            "åå¥½å¯¹é½": [
                ("hh_rlhf_en", "äººç±»åå¥½æ•°æ®", "RLHF"),
                ("ultrafeedback", "åé¦ˆæ•°æ®", "DPO"),
            ]
        }
        
        print("å¸¸ç”¨æ•°æ®é›†åˆ—è¡¨:\n")
        for category, dataset_list in datasets.items():
            print(f"## {category}")
            for name, desc, task in dataset_list:
                print(f"  - {name}: {desc} ({task})")
            print()
        
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  1. åœ¨Web UIçš„Datasetä¸‹æ‹‰èœå•ä¸­é€‰æ‹©")
        print("  2. æˆ–åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®š: --dataset alpaca_en")
    
    @staticmethod
    def explain_template():
        """è§£é‡Šæ¨¡æ¿"""
        print("\n=== å¯¹è¯æ¨¡æ¿ ===\n")
        
        print("æ¨¡æ¿ä½œç”¨: å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼")
        print()
        print("å¸¸ç”¨æ¨¡æ¿:")
        templates = [
            ("default", "é€šç”¨æ ¼å¼", "é€‚ç”¨å¤§éƒ¨åˆ†æ¨¡å‹"),
            ("alpaca", "Alpacaæ ¼å¼", "Below is an instruction..."),
            ("vicuna", "Vicunaæ ¼å¼", "USER: ... ASSISTANT:"),
            ("llama2", "Llama-2æ ¼å¼", "[INST] ... [/INST]"),
            ("chatml", "ChatMLæ ¼å¼", "<|im_start|>user\\n..."),
            ("qwen", "é€šä¹‰åƒé—®æ ¼å¼", "<|im_start|>user\\n..."),
        ]
        
        for name, desc, example in templates:
            print(f"  - {name}: {desc}")
            print(f"    ç¤ºä¾‹: {example}")
            print()
        
        print("é€‰æ‹©å»ºè®®:")
        print("  - ä½¿ç”¨å®˜æ–¹æ¨¡æ¿ï¼ˆå¦‚Llama-2ç”¨llama2ï¼‰æ•ˆæœæœ€å¥½")
        print("  - ä¸ç¡®å®šæ—¶é€‰æ‹©default")

DatasetConfiguration.display_builtin_datasets()
DatasetConfiguration.explain_template()
```

---


#### 3. æ­¥éª¤3ï¼šè°ƒæ•´è¶…å‚æ•°

```python
from dataclasses import dataclass
from typing import Dict

@dataclass
class HyperparameterTuning:
    """è¶…å‚æ•°è°ƒä¼˜"""
    
    @staticmethod
    def display_important_params():
        """å±•ç¤ºé‡è¦å‚æ•°"""
        print("=== å…³é”®è¶…å‚æ•° ===\n")
        
        params = {
            "å­¦ä¹ ç‡ (Learning Rate)": {
                "èŒƒå›´": "1e-5 åˆ° 5e-5",
                "é»˜è®¤": "5e-5",
                "è¯´æ˜": "LoRAé€šå¸¸ç”¨è¾ƒå¤§å­¦ä¹ ç‡",
                "è°ƒä¼˜å»ºè®®": [
                    "å…¨é‡å¾®è°ƒ: 1e-5 ~ 2e-5",
                    "LoRAå¾®è°ƒ: 1e-4 ~ 5e-4",
                    "QLoRAå¾®è°ƒ: 2e-4 ~ 1e-3",
                    "æ¨¡å‹è¶Šå¤§ï¼Œå­¦ä¹ ç‡è¶Šå°"
                ]
            },
            "LoRAç§© (LoRA Rank)": {
                "èŒƒå›´": "4 åˆ° 64",
                "é»˜è®¤": "8",
                "è¯´æ˜": "ç§©è¶Šå¤§ï¼Œè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†å†…å­˜å ç”¨å¢åŠ ",
                "è°ƒä¼˜å»ºè®®": [
                    "ç®€å•ä»»åŠ¡: r=4 æˆ– r=8",
                    "å¤æ‚ä»»åŠ¡: r=16 æˆ– r=32",
                    "é¢†åŸŸé€‚é…: r=64",
                    "é€šå¸¸ r=8 å·²è¶³å¤Ÿ"
                ]
            },
            "LoRA Alpha": {
                "èŒƒå›´": "8 åˆ° 32",
                "é»˜è®¤": "16",
                "è¯´æ˜": "ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸ºrankçš„2å€",
                "è°ƒä¼˜å»ºè®®": [
                    "alpha = 2 Ã— rankï¼ˆå¸¸ç”¨ï¼‰",
                    "æ›´æ¿€è¿›: alpha = 4 Ã— rank"
                ]
            },
            "æ‰¹å¤§å° (Batch Size)": {
                "èŒƒå›´": "1 åˆ° 128",
                "é»˜è®¤": "4",
                "è¯´æ˜": "å—GPUæ˜¾å­˜é™åˆ¶",
                "è°ƒä¼˜å»ºè®®": [
                    "æ˜¾å­˜å……è¶³: å°½é‡å¢å¤§",
                    "æ˜¾å­˜ä¸è¶³: å‡å°batch_sizeï¼Œå¢åŠ gradient_accumulation_steps",
                    "æœ‰æ•ˆbatch = batch_size Ã— gradient_accumulation_steps Ã— num_gpus"
                ]
            },
            "è®­ç»ƒè½®æ•° (Epochs)": {
                "èŒƒå›´": "1 åˆ° 10",
                "é»˜è®¤": "3",
                "è¯´æ˜": "æ•°æ®é›†è§„æ¨¡å†³å®š",
                "è°ƒä¼˜å»ºè®®": [
                    "å¤§æ•°æ®é›†(>10K): 1-3 epochs",
                    "ä¸­æ•°æ®é›†(1K-10K): 3-5 epochs",
                    "å°æ•°æ®é›†(<1K): 5-10 epochs",
                    "é¿å…è¿‡æ‹Ÿåˆ: ç›‘æ§éªŒè¯loss"
                ]
            }
        }
        
        for param_name, info in params.items():
            print(f"## {param_name}")
            print(f"èŒƒå›´: {info['èŒƒå›´']}")
            print(f"é»˜è®¤: {info['é»˜è®¤']}")
            print(f"è¯´æ˜: {info['è¯´æ˜']}")
            print("\nè°ƒä¼˜å»ºè®®:")
            for tip in info['è°ƒä¼˜å»ºè®®']:
                print(f"  â€¢ {tip}")
            print()
    
    @staticmethod
    def recommend_config(task_type: str, data_size: int, gpu_memory: int):
        """æ¨èé…ç½®"""
        print(f"\n=== é…ç½®æ¨è ===\n")
        print(f"ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"æ•°æ®è§„æ¨¡: {data_size:,}æ¡")
        print(f"GPUæ˜¾å­˜: {gpu_memory}GB")
        print()
        
        # åŸºç¡€é…ç½®
        if gpu_memory >= 40:
            batch_size = 8
            quant = "None (FP16)"
        elif gpu_memory >= 24:
            batch_size = 4
            quant = "4-bit"
        else:
            batch_size = 1
            quant = "4-bit"
        
        # å­¦ä¹ ç‡
        if task_type == "æŒ‡ä»¤å¾®è°ƒ":
            lr = "5e-5"
            epochs = 3 if data_size > 10000 else 5
        elif task_type == "é¢†åŸŸé€‚é…":
            lr = "1e-4"
            epochs = 5
        else:
            lr = "5e-5"
            epochs = 3
        
        # LoRAé…ç½®
        if data_size < 1000:
            lora_rank = 4
        elif data_size < 10000:
            lora_rank = 8
        else:
            lora_rank = 16
        
        print("æ¨èé…ç½®:")
        print(f"  é‡åŒ–: {quant}")
        print(f"  Batch Size: {batch_size}")
        print(f"  Gradient Accumulation: {16 // batch_size}")
        print(f"  Learning Rate: {lr}")
        print(f"  Epochs: {epochs}")
        print(f"  LoRA Rank: {lora_rank}")
        print(f"  LoRA Alpha: {lora_rank * 2}")

tuner = HyperparameterTuning()
tuner.display_important_params()
tuner.recommend_config(task_type="æŒ‡ä»¤å¾®è°ƒ", data_size=50000, gpu_memory=24)
```

---

#### 4. æ­¥éª¤4ï¼šå¼€å§‹è®­ç»ƒ

```python
from dataclasses import dataclass
from typing import List

@dataclass
class TrainingProcess:
    """è®­ç»ƒè¿‡ç¨‹"""
    
    @staticmethod
    def explain_training_flow():
        """è§£é‡Šè®­ç»ƒæµç¨‹"""
        print("=== è®­ç»ƒæµç¨‹ ===\n")
        
        print("ç‚¹å‡»ã€ŒStart Trainingã€åå‘ç”Ÿä»€ä¹ˆï¼š\n")
        
        steps = [
            ("1. ç¯å¢ƒæ£€æŸ¥", [
                "æ£€æŸ¥GPUå¯ç”¨æ€§",
                "æ£€æŸ¥ä¾èµ–åŒ…ç‰ˆæœ¬",
                "æ£€æŸ¥ç£ç›˜ç©ºé—´"
            ]),
            ("2. æ¨¡å‹åŠ è½½", [
                "ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰",
                "åº”ç”¨é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰",
                "åŠ è½½LoRAé€‚é…å™¨ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒï¼‰",
                "å†»ç»“åŸºåº§å‚æ•°ï¼ˆLoRAæ¨¡å¼ï¼‰"
            ]),
            ("3. æ•°æ®å‡†å¤‡", [
                "åŠ è½½æ•°æ®é›†",
                "åº”ç”¨å¯¹è¯æ¨¡æ¿",
                "Tokenization",
                "æ„å»ºDataLoader"
            ]),
            ("4. è®­ç»ƒå¾ªç¯", [
                "å‰å‘ä¼ æ’­",
                "è®¡ç®—loss",
                "åå‘ä¼ æ’­",
                "æ›´æ–°å‚æ•°ï¼ˆä»…LoRAå‚æ•°ï¼‰",
                "è®°å½•metrics"
            ]),
            ("5. ä¿å­˜æ£€æŸ¥ç‚¹", [
                "æ¯save_stepsä¿å­˜ä¸€æ¬¡",
                "ä¿å­˜LoRAæƒé‡",
                "ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼ˆoptimizerã€lr_schedulerï¼‰",
                "ç”Ÿæˆadapter_config.json"
            ])
        ]
        
        for step, substeps in steps:
            print(f"{step}")
            for substep in substeps:
                print(f"  â†’ {substep}")
            print()
    
    @staticmethod
    def monitor_training():
        """ç›‘æ§è®­ç»ƒ"""
        print("=== è®­ç»ƒç›‘æ§ ===\n")
        
        print("å®æ—¶ç›‘æ§æŒ‡æ ‡:\n")
        
        metrics = [
            ("Loss", "è®­ç»ƒæŸå¤±", "åº”æŒç»­ä¸‹é™", "å¦‚æœä¸é™ï¼Œæ£€æŸ¥å­¦ä¹ ç‡"),
            ("Learning Rate", "å­¦ä¹ ç‡", "æ ¹æ®schedulerå˜åŒ–", "warmupåé€æ¸è¡°å‡"),
            ("GPU Memory", "æ˜¾å­˜å ç”¨", "åº”ç¨³å®šåœ¨é˜ˆå€¼å†…", "è¶…å‡ºä¼šOOM"),
            ("Tokens/s", "è®­ç»ƒé€Ÿåº¦", "è¶Šé«˜è¶Šå¥½", "FlashAttentionå¯æå‡2-4å€"),
            ("ETA", "é¢„è®¡å‰©ä½™æ—¶é—´", "å‚è€ƒå€¼", "æ ¹æ®å½“å‰é€Ÿåº¦ä¼°ç®—")
        ]
        
        for metric, desc, expected, note in metrics:
            print(f"  â€¢ {metric}: {desc}")
            print(f"    æœŸæœ›: {expected}")
            print(f"    å¤‡æ³¨: {note}")
            print()
        
        print("Lossæ›²çº¿åˆ†æ:")
        print("""
æ­£å¸¸æ›²çº¿:
  Loss
   â”‚ â•²
   â”‚  â•²___
   â”‚      â€¾â€¾â€¾___
   â”‚           â€¾â€¾â€¾___
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Steps

è¿‡æ‹Ÿåˆ:
  Loss
   â”‚ â•²     â•± â† éªŒè¯lossä¸Šå‡
   â”‚  â•²___â•±
   â”‚  â•±
   â”‚ â•± â† è®­ç»ƒlossç»§ç»­ä¸‹é™
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Steps
   
æ¬ æ‹Ÿåˆ:
  Loss
   â”‚ â•²
   â”‚  â•²___  â† lossä¸‹é™ç¼“æ…¢
   â”‚      â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Steps
        """)

process = TrainingProcess()
process.explain_training_flow()
process.monitor_training()
```

---

### ä¸‰ã€Chatæµ‹è¯•ä¸Exportå¯¼å‡º

#### 1. Chatæµ‹è¯•

```python
@dataclass
class ChatTesting:
    """å¯¹è¯æµ‹è¯•"""
    
    @staticmethod
    def explain_chat_tab():
        """è§£é‡ŠChat Tab"""
        print("=== Chat Tabä½¿ç”¨ ===\n")
        
        print("åŠŸèƒ½: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ–è®­ç»ƒåæµ‹è¯•æ¨¡å‹æ•ˆæœ\n")
        
        print("æ­¥éª¤:")
        print("  1. åŠ è½½æ¨¡å‹:")
        print("     - Base Model: åŸºåº§æ¨¡å‹è·¯å¾„")
        print("     - Adapter Path: LoRAé€‚é…å™¨è·¯å¾„")
        print("     - è‡ªåŠ¨åˆå¹¶adapteråˆ°base model")
        print()
        print("  2. é…ç½®ç”Ÿæˆå‚æ•°:")
        print("     - Temperature: æ¸©åº¦ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰")
        print("     - Top-p: æ ¸é‡‡æ ·é˜ˆå€¼")
        print("     - Max Length: æœ€å¤§ç”Ÿæˆé•¿åº¦")
        print()
        print("  3. å¯¹è¯æµ‹è¯•:")
        print("     - è¾“å…¥é—®é¢˜")
        print("     - æŸ¥çœ‹æ¨¡å‹å›å¤")
        print("     - æ”¯æŒå¤šè½®å¯¹è¯")
        print()
        
        print("ç¤ºä¾‹å¯¹è¯:")
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Assistant: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ª  â”‚
â”‚ åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹  â”‚
â”‚ çš„æƒ…å†µä¸‹ä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡ç®—  â”‚
â”‚ æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥è¯†åˆ«  â”‚
â”‚ æ¨¡å¼ã€åšå‡ºé¢„æµ‹å’Œå†³ç­–ã€‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: ç»™æˆ‘ä¸¾ä¸ªä¾‹å­                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Assistant: ä¸€ä¸ªå¸¸è§çš„ä¾‹å­æ˜¯åƒåœ¾é‚®ä»¶ â”‚
â”‚ è¿‡æ»¤å™¨ã€‚é€šè¿‡å­¦ä¹ å¤§é‡æ ‡è®°ä¸º"åƒåœ¾"å’Œâ”‚
â”‚ "æ­£å¸¸"çš„é‚®ä»¶æ ·æœ¬ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿâ”‚
â”‚ è‡ªåŠ¨è¯†åˆ«æ–°é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶...    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("è¯„ä¼°è¦ç‚¹:")
        print("  âœ“ å›ç­”æ˜¯å¦å‡†ç¡®")
        print("  âœ“ è¯­è¨€æ˜¯å¦æµç•…")
        print("  âœ“ æ˜¯å¦éµå¾ªæŒ‡ä»¤")
        print("  âœ“ æ˜¯å¦åŒ…å«å¹»è§‰")
        print("  âœ“ å¤šè½®å¯¹è¯çš„è¿è´¯æ€§")

ChatTesting.explain_chat_tab()
```

---

#### 2. æ¨¡å‹å¯¼å‡º

```python
@dataclass
class ModelExport:
    """æ¨¡å‹å¯¼å‡º"""
    
    @staticmethod
    def explain_export_options():
        """è§£é‡Šå¯¼å‡ºé€‰é¡¹"""
        print("=== æ¨¡å‹å¯¼å‡º ===\n")
        
        print("Export Tabæä¾›3ç§å¯¼å‡ºæ–¹å¼:\n")
        
        options = [
            {
                "åç§°": "1. ä»…å¯¼å‡ºLoRAé€‚é…å™¨",
                "æè¿°": "åªä¿å­˜LoRAæƒé‡ï¼ˆadapter_model.binï¼‰",
                "å¤§å°": "~10-100MB",
                "ç”¨é€”": "ç»§ç»­è®­ç»ƒã€ç‰ˆæœ¬ç®¡ç†",
                "ä¼˜ç‚¹": "ä½“ç§¯å°ï¼Œçµæ´»",
                "ç¼ºç‚¹": "æ¨ç†æ—¶éœ€è¦base model + adapter"
            },
            {
                "åç§°": "2. åˆå¹¶LoRAåˆ°base model",
                "æè¿°": "å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºåº§æ¨¡å‹",
                "å¤§å°": "ä¸base modelç›¸åŒï¼ˆå¦‚7Bæ¨¡å‹ ~14GBï¼‰",
                "ç”¨é€”": "ç‹¬ç«‹éƒ¨ç½²",
                "ä¼˜ç‚¹": "æ— éœ€adapterï¼Œæ¨ç†æ›´å¿«",
                "ç¼ºç‚¹": "ä½“ç§¯å¤§"
            },
            {
                "åç§°": "3. å¯¼å‡ºé‡åŒ–æ¨¡å‹",
                "æè¿°": "åˆå¹¶åé‡åŒ–ä¸ºGPTQ/AWQ",
                "å¤§å°": "å‹ç¼©75%ï¼ˆå¦‚7Bæ¨¡å‹ ~3.5GBï¼‰",
                "ç”¨é€”": "ç”Ÿäº§éƒ¨ç½²",
                "ä¼˜ç‚¹": "ä½“ç§¯å°ï¼Œæ¨ç†å¿«",
                "ç¼ºç‚¹": "éœ€è¦é¢å¤–é‡åŒ–æ—¶é—´"
            }
        ]
        
        for option in options:
            print(f"{option['åç§°']}")
            print(f"  æè¿°: {option['æè¿°']}")
            print(f"  å¤§å°: {option['å¤§å°']}")
            print(f"  ç”¨é€”: {option['ç”¨é€”']}")
            print(f"  ä¼˜ç‚¹: {option['ä¼˜ç‚¹']}")
            print(f"  ç¼ºç‚¹: {option['ç¼ºç‚¹']}")
            print()
    
    @staticmethod
    def demonstrate_export_commands():
        """æ¼”ç¤ºå¯¼å‡ºå‘½ä»¤"""
        print("=== å¯¼å‡ºå‘½ä»¤ç¤ºä¾‹ ===\n")
        
        print("1. åˆå¹¶LoRAï¼ˆå‘½ä»¤è¡Œï¼‰:")
        print("""
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-merged \\
  --export_size 2
        """)
        
        print("\n2. é‡åŒ–å¯¼å‡ºï¼ˆGPTQï¼‰:")
        print("""
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-gptq \\
  --export_quantization_bit 4 \\
  --export_quantization_dataset alpaca
        """)
        
        print("\n3. å¯¼å‡ºåçš„ç›®å½•ç»“æ„:")
        print("""
output/llama2-7b-alpaca-merged/
â”œâ”€â”€ config.json                 # æ¨¡å‹é…ç½®
â”œâ”€â”€ generation_config.json      # ç”Ÿæˆé…ç½®
â”œâ”€â”€ tokenizer.json              # åˆ†è¯å™¨
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ pytorch_model.bin           # æ¨¡å‹æƒé‡ï¼ˆæˆ–.safetensorsï¼‰
â””â”€â”€ adapter_config.json         # ï¼ˆå¦‚æœæœªå®Œå…¨åˆå¹¶ï¼‰
        """)

exporter = ModelExport()
exporter.explain_export_options()
exporter.demonstrate_export_commands()
```

---

## ç¬¬ä¸‰èŠ‚ï¼šå‘½ä»¤è¡Œé«˜çº§å¾®è°ƒ

> æŒæ¡é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼Œå®ç°æ›´çµæ´»çš„å¾®è°ƒã€‚

### ä¸€ã€é…ç½®æ–‡ä»¶è¯¦è§£

#### 1. YAMLé…ç½®æ–‡ä»¶

```yaml
# config/llama2_lora_sft.yaml
# Llama-2 LoRAå¾®è°ƒå®Œæ•´é…ç½®

### Model arguments
model_name_or_path: meta-llama/Llama-2-7b-hf
quantization_bit: 4                    # 4-bité‡åŒ–
use_unsloth: true                      # å¯ç”¨UnslothåŠ é€Ÿ

### Data arguments
dataset: alpaca_en,sharegpt            # å¤šæ•°æ®é›†
template: llama2                       # Llama-2æ¨¡æ¿
cutoff_len: 2048                       # æœ€å¤§åºåˆ—é•¿åº¦
preprocessing_num_workers: 8           # é¢„å¤„ç†å¹¶è¡Œæ•°

### Training arguments
output_dir: output/llama2-7b-lora
overwrite_output_dir: true

do_train: true
per_device_train_batch_size: 2
gradient_accumulation_steps: 8         # æœ‰æ•ˆbatch=2Ã—8=16
learning_rate: 5.0e-5
num_train_epochs: 3.0

lr_scheduler_type: cosine
warmup_ratio: 0.1                      # 10% warmup

fp16: true                             # æ··åˆç²¾åº¦
ddp_timeout: 180000000                 # DDPè¶…æ—¶

logging_steps: 5
save_steps: 500
save_total_limit: 3                    # æœ€å¤šä¿ç•™3ä¸ªcheckpoint

### LoRA arguments
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: all                       # å¯¹æ‰€æœ‰linearå±‚åº”ç”¨LoRA

### Generation arguments (for evaluation)
do_predict: true
predict_with_generate: true
max_new_tokens: 512
temperature: 0.7
top_p: 0.9
```

ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒï¼š

```bash
llamafactory-cli train config/llama2_lora_sft.yaml
```

---

#### 2. å¤šç§PEFTæ–¹æ³•é…ç½®

```python
from dataclasses import dataclass
from typing import Dict

@dataclass
class PEFTMethodsConfig:
    """PEFTæ–¹æ³•é…ç½®"""
    
    @staticmethod
    def display_methods():
        """å±•ç¤ºå„ç§PEFTæ–¹æ³•é…ç½®"""
        print("=== PEFTæ–¹æ³•é…ç½® ===\n")
        
        methods = {
            "LoRA (æ ‡å‡†)": {
                "config": """
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: all
                """,
                "ç‰¹ç‚¹": "æœ€å¸¸ç”¨ï¼Œæ•ˆæœç¨³å®š",
                "å†…å­˜": "~25% base model",
                "é€Ÿåº¦": "1x"
            },
            "QLoRA (é‡åŒ–LoRA)": {
                "config": """
finetuning_type: lora
quantization_bit: 4        # 4-bité‡åŒ–
lora_rank: 64              # å¯ç”¨æ›´å¤§çš„rank
lora_alpha: 128
lora_dropout: 0.05
lora_target: all
                """,
                "ç‰¹ç‚¹": "æ˜¾å­˜å ç”¨æä½",
                "å†…å­˜": "~10% base model",
                "é€Ÿåº¦": "0.8xï¼ˆé‡åŒ–å¼€é”€ï¼‰"
            },
            "DoRA (æƒé‡åˆ†è§£LoRA)": {
                "config": """
finetuning_type: lora
use_dora: true             # å¯ç”¨DoRA
lora_rank: 8
lora_alpha: 16
lora_target: all
                """,
                "ç‰¹ç‚¹": "åˆ†è§£magnitudeå’Œdirection",
                "å†…å­˜": "~30% base model",
                "é€Ÿåº¦": "0.9x"
            },
            "AdaLoRA (è‡ªé€‚åº”ç§©)": {
                "config": """
finetuning_type: adalora
adalora_target_r: 8        # ç›®æ ‡å¹³å‡ç§©
adalora_init_r: 12         # åˆå§‹ç§©
adalora_tinit: 200         # warmup steps
adalora_tfinal: 1000       # æœ€ç»ˆæ”¶æ•›æ­¥æ•°
adalora_delta_t: 10        # æ›´æ–°é—´éš”
lora_alpha: 32
                """,
                "ç‰¹ç‚¹": "è‡ªåŠ¨è°ƒæ•´æ¯å±‚çš„ç§©",
                "å†…å­˜": "~28% base model",
                "é€Ÿåº¦": "0.85xï¼ˆé¢å¤–è®¡ç®—ï¼‰"
            },
            "LoRA+ (æ”¹è¿›åˆå§‹åŒ–)": {
                "config": """
finetuning_type: lora
use_rslora: true           # å¯ç”¨RSLoRAåˆå§‹åŒ–
loraplus_lr_ratio: 16      # BçŸ©é˜µå­¦ä¹ ç‡å€æ•°
lora_rank: 8
lora_alpha: 16
                """,
                "ç‰¹ç‚¹": "æ”¶æ•›æ›´å¿«ï¼Œæ•ˆæœæ›´å¥½",
                "å†…å­˜": "~25% base model",
                "é€Ÿåº¦": "1x"
            },
            "å…¨é‡å¾®è°ƒ": {
                "config": """
finetuning_type: full
pure_bf16: true            # ä½¿ç”¨BF16
deepspeed: config/ds_z3_config.json  # DeepSpeed ZeRO-3
                """,
                "ç‰¹ç‚¹": "æœ€é«˜ç²¾åº¦ï¼Œéœ€å¤§æ˜¾å­˜",
                "å†…å­˜": "100% base model + optimizer",
                "é€Ÿåº¦": "0.5xï¼ˆæ›´æ–°æ‰€æœ‰å‚æ•°ï¼‰"
            }
        }
        
        for method, info in methods.items():
            print(f"## {method}")
            print(f"ç‰¹ç‚¹: {info['ç‰¹ç‚¹']}")
            print(f"å†…å­˜å ç”¨: {info['å†…å­˜']}")
            print(f"è®­ç»ƒé€Ÿåº¦: {info['é€Ÿåº¦']}")
            print("\né…ç½®:")
            print(info['config'])
            print()
    
    @staticmethod
    def compare_methods():
        """å¯¹æ¯”ä¸åŒæ–¹æ³•"""
        print("=== PEFTæ–¹æ³•å¯¹æ¯” ===\n")
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ–¹æ³•     â”‚ å†…å­˜å ç”¨  â”‚ è®­ç»ƒé€Ÿåº¦  â”‚ æ•ˆæœ     â”‚  æ¨èåœºæ™¯â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Full FT    â”‚  æœ€é«˜    â”‚   æ…¢     â”‚  æœ€å¥½    â”‚ å¤§è§„æ¨¡æ•°æ®â”‚
â”‚ LoRA       â”‚  ä½      â”‚   å¿«     â”‚  å¥½      â”‚ é€šç”¨åœºæ™¯ â”‚
â”‚ QLoRA      â”‚  æœ€ä½    â”‚   ä¸­     â”‚  å¥½      â”‚ å°æ˜¾å­˜GPUâ”‚
â”‚ DoRA       â”‚  ä¸­      â”‚   ä¸­     â”‚  æ›´å¥½    â”‚ å¤æ‚ä»»åŠ¡ â”‚
â”‚ AdaLoRA    â”‚  ä¸­      â”‚   è¾ƒæ…¢   â”‚  å¥½      â”‚ å‚æ•°æ•ˆç‡ â”‚
â”‚ LoRA+      â”‚  ä½      â”‚   å¿«     â”‚  æ›´å¥½    â”‚ å¿«é€Ÿæ”¶æ•› â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)

PEFTMethodsConfig.display_methods()
PEFTMethodsConfig.compare_methods()
```

---

### äºŒã€æ•°æ®å·¥ç¨‹

#### 1. æ•°æ®æ ¼å¼è§„èŒƒ

```python
from dataclasses import dataclass
from typing import List, Dict
import json

@dataclass
class DataFormat:
    """æ•°æ®æ ¼å¼è§„èŒƒ"""
    
    @staticmethod
    def explain_formats():
        """è§£é‡Šå„ç§æ•°æ®æ ¼å¼"""
        print("=== LLaMA-Factoryæ”¯æŒçš„æ•°æ®æ ¼å¼ ===\n")
        
        print("1. Alpacaæ ¼å¼ï¼ˆå•è½®æŒ‡ä»¤ï¼‰:")
        print("""
{
  "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
  "input": "",                           # å¯é€‰çš„é¢å¤–è¾“å…¥
  "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
}
        """)
        
        print("\n2. ShareGPTæ ¼å¼ï¼ˆå¤šè½®å¯¹è¯ï¼‰:")
        print("""
{
  "conversations": [
    {
      "from": "human",
      "value": "ä½ å¥½"
    },
    {
      "from": "gpt",
      "value": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    },
    {
      "from": "human",
      "value": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"
    },
    {
      "from": "gpt",
      "value": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€..."
    }
  ]
}
        """)
        
        print("\n3. OpenAIæ ¼å¼:")
        print("""
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI stands for..."}
  ]
}
        """)
        
        print("\n4. åå¥½å¯¹é½æ ¼å¼ï¼ˆDPO/ORPOï¼‰:")
        print("""
{
  "instruction": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
  "input": "",
  "output": [
    "æ˜¥é£æ‹‚é¢æš–...",                    # chosen (å¥½çš„å›ç­”)
    "æ˜¥å¤©åˆ°äº†ã€‚"                        # rejected (å·®çš„å›ç­”)
  ]
}
        """)
    
    @staticmethod
    def create_custom_dataset():
        """åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ç¤ºä¾‹"""
        print("\n=== åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›† ===\n")
        
        print("æ­¥éª¤1: å‡†å¤‡æ•°æ®æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰")
        
        # ç¤ºä¾‹æ•°æ®
        dataset = [
            {
                "instruction": "å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±æ–‡",
                "input": "æˆ‘å–œæ¬¢ç¼–ç¨‹",
                "output": "I like programming."
            },
            {
                "instruction": "è§£é‡Šä»¥ä¸‹æ¦‚å¿µ",
                "input": "æ·±åº¦å­¦ä¹ ",
                "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ..."
            },
            {
                "instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°",
                "input": "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
                "output": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
            }
        ]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "data/my_dataset.json"
        print(f"\nä¿å­˜åˆ°: {output_file}")
        print(f"ç¤ºä¾‹æ•°æ®:\n{json.dumps(dataset[0], ensure_ascii=False, indent=2)}")
        
        print("\næ­¥éª¤2: æ³¨å†Œæ•°æ®é›†åˆ°dataset_info.json")
        dataset_info = {
            "my_dataset": {
                "file_name": "my_dataset.json",
                "formatting": "alpaca",
                "columns": {
                    "prompt": "instruction",
                    "query": "input",
                    "response": "output"
                }
            }
        }
        
        print(f"\nåœ¨dataset_info.jsonä¸­æ·»åŠ :\n{json.dumps(dataset_info, ensure_ascii=False, indent=2)}")
        
        print("\næ­¥éª¤3: ä½¿ç”¨æ•°æ®é›†")
        print("""
llamafactory-cli train \\
  --dataset my_dataset \\
  --template default \\
  ...
        """)

formatter = DataFormat()
formatter.explain_formats()
formatter.create_custom_dataset()
```

---

#### 2. æ•°æ®è´¨é‡ä¼˜åŒ–

```python
@dataclass
class DataQuality:
    """æ•°æ®è´¨é‡ä¼˜åŒ–"""
    
    @staticmethod
    def display_best_practices():
        """å±•ç¤ºæœ€ä½³å®è·µ"""
        print("=== æ•°æ®è´¨é‡æœ€ä½³å®è·µ ===\n")
        
        practices = [
            {
                "åŸåˆ™": "1. æ•°æ®æ¸…æ´—",
                "è¦ç‚¹": [
                    "ç§»é™¤é‡å¤æ ·æœ¬",
                    "è¿‡æ»¤ä½è´¨é‡å›ç­”ï¼ˆå¤ªçŸ­ã€æ— æ„ä¹‰ï¼‰",
                    "ç»Ÿä¸€æ ¼å¼ï¼ˆæ ‡ç‚¹ã€ç©ºæ ¼ï¼‰",
                    "ç§»é™¤ä¸ªäººéšç§ä¿¡æ¯"
                ],
                "ä»£ç ç¤ºä¾‹": """
# è¿‡æ»¤çŸ­å›ç­”
filtered_data = [
    item for item in data 
    if len(item['output']) > 20
]

# ç§»é™¤é‡å¤
seen = set()
unique_data = []
for item in data:
    key = item['instruction'] + item['output']
    if key not in seen:
        seen.add(key)
        unique_data.append(item)
                """
            },
            {
                "åŸåˆ™": "2. æ•°æ®å¹³è¡¡",
                "è¦ç‚¹": [
                    "ä¸åŒç±»å‹ä»»åŠ¡æ¯”ä¾‹å‡è¡¡",
                    "éš¾åº¦åˆ†å¸ƒåˆç†",
                    "é•¿åº¦åˆ†å¸ƒåˆç†ï¼ˆé¿å…å…¨æ˜¯çŸ­/é•¿æ–‡æœ¬ï¼‰"
                ],
                "ä»£ç ç¤ºä¾‹": """
from collections import Counter

# ç»Ÿè®¡ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
task_types = [classify_task(item) for item in data]
type_counts = Counter(task_types)

# å¹³è¡¡é‡‡æ ·
balanced_data = []
target_per_type = 1000
for task_type in type_counts:
    samples = [item for item in data if classify_task(item) == task_type]
    balanced_data.extend(samples[:target_per_type])
                """
            },
            {
                "åŸåˆ™": "3. æ•°æ®å¢å¼º",
                "è¦ç‚¹": [
                    "åŒä¹‰æ”¹å†™ï¼ˆä¿æŒè¯­ä¹‰ï¼‰",
                    "Back Translation",
                    "Few-shotç¤ºä¾‹å˜æ¢",
                    "LLMç”Ÿæˆæ›´å¤šæ ·æœ¬"
                ],
                "ä»£ç ç¤ºä¾‹": """
# ä½¿ç”¨LLMç”Ÿæˆæ›´å¤šæ ·æœ¬
from openai import OpenAI
client = OpenAI()

def augment_data(item):
    prompt = f\"\"\"
è¯·ç”Ÿæˆ3ä¸ªç±»ä¼¼çš„æŒ‡ä»¤å¾®è°ƒæ ·æœ¬ï¼š
åŸå§‹æ ·æœ¬:
- æŒ‡ä»¤: {item['instruction']}
- è¾“å…¥: {item['input']}
- è¾“å‡º: {item['output']}

ç”Ÿæˆæ ¼å¼: JSONåˆ—è¡¨
    \"\"\"
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return json.loads(response.choices[0].message.content)
                """
            },
            {
                "åŸåˆ™": "4. æ•°æ®éªŒè¯",
                "è¦ç‚¹": [
                    "äººå·¥æŠ½æ ·æ£€æŸ¥",
                    "è‡ªåŠ¨åŒ–è´¨é‡è¯„åˆ†",
                    "A/Bæµ‹è¯•ä¸åŒæ•°æ®é›†"
                ],
                "ä»£ç ç¤ºä¾‹": """
# è‡ªåŠ¨è´¨é‡è¯„åˆ†
def quality_score(item):
    score = 0
    
    # é•¿åº¦æ£€æŸ¥
    if 20 < len(item['output']) < 500:
        score += 1
    
    # è¯­è¨€æµç•…åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if 'ã€‚' in item['output'] or 'ï¼' in item['output']:
        score += 1
    
    # ç›¸å…³æ€§ï¼ˆinstructionå’Œoutputçš„ç›¸ä¼¼åº¦ï¼‰
    # ... (ä½¿ç”¨embeddingè®¡ç®—)
    
    return score

# è¿‡æ»¤ä½è´¨é‡æ ·æœ¬
high_quality_data = [
    item for item in data 
    if quality_score(item) >= 2
]
                """
            }
        ]
        
        for practice in practices:
            print(f"{practice['åŸåˆ™']}")
            print("\nè¦ç‚¹:")
            for point in practice['è¦ç‚¹']:
                print(f"  â€¢ {point}")
            print(f"\nä»£ç ç¤ºä¾‹:")
            print(practice['ä»£ç ç¤ºä¾‹'])
            print()

DataQuality.display_best_practices()
```

---


## ç¬¬å››èŠ‚ï¼šç”Ÿäº§å®æˆ˜

> ä»å¾®è°ƒåˆ°éƒ¨ç½²ï¼Œæ‰“é€šå…¨æµç¨‹ã€‚

### ä¸€ã€æ¨¡å‹åˆå¹¶ä¸å¯¼å‡º

#### 1. LoRAæƒé‡åˆå¹¶

```python
from dataclasses import dataclass
import torch

@dataclass
class LoRAMerger:
    """LoRAåˆå¹¶å™¨"""
    
    @staticmethod
    def explain_merge_process():
        """è§£é‡Šåˆå¹¶è¿‡ç¨‹"""
        print("=== LoRAæƒé‡åˆå¹¶åŸç† ===\n")
        
        print("LoRAçš„æ•°å­¦å½¢å¼:")
        print("""
åŸå§‹æƒé‡çŸ©é˜µ: W âˆˆ R^(dÃ—k)

LoRAåˆ†è§£:
  Î”W = B @ A
  å…¶ä¸­ B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k), r << min(d, k)

å¾®è°ƒåçš„æƒé‡:
  W' = W + Î±/r Ã— Î”W
  
åˆå¹¶è¿‡ç¨‹:
  W_merged = W_pretrained + Î±/r Ã— (B @ A)
        """)
        
        print("ä»£ç å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰:")
        print("""
import torch
from peft import PeftModel

# 1. åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf"
)

# 2. åŠ è½½LoRAé€‚é…å™¨
model = PeftModel.from_pretrained(
    base_model,
    "output/llama2-7b-alpaca-lora"
)

# 3. åˆå¹¶æƒé‡
model = model.merge_and_unload()

# 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
model.save_pretrained("output/llama2-7b-alpaca-merged")
tokenizer.save_pretrained("output/llama2-7b-alpaca-merged")
        """)
        
        print("\nåˆå¹¶çš„å¥½å¤„:")
        print("  âœ“ æ¨ç†æ—¶æ— éœ€åŠ è½½adapterï¼Œé€Ÿåº¦æ›´å¿«")
        print("  âœ“ å¯ä»¥ç›´æ¥ç”¨æ ‡å‡†HF TransformersåŠ è½½")
        print("  âœ“ å…¼å®¹å„ç§æ¨ç†æ¡†æ¶ï¼ˆvLLM, TGIç­‰ï¼‰")
        
        print("\næ³¨æ„äº‹é¡¹:")
        print("  âš ï¸  åˆå¹¶åæ¨¡å‹ä½“ç§¯ = åŸºåº§æ¨¡å‹å¤§å°")
        print("  âš ï¸  æ— æ³•å†å•ç‹¬æ›´æ–°LoRAæƒé‡")
        print("  âš ï¸  å»ºè®®ä¿ç•™åŸå§‹LoRAæƒé‡å¤‡ä»½")

LoRAMerger.explain_merge_process()
```

---

#### 2. é‡åŒ–å‹ç¼©

```python
@dataclass
class ModelQuantization:
    """æ¨¡å‹é‡åŒ–"""
    
    @staticmethod
    def explain_quantization():
        """è§£é‡Šé‡åŒ–æ–¹æ³•"""
        print("\n=== æ¨¡å‹é‡åŒ– ===\n")
        
        print("æ”¯æŒçš„é‡åŒ–æ–¹æ³•:\n")
        
        methods = [
            {
                "æ–¹æ³•": "GPTQ",
                "ç²¾åº¦": "4-bit",
                "å‹ç¼©æ¯”": "75%",
                "æ¨ç†æ¡†æ¶": "AutoGPTQ, vLLM, TGI",
                "ç‰¹ç‚¹": "éœ€è¦æ ¡å‡†æ•°æ®ï¼Œç²¾åº¦æŸå¤±å°",
                "å‘½ä»¤": """
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-gptq \\
  --export_quantization_bit 4 \\
  --export_quantization_dataset alpaca \\
  --export_quantization_method gptq
                """
            },
            {
                "æ–¹æ³•": "AWQ",
                "ç²¾åº¦": "4-bit",
                "å‹ç¼©æ¯”": "75%",
                "æ¨ç†æ¡†æ¶": "vLLM, TGI",
                "ç‰¹ç‚¹": "Activation-awareï¼Œç²¾åº¦æ›´é«˜",
                "å‘½ä»¤": """
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-awq \\
  --export_quantization_bit 4 \\
  --export_quantization_method awq
                """
            },
            {
                "æ–¹æ³•": "BitsAndBytes (åŠ¨æ€é‡åŒ–)",
                "ç²¾åº¦": "4/8-bit",
                "å‹ç¼©æ¯”": "50-75%",
                "æ¨ç†æ¡†æ¶": "HF Transformers",
                "ç‰¹ç‚¹": "æ— éœ€ç¦»çº¿é‡åŒ–ï¼ŒåŠ¨æ€åŠ è½½",
                "å‘½ä»¤": """
# æ¨ç†æ—¶åŠ¨æ€é‡åŒ–
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    "output/llama2-7b-alpaca-merged",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16
    )
)
                """
            }
        ]
        
        for method in methods:
            print(f"## {method['æ–¹æ³•']}")
            print(f"ç²¾åº¦: {method['ç²¾åº¦']}")
            print(f"å‹ç¼©æ¯”: {method['å‹ç¼©æ¯”']}")
            print(f"æ¨ç†æ¡†æ¶: {method['æ¨ç†æ¡†æ¶']}")
            print(f"ç‰¹ç‚¹: {method['ç‰¹ç‚¹']}")
            print(f"\nä½¿ç”¨æ–¹æ³•:\n{method['å‘½ä»¤']}")
            print()
    
    @staticmethod
    def compare_quantization():
        """å¯¹æ¯”é‡åŒ–æ–¹æ³•"""
        print("=== é‡åŒ–æ–¹æ³•å¯¹æ¯” ===\n")
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–¹æ³•    â”‚ ç²¾åº¦  â”‚ å‹ç¼©æ¯”  â”‚ ç²¾åº¦æŸå¤± â”‚  æ¨ç†é€Ÿåº¦â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FP16     â”‚ 16bit â”‚   -     â”‚    0%    â”‚   1x     â”‚
â”‚ INT8     â”‚ 8bit  â”‚  50%    â”‚  <1%     â”‚  1.5x    â”‚
â”‚ GPTQ     â”‚ 4bit  â”‚  75%    â”‚  ~1%     â”‚  2-3x    â”‚
â”‚ AWQ      â”‚ 4bit  â”‚  75%    â”‚  <1%     â”‚  2-3x    â”‚
â”‚ BnB 4bit â”‚ 4bit  â”‚  75%    â”‚  ~1.5%   â”‚  2x      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®:
  â€¢ è¿½æ±‚ç²¾åº¦: FP16 æˆ– INT8
  â€¢ å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦: AWQ (æ¨è)
  â€¢ æ˜¾å­˜å—é™: GPTQ æˆ– AWQ
  â€¢ å¿«é€Ÿå®éªŒ: BitsAndBytes (åŠ¨æ€é‡åŒ–)
        """)

quantizer = ModelQuantization()
quantizer.explain_quantization()
quantizer.compare_quantization()
```

---

### äºŒã€vLLMéƒ¨ç½²é›†æˆ

#### 1. éƒ¨ç½²å¾®è°ƒæ¨¡å‹

```python
@dataclass
class VLLMDeployment:
    """vLLMéƒ¨ç½²"""
    
    @staticmethod
    def deploy_finetuned_model():
        """éƒ¨ç½²å¾®è°ƒæ¨¡å‹"""
        print("=== ä½¿ç”¨vLLMéƒ¨ç½²å¾®è°ƒæ¨¡å‹ ===\n")
        
        print("æ–¹æ¡ˆ1: éƒ¨ç½²åˆå¹¶åçš„æ¨¡å‹ï¼ˆæ¨èï¼‰\n")
        print("""
# æ­¥éª¤1: åˆå¹¶LoRA
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-merged

# æ­¥éª¤2: ä½¿ç”¨vLLMéƒ¨ç½²
python -m vllm.entrypoints.openai.api_server \\
  --model output/llama2-7b-alpaca-merged \\
  --host 0.0.0.0 \\
  --port 8000 \\
  --gpu-memory-utilization 0.9
        """)
        
        print("\næ–¹æ¡ˆ2: ç›´æ¥åŠ è½½LoRAé€‚é…å™¨ï¼ˆå®éªŒæ€§ï¼‰\n")
        print("""
# vLLMåŸç”Ÿä¸æ”¯æŒLoRAï¼Œéœ€è¦å…ˆåˆå¹¶
# æˆ–ä½¿ç”¨æ”¯æŒLoRAçš„æ¨ç†æ¡†æ¶ï¼ˆå¦‚TGIï¼‰

# Text Generation Inference (TGI)
docker run --gpus all \\
  -p 8080:80 \\
  -v $(pwd)/output:/data \\
  ghcr.io/huggingface/text-generation-inference:latest \\
  --model-id /data/llama2-7b-alpaca-merged \\
  --max-input-length 2048 \\
  --max-total-tokens 4096
        """)
        
        print("\næ–¹æ¡ˆ3: é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰\n")
        print("""
# æ­¥éª¤1: é‡åŒ–
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-7b-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-7b-alpaca-awq \\
  --export_quantization_bit 4 \\
  --export_quantization_method awq

# æ­¥éª¤2: vLLMéƒ¨ç½²é‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \\
  --model output/llama2-7b-alpaca-awq \\
  --quantization awq \\
  --gpu-memory-utilization 0.9
        """)
    
    @staticmethod
    def test_deployed_model():
        """æµ‹è¯•éƒ¨ç½²çš„æ¨¡å‹"""
        print("\n=== æµ‹è¯•éƒ¨ç½²çš„æ¨¡å‹ ===\n")
        
        print("Pythonå®¢æˆ·ç«¯:")
        print("""
import openai

openai.api_key = "EMPTY"
openai.api_base = "http://localhost:8000/v1"

response = openai.ChatCompletion.create(
    model="llama2-7b-alpaca-merged",
    messages=[
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ],
    temperature=0.7,
    max_tokens=512
)

print(response.choices[0].message.content)
        """)
        
        print("\ncURLæµ‹è¯•:")
        print("""
curl http://localhost:8000/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama2-7b-alpaca-merged",
    "messages": [
      {"role": "user", "content": "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åº"}
    ],
    "temperature": 0.7,
    "max_tokens": 512
  }'
        """)

deployment = VLLMDeployment()
deployment.deploy_finetuned_model()
deployment.test_deployed_model()
```

---

### ä¸‰ã€ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

#### 1. å®Œæ•´éƒ¨ç½²æµç¨‹

```python
@dataclass
class ProductionPipeline:
    """ç”Ÿäº§éƒ¨ç½²æµç¨‹"""
    
    @staticmethod
    def display_pipeline():
        """å±•ç¤ºå®Œæ•´æµç¨‹"""
        print("=== ç”Ÿäº§éƒ¨ç½²å®Œæ•´æµç¨‹ ===\n")
        
        pipeline = [
            {
                "é˜¶æ®µ": "1. æ•°æ®å‡†å¤‡",
                "ä»»åŠ¡": [
                    "æ”¶é›†é«˜è´¨é‡æ•°æ®",
                    "æ•°æ®æ¸…æ´—å’Œå»é‡",
                    "æ ¼å¼è½¬æ¢ï¼ˆAlpaca/ShareGPTï¼‰",
                    "åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†"
                ],
                "äº§å‡º": "data/train.json, data/val.json"
            },
            {
                "é˜¶æ®µ": "2. æ¨¡å‹å¾®è°ƒ",
                "ä»»åŠ¡": [
                    "é€‰æ‹©åŸºåº§æ¨¡å‹å’ŒPEFTæ–¹æ³•",
                    "é…ç½®è¶…å‚æ•°",
                    "å¯åŠ¨è®­ç»ƒï¼ˆWeb UIæˆ–CLIï¼‰",
                    "ç›‘æ§lossæ›²çº¿"
                ],
                "äº§å‡º": "output/model-lora/adapter_model.bin"
            },
            {
                "é˜¶æ®µ": "3. æ¨¡å‹è¯„ä¼°",
                "ä»»åŠ¡": [
                    "åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°",
                    "äººå·¥æµ‹è¯•å¯¹è¯è´¨é‡",
                    "å¯¹æ¯”baselineæ¨¡å‹",
                    "A/Bæµ‹è¯•"
                ],
                "äº§å‡º": "è¯„ä¼°æŠ¥å‘Š"
            },
            {
                "é˜¶æ®µ": "4. æ¨¡å‹å¯¼å‡º",
                "ä»»åŠ¡": [
                    "åˆå¹¶LoRAåˆ°base model",
                    "é‡åŒ–ï¼ˆGPTQ/AWQï¼‰",
                    "éªŒè¯å¯¼å‡ºæ¨¡å‹",
                    "ä¸Šä¼ åˆ°HuggingFace Hub"
                ],
                "äº§å‡º": "output/model-merged, output/model-awq"
            },
            {
                "é˜¶æ®µ": "5. éƒ¨ç½²ä¸Šçº¿",
                "ä»»åŠ¡": [
                    "ä½¿ç”¨vLLMå¯åŠ¨æ¨ç†æœåŠ¡",
                    "é…ç½®è´Ÿè½½å‡è¡¡",
                    "å¯ç”¨ç›‘æ§ï¼ˆPrometheusï¼‰",
                    "è®¾ç½®å‘Šè­¦"
                ],
                "äº§å‡º": "ç”Ÿäº§APIæœåŠ¡"
            },
            {
                "é˜¶æ®µ": "6. æŒç»­ä¼˜åŒ–",
                "ä»»åŠ¡": [
                    "æ”¶é›†ç”¨æˆ·åé¦ˆ",
                    "æ ‡æ³¨badcase",
                    "å¢é‡å¾®è°ƒ",
                    "è¿­ä»£æ›´æ–°"
                ],
                "äº§å‡º": "v2, v3, ..."
            }
        ]
        
        for stage in pipeline:
            print(f"{stage['é˜¶æ®µ']}")
            for task in stage['ä»»åŠ¡']:
                print(f"  â–¡ {task}")
            print(f"  â†’ äº§å‡º: {stage['äº§å‡º']}")
            print()

ProductionPipeline.display_pipeline()
```

---

#### 2. éƒ¨ç½²æ£€æŸ¥æ¸…å•

```python
@dataclass
class DeploymentChecklist:
    """éƒ¨ç½²æ£€æŸ¥æ¸…å•"""
    
    @staticmethod
    def display_checklist():
        """æ˜¾ç¤ºæ£€æŸ¥æ¸…å•"""
        print("=== éƒ¨ç½²æ£€æŸ¥æ¸…å• ===\n")
        
        checklist = {
            "æ¨¡å‹è´¨é‡": [
                "â–¡ åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°ç›®æ ‡æŒ‡æ ‡",
                "â–¡ äººå·¥æµ‹è¯•é€šè¿‡ï¼ˆè‡³å°‘50ä¸ªæ ·æœ¬ï¼‰",
                "â–¡ æ— æ˜æ˜¾å¹»è§‰æˆ–é”™è¯¯",
                "â–¡ å¯¹æ¯”baselineæœ‰æå‡"
            ],
            "æŠ€æœ¯å‡†å¤‡": [
                "â–¡ æ¨¡å‹å·²æ­£ç¡®åˆå¹¶å’Œå¯¼å‡º",
                "â–¡ æ¨ç†æœåŠ¡å¯æ­£å¸¸å¯åŠ¨",
                "â–¡ APIæ¥å£æµ‹è¯•é€šè¿‡",
                "â–¡ è´Ÿè½½æµ‹è¯•å®Œæˆï¼ˆQPSã€å»¶è¿Ÿï¼‰"
            ],
            "ç›‘æ§å‘Šè­¦": [
                "â–¡ Prometheus metricsé…ç½®å®Œæˆ",
                "â–¡ Grafana dashboardåˆ›å»º",
                "â–¡ å‘Šè­¦è§„åˆ™è®¾ç½®ï¼ˆé«˜å»¶è¿Ÿã€é«˜é”™è¯¯ç‡ï¼‰",
                "â–¡ æ—¥å¿—æ”¶é›†é…ç½®"
            ],
            "å®‰å…¨åˆè§„": [
                "â–¡ APIè®¤è¯å¯ç”¨",
                "â–¡ Rate limitingé…ç½®",
                "â–¡ æ•æ„Ÿä¿¡æ¯è¿‡æ»¤",
                "â–¡ ç”¨æˆ·åè®®å’Œå…è´£å£°æ˜"
            ],
            "è¿ç»´å‡†å¤‡": [
                "â–¡ éƒ¨ç½²æ–‡æ¡£ç¼–å†™",
                "â–¡ å›æ»šæ–¹æ¡ˆå‡†å¤‡",
                "â–¡ On-callè½®å€¼å®‰æ’",
                "â–¡ äº‹æ•…å“åº”æµç¨‹"
            ]
        }
        
        for category, items in checklist.items():
            print(f"## {category}")
            for item in items:
                print(f"  {item}")
            print()

DeploymentChecklist.display_checklist()
```

---

## æœ¬ç« å°ç»“

> LLaMA-Factoryè®©LLMå¾®è°ƒè§¦æ‰‹å¯åŠï¼Œä»æ•°æ®åˆ°éƒ¨ç½²å…¨æµç¨‹æ‰“é€šã€‚

### ä¸€ã€æ ¸å¿ƒçŸ¥è¯†å›é¡¾

```python
print("=== LLaMA-Factoryæ ¸å¿ƒè¦ç‚¹ ===\n")

print("1. æ ¸å¿ƒç‰¹æ€§:")
print("   âœ“ æ”¯æŒ100+ä¸»æµæ¨¡å‹ï¼ˆLLaMA/Qwen/Mistralç­‰ï¼‰")
print("   âœ“ å¤šç§PEFTæ–¹æ³•ï¼ˆLoRA/QLoRA/DoRA/AdaLoRAï¼‰")
print("   âœ“ Web UIé›¶ä»£ç  + å‘½ä»¤è¡Œé«˜çº§æ§åˆ¶")
print("   âœ“ å†…ç½®100+æ•°æ®é›†ï¼Œå¼€ç®±å³ç”¨")
print()

print("2. ä½¿ç”¨æ–¹å¼:")
print("   â€¢ Web UIï¼ˆLLaMA Boardï¼‰: é€‚åˆå¿«é€Ÿå®éªŒ")
print("   â€¢ å‘½ä»¤è¡Œ: é€‚åˆæ‰¹é‡è®­ç»ƒ")
print("   â€¢ Python API: é€‚åˆé›†æˆåˆ°pipeline")
print()

print("3. PEFTæ–¹æ³•é€‰æ‹©:")
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ–¹æ³•     â”‚ å†…å­˜å ç”¨  â”‚   æ•ˆæœ   â”‚  æ¨èåœºæ™¯  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LoRA       â”‚   ä½     â”‚   å¥½     â”‚  é€šç”¨é¦–é€‰  â”‚
â”‚ QLoRA      â”‚   æœ€ä½   â”‚   å¥½     â”‚  å°æ˜¾å­˜GPU â”‚
â”‚ LoRA+      â”‚   ä½     â”‚   æ›´å¥½   â”‚  å¿«é€Ÿæ”¶æ•›  â”‚
â”‚ DoRA       â”‚   ä¸­     â”‚   æ›´å¥½   â”‚  å¤æ‚ä»»åŠ¡  â”‚
â”‚ Full FT    â”‚   æœ€é«˜   â”‚   æœ€å¥½   â”‚  å¤§è§„æ¨¡æ•°æ®â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

print("4. å…³é”®è¶…å‚æ•°:")
print("   â€¢ Learning Rate: 1e-4 ~ 5e-4 (LoRA)")
print("   â€¢ LoRA Rank: 8 (é€šç”¨), 16-32 (å¤æ‚)")
print("   â€¢ Batch Size: å°½é‡å¤§ï¼Œå—æ˜¾å­˜é™åˆ¶")
print("   â€¢ Epochs: 3 (å¤§æ•°æ®), 5-10 (å°æ•°æ®)")
print()

print("5. æ•°æ®å·¥ç¨‹:")
print("   â€¢ æ ¼å¼: Alpacaï¼ˆå•è½®ï¼‰/ ShareGPTï¼ˆå¤šè½®ï¼‰")
print("   â€¢ è´¨é‡: å»é‡ã€æ¸…æ´—ã€å¹³è¡¡")
print("   â€¢ è§„æ¨¡: è‡³å°‘1Ké«˜è´¨é‡æ ·æœ¬")
print()

print("6. éƒ¨ç½²æµç¨‹:")
print("   å¾®è°ƒ â†’ è¯„ä¼° â†’ åˆå¹¶LoRA â†’ é‡åŒ– â†’ vLLMéƒ¨ç½²")
```

---

### äºŒã€å¿«é€Ÿå‚è€ƒ

```python
from dataclasses import dataclass

@dataclass
class QuickReference:
    """å¿«é€Ÿå‚è€ƒ"""
    
    @staticmethod
    def display():
        """æ˜¾ç¤ºå¿«é€Ÿå‚è€ƒ"""
        print("\n=== å¿«é€Ÿå‚è€ƒ ===\n")
        
        print("å¸¸ç”¨å‘½ä»¤:")
        print("""
# å¯åŠ¨Web UI
llamafactory-cli webui

# è®­ç»ƒï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
llamafactory-cli train config.yaml

# è®­ç»ƒï¼ˆå‘½ä»¤è¡Œå‚æ•°ï¼‰
llamafactory-cli train \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --dataset alpaca_en \\
  --finetuning_type lora \\
  --output_dir output/llama2-lora \\
  --per_device_train_batch_size 4 \\
  --learning_rate 5e-5 \\
  --num_train_epochs 3

# å¯¼å‡ºæ¨¡å‹
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-lora \\
  --export_dir output/llama2-merged

# å¯¹è¯æµ‹è¯•
llamafactory-cli chat \\
  --model_name_or_path output/llama2-merged \\
  --template default
        """)
        
        print("\nç›®å½•ç»“æ„:")
        print("""
LLaMA-Factory/
â”œâ”€â”€ data/                          # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ dataset_info.json          # æ•°æ®é›†æ³¨å†Œ
â”‚   â”œâ”€â”€ alpaca_en.json             # ç¤ºä¾‹æ•°æ®
â”‚   â””â”€â”€ my_dataset.json            # è‡ªå®šä¹‰æ•°æ®
â”œâ”€â”€ config/                        # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ llama2_lora_sft.yaml       # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ ds_z3_config.json          # DeepSpeedé…ç½®
â”œâ”€â”€ output/                        # è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ model-lora/                # LoRAæƒé‡
â”‚   â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â”‚   â””â”€â”€ adapter_config.json
â”‚   â””â”€â”€ model-merged/              # åˆå¹¶åæ¨¡å‹
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ config.json
â””â”€â”€ saves/                         # æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒä¸­ï¼‰
        """)

QuickReference.display()
```

---

### å®æˆ˜ç»ƒä¹ 

#### ç»ƒä¹ 1ï¼šåŸºç¡€å¾®è°ƒ â­â­

**ä»»åŠ¡**ï¼šä½¿ç”¨LLaMA-Factoryå¾®è°ƒLlama-2-7B

è¦æ±‚ï¼š
1. ä½¿ç”¨alpaca_enæ•°æ®é›†
2. LoRAå¾®è°ƒï¼ˆr=8, alpha=16ï¼‰
3. è®­ç»ƒ3ä¸ªepoch
4. å¯¼å‡ºåˆå¹¶åçš„æ¨¡å‹

<details>
<summary>ğŸ’¡ å‚è€ƒç­”æ¡ˆ</summary>

```bash
# æ–¹æ³•1: Web UI
llamafactory-cli webui
# åœ¨ç•Œé¢ä¸­é€‰æ‹©:
# - Model: meta-llama/Llama-2-7b-hf
# - Dataset: alpaca_en
# - Finetuning Type: lora
# - LoRA Rank: 8, Alpha: 16
# - Epochs: 3
# ç‚¹å‡» Start Training

# æ–¹æ³•2: å‘½ä»¤è¡Œ
llamafactory-cli train \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --dataset alpaca_en \\
  --template default \\
  --finetuning_type lora \\
  --lora_rank 8 \\
  --lora_alpha 16 \\
  --output_dir output/llama2-alpaca-lora \\
  --per_device_train_batch_size 4 \\
  --gradient_accumulation_steps 4 \\
  --learning_rate 5e-5 \\
  --num_train_epochs 3 \\
  --fp16

# å¯¼å‡º
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-alpaca-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-alpaca-merged
```
</details>

---

#### ç»ƒä¹ 2ï¼šè‡ªå®šä¹‰æ•°æ®é›† â­â­â­

**ä»»åŠ¡**ï¼šåˆ›å»ºè‡ªå·±çš„æ•°æ®é›†å¹¶å¾®è°ƒ

è¦æ±‚ï¼š
1. å‡†å¤‡è‡³å°‘100æ¡æ•°æ®ï¼ˆAlpacaæ ¼å¼ï¼‰
2. æ³¨å†Œåˆ°dataset_info.json
3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒ
4. åœ¨Chat Tabæµ‹è¯•æ•ˆæœ

<details>
<summary>ğŸ’¡ å‚è€ƒç­”æ¡ˆ</summary>

```python
# 1. å‡†å¤‡æ•°æ®ï¼ˆdata/my_dataset.jsonï¼‰
import json

data = [
    {
        "instruction": "è§£é‡Šä»¥ä¸‹ç¼–ç¨‹æ¦‚å¿µ",
        "input": "é—­åŒ…",
        "output": "é—­åŒ…æ˜¯æŒ‡å‡½æ•°èƒ½å¤Ÿè®¿é—®å…¶è¯æ³•ä½œç”¨åŸŸä¹‹å¤–çš„å˜é‡..."
    },
    # ... æ›´å¤šæ•°æ®
]

with open("data/my_dataset.json", "w") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# 2. æ³¨å†Œæ•°æ®é›†ï¼ˆåœ¨dataset_info.jsonä¸­æ·»åŠ ï¼‰
{
  "my_dataset": {
    "file_name": "my_dataset.json",
    "formatting": "alpaca"
  }
}

# 3. è®­ç»ƒ
llamafactory-cli train \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --dataset my_dataset \\
  --template default \\
  --finetuning_type lora \\
  --output_dir output/my-model-lora \\
  --num_train_epochs 5  # å°æ•°æ®é›†å¤šè®­ç»ƒ
```
</details>

---

#### ç»ƒä¹ 3ï¼šå¤šæ•°æ®é›†æ··åˆå¾®è°ƒ â­â­â­â­

**ä»»åŠ¡**ï¼šä½¿ç”¨å¤šä¸ªæ•°æ®é›†è”åˆå¾®è°ƒ

è¦æ±‚ï¼š
1. æ··åˆalpaca_enã€belle_school_mathã€code_alpaca
2. QLoRAå¾®è°ƒï¼ˆ4-bité‡åŒ–ï¼‰
3. è¯„ä¼°åœ¨å„æ•°æ®é›†ä¸Šçš„è¡¨ç°

<details>
<summary>ğŸ’¡ å‚è€ƒç­”æ¡ˆ</summary>

```yaml
# config/multi_dataset.yaml
model_name_or_path: meta-llama/Llama-2-13b-hf
quantization_bit: 4                    # 4-bité‡åŒ–

dataset: alpaca_en,belle_school_math,code_alpaca  # å¤šæ•°æ®é›†
template: default
cutoff_len: 2048

finetuning_type: lora
lora_rank: 64                          # QLoRAå¯ç”¨æ›´å¤§rank
lora_alpha: 128
lora_target: all

output_dir: output/llama2-13b-multi-qlora
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2e-4                    # QLoRAå­¦ä¹ ç‡ç•¥é«˜
num_train_epochs: 3

# å¯åŠ¨è®­ç»ƒ
llamafactory-cli train config/multi_dataset.yaml

# åˆ†åˆ«è¯„ä¼°
llamafactory-cli eval \\
  --model_name_or_path meta-llama/Llama-2-13b-hf \\
  --adapter_name_or_path output/llama2-13b-multi-qlora \\
  --dataset alpaca_en \\
  --template default

# é‡å¤è¯„ä¼°å…¶ä»–æ•°æ®é›†
```
</details>

---

#### ç»ƒä¹ 4ï¼šç”Ÿäº§éƒ¨ç½²å®Œæ•´æµç¨‹ï¼ˆç»¼åˆï¼‰ â­â­â­â­â­

**ä»»åŠ¡**ï¼šå®Œæ•´çš„å¾®è°ƒåˆ°éƒ¨ç½²æµç¨‹

è¦æ±‚ï¼š
1. å¾®è°ƒæ¨¡å‹
2. è¯„ä¼°è´¨é‡
3. é‡åŒ–ä¸ºAWQ
4. vLLMéƒ¨ç½²
5. æ€§èƒ½æµ‹è¯•

<details>
<summary>ğŸ’¡ å‚è€ƒç­”æ¡ˆ</summary>

```bash
# æ­¥éª¤1: å¾®è°ƒ
llamafactory-cli train \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --dataset alpaca_zh \\
  --template default \\
  --finetuning_type lora \\
  --output_dir output/llama2-zh-lora \\
  --num_train_epochs 3

# æ­¥éª¤2: è¯„ä¼°ï¼ˆäººå·¥æµ‹è¯•ï¼‰
llamafactory-cli chat \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-zh-lora \\
  --template default

# æ­¥éª¤3: é‡åŒ–
llamafactory-cli export \\
  --model_name_or_path meta-llama/Llama-2-7b-hf \\
  --adapter_name_or_path output/llama2-zh-lora \\
  --template default \\
  --finetuning_type lora \\
  --export_dir output/llama2-zh-awq \\
  --export_quantization_bit 4 \\
  --export_quantization_method awq

# æ­¥éª¤4: vLLMéƒ¨ç½²
python -m vllm.entrypoints.openai.api_server \\
  --model output/llama2-zh-awq \\
  --quantization awq \\
  --host 0.0.0.0 \\
  --port 8000 \\
  --gpu-memory-utilization 0.9

# æ­¥éª¤5: æ€§èƒ½æµ‹è¯•
# å®‰è£…æµ‹è¯•å·¥å…·
pip install locust

# ç¼–å†™æµ‹è¯•è„šæœ¬ï¼ˆlocustfile.pyï¼‰
from locust import HttpUser, task

class ChatUser(HttpUser):
    @task
    def chat(self):
        self.client.post("/v1/chat/completions", json={
            "model": "llama2-zh-awq",
            "messages": [{"role": "user", "content": "ä½ å¥½"}],
            "max_tokens": 100
        })

# è¿è¡Œå‹æµ‹
locust -f locustfile.py --host http://localhost:8000
```
</details>

---

### ä¸‹ä¸€ç« é¢„å‘Š

åœ¨ä¸‹ä¸€ç« ã€Šå¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸LLMåº”ç”¨ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ï¼š

- **MDPä¸ç­–ç•¥æ¢¯åº¦**ï¼šå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€
- **PPOç®—æ³•è¯¦è§£**ï¼šRLHFçš„æ ¸å¿ƒç®—æ³•
- **Reward Modeling**ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹
- **RLHFå®Œæ•´æµç¨‹**ï¼šä»SFTåˆ°PPOå®æˆ˜

ä»ç›‘ç£å¾®è°ƒåˆ°å¼ºåŒ–å­¦ä¹ ï¼Œä½ å°†æŒæ¡å¯¹é½LLMçš„å®Œæ•´æŠ€æœ¯æ ˆï¼

---

**æ­å–œä½ å®Œæˆç¬¬4ç« ï¼** ğŸ‰

ä½ å·²ç»æŒæ¡äº†LLaMA-Factoryè¿™ä¸ªå¼ºå¤§çš„å¾®è°ƒå·¥å…·ç®±ï¼Œä»Web UIé›¶ä»£ç åˆ°å‘½ä»¤è¡Œé«˜çº§æ§åˆ¶ï¼Œä»æ•°æ®å‡†å¤‡åˆ°ç”Ÿäº§éƒ¨ç½²ï¼Œå…¨æµç¨‹æ‰“é€šã€‚

è®°ä½ï¼š**å¾®è°ƒçš„æœ¬è´¨æ˜¯è®©é€šç”¨æ¨¡å‹é€‚åº”ç‰¹å®šä»»åŠ¡**ï¼ŒLLaMA-Factoryé€šè¿‡å¼€ç®±å³ç”¨çš„è®¾è®¡ï¼Œè®©è¿™ä¸€è¿‡ç¨‹å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€‚

