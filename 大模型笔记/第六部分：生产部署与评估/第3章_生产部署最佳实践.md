# 第3章：生产部署最佳实践

> 像运营 Google 搜索一样运营你的 LLM API：稳定、高效、可观测。

---

## 目录
- [第一节：部署架构设计](#第一节部署架构设计)
- [第二节：API 服务最佳实践 (FastAPI)](#第二节api-服务最佳实践-fastapi)
- [第三节：流式响应 (Streaming)](#第三节流式响应-streaming)
- [第四节：容错与降级](#第四节容错与降级)
- [第五节：可观测性 (Observability)](#第五节可观测性-observability)
- [本章小结](#本章小结)

---

## 第一节：部署架构设计

### 1.1 典型生产架构

一个高可用的 LLM 服务不仅是运行 `python app.py` 那么简单。

```mermaid
graph LR
    User[Client] --> LB[Load Balancer (Nginx/AWS ELB)]
    LB --> Gateway[API Gateway (FastAPI)]

    subgraph "推理集群 (Inference Cluster)"
        Gateway --> Worker1[vLLM Worker 1 (GPU Node)]
        Gateway --> Worker2[vLLM Worker 2 (GPU Node)]
        Gateway --> Worker3[vLLM Worker 3 (GPU Node)]
    end

    Gateway --> Redis[Redis Queue (Optional)]
    Worker1 --> Prom[Prometheus]
    Prom --> Grafana[Grafana Dashboard]
```

### 1.2 组件职责

1.  **Load Balancer (ALB/Nginx)**: 负责 SSL 终结 (HTTPS -> HTTP) 和简单的轮询分发。
2.  **API Gateway (FastAPI)**:
    *   **鉴权 (Auth)**: 验证 API Key。
    *   **限流 (Rate Limiting)**: 防止某个用户打爆集群。
    *   **路由 (Routing)**: 根据模型名称 (`model="llama-3"`) 将请求转发给对应的 Worker 组。
3.  **Worker Node (vLLM)**: 纯粹的计算节点，只负责接收 JSON 请求，吐出 JSON 响应。

---

## 第二节：Streaming 响应 (SSE)

### 2.1 为什么需要 Streaming？

LLM 生成 500 字可能需要 10 秒。
*   **非流式**：用户看着空白屏幕等 10 秒，以为网断了。
*   **流式 (SSE)**：用户在 0.5 秒内看到第一个字，之后字一个接一个蹦出来（打字机效果）。

### 2.2 Python SSE 完整实现

我们使用 FastAPI 来实现一个兼容 OpenAI 格式的流式接口。

```python
import asyncio
import json
import time
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

app = FastAPI()

class ChatRequest(BaseModel):
    message: str

async def fake_llm_generator(message: str):
    """
    模拟 LLM 的生成过程。
    在真实场景中，这里会调用 vLLM 的 AsyncEngine。
    """
    response_text = f"这里是针对'{message}'的回复，正在逐字生成中..."

    for i, char in enumerate(response_text):
        # 模仿 Token 生成的延迟
        await asyncio.sleep(0.1)

        # 构造 OpenAI 兼容的 chunk 格式
        chunk = {
            "id": "chatcmpl-123",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "gpt-3.5-turbo",
            "choices": [
                {
                    "index": 0,
                    "delta": {"content": char},
                    "finish_reason": None
                }
            ]
        }

        #SSE 格式要求：以 "data: " 开头，双换行结尾
        yield f"data: {json.dumps(chunk)}\n\n"

    # 发送结束标记
    yield "data: [DONE]\n\n"

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    return StreamingResponse(
        fake_llm_generator(request.message),
        media_type="text/event-stream"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**客户端测试**：
```bash
curl -N -X POST http://localhost:8000/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{"message": "Hello"}'
```

---

## 第三节：压力测试 (Locust / wrk)

### 3.1 为什么需要压测？

你必须知道你的集群在**崩溃前**能抗多少 QPS (Queries Per Second)。
LLM 的压测有两个特殊指标：
*   **TTFT (Time To First Token)**: 首字延迟。用户感知的"响应速度"。
*   **TPOT (Time Per Output Token)**: 生成速度。字蹦出来的快慢。

### 3.2 使用 Locust 进行压测

Locust 是一个 Python 编写的压测工具，非常适合编写复杂的 LLM 对话场景。

#### 1. 安装
```bash
pip install locust
```

#### 2. 编写压测脚本 (`locustfile.py`)

```python
import json
from locust import HttpUser, task, between

class LLMUser(HttpUser):
    # 模拟用户思考时间：1~5秒发一次请求
    wait_time = between(1, 5)

    @task
    def chat_completion(self):
        payload = {
            "model": "meta-llama/Meta-Llama-3-8B-Instruct",
            "messages": [
                {"role": "user", "content": "Tell me a joke about engineers."}
            ],
            "max_tokens": 100
        }

        # 发送请求
        with self.client.post(
            "/v1/chat/completions",
            json=payload,
            stream=True,
            catch_response=True
        ) as response:
            if response.status_code != 200:
                response.failure(f"Status code: {response.status_code}")
            else:
                # 在这里可以解析 SSE 统计 TTFT
                # 为了简单，这里只统计请求成功
                response.success()

```

#### 3. 启动压测
```bash
locust -f locustfile.py --host=http://localhost:8000
```
打开浏览器访问 `http://localhost:8089`，设置并发用户数（比如 50），开始轰炸你的 API。

### 3.3 使用 wrk (更底层的 HTTP 压测)

如果你只想测网关吞吐量（不关心生成内容），用 `wrk` 更猛。

```bash
# post.lua
wrk.method = "POST"
wrk.body   = '{"model": "llama-3", "messages": [{"role": "user", "content": "hi"}]}'
wrk.headers["Content-Type"] = "application/json"
```

```bash
wrk -t12 -c400 -d30s -s post.lua http://localhost:8000/v1/chat/completions
```

---

## 第四节：监控与可观测性

在生产环境，盲飞是自杀行为。你需要监控以下核心指标（Golden Signals）：

1.  **Latency (延迟)**: P95, P99 的 TTFT。
2.  **Traffic (流量)**: 当前通过 gateway 的 QPS。
3.  **Errors (错误)**: 5xx 错误率。
4.  **Saturation (饱和度)**:
    *   **KV Cache Usage**: vLLM 显存使用了多少？如果长期 > 95%，说明需要扩容。
    *   **Queue Length**: 有多少请求在 vLLM 队列里排队？

**vLLM 原生支持 Prometheus**：
部署时访问 `http://localhost:8000/metrics` 即可拉取这些数据到 Grafana。

---

## 第3章小结

1.  **架构分层**：网关负责脏活累活（鉴权限流），vLLM 负责纯粹的计算。
2.  **SSE 是必须的**：即便后端生成慢，流式输出也能安抚用户的焦躁。
3.  **压测不要停**：每次模型更新或参数调整（如 Batch Size），都要重新跑 Locust 确保 SLA 不降级。

**下一章预告**：
模型部署上线了，但它回答得准不准？有没有在那一本正经地胡说八道？我们将进入最后一章：**模型评估体系**。
