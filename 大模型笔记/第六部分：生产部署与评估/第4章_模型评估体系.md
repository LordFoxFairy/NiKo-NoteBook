# 第4章：模型评估体系

> 别信感觉，信数据。拒绝"体感评估"，建立科学的打分流水线。

---

## 目录
- [第一节：评估维度与权威榜单](#第一节评估维度与权威榜单)
- [第二节：自动化评估 (LM-Evaluation-Harness)](#第二节自动化评估-lm-evaluation-harness)
- [第三节：LLM-as-a-Judge (用大模型评测大模型)](#第三节llm-as-a-judge-用大模型评测大模型)
- [第四节：主观评估与 A/B Test](#第四节主观评估与-ab-test)
- [本章小结](#本章小结)

---

## 第一节：评估维度与权威榜单

评估大模型是一个极其困难的开放性问题。通常我们从三个维度入手：

1.  **知识与推理 (Capability)**:
    *   **MMLU**: 综合学科知识（数理化生、历史地理）。
    *   **GSM8K**: 小学数学应用题，测试 CoT 推理能力。
    *   **HumanEval**: Python 代码生成能力。
2.  **指令遵循 (Instruction Following)**:
    *   **IFEval**: 评估模型是否能严格遵守"回复不超过 50 字"、"JSON 格式输出"等硬性约束。
3.  **对话质量 (Chat)**:
    *   **MT-Bench**: 多轮对话评估，更加贴近真实用户的使用场景。

### 1.1 MT-Bench 详解
MT-Bench 是目前业界公认度最高的 Chatbot 评估集之一。它包含 80 个高质量的多轮对话问题，覆盖写作、角色扮演、提取信息等 8 个类别。

---

## 第二节：LLM-as-a-Judge (用强模型当裁判)

### 2.1 为什么要 LLM-as-a-Judge？
人工评估太贵太慢；BLEU/ROUGE 等传统 NLP 指标对生成式任务完全失效（字不一样，意思可以一样）。

**LLM-as-a-Judge** 的核心思想是：用一个更强、更公正的模型（通常是 GPT-4o 或 Claude-3.5-Sonnet）来评价两个模型的回答，通过打分或胜负判决来决定优劣。

### 2.2 实战：编写自动打分脚本

下面是一个标准的 LLM-as-a-Judge 评测脚本模板：

```python
from openai import OpenAI
import json

# 评判者客户端 (使用 GPT-4 作为裁判)
judge_client = OpenAI(api_key="sk-proj-...", base_url="https://api.openai.com/v1")

def get_judge_score(question, answer_a, answer_b):
    """
    让 GPT-4 比较 Answer A 和 Answer B，并给出理由和胜者。
    """

    prompt = f"""
    [System]
    You are a helpful and precise assistant for checking the quality of the answer.

    [Question]
    {question}

    [The Start of Assistant A's Answer]
    {answer_a}
    [The End of Assistant A's Answer]

    [The Start of Assistant B's Answer]
    {answer_b}
    [The End of Assistant B's Answer]

    [Task]
    We would like to request your feedback on the performance of two AI assistants.
    Please rate the helpfulness, relevance, accuracy, and level of detail of their responses.
    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.

    Please output your response in JSON format:
    {{
        "reason": "...",
        "score_a": 8,
        "score_b": 9,
        "winner": "B"
    }}
    """

    response = judge_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# 测试数据
q = "上海有什么好吃的？推荐3个。"
ans_llama = "上海有小笼包、生煎馒头和红烧肉。"
ans_qwen = "1. 南翔小笼：皮薄肉嫩。\n2. 生煎：底部酥脆。\n3. 葱油拌面：老上海的味道。"

# 执行评测
result = get_judge_score(q, ans_llama, ans_qwen)
print(json.dumps(result, indent=2, ensure_ascii=False))
```

---

## 第三节：RAG 专项评估 (RAGAS)

对于 RAG (检索增强生成) 应用，仅评估生成的答案是不够的，我们必须评估**检索**的质量。

### 3.1 RAGAS 核心指标
RAGAS (Retrieval Augmented Generation Assessment) 定义了 RAG 的黄金三角：

1.  **Faithfulness (忠实度)**: 答案是否完全基于检索到的 Context？（防幻觉）
2.  **Answer Relevance (答案相关性)**: 答案是否回答了用户的问题？
3.  **Context Precision (上下文精确度)**: 检索回来的文档里，包含多少无用的噪音？

### 3.2 RAGAS 实战代码

```bash
pip install ragas langchain
```

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
)
from datasets import Dataset

# 准备评测数据
# 必须包含: question, answer (生成结果), contexts (检索到的文档片段), ground_truth (标准答案)
data = {
    'question': ['如何重置 Linux root 密码？'],
    'answer': ['你需要重启并在 GRUB 菜单中选择恢复模式，然后输入 passwd 命令。'],
    'contexts': [['在 GRUB 引导加载加载程序中，选择高级选项...输入 passwd root...']],
    'ground_truth': ['重启进入单用户模式或恢复模式，使用 passwd 命令重置。']
}

dataset = Dataset.from_dict(data)

# 开始评估 (背后会调用 OpenAI API 进行打分)
results = evaluate(
    dataset=dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
    ],
)

print(results)
# Output: {'faithfulness': 0.95, 'answer_relevancy': 0.98, 'context_precision': 1.0}
```

---

## 第四节：自动化评估流水线整合

最后，我们将上述工具整合为一个 `AutoEval` 脚本，用于每次模型微调后的自动回归测试。

### 4.1 整合脚本 (`run_eval.py`)

这是一个概念性的整合脚本结构：

```python
import argparse
from my_evals import run_mmlu, run_gsm8k, run_ragas_eval

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--tasks", type=str, default="all")
    args = parser.parse_args()

    report = {}

    # 1. 基础能力测试
    if "base" in args.tasks or args.tasks == "all":
        print("Running MMLU...")
        report["mmlu_score"] = run_mmlu(args.model_path)

        print("Running GSM8K...")
        report["gsm8k_score"] = run_gsm8k(args.model_path)

    # 2. 业务场景测试 (RAG)
    if "rag" in args.tasks or args.tasks == "all":
        print("Running RAGAS on internal knowledge base...")
        # 这里加载公司内部的验证集
        report["ragas_metrics"] = run_ragas_eval(args.model_path, dataset="company_wiki_qa.json")

    # 3. 输出报告
    print("\n====== Evaluation Report ======")
    for k, v in report.items():
        print(f"{k}: {v}")

    # 4. 自动判断是否通过 CI/CD
    if report["ragas_metrics"]["faithfulness"] < 0.8:
        print("❌ FAILED: Faithfulness too low!")
        exit(1)
    else:
        print("✅ PASSED")
        exit(0)

if __name__ == "__main__":
    main()
```

---

## 第4章小结

1.  **多维评估**：不要只看一个分数。MMLU 测智商，IFEval 测情商（听话程度），RAGAS 测业务能力。
2.  **LLM-as-a-Judge 是主流**：在缺乏大规模人工标注的情况下，用 GPT-4 来评估微调模型是最具性价比的方案。
3.  **流水线化**：评估不应该是一次性的，而应作为 CI/CD 的一部分。模型变动后，必须跑一遍 RAGAS 确保没有发生"灾难性遗忘"或幻觉率飙升。

**致此，第六部分《生产部署与评估》结束。**
下一部分我们将进入更前沿的 Agent 与多模态开发领域。
