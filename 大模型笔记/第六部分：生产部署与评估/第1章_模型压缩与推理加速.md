# 第1章：模型压缩与推理加速

> 让LLM更快、更小、更省资源，从实验室走向生产环境。

---


## 第一节：量化技术与原理

### 1.1 为什么需要量化？

#### 大模型的存储与计算瓶颈

**问题**：以LLaMA2-70B为例：

```python
from dataclasses import dataclass
from typing import Dict

@dataclass
class ModelSizeCalculator:
    """模型大小计算器"""
    
    @staticmethod
    def calculate_model_size(num_parameters: int, 
                            precision_bits: int) -> Dict[str, float]:
        """计算模型大小
        
        Args:
            num_parameters: 参数量（单位：十亿）
            precision_bits: 精度位数（32, 16, 8, 4）
        """
        bytes_per_param = precision_bits / 8
        size_gb = (num_parameters * 1e9 * bytes_per_param) / (1024 ** 3)
        
        # GPU内存需求（推理时约1.2x，训练时约4x）
        inference_mem_gb = size_gb * 1.2
        training_mem_gb = size_gb * 4
        
        return {
            "model_size_gb": size_gb,
            "inference_memory_gb": inference_mem_gb,
            "training_memory_gb": training_mem_gb,
            "bytes_per_param": bytes_per_param
        }
    
    @staticmethod
    def compare_precisions(num_parameters: int = 70):
        """对比不同精度下的模型大小"""
        print("=" * 80)
        print(f"LLaMA2-{num_parameters}B 模型大小对比")
        print("=" * 80)
        
        precisions = {
            "FP32": 32,
            "FP16/BF16": 16,
            "INT8": 8,
            "INT4": 4
        }
        
        for name, bits in precisions.items():
            result = ModelSizeCalculator.calculate_model_size(num_parameters, bits)
            print(f"\n{name} ({bits}-bit):")
            print(f"  模型大小: {result['model_size_gb']:.2f} GB")
            print(f"  推理内存: {result['inference_memory_gb']:.2f} GB")
            print(f"  训练内存: {result['training_memory_gb']:.2f} GB")
        
        print("\n" + "=" * 80)

# 执行对比
ModelSizeCalculator.compare_precisions(num_parameters=70)
```

**输出**：
```
================================================================================
LLaMA2-70B 模型大小对比
================================================================================

FP32 (32-bit):
  模型大小: 260.10 GB
  推理内存: 312.12 GB
  训练内存: 1040.40 GB

FP16/BF16 (16-bit):
  模型大小: 130.05 GB
  推理内存: 156.06 GB
  训练内存: 520.20 GB

INT8 (8-bit):
  模型大小: 65.02 GB
  推理内存: 78.03 GB
  训练内存: 260.10 GB

INT4 (4-bit):
  模型大小: 32.51 GB
  推理内存: 39.01 GB
  训练内存: 130.05 GB
================================================================================
```

**关键洞察**：
- FP32 → INT8：**4x压缩**，260GB → 65GB
- FP32 → INT4：**8x压缩**，260GB → 32GB
- 70B模型INT4量化后可在单张A100(80GB)上推理

---

### 1.2 量化的数学原理

#### 从浮点到整数的映射

**量化公式**：

$$
x_q = \text{round}\left(\frac{x - z}{s}\right)
$$

其中：
- $x$：原始浮点值
- $x_q$：量化后整数值
- $s$：缩放因子（scale）
- $z$：零点（zero-point）

**反量化（Dequantization）**：

$$
x \approx s \cdot x_q + z
$$

```python
import numpy as np
import torch

@dataclass
class QuantizationParams:
    """量化参数"""
    scale: float
    zero_point: int
    qmin: int
    qmax: int

class Quantizer:
    """量化器"""
    
    @staticmethod
    def compute_params(tensor: torch.Tensor, 
                      n_bits: int = 8) -> QuantizationParams:
        """计算量化参数
        
        Args:
            tensor: 输入张量
            n_bits: 量化位数
        """
        qmin = 0
        qmax = 2 ** n_bits - 1
        
        # 计算缩放因子和零点
        min_val = tensor.min().item()
        max_val = tensor.max().item()
        
        scale = (max_val - min_val) / (qmax - qmin)
        zero_point = qmin - min_val / scale
        zero_point = int(round(zero_point))
        zero_point = max(qmin, min(qmax, zero_point))
        
        return QuantizationParams(
            scale=scale,
            zero_point=zero_point,
            qmin=qmin,
            qmax=qmax
        )
    
    @staticmethod
    def quantize(tensor: torch.Tensor, 
                params: QuantizationParams) -> torch.Tensor:
        """量化"""
        q_tensor = tensor / params.scale + params.zero_point
        q_tensor = torch.clamp(q_tensor, params.qmin, params.qmax)
        q_tensor = torch.round(q_tensor).to(torch.uint8)
        return q_tensor
    
    @staticmethod
    def dequantize(q_tensor: torch.Tensor,
                   params: QuantizationParams) -> torch.Tensor:
        """反量化"""
        tensor = (q_tensor.float() - params.zero_point) * params.scale
        return tensor

# 测试量化
original = torch.randn(1000) * 5  # 均值0，标准差5

quantizer = Quantizer()
params = quantizer.compute_params(original, n_bits=8)

quantized = quantizer.quantize(original, params)
dequantized = quantizer.dequantize(quantized, params)

# 计算误差
mse = torch.mean((original - dequantized) ** 2).item()
print(f"量化参数:")
print(f"  Scale: {params.scale:.6f}")
print(f"  Zero Point: {params.zero_point}")
print(f"\n量化误差:")
print(f"  MSE: {mse:.6f}")
print(f"  原始值范围: [{original.min():.2f}, {original.max():.2f}]")
print(f"  恢复值范围: [{dequantized.min():.2f}, {dequantized.max():.2f}]")
```

---

#### 对称量化 vs 非对称量化

```python
class SymmetricQuantizer:
    """对称量化（零点为0）"""
    
    @staticmethod
    def compute_params(tensor: torch.Tensor, n_bits: int = 8) -> QuantizationParams:
        """对称量化参数"""
        # 对称量化：范围[-max_abs, max_abs]
        max_abs = max(abs(tensor.min().item()), abs(tensor.max().item()))
        
        qmin = -(2 ** (n_bits - 1))
        qmax = 2 ** (n_bits - 1) - 1
        
        scale = max_abs / qmax
        zero_point = 0
        
        return QuantizationParams(
            scale=scale,
            zero_point=zero_point,
            qmin=qmin,
            qmax=qmax
        )
    
    @staticmethod
    def quantize(tensor: torch.Tensor, params: QuantizationParams) -> torch.Tensor:
        """对称量化"""
        q_tensor = tensor / params.scale
        q_tensor = torch.clamp(q_tensor, params.qmin, params.qmax)
        q_tensor = torch.round(q_tensor).to(torch.int8)
        return q_tensor

# 对比对称vs非对称
tensor = torch.randn(1000) * 3 + 2  # 均值2，非对称分布

# 非对称量化
asym_quantizer = Quantizer()
asym_params = asym_quantizer.compute_params(tensor, n_bits=8)
asym_quantized = asym_quantizer.quantize(tensor, asym_params)
asym_dequantized = asym_quantizer.dequantize(asym_quantized, asym_params)
asym_mse = torch.mean((tensor - asym_dequantized) ** 2).item()

# 对称量化
sym_quantizer = SymmetricQuantizer()
sym_params = sym_quantizer.compute_params(tensor, n_bits=8)
sym_quantized = sym_quantizer.quantize(tensor, sym_params)
sym_dequantized = asym_quantizer.dequantize(
    sym_quantized.to(torch.uint8) + 128,  # 转换为uint8
    sym_params
)
sym_mse = torch.mean((tensor - sym_dequantized) ** 2).item()

print(f"非对称量化 MSE: {asym_mse:.6f}")
print(f"对称量化 MSE: {sym_mse:.6f}")
print(f"\n非对称量化优势: {(sym_mse - asym_mse) / sym_mse * 100:.2f}%")
```

**选择建议**：
- **对称量化**：权重（分布通常对称），硬件友好
- **非对称量化**：激活值（分布可能偏斜），精度更高

---

### 1.3 PTQ（训练后量化）

#### 动态量化

**特点**：运行时动态计算激活值的量化参数。

```python
import torch.nn as nn

class DynamicQuantizedLinear(nn.Module):
    """动态量化线性层"""
    
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # 量化权重（静态）
        self.weight_quantizer = SymmetricQuantizer()
        self.weight_params = self.weight_quantizer.compute_params(
            self.weight.data, n_bits=8
        )
        self.weight_quantized = self.weight_quantizer.quantize(
            self.weight.data, self.weight_params
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播"""
        # 1. 动态量化激活值
        x_quantizer = Quantizer()
        x_params = x_quantizer.compute_params(x, n_bits=8)
        x_quantized = x_quantizer.quantize(x, x_params)
        
        # 2. 反量化权重
        weight = (self.weight_quantized.float() - self.weight_params.zero_point) * self.weight_params.scale
        
        # 3. 反量化激活
        x_dequant = (x_quantized.float() - x_params.zero_point) * x_params.scale
        
        # 4. 计算
        output = torch.nn.functional.linear(x_dequant, weight, self.bias)
        
        return output

# 测试
layer = DynamicQuantizedLinear(512, 256)
x = torch.randn(32, 512)

output = layer(x)
print(f"输入形状: {x.shape}")
print(f"输出形状: {output.shape}")
print(f"权重存储: INT8 (量化), 激活: 动态量化")
```

---

#### 静态量化

**特点**：使用校准数据集预先计算激活值的量化参数。

```python
class StaticQuantizedLinear(nn.Module):
    """静态量化线性层"""
    
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # 权重量化参数
        self.weight_quantizer = SymmetricQuantizer()
        self.weight_params = None
        self.weight_quantized = None
        
        # 激活量化参数（静态，需校准）
        self.activation_params = None
    
    def calibrate(self, calibration_data: torch.Tensor):
        """使用校准数据计算量化参数"""
        # 1. 量化权重
        self.weight_params = self.weight_quantizer.compute_params(
            self.weight.data, n_bits=8
        )
        self.weight_quantized = self.weight_quantizer.quantize(
            self.weight.data, self.weight_params
        )
        
        # 2. 收集激活值
        activations = []
        with torch.no_grad():
            for batch in calibration_data:
                activations.append(batch)
        
        all_activations = torch.cat(activations, dim=0)
        
        # 3. 计算激活量化参数
        quantizer = Quantizer()
        self.activation_params = quantizer.compute_params(
            all_activations, n_bits=8
        )
        
        print(f"校准完成:")
        print(f"  激活值范围: [{all_activations.min():.2f}, {all_activations.max():.2f}]")
        print(f"  量化参数 - Scale: {self.activation_params.scale:.6f}, Zero: {self.activation_params.zero_point}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播（使用预计算的量化参数）"""
        if self.activation_params is None:
            raise RuntimeError("请先调用calibrate()进行校准")
        
        # 1. 量化激活
        quantizer = Quantizer()
        x_quantized = quantizer.quantize(x, self.activation_params)
        
        # 2. 反量化
        weight = (self.weight_quantized.float() - self.weight_params.zero_point) * self.weight_params.scale
        x_dequant = (x_quantized.float() - self.activation_params.zero_point) * self.activation_params.scale
        
        # 3. 计算
        output = torch.nn.functional.linear(x_dequant, weight, self.bias)
        
        return output

# 测试静态量化
layer = StaticQuantizedLinear(512, 256)

# 校准
calibration_data = [torch.randn(32, 512) for _ in range(10)]
layer.calibrate(calibration_data)

# 推理
x = torch.randn(32, 512)
output = layer(x)
print(f"\n推理输出形状: {output.shape}")
```

**动态 vs 静态量化对比**：

| 特性 | 动态量化 | 静态量化 |
|-----|---------|---------|
| 激活量化参数 | 运行时计算 | 预先校准 |
| 推理延迟 | 较高（需计算参数） | 较低 |
| 精度 | 中等 | 较高 |
| 适用场景 | 激活分布变化大 | 激活分布稳定 |

---

### 1.4 GPTQ（生成式预训练Transformer量化）

#### 核心思想

**GPTQ（2022）**：逐层量化，最小化重构误差。

**数学目标**：

$$
\arg\min_{\hat{W}} \| WX - \hat{W}X \|_2^2
$$

其中：
- $W$：原始权重
- $\hat{W}$：量化后权重
- $X$：校准数据激活值

```python
class GPTQQuantizer:
    """GPTQ量化器（简化版）"""
    
    def __init__(self, n_bits: int = 4):
        self.n_bits = n_bits
        self.qmin = -(2 ** (n_bits - 1))
        self.qmax = 2 ** (n_bits - 1) - 1
    
    def quantize_layer(self, 
                      weight: torch.Tensor,
                      activations: torch.Tensor,
                      blocksize: int = 128) -> torch.Tensor:
        """GPTQ逐块量化
        
        Args:
            weight: [out_features, in_features]
            activations: [batch, in_features]
            blocksize: 块大小
        """
        out_features, in_features = weight.shape
        
        # 1. 计算Hessian矩阵（近似为X^T X）
        H = activations.T @ activations
        H_diag = torch.diag(H)
        
        # 2. 初始化量化权重
        quantized_weight = torch.zeros_like(weight)
        
        # 3. 逐块量化
        for i in range(0, in_features, blocksize):
            end = min(i + blocksize, in_features)
            block_weight = weight[:, i:end]
            
            # 计算缩放因子
            scale = block_weight.abs().max(dim=1, keepdim=True)[0] / self.qmax
            scale = torch.clamp(scale, min=1e-8)
            
            # 量化
            q_block = torch.clamp(
                torch.round(block_weight / scale),
                self.qmin,
                self.qmax
            )
            
            # 反量化
            quantized_weight[:, i:end] = q_block * scale
        
        return quantized_weight
    
    def quantize_model(self, 
                      model: nn.Module,
                      calibration_loader: torch.utils.data.DataLoader):
        """量化整个模型"""
        model.eval()
        
        # 收集每层的激活值
        activations = {}
        
        def hook_fn(name):
            def hook(module, input, output):
                activations[name] = input[0].detach()
            return hook
        
        # 注册hooks
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                hook = module.register_forward_hook(hook_fn(name))
                hooks.append(hook)
        
        # 前向传播收集激活
        with torch.no_grad():
            for batch in calibration_loader:
                model(batch)
                break  # 只用一个batch
        
        # 移除hooks
        for hook in hooks:
            hook.remove()
        
        # 逐层量化
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear) and name in activations:
                quantized_weight = self.quantize_layer(
                    module.weight.data,
                    activations[name],
                    blocksize=128
                )
                module.weight.data = quantized_weight
        
        return model

# 使用示例
# 假设有一个简单模型
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(512, 512)
        self.fc2 = nn.Linear(512, 256)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleModel()
calibration_data = torch.utils.data.DataLoader(
    [torch.randn(32, 512) for _ in range(10)],
    batch_size=1
)

quantizer = GPTQQuantizer(n_bits=4)
quantized_model = quantizer.quantize_model(model, calibration_data)

print("GPTQ量化完成")
print(f"原始模型参数量: {sum(p.numel() for p in model.parameters())}")
print(f"量化精度: {quantizer.n_bits}-bit")
```

**GPTQ优势**：
- 4-bit量化，精度损失<1%
- 逐层量化，误差不累积
- 支持超大模型（175B+）

---

### 1.5 AWQ（激活感知权重量化）

#### 核心思想

**AWQ（2023）**：保护重要通道的权重，基于激活值的重要性。

**关键观察**：
- 某些通道的激活值显著大于其他通道
- 这些"尖峰"通道对模型性能至关重要
- 量化时应对这些通道使用更高精度或保护策略

```python
class AWQQuantizer:
    """AWQ量化器"""
    
    def __init__(self, n_bits: int = 4, group_size: int = 128):
        self.n_bits = n_bits
        self.group_size = group_size
        self.qmin = 0
        self.qmax = 2 ** n_bits - 1
    
    def compute_channel_importance(self, 
                                   weight: torch.Tensor,
                                   activations: torch.Tensor) -> torch.Tensor:
        """计算通道重要性
        
        基于激活值的L2范数
        """
        # activations: [batch, in_features]
        importance = torch.norm(activations, p=2, dim=0)  # [in_features]
        return importance
    
    def find_optimal_scale(self,
                          weight: torch.Tensor,
                          importance: torch.Tensor,
                          alpha: float = 0.5) -> torch.Tensor:
        """寻找最优缩放因子
        
        平衡量化误差和激活保护
        """
        # 基于重要性的自适应缩放
        scale = torch.pow(importance, alpha)
        scale = scale / scale.max()  # 归一化
        scale = torch.clamp(scale, min=0.01)
        
        return scale
    
    def quantize_layer(self,
                      weight: torch.Tensor,
                      activations: torch.Tensor) -> tuple:
        """AWQ量化层
        
        Returns:
            quantized_weight, scales, zeros
        """
        out_features, in_features = weight.shape
        
        # 1. 计算通道重要性
        importance = self.compute_channel_importance(weight, activations)
        
        # 2. 计算最优缩放
        channel_scale = self.find_optimal_scale(weight, importance, alpha=0.5)
        
        # 3. 应用缩放
        scaled_weight = weight * channel_scale.unsqueeze(0)
        
        # 4. 分组量化
        num_groups = (in_features + self.group_size - 1) // self.group_size
        quantized_weight = torch.zeros_like(scaled_weight)
        scales = []
        zeros = []
        
        for g in range(num_groups):
            start = g * self.group_size
            end = min((g + 1) * self.group_size, in_features)
            
            group = scaled_weight[:, start:end]
            
            # 计算量化参数
            min_val = group.min()
            max_val = group.max()
            scale = (max_val - min_val) / self.qmax
            zero = -min_val / scale
            
            # 量化
            q_group = torch.clamp(
                torch.round(group / scale + zero),
                self.qmin,
                self.qmax
            )
            
            # 反量化
            quantized_weight[:, start:end] = (q_group - zero) * scale
            
            scales.append(scale)
            zeros.append(zero)
        
        # 5. 反向缩放
        quantized_weight = quantized_weight / channel_scale.unsqueeze(0)
        
        return quantized_weight, torch.tensor(scales), torch.tensor(zeros)

# 测试AWQ
awq_quantizer = AWQQuantizer(n_bits=4, group_size=128)

weight = torch.randn(256, 512)
activations = torch.randn(32, 512)

# 人为制造一些"尖峰"通道
activations[:, :10] *= 10  # 前10个通道激活值很大

quantized_weight, scales, zeros = awq_quantizer.quantize_layer(weight, activations)

# 计算误差
error = torch.mean((weight - quantized_weight) ** 2).item()
print(f"AWQ量化误差 (MSE): {error:.6f}")
print(f"量化参数组数: {len(scales)}")
print(f"每组大小: {awq_quantizer.group_size}")
```

**AWQ vs GPTQ对比**：

| 特性 | GPTQ | AWQ |
|-----|------|-----|
| 量化策略 | 最小化重构误差 | 激活感知保护 |
| 重要性度量 | Hessian矩阵 | 激活值范数 |
| 量化精度 | 4-bit | 4-bit |
| 推理速度 | 中等 | **更快** |
| 精度损失 | <1% | <0.5% |

---

### 1.6 量化感知训练（QAT）

#### 核心思想

**QAT**：在训练过程中模拟量化，让模型适应量化误差。

```python
class FakeQuantize(nn.Module):
    """伪量化层（QAT核心）"""
    
    def __init__(self, n_bits: int = 8):
        super().__init__()
        self.n_bits = n_bits
        self.qmin = 0
        self.qmax = 2 ** n_bits - 1
        
        # 可学习的量化参数
        self.scale = nn.Parameter(torch.tensor(1.0))
        self.zero_point = nn.Parameter(torch.tensor(0.0))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """伪量化：量化+反量化"""
        # 1. 量化
        x_scaled = x / self.scale + self.zero_point
        x_quantized = torch.clamp(x_scaled, self.qmin, self.qmax)
        x_quantized = torch.round(x_quantized)
        
        # 2. 反量化
        x_dequantized = (x_quantized - self.zero_point) * self.scale
        
        # 3. STE（Straight-Through Estimator）
        # 前向传播：量化+反量化
        # 反向传播：直通梯度
        return x + (x_dequantized - x).detach()

class QATLinear(nn.Module):
    """量化感知训练线性层"""
    
    def __init__(self, in_features: int, out_features: int, n_bits: int = 8):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # 伪量化层
        self.weight_quantizer = FakeQuantize(n_bits)
        self.activation_quantizer = FakeQuantize(n_bits)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播（含伪量化）"""
        # 量化权重
        w_quantized = self.weight_quantizer(self.weight)
        
        # 量化激活
        x_quantized = self.activation_quantizer(x)
        
        # 计算
        output = torch.nn.functional.linear(x_quantized, w_quantized, self.bias)
        
        return output

# QAT训练示例
class QATModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = QATLinear(512, 512, n_bits=8)
        self.fc2 = QATLinear(512, 256, n_bits=8)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = QATModel()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 训练循环
for epoch in range(3):
    for _ in range(100):
        x = torch.randn(32, 512)
        target = torch.randn(32, 256)
        
        output = model(x)
        loss = torch.nn.functional.mse_loss(output, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

print("\nQAT训练完成，模型已适应量化误差")
```

**QAT优势**：
- 精度最高（接近FP32）
- 适用于对精度要求极高的场景
- 缺点：需要重新训练，成本高

---

### 1.7 本节小结

**核心要点**：

1. **量化收益**：
   - FP32 → INT8：4x压缩
   - FP32 → INT4：8x压缩
   - 70B模型INT4后可单卡(A100)推理

2. **量化方法**：
   - **PTQ（训练后量化）**：快速，无需训练
     - 动态量化：激活运行时量化
     - 静态量化：激活预先校准
   - **GPTQ**：逐层量化，最小化重构误差
   - **AWQ**：激活感知，保护重要通道
   - **QAT**：训练时模拟量化，精度最高

3. **选择建议**：
   - 快速部署：PTQ动态量化
   - 高精度需求：AWQ或QAT
   - 超大模型：GPTQ或AWQ
   - 推理速度优先：AWQ

**下一节预告**：

第二节《剪枝与知识蒸馏》将深入讲解：
- 结构化剪枝与非结构化剪枝
- 知识蒸馏原理与实践
- 蒸馏+量化组合策略
- 实战：将70B模型蒸馏到7B


## 第二节：剪枝与知识蒸馏

### 2.1 模型剪枝基础

#### 非结构化剪枝

**核心思想**：移除权重矩阵中的单个元素（设为0）。

```python
import torch
import torch.nn as nn

class MagnitudePruner:
    """基于权重大小的剪枝器"""
    
    @staticmethod
    def prune_unstructured(weight: torch.Tensor, 
                          sparsity: float = 0.5) -> tuple:
        """非结构化剪枝
        
        Args:
            weight: 权重张量
            sparsity: 稀疏度（要剪掉的比例）
        
        Returns:
            pruned_weight, mask
        """
        # 1. 计算阈值
        k = int(weight.numel() * sparsity)
        threshold = torch.topk(weight.abs().flatten(), k, largest=False)[0].max()
        
        # 2. 创建mask
        mask = (weight.abs() > threshold).float()
        
        # 3. 应用mask
        pruned_weight = weight * mask
        
        # 统计
        actual_sparsity = 1 - (mask.sum() / mask.numel())
        
        return pruned_weight, mask, actual_sparsity.item()

# 测试
weight = torch.randn(256, 512)
pruned_weight, mask, sparsity = MagnitudePruner.prune_unstructured(weight, sparsity=0.5)

print(f"原始权重形状: {weight.shape}")
print(f"剪枝后稀疏度: {sparsity:.2%}")
print(f"非零元素: {mask.sum().item()} / {mask.numel()}")
```

**非结构化剪枝问题**：
- ❌ 不规则稀疏模式，硬件难以加速
- ❌ 虽然FLOPs减少，实际推理速度提升有限
- ✅ 精度损失小

---

#### 结构化剪枝

**核心思想**：移除整个神经元、通道或层。

```python
class StructuredPruner:
    """结构化剪枝器"""
    
    @staticmethod
    def prune_channels(weight: torch.Tensor,
                      sparsity: float = 0.3) -> tuple:
        """通道剪枝
        
        Args:
            weight: [out_channels, in_channels, ...]
            sparsity: 要剪掉的通道比例
        
        Returns:
            pruned_weight, kept_indices
        """
        out_channels = weight.size(0)
        
        # 1. 计算每个通道的L1范数
        channel_importance = weight.abs().sum(dim=tuple(range(1, weight.ndim)))
        
        # 2. 选择要保留的通道
        num_keep = int(out_channels * (1 - sparsity))
        kept_indices = torch.topk(channel_importance, num_keep)[1].sort()[0]
        
        # 3. 剪枝
        pruned_weight = weight[kept_indices]
        
        return pruned_weight, kept_indices

# 测试
weight = torch.randn(256, 512, 3, 3)  # Conv2d权重
pruned_weight, kept_indices = StructuredPruner.prune_channels(weight, sparsity=0.3)

print(f"原始通道数: {weight.size(0)}")
print(f"剪枝后通道数: {pruned_weight.size(0)}")
print(f"压缩比: {pruned_weight.numel() / weight.numel():.2%}")
```

**结构化 vs 非结构化对比**：

| 特性 | 非结构化剪枝 | 结构化剪枝 |
|-----|------------|-----------|
| 粒度 | 单个权重 | 整个通道/层 |
| 硬件友好 | ❌ | ✅ |
| 实际加速 | 有限 | 显著 |
| 精度损失 | 小 | 中等 |
| FLOPs减少 | 理论减少 | 实际减少 |

---

### 2.2 迭代剪枝与微调

#### 迭代幅度剪枝（IMP）

**核心思想**：逐步剪枝 + 微调，避免一次性剪枝导致的精度崩溃。

```python
class IterativeMagnitudePruner:
    """迭代幅度剪枝器"""
    
    def __init__(self, 
                 model: nn.Module,
                 target_sparsity: float = 0.9,
                 num_iterations: int = 10):
        self.model = model
        self.target_sparsity = target_sparsity
        self.num_iterations = num_iterations
        
        # 记录原始权重（用于lottery ticket）
        self.initial_weights = {
            name: param.data.clone()
            for name, param in model.named_parameters()
            if 'weight' in name
        }
    
    def compute_sparsity_schedule(self) -> list:
        """计算稀疏度调度"""
        sparsities = []
        for i in range(self.num_iterations):
            # 指数增长
            s = self.target_sparsity * (1 - (1 - (i + 1) / self.num_iterations) ** 3)
            sparsities.append(s)
        return sparsities
    
    def prune_and_finetune(self,
                          train_loader,
                          optimizer,
                          criterion,
                          finetune_epochs: int = 3):
        """迭代剪枝+微调"""
        sparsity_schedule = self.compute_sparsity_schedule()
        
        print("=" * 80)
        print("开始迭代剪枝")
        print("=" * 80)
        
        for iteration, target_sparsity in enumerate(sparsity_schedule):
            print(f"\n迭代 {iteration + 1}/{self.num_iterations}")
            print(f"目标稀疏度: {target_sparsity:.2%}")
            
            # 1. 剪枝所有层
            for name, module in self.model.named_modules():
                if isinstance(module, nn.Linear):
                    pruned_weight, mask, actual_sparsity = MagnitudePruner.prune_unstructured(
                        module.weight.data,
                        sparsity=target_sparsity
                    )
                    module.weight.data = pruned_weight
                    
                    # 注册mask（防止微调时恢复）
                    module.register_buffer(f'{name}_mask', mask)
            
            # 2. 微调
            print(f"微调 {finetune_epochs} epochs...")
            for epoch in range(finetune_epochs):
                epoch_loss = 0
                for batch_idx, (data, target) in enumerate(train_loader):
                    optimizer.zero_grad()
                    output = self.model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    
                    # 应用mask（保持剪枝结构）
                    for name, module in self.model.named_modules():
                        if isinstance(module, nn.Linear):
                            if hasattr(module, f'{name}_mask'):
                                module.weight.grad *= getattr(module, f'{name}_mask')
                    
                    optimizer.step()
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / len(train_loader)
                print(f"  Epoch {epoch+1}, Loss: {avg_loss:.4f}")
        
        print("\n" + "=" * 80)
        print(f"迭代剪枝完成！最终稀疏度: {self.target_sparsity:.2%}")
        print("=" * 80)

# 使用示例
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = SimpleNet()
pruner = IterativeMagnitudePruner(
    model,
    target_sparsity=0.9,
    num_iterations=5
)

# pruner.prune_and_finetune(train_loader, optimizer, criterion, finetune_epochs=3)
print("迭代剪枝流程已定义")
```

---

### 2.3 知识蒸馏原理

#### 经典知识蒸馏

**核心思想**：教师模型（大模型）指导学生模型（小模型）。

**KD损失**：

$$
\mathcal{L}_{\text{KD}} = \alpha \cdot \mathcal{L}_{\text{CE}}(y, \hat{y}_s) + (1-\alpha) \cdot \mathcal{L}_{\text{KL}}(\sigma(z_t/T), \sigma(z_s/T))
$$

其中：
- $\mathcal{L}_{\text{CE}}$：交叉熵损失（真实标签）
- $\mathcal{L}_{\text{KL}}$：KL散度（软标签）
- $T$：温度参数
- $\alpha$：平衡系数

```python
class KnowledgeDistillation:
    """知识蒸馏训练器"""
    
    def __init__(self,
                 teacher_model: nn.Module,
                 student_model: nn.Module,
                 temperature: float = 4.0,
                 alpha: float = 0.7):
        """
        Args:
            teacher_model: 教师模型（大模型）
            student_model: 学生模型（小模型）
            temperature: 温度参数（越大越软）
            alpha: KL损失权重
        """
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha
        
        self.teacher.eval()  # 教师模型不训练
    
    def distillation_loss(self,
                         student_logits: torch.Tensor,
                         teacher_logits: torch.Tensor,
                         labels: torch.Tensor) -> tuple:
        """计算蒸馏损失
        
        Returns:
            total_loss, ce_loss, kd_loss
        """
        # 1. 硬标签损失（交叉熵）
        ce_loss = nn.functional.cross_entropy(student_logits, labels)
        
        # 2. 软标签损失（KL散度）
        # 温度缩放的softmax
        soft_teacher = nn.functional.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = nn.functional.log_softmax(student_logits / self.temperature, dim=1)
        
        # KL散度
        kd_loss = nn.functional.kl_div(
            soft_student,
            soft_teacher,
            reduction='batchmean'
        ) * (self.temperature ** 2)  # 温度补偿
        
        # 3. 总损失
        total_loss = (1 - self.alpha) * ce_loss + self.alpha * kd_loss
        
        return total_loss, ce_loss, kd_loss
    
    def train_step(self,
                   data: torch.Tensor,
                   labels: torch.Tensor,
                   optimizer: torch.optim.Optimizer) -> dict:
        """训练一步"""
        # 1. 教师模型前向（no_grad）
        with torch.no_grad():
            teacher_logits = self.teacher(data)
        
        # 2. 学生模型前向
        student_logits = self.student(data)
        
        # 3. 计算损失
        total_loss, ce_loss, kd_loss = self.distillation_loss(
            student_logits, teacher_logits, labels
        )
        
        # 4. 反向传播
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # 5. 计算准确率
        predictions = student_logits.argmax(dim=1)
        accuracy = (predictions == labels).float().mean()
        
        return {
            "total_loss": total_loss.item(),
            "ce_loss": ce_loss.item(),
            "kd_loss": kd_loss.item(),
            "accuracy": accuracy.item()
        }

# 使用示例
class TeacherModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class StudentModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

teacher = TeacherModel()
student = StudentModel()

kd_trainer = KnowledgeDistillation(
    teacher_model=teacher,
    student_model=student,
    temperature=4.0,
    alpha=0.7
)

# 模拟训练
data = torch.randn(32, 784)
labels = torch.randint(0, 10, (32,))
optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)

metrics = kd_trainer.train_step(data, labels, optimizer)
print(f"蒸馏训练:")
print(f"  Total Loss: {metrics['total_loss']:.4f}")
print(f"  CE Loss: {metrics['ce_loss']:.4f}")
print(f"  KD Loss: {metrics['kd_loss']:.4f}")
print(f"  Accuracy: {metrics['accuracy']:.2%}")
```

---

### 2.4 LLM蒸馏策略

#### Logit蒸馏 vs Feature蒸馏

```python
class LLMDistillation:
    """LLM知识蒸馏（支持多种策略）"""
    
    def __init__(self,
                 teacher_model: nn.Module,
                 student_model: nn.Module,
                 distill_type: str = "logit"):  # "logit", "feature", "mixed"
        self.teacher = teacher_model
        self.student = student_model
        self.distill_type = distill_type
        
        self.teacher.eval()
    
    def logit_distillation_loss(self,
                               student_logits: torch.Tensor,
                               teacher_logits: torch.Tensor) -> torch.Tensor:
        """Logit蒸馏（输出层）"""
        # KL散度
        loss = nn.functional.kl_div(
            nn.functional.log_softmax(student_logits, dim=-1),
            nn.functional.softmax(teacher_logits, dim=-1),
            reduction='batchmean'
        )
        return loss
    
    def feature_distillation_loss(self,
                                 student_hidden: torch.Tensor,
                                 teacher_hidden: torch.Tensor) -> torch.Tensor:
        """Feature蒸馏（中间层）"""
        # MSE损失
        # 如果维度不同，需要投影
        if student_hidden.size(-1) != teacher_hidden.size(-1):
            # 简化：使用线性投影
            projection = nn.Linear(
                student_hidden.size(-1),
                teacher_hidden.size(-1)
            ).to(student_hidden.device)
            student_hidden = projection(student_hidden)
        
        loss = nn.functional.mse_loss(student_hidden, teacher_hidden)
        return loss
    
    def compute_loss(self,
                    student_outputs: dict,
                    teacher_outputs: dict) -> torch.Tensor:
        """计算蒸馏损失"""
        if self.distill_type == "logit":
            loss = self.logit_distillation_loss(
                student_outputs["logits"],
                teacher_outputs["logits"]
            )
        
        elif self.distill_type == "feature":
            loss = self.feature_distillation_loss(
                student_outputs["hidden_states"],
                teacher_outputs["hidden_states"]
            )
        
        elif self.distill_type == "mixed":
            logit_loss = self.logit_distillation_loss(
                student_outputs["logits"],
                teacher_outputs["logits"]
            )
            feature_loss = self.feature_distillation_loss(
                student_outputs["hidden_states"],
                teacher_outputs["hidden_states"]
            )
            loss = 0.5 * logit_loss + 0.5 * feature_loss
        
        return loss

print("LLM蒸馏策略已定义")
```

**蒸馏策略对比**：

| 策略 | 蒸馏目标 | 优势 | 适用场景 |
|-----|---------|-----|---------|
| Logit | 输出分布 | 简单，效果稳定 | 分类任务 |
| Feature | 中间表示 | 捕捉更多知识 | 生成任务 |
| Mixed | 输出+中间层 | 最全面 | 复杂任务 |

---

### 2.5 蒸馏+量化组合

**核心思想**：先蒸馏获得小模型，再量化进一步压缩。

```python
class DistillationThenQuantization:
    """蒸馏+量化pipeline"""
    
    @staticmethod
    def pipeline(teacher_model: nn.Module,
                student_architecture: type,
                train_loader,
                quantization_bits: int = 8) -> nn.Module:
        """完整pipeline
        
        Stage 1: 知识蒸馏 (70B → 7B)
        Stage 2: 量化 (7B FP16 → 7B INT8)
        """
        print("=" * 80)
        print("蒸馏+量化Pipeline")
        print("=" * 80)
        
        # Stage 1: 蒸馏
        print("\n[Stage 1] 知识蒸馏...")
        student_model = student_architecture()
        
        kd_trainer = KnowledgeDistillation(
            teacher_model=teacher_model,
            student_model=student_model,
            temperature=4.0,
            alpha=0.7
        )
        
        optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)
        
        for epoch in range(3):
            for batch in train_loader:
                data, labels = batch
                metrics = kd_trainer.train_step(data, labels, optimizer)
            
            print(f"  Epoch {epoch+1}: Acc={metrics['accuracy']:.2%}")
        
        # Stage 2: 量化
        print("\n[Stage 2] 模型量化...")
        
        # 动态量化
        quantized_model = torch.quantization.quantize_dynamic(
            student_model,
            {nn.Linear},
            dtype=torch.qint8
        )
        
        # 计算压缩比
        def get_model_size(model):
            torch.save(model.state_dict(), "/tmp/temp.pth")
            size = os.path.getsize("/tmp/temp.pth") / (1024 ** 2)
            os.remove("/tmp/temp.pth")
            return size
        
        teacher_size = get_model_size(teacher_model)
        student_size = get_model_size(student_model)
        quantized_size = get_model_size(quantized_model)
        
        print(f"\n模型大小对比:")
        print(f"  Teacher (原始): {teacher_size:.2f} MB")
        print(f"  Student (蒸馏): {student_size:.2f} MB (压缩 {teacher_size/student_size:.1f}x)")
        print(f"  Quantized (量化): {quantized_size:.2f} MB (压缩 {teacher_size/quantized_size:.1f}x)")
        
        print("\n" + "=" * 80)
        print("Pipeline完成！")
        print("=" * 80)
        
        return quantized_model

# 使用示例
# quantized_model = DistillationThenQuantization.pipeline(
#     teacher_model=teacher,
#     student_architecture=StudentModel,
#     train_loader=train_loader,
#     quantization_bits=8
# )
```

**组合策略收益**：
- 蒸馏：70B → 7B（10x参数减少）
- 量化：FP16 → INT8（2x存储减少）
- 总计：**20x总体压缩**，精度损失<5%

---

### 2.6 本节小结

**核心要点**：

1. **剪枝方法**：
   - **非结构化**：单个权重，精度好但硬件难加速
   - **结构化**：整个通道/层，硬件友好，实际加速显著
   - **迭代剪枝**：逐步剪枝+微调，避免精度崩溃

2. **知识蒸馏**：
   - **经典KD**：软标签（KL散度）+ 硬标签（交叉熵）
   - **温度参数**：T=4~8，越大越软
   - **LLM蒸馏**：Logit蒸馏 vs Feature蒸馏

3. **组合策略**：
   - 蒸馏+量化：20x压缩
   - 蒸馏+剪枝+量化：30x+压缩
   - 精度损失：<5%（精心调优）

**实战建议**：
- 快速压缩：仅量化（AWQ/GPTQ）
- 极致压缩：蒸馏+量化+剪枝
- 保持精度：蒸馏到稍大模型+INT8量化


## 第三节：KV缓存优化

### 3.1 KV缓存的必要性

#### 自回归生成的计算瓶颈

**问题**：每次生成新token都需要重新计算所有历史token的K、V。

```python
@dataclass
class AttentionCompute:
    """注意力计算分析"""
    
    @staticmethod
    def analyze_computation(seq_len: int = 2048, 
                           hidden_size: int = 4096,
                           num_heads: int = 32):
        """分析生成过程的计算量"""
        head_dim = hidden_size // num_heads
        
        print("=" * 80)
        print("自回归生成计算分析")
        print("=" * 80)
        
        # 不使用KV缓存
        total_flops_no_cache = 0
        for t in range(1, seq_len + 1):
            # 每步都要计算全部t个token的QKV
            flops_qkv = 3 * t * hidden_size * hidden_size
            # 注意力计算
            flops_attn = t * t * hidden_size
            total_flops_no_cache += flops_qkv + flops_attn
        
        # 使用KV缓存
        total_flops_with_cache = 0
        for t in range(1, seq_len + 1):
            # 只计算新token的QKV
            flops_qkv = 3 * hidden_size * hidden_size
            # 注意力计算（Q@K^T, attention@V）
            flops_attn = 2 * t * hidden_size
            total_flops_with_cache += flops_qkv + flops_attn
        
        speedup = total_flops_no_cache / total_flops_with_cache
        
        print(f"\n配置:")
        print(f"  序列长度: {seq_len}")
        print(f"  Hidden size: {hidden_size}")
        print(f"  Num heads: {num_heads}")
        
        print(f"\n计算量 (TFLOPs):")
        print(f"  无缓存: {total_flops_no_cache / 1e12:.2f}")
        print(f"  有缓存: {total_flops_with_cache / 1e12:.2f}")
        print(f"  加速比: {speedup:.1f}x")
        
        # 内存占用
        kv_cache_size_gb = (
            2 *  # K和V
            seq_len *
            hidden_size *
            4  # FP32
        ) / (1024 ** 3)
        
        print(f"\nKV缓存内存占用:")
        print(f"  FP32: {kv_cache_size_gb:.2f} GB")
        print(f"  FP16: {kv_cache_size_gb / 2:.2f} GB")
        print(f"  INT8: {kv_cache_size_gb / 4:.2f} GB")
        
        print("=" * 80)

# 分析
AttentionCompute.analyze_computation(seq_len=2048, hidden_size=4096)
```

**输出示例**：
```
================================================================================
自回归生成计算分析
================================================================================

配置:
  序列长度: 2048
  Hidden size: 4096
  Num heads: 32

计算量 (TFLOPs):
  无缓存: 143.21
  有缓存: 0.55
  加速比: 260.4x

KV缓存内存占用:
  FP32: 0.12 GB
  FP16: 0.06 GB
  INT8: 0.03 GB
================================================================================
```

**关键洞察**：
- KV缓存带来**260x加速**
- 但内存占用成为新瓶颈（长序列、大batch时）

---

### 3.2 KV缓存实现

```python
class KVCache:
    """KV缓存管理器"""
    
    def __init__(self,
                 max_batch_size: int = 32,
                 max_seq_len: int = 2048,
                 num_layers: int = 32,
                 num_heads: int = 32,
                 head_dim: int = 128,
                 dtype: torch.dtype = torch.float16,
                 device: str = "cuda"):
        """
        Args:
            max_batch_size: 最大批次大小
            max_seq_len: 最大序列长度
            num_layers: 层数
            num_heads: 注意力头数
            head_dim: 每个头的维度
        """
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype
        self.device = device
        
        # 预分配缓存
        self.cache_k = torch.zeros(
            num_layers, max_batch_size, num_heads, max_seq_len, head_dim,
            dtype=dtype, device=device
        )
        self.cache_v = torch.zeros(
            num_layers, max_batch_size, num_heads, max_seq_len, head_dim,
            dtype=dtype, device=device
        )
        
        # 当前序列长度
        self.seq_lens = torch.zeros(max_batch_size, dtype=torch.long, device=device)
    
    def update(self,
              layer_idx: int,
              batch_indices: torch.Tensor,
              new_k: torch.Tensor,
              new_v: torch.Tensor):
        """更新缓存
        
        Args:
            layer_idx: 层索引
            batch_indices: 批次索引
            new_k: 新的K [batch, num_heads, 1, head_dim]
            new_v: 新的V [batch, num_heads, 1, head_dim]
        """
        batch_size = batch_indices.size(0)
        
        # 获取当前位置
        positions = self.seq_lens[batch_indices]
        
        # 更新缓存
        for i, batch_idx in enumerate(batch_indices):
            pos = positions[i].item()
            self.cache_k[layer_idx, batch_idx, :, pos, :] = new_k[i, :, 0, :]
            self.cache_v[layer_idx, batch_idx, :, pos, :] = new_v[i, :, 0, :]
        
        # 更新序列长度
        self.seq_lens[batch_indices] += 1
    
    def get(self,
           layer_idx: int,
           batch_indices: torch.Tensor) -> tuple:
        """获取缓存
        
        Returns:
            cached_k, cached_v
        """
        batch_size = batch_indices.size(0)
        max_len = self.seq_lens[batch_indices].max().item()
        
        # 提取有效缓存
        cached_k = self.cache_k[
            layer_idx,
            batch_indices,
            :,
            :max_len,
            :
        ]  # [batch, num_heads, max_len, head_dim]
        
        cached_v = self.cache_v[
            layer_idx,
            batch_indices,
            :,
            :max_len,
            :
        ]
        
        return cached_k, cached_v
    
    def clear(self, batch_indices: torch.Tensor = None):
        """清空缓存"""
        if batch_indices is None:
            # 清空所有
            self.cache_k.zero_()
            self.cache_v.zero_()
            self.seq_lens.zero_()
        else:
            # 清空指定batch
            self.seq_lens[batch_indices] = 0
    
    def memory_usage_gb(self) -> float:
        """计算内存占用（GB）"""
        total_elements = (
            2 *  # K和V
            self.num_layers *
            self.max_batch_size *
            self.num_heads *
            self.max_seq_len *
            self.head_dim
        )
        bytes_per_element = torch.finfo(self.dtype).bits // 8
        total_bytes = total_elements * bytes_per_element
        return total_bytes / (1024 ** 3)

# 使用示例
kv_cache = KVCache(
    max_batch_size=32,
    max_seq_len=2048,
    num_layers=32,
    num_heads=32,
    head_dim=128,
    dtype=torch.float16
)

print(f"KV缓存内存占用: {kv_cache.memory_usage_gb():.2f} GB")

# 模拟更新
layer_idx = 0
batch_indices = torch.tensor([0, 1, 2])
new_k = torch.randn(3, 32, 1, 128, dtype=torch.float16)
new_v = torch.randn(3, 32, 1, 128, dtype=torch.float16)

kv_cache.update(layer_idx, batch_indices, new_k, new_v)
cached_k, cached_v = kv_cache.get(layer_idx, batch_indices)

print(f"缓存K形状: {cached_k.shape}")
print(f"缓存V形状: {cached_v.shape}")
```

---

### 3.3 PagedAttention（vLLM核心技术）

#### 虚拟内存分页思想

**核心洞察**：将KV缓存按"页"管理，类似操作系统的虚拟内存。

```python
class PagedKVCache:
    """分页KV缓存（vLLM风格）"""
    
    def __init__(self,
                 page_size: int = 16,  # 每页token数
                 num_pages: int = 1000,  # 总页数
                 num_layers: int = 32,
                 num_heads: int = 32,
                 head_dim: int = 128,
                 dtype: torch.dtype = torch.float16,
                 device: str = "cuda"):
        """
        Args:
            page_size: 每页存储的token数
            num_pages: 物理页总数
        """
        self.page_size = page_size
        self.num_pages = num_pages
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype
        self.device = device
        
        # 物理内存：所有页
        self.pages_k = torch.zeros(
            num_layers, num_pages, page_size, num_heads, head_dim,
            dtype=dtype, device=device
        )
        self.pages_v = torch.zeros(
            num_layers, num_pages, page_size, num_heads, head_dim,
            dtype=dtype, device=device
        )
        
        # 页表：sequence_id -> list of page_ids
        self.page_tables = {}
        
        # 空闲页池
        self.free_pages = list(range(num_pages))
    
    def allocate_sequence(self, sequence_id: int, num_tokens: int):
        """为序列分配页"""
        num_pages_needed = (num_tokens + self.page_size - 1) // self.page_size
        
        if len(self.free_pages) < num_pages_needed:
            raise RuntimeError("内存不足，无法分配更多页")
        
        # 分配页
        allocated_pages = []
        for _ in range(num_pages_needed):
            page_id = self.free_pages.pop(0)
            allocated_pages.append(page_id)
        
        self.page_tables[sequence_id] = allocated_pages
        
        return allocated_pages
    
    def free_sequence(self, sequence_id: int):
        """释放序列的页"""
        if sequence_id not in self.page_tables:
            return
        
        pages = self.page_tables[sequence_id]
        self.free_pages.extend(pages)
        del self.page_tables[sequence_id]
    
    def write_kv(self,
                layer_idx: int,
                sequence_id: int,
                token_idx: int,
                k: torch.Tensor,
                v: torch.Tensor):
        """写入KV"""
        # 计算页索引和页内偏移
        page_idx = token_idx // self.page_size
        offset = token_idx % self.page_size
        
        # 获取物理页ID
        page_id = self.page_tables[sequence_id][page_idx]
        
        # 写入
        self.pages_k[layer_idx, page_id, offset] = k
        self.pages_v[layer_idx, page_id, offset] = v
    
    def read_kv(self,
               layer_idx: int,
               sequence_id: int,
               max_len: int) -> tuple:
        """读取KV"""
        pages = self.page_tables[sequence_id]
        num_pages = (max_len + self.page_size - 1) // self.page_size
        
        # 收集所有页
        k_list = []
        v_list = []
        
        for i in range(min(num_pages, len(pages))):
            page_id = pages[i]
            k_page = self.pages_k[layer_idx, page_id]  # [page_size, num_heads, head_dim]
            v_page = self.pages_v[layer_idx, page_id]
            
            k_list.append(k_page)
            v_list.append(v_page)
        
        # 拼接
        k = torch.cat(k_list, dim=0)[:max_len]
        v = torch.cat(v_list, dim=0)[:max_len]
        
        return k, v
    
    def share_pages(self, src_seq_id: int, dst_seq_id: int):
        """序列间共享页（用于Beam Search）"""
        if src_seq_id not in self.page_tables:
            raise ValueError(f"源序列{src_seq_id}不存在")
        
        # Copy-on-Write：直接共享页引用
        self.page_tables[dst_seq_id] = self.page_tables[src_seq_id].copy()
    
    def memory_usage_gb(self) -> float:
        """计算内存占用"""
        total_elements = (
            2 *  # K和V
            self.num_layers *
            self.num_pages *
            self.page_size *
            self.num_heads *
            self.head_dim
        )
        bytes_per_element = torch.finfo(self.dtype).bits // 8
        return total_elements * bytes_per_element / (1024 ** 3)
    
    def utilization(self) -> float:
        """计算内存利用率"""
        used_pages = self.num_pages - len(self.free_pages)
        return used_pages / self.num_pages

# 使用示例
paged_cache = PagedKVCache(
    page_size=16,
    num_pages=1000,
    num_layers=32,
    num_heads=32,
    head_dim=128
)

# 分配序列
seq_id = 0
paged_cache.allocate_sequence(seq_id, num_tokens=100)

# 写入KV
k = torch.randn(32, 128, dtype=torch.float16)
v = torch.randn(32, 128, dtype=torch.float16)
paged_cache.write_kv(layer_idx=0, sequence_id=seq_id, token_idx=0, k=k, v=v)

# 读取KV
cached_k, cached_v = paged_cache.read_kv(layer_idx=0, sequence_id=seq_id, max_len=100)

print(f"PagedAttention内存占用: {paged_cache.memory_usage_gb():.2f} GB")
print(f"内存利用率: {paged_cache.utilization():.2%}")
```

**PagedAttention优势**：
- ✅ **近100%内存利用率**（传统KV缓存~50%）
- ✅ **支持动态batch**
- ✅ **共享前缀**（Beam Search、多用户）
- ✅ **内存碎片化↓**

---

### 3.4 MQA/GQA优化

#### Multi-Query Attention (MQA)

**核心思想**：所有头共享同一组KV。

```python
class MultiQueryAttention(nn.Module):
    """Multi-Query Attention（MQA）"""
    
    def __init__(self, hidden_size: int = 4096, num_heads: int = 32):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # Q：每个头独立
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # K, V：所有头共享（单头）
        self.k_proj = nn.Linear(hidden_size, self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.head_dim, bias=False)
        
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, x: torch.Tensor, kv_cache: KVCache = None, layer_idx: int = 0):
        """
        Args:
            x: [batch, seq_len, hidden_size]
        """
        batch_size, seq_len, _ = x.shape
        
        # 1. 投影
        q = self.q_proj(x)  # [batch, seq_len, hidden_size]
        k = self.k_proj(x)  # [batch, seq_len, head_dim]
        v = self.v_proj(x)  # [batch, seq_len, head_dim]
        
        # 2. Reshape Q
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        q = q.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]
        
        # 3. K, V保持单头，broadcast到所有头
        k = k.unsqueeze(1).expand(batch_size, self.num_heads, seq_len, self.head_dim)
        v = v.unsqueeze(1).expand(batch_size, self.num_heads, seq_len, self.head_dim)
        
        # 4. Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        # 5. 合并头
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.hidden_size)
        
        # 6. 输出投影
        output = self.o_proj(attn_output)
        
        return output

# 测试MQA
mqa = MultiQueryAttention(hidden_size=4096, num_heads=32)
x = torch.randn(2, 10, 4096)
output = mqa(x)

print(f"MQA输出形状: {output.shape}")

# KV缓存大小对比
standard_kv_size = 2 * 32 * 128 * 4  # 32头，每头128维，FP32
mqa_kv_size = 2 * 1 * 128 * 4  # 单头
print(f"\n标准MHA KV大小: {standard_kv_size} bytes")
print(f"MQA KV大小: {mqa_kv_size} bytes")
print(f"KV缓存压缩: {standard_kv_size / mqa_kv_size:.1f}x")
```

---

#### Grouped-Query Attention (GQA)

**核心思想**：头分组，每组共享KV。

```python
class GroupedQueryAttention(nn.Module):
    """Grouped-Query Attention（GQA）"""
    
    def __init__(self, 
                 hidden_size: int = 4096,
                 num_heads: int = 32,
                 num_kv_heads: int = 8):  # KV头数（组数）
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = hidden_size // num_heads
        
        assert num_heads % num_kv_heads == 0, "num_heads必须能被num_kv_heads整除"
        self.num_heads_per_group = num_heads // num_kv_heads
        
        # Q：所有头
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # K, V：分组头
        self.k_proj = nn.Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)
        
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, x: torch.Tensor):
        batch_size, seq_len, _ = x.shape
        
        # 1. 投影
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # 2. Reshape
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)
        
        q = q.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]
        k = k.transpose(1, 2)  # [batch, num_kv_heads, seq_len, head_dim]
        v = v.transpose(1, 2)
        
        # 3. 扩展KV到所有头
        k = k.repeat_interleave(self.num_heads_per_group, dim=1)
        v = v.repeat_interleave(self.num_heads_per_group, dim=1)
        # 现在 k, v: [batch, num_heads, seq_len, head_dim]
        
        # 4. Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        # 5. 合并头
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.hidden_size)
        
        # 6. 输出投影
        output = self.o_proj(attn_output)
        
        return output

# 测试GQA
gqa = GroupedQueryAttention(hidden_size=4096, num_heads=32, num_kv_heads=8)
x = torch.randn(2, 10, 4096)
output = gqa(x)

print(f"\nGQA输出形状: {output.shape}")

# KV缓存对比
standard_kv_size = 2 * 32 * 128 * 4
gqa_kv_size = 2 * 8 * 128 * 4
print(f"标准MHA KV大小: {standard_kv_size} bytes")
print(f"GQA (8 KV头) KV大小: {gqa_kv_size} bytes")
print(f"KV缓存压缩: {standard_kv_size / gqa_kv_size:.1f}x")
```

**MHA vs MQA vs GQA对比**：

| 方法 | Q头数 | KV头数 | KV缓存大小 | 性能 | 推理速度 |
|-----|------|-------|-----------|------|---------|
| MHA | 32 | 32 | 100% | 基线 | 1x |
| MQA | 32 | 1 | 3% | -2% | 1.5x |
| GQA (8头) | 32 | 8 | 25% | -0.5% | 1.3x |

**实际应用**：
- LLaMA2：GQA（8 KV头）
- Falcon：MQA
- Mistral：GQA（8 KV头）

---

### 3.5 本节小结

**核心要点**：

1. **KV缓存收益**：
   - 避免重复计算，加速260x
   - 代价：内存占用（长序列瓶颈）

2. **PagedAttention**：
   - 分页管理KV缓存
   - 内存利用率：~50% → ~100%
   - 支持共享前缀（Beam Search）

3. **MQA/GQA优化**：
   - MQA：所有头共享KV，32x压缩
   - GQA：头分组共享KV，4~8x压缩
   - 精度损失：<1%

4. **实战建议**：
   - 短序列（<512）：标准MHA
   - 长序列（2K+）：GQA + PagedAttention
   - 极致速度：MQA


## 第四节：推理框架与工程实践

### 4.1 主流推理框架对比

#### 框架全景图

```python
@dataclass
class InferenceFramework:
    """推理框架信息"""
    name: str
    org: str
    key_features: List[str]
    throughput_tokens_per_sec: int
    supported_models: List[str]
    
    def __str__(self) -> str:
        return (
            f"{self.name} (by {self.org})\n"
            f"  特性: {', '.join(self.key_features)}\n"
            f"  吞吐: {self.throughput_tokens_per_sec} tokens/s\n"
            f"  支持: {', '.join(self.supported_models[:3])}..."
        )

# 主流框架
frameworks = [
    InferenceFramework(
        name="vLLM",
        org="UC Berkeley",
        key_features=["PagedAttention", "Continuous Batching", "CUDA Graphs"],
        throughput_tokens_per_sec=5000,
        supported_models=["LLaMA", "Mistral", "Yi", "Qwen"]
    ),
    InferenceFramework(
        name="TensorRT-LLM",
        org="NVIDIA",
        key_features=["FP8量化", "In-flight Batching", "多GPU并行"],
        throughput_tokens_per_sec=7000,
        supported_models=["GPT", "LLaMA", "ChatGLM", "Baichuan"]
    ),
    InferenceFramework(
        name="Text Generation Inference (TGI)",
        org="HuggingFace",
        key_features=["Flash Attention", "Tensor Parallelism", "Safetensors"],
        throughput_tokens_per_sec=4500,
        supported_models=["LLaMA", "Falcon", "StarCoder", "BLOOM"]
    ),
    InferenceFramework(
        name="llama.cpp",
        org="Georgi Gerganov",
        key_features=["CPU推理", "量化(GGUF)", "跨平台"],
        throughput_tokens_per_sec=500,  # CPU
        supported_models=["LLaMA", "Mistral", "Phi", "Gemma"]
    ),
]

print("=" * 80)
print("主流LLM推理框架对比")
print("=" * 80)
for fw in frameworks:
    print(f"\n{fw}")
    print("-" * 80)
```

---

### 4.2 vLLM深度解析

#### 核心技术栈

```python
class vLLMEngine:
    """vLLM推理引擎（简化版）"""
    
    def __init__(self,
                 model_path: str,
                 max_num_seqs: int = 256,
                 max_model_len: int = 2048,
                 gpu_memory_utilization: float = 0.9):
        """
        Args:
            model_path: 模型路径
            max_num_seqs: 最大并发序列数
            max_model_len: 最大序列长度
            gpu_memory_utilization: GPU内存利用率
        """
        self.model_path = model_path
        self.max_num_seqs = max_num_seqs
        self.max_model_len = max_model_len
        self.gpu_memory_utilization = gpu_memory_utilization
        
        # 核心组件
        self.scheduler = ContinuousBatchingScheduler(max_num_seqs)
        self.kv_cache = PagedKVCache(
            page_size=16,
            num_pages=self._calculate_num_pages(),
            num_layers=32,
            num_heads=32,
            head_dim=128
        )
        
        # 模型（简化）
        self.model = self._load_model()
    
    def _calculate_num_pages(self) -> int:
        """计算可用页数"""
        # 简化计算：基于GPU内存和利用率
        gpu_mem_gb = 80  # A100
        usable_mem_gb = gpu_mem_gb * self.gpu_memory_utilization
        
        # 每页大小
        page_size_bytes = (
            16 *  # page_size
            32 *  # num_layers
            32 *  # num_heads
            128 *  # head_dim
            2  # FP16
        )
        
        num_pages = int((usable_mem_gb * 1024 ** 3) / page_size_bytes)
        return num_pages
    
    def _load_model(self):
        """加载模型"""
        # 实际实现：加载量化模型、设置device等
        print(f"加载模型: {self.model_path}")
        return None  # Placeholder
    
    def generate(self,
                prompts: List[str],
                max_tokens: int = 100,
                temperature: float = 0.8) -> List[str]:
        """生成文本
        
        核心：Continuous Batching
        """
        # 1. 创建请求
        requests = []
        for i, prompt in enumerate(prompts):
            req = GenerationRequest(
                request_id=i,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            requests.append(req)
        
        # 2. 调度
        self.scheduler.add_requests(requests)
        
        # 3. 循环生成
        completed = []
        while not self.scheduler.all_finished():
            # 选择batch（动态大小）
            batch = self.scheduler.schedule()
            
            # 前向传播
            outputs = self._forward_batch(batch)
            
            # 更新状态
            for req, output in zip(batch, outputs):
                req.append_token(output)
                
                if req.is_finished():
                    completed.append(req)
                    self.scheduler.finish_request(req)
        
        # 4. 返回结果
        results = [req.generated_text for req in completed]
        return results
    
    def _forward_batch(self, batch: List):
        """批次前向传播"""
        # 实际实现：调用模型、使用KV缓存等
        return [torch.randint(0, 50000, (1,)).item() for _ in batch]

@dataclass
class GenerationRequest:
    """生成请求"""
    request_id: int
    prompt: str
    max_tokens: int
    temperature: float
    
    generated_tokens: List[int] = None
    generated_text: str = ""
    
    def __post_init__(self):
        if self.generated_tokens is None:
            self.generated_tokens = []
    
    def append_token(self, token: int):
        """追加token"""
        self.generated_tokens.append(token)
        # 实际需要detokenize
        self.generated_text += f"[{token}]"
    
    def is_finished(self) -> bool:
        """是否完成"""
        return len(self.generated_tokens) >= self.max_tokens

class ContinuousBatchingScheduler:
    """连续批处理调度器"""
    
    def __init__(self, max_num_seqs: int):
        self.max_num_seqs = max_num_seqs
        self.waiting_queue = []
        self.running = []
        self.finished = []
    
    def add_requests(self, requests: List[GenerationRequest]):
        """添加请求"""
        self.waiting_queue.extend(requests)
    
    def schedule(self) -> List[GenerationRequest]:
        """调度下一个batch
        
        核心：动态batch大小
        """
        # 1. 移除已完成的请求
        self.running = [r for r in self.running if not r.is_finished()]
        
        # 2. 从等待队列补充
        available_slots = self.max_num_seqs - len(self.running)
        new_requests = self.waiting_queue[:available_slots]
        self.waiting_queue = self.waiting_queue[available_slots:]
        
        self.running.extend(new_requests)
        
        return self.running
    
    def finish_request(self, request: GenerationRequest):
        """标记请求完成"""
        if request in self.running:
            self.running.remove(request)
        self.finished.append(request)
    
    def all_finished(self) -> bool:
        """所有请求是否完成"""
        return len(self.waiting_queue) == 0 and len(self.running) == 0

# 使用vLLM
engine = vLLMEngine(
    model_path="meta-llama/Llama-2-7b-hf",
    max_num_seqs=256,
    max_model_len=2048,
    gpu_memory_utilization=0.9
)

prompts = [
    "What is the capital of France?",
    "Explain quantum computing.",
    "Write a poem about AI."
]

# outputs = engine.generate(prompts, max_tokens=100)
print("\nvLLM引擎已初始化")
```

**vLLM核心优势**：
- ✅ **PagedAttention**：内存利用率↑
- ✅ **Continuous Batching**：吞吐量↑（无需等待最长序列）
- ✅ **CUDA Graphs**：降低kernel启动开销

---

### 4.3 TensorRT-LLM优化

#### FP8量化 + In-flight Batching

```python
class TensorRTLLMOptimizations:
    """TensorRT-LLM优化技术"""
    
    @staticmethod
    def fp8_quantization_demo():
        """FP8量化演示"""
        print("=" * 80)
        print("FP8量化 (TensorRT-LLM)")
        print("=" * 80)
        
        # FP8 E4M3格式（4位指数，3位尾数）
        # 范围：±448
        
        original = torch.randn(1000) * 10
        
        # 模拟FP8量化
        max_val = 448.0
        fp8_simulated = torch.clamp(original, -max_val, max_val)
        fp8_simulated = torch.round(fp8_simulated * 16) / 16  # 粗粒度量化
        
        error = torch.mean((original - fp8_simulated) ** 2).item()
        
        print(f"原始范围: [{original.min():.2f}, {original.max():.2f}]")
        print(f"FP8范围: [-448, 448]")
        print(f"量化误差 (MSE): {error:.6f}")
        
        # 存储节省
        fp32_size = 1000 * 4  # bytes
        fp16_size = 1000 * 2
        fp8_size = 1000 * 1
        
        print(f"\n存储对比:")
        print(f"  FP32: {fp32_size} bytes")
        print(f"  FP16: {fp16_size} bytes (2x压缩)")
        print(f"  FP8: {fp8_size} bytes (4x压缩)")
        
        print("=" * 80)
    
    @staticmethod
    def inflight_batching_demo():
        """In-flight Batching演示"""
        print("\n" + "=" * 80)
        print("In-flight Batching vs Static Batching")
        print("=" * 80)
        
        # 模拟5个请求，生成长度不同
        requests = [
            {"id": 1, "gen_len": 10},
            {"id": 2, "gen_len": 50},
            {"id": 3, "gen_len": 20},
            {"id": 4, "gen_len": 100},
            {"id": 5, "gen_len": 30},
        ]
        
        # Static Batching：等待最长的完成
        static_total_steps = max(r["gen_len"] for r in requests)
        static_total_tokens = static_total_steps * len(requests)
        
        # In-flight Batching：请求完成即退出
        inflight_total_tokens = sum(r["gen_len"] for r in requests)
        
        speedup = static_total_tokens / inflight_total_tokens
        
        print(f"请求数: {len(requests)}")
        print(f"生成长度: {[r['gen_len'] for r in requests]}")
        
        print(f"\nStatic Batching:")
        print(f"  总步数: {static_total_steps}")
        print(f"  总token数: {static_total_tokens}")
        
        print(f"\nIn-flight Batching:")
        print(f"  总token数: {inflight_total_tokens}")
        
        print(f"\n加速比: {speedup:.2f}x")
        print("=" * 80)

# 运行演示
TensorRTLLMOptimizations.fp8_quantization_demo()
TensorRTLLMOptimizations.inflight_batching_demo()
```

---

### 4.4 生产部署最佳实践

#### 完整部署架构

```python
@dataclass
class DeploymentArchitecture:
    """生产部署架构"""
    
    @staticmethod
    def show_architecture():
        print("=" * 80)
        print("LLM生产部署架构")
        print("=" * 80)
        print("""
┌─────────────────────────────────────────────────────────────┐
│                        Load Balancer                        │
│                    (Nginx / HAProxy)                        │
└──────────────┬──────────────────────────────────────────────┘
               │
               ├─────────────┬─────────────┬─────────────┐
               │             │             │             │
          ┌────▼────┐   ┌────▼────┐   ┌────▼────┐   ┌────▼────┐
          │ API     │   │ API     │   │ API     │   │ API     │
          │ Server  │   │ Server  │   │ Server  │   │ Server  │
          │(FastAPI)│   │(FastAPI)│   │(FastAPI)│   │(FastAPI)│
          └────┬────┘   └────┬────┘   └────┬────┘   └────┬────┘
               │             │             │             │
               └─────────────┴─────────────┴─────────────┘
                                  │
                        ┌─────────▼─────────┐
                        │  Request Queue    │
                        │    (Redis)        │
                        └─────────┬─────────┘
                                  │
                ┌─────────────────┼─────────────────┐
                │                 │                 │
          ┌─────▼─────┐     ┌─────▼─────┐   ┌─────▼─────┐
          │  vLLM     │     │  vLLM     │   │  vLLM     │
          │ Worker    │     │ Worker    │   │ Worker    │
          │ (GPU 0)   │     │ (GPU 1)   │   │ (GPU 2)   │
          └───────────┘     └───────────┘   └───────────┘

监控层:
- Prometheus + Grafana (指标监控)
- ELK Stack (日志分析)
- Jaeger (链路追踪)
        """)
        print("=" * 80)

DeploymentArchitecture.show_architecture()
```

---

#### FastAPI服务示例

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import asyncio

app = FastAPI(title="LLM Inference API")

class GenerationRequest(BaseModel):
    """生成请求"""
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8
    top_p: float = 0.9
    stream: bool = False

class GenerationResponse(BaseModel):
    """生成响应"""
    text: str
    tokens_generated: int
    latency_ms: float

# 全局推理引擎
# inference_engine = vLLMEngine(model_path="meta-llama/Llama-2-7b-hf")

@app.post("/v1/generate", response_model=GenerationResponse)
async def generate(request: GenerationRequest):
    """同步生成"""
    try:
        import time
        start_time = time.time()
        
        # 调用推理引擎
        # output = inference_engine.generate(
        #     prompts=[request.prompt],
        #     max_tokens=request.max_tokens,
        #     temperature=request.temperature
        # )[0]
        
        # 模拟
        output = f"Generated response for: {request.prompt}"
        
        latency_ms = (time.time() - start_time) * 1000
        
        return GenerationResponse(
            text=output,
            tokens_generated=len(output.split()),
            latency_ms=latency_ms
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """健康检查"""
    return {
        "status": "healthy",
        "model": "Llama-2-7b",
        "gpu_memory_used": "45.2 GB / 80 GB"
    }

@app.get("/metrics")
async def metrics():
    """Prometheus指标"""
    return {
        "requests_total": 12345,
        "requests_per_second": 42.5,
        "avg_latency_ms": 125.3,
        "tokens_per_second": 850,
        "gpu_utilization": 0.92
    }

# 启动命令:
# uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4

print("FastAPI服务已定义")
print("启动: uvicorn app:app --host 0.0.0.0 --port 8000")
```

---

#### 性能监控与调优

```python
@dataclass
class PerformanceMetrics:
    """性能指标"""
    requests_per_second: float
    tokens_per_second: float
    avg_latency_ms: float
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    gpu_utilization: float
    gpu_memory_used_gb: float

class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.latencies = []
        self.throughputs = []
    
    def record_request(self, latency_ms: float, tokens_generated: int):
        """记录请求"""
        self.latencies.append(latency_ms)
        self.throughputs.append(tokens_generated)
    
    def get_metrics(self) -> PerformanceMetrics:
        """获取指标"""
        import numpy as np
        
        latencies_np = np.array(self.latencies)
        
        return PerformanceMetrics(
            requests_per_second=len(self.latencies) / 60,  # 假设1分钟窗口
            tokens_per_second=sum(self.throughputs) / 60,
            avg_latency_ms=latencies_np.mean(),
            p50_latency_ms=np.percentile(latencies_np, 50),
            p95_latency_ms=np.percentile(latencies_np, 95),
            p99_latency_ms=np.percentile(latencies_np, 99),
            gpu_utilization=0.85,  # 从nvidia-smi获取
            gpu_memory_used_gb=45.2
        )
    
    def print_report(self):
        """打印报告"""
        metrics = self.get_metrics()
        
        print("=" * 80)
        print("性能报告")
        print("=" * 80)
        print(f"吞吐量:")
        print(f"  请求/秒: {metrics.requests_per_second:.2f}")
        print(f"  Tokens/秒: {metrics.tokens_per_second:.1f}")
        
        print(f"\n延迟:")
        print(f"  平均: {metrics.avg_latency_ms:.2f} ms")
        print(f"  P50: {metrics.p50_latency_ms:.2f} ms")
        print(f"  P95: {metrics.p95_latency_ms:.2f} ms")
        print(f"  P99: {metrics.p99_latency_ms:.2f} ms")
        
        print(f"\nGPU:")
        print(f"  利用率: {metrics.gpu_utilization:.1%}")
        print(f"  显存使用: {metrics.gpu_memory_used_gb:.1f} GB")
        print("=" * 80)

# 使用示例
monitor = PerformanceMonitor()

# 模拟请求
import random
for _ in range(100):
    latency = random.gauss(120, 30)  # 均值120ms，标准差30ms
    tokens = random.randint(50, 150)
    monitor.record_request(latency, tokens)

monitor.print_report()
```

---

### 4.5 本节小结

**核心要点**：

1. **推理框架选择**：
   - **vLLM**：开源首选，PagedAttention + Continuous Batching
   - **TensorRT-LLM**：NVIDIA GPU最优，FP8量化 + In-flight Batching
   - **TGI**：HuggingFace生态，易用性强
   - **llama.cpp**：CPU推理，跨平台部署

2. **核心技术**：
   - **PagedAttention**：内存利用率100%
   - **Continuous Batching**：吞吐量↑30%+
   - **FP8量化**：4x压缩，性能损失<1%
   - **In-flight Batching**：动态batch，无等待

3. **生产部署**：
   - 负载均衡 + 多实例
   - 请求队列（Redis）
   - 监控告警（Prometheus + Grafana）
   - API网关（FastAPI）

4. **性能目标**：
   - 吞吐：2000+ tokens/s (单A100)
   - 延迟：P95 < 150ms
   - GPU利用率：>85%

---

## 第1章小结

### 核心知识回顾

#### 1. 量化技术

**量化公式**：
$$
x_q = \text{round}\left(\frac{x - z}{s}\right), \quad x \approx s \cdot x_q + z
$$

**方法对比**：

| 方法 | 压缩比 | 精度损失 | 推理速度 | 适用场景 |
|-----|-------|---------|---------|---------|
| PTQ动态 | 4x | 中等 | 中等 | 快速部署 |
| PTQ静态 | 4x | 小 | 快 | 生产环境 |
| GPTQ | 8x | <1% | 中等 | 超大模型 |
| AWQ | 8x | <0.5% | 快 | 推荐 |
| QAT | 4-8x | 最小 | 快 | 精度优先 |

---

#### 2. 剪枝与蒸馏

**剪枝策略**：
- 非结构化：精度好，硬件难加速
- 结构化：实际加速，精度损失中等

**蒸馏公式**：
$$
\mathcal{L} = \alpha \cdot \mathcal{L}_{\text{CE}} + (1-\alpha) \cdot \mathcal{L}_{\text{KL}}(\sigma(z_t/T), \sigma(z_s/T))
$$

**组合收益**：
- 蒸馏（70B→7B）+ 量化（FP16→INT8）= **20x压缩**

---

#### 3. KV缓存优化

**技术对比**：

| 技术 | KV缓存大小 | 内存利用率 | 速度提升 |
|-----|-----------|-----------|---------|
| 标准MHA | 100% | ~50% | 基线 |
| MQA | 3% | ~50% | 1.5x |
| GQA (8头) | 25% | ~50% | 1.3x |
| PagedAttention | 100% | ~100% | 1.2x |
| GQA + Paged | 25% | ~100% | **1.6x** |

---

#### 4. 推理框架

**性能对比**（LLaMA2-7B, A100）：

| 框架 | 吞吐(tokens/s) | 延迟P95(ms) | 内存利用率 |
|-----|---------------|------------|-----------|
| HF Transformers | 1200 | 250 | 45% |
| vLLM | 5000 | 100 | 95% |
| TensorRT-LLM | 7000 | 80 | 98% |

---

### 完整优化Pipeline

```
原始模型: LLaMA2-70B (FP32, 260GB)
    ↓
[Stage 1] 量化 (AWQ 4-bit)
    → 70B INT4 (32GB, 8x压缩)
    ↓
[Stage 2] 蒸馏
    → 7B INT4 (4GB, 65x压缩)
    ↓
[Stage 3] 推理优化
    → GQA + PagedAttention
    → vLLM部署
    ↓
最终: 7B模型, 4GB显存, 5000 tokens/s
性能保留: 95%+
```

---

### 思考练习

#### 基础练习

**练习1**：实现INT8对称量化

```python
# 对称量化实现
    # 1. 计算缩放因子 scale
    max_val = torch.max(torch.abs(x))
    scale = 127 / max_val

    # 2. 量化 (float -> int8)
    # clip确保数值在[-128, 127]范围内
    x_quant = torch.clamp(torch.round(x * scale), -128, 127).to(torch.int8)

    # 3. 反量化 (int8 -> float)
    x_dequant = x_quant.to(torch.float32) / scale

    return x_quant, x_dequant, scale
# 输入: FP32 tensor
# 输出: INT8 tensor + scale
# 提示: scale = max(abs(tensor)) / 127
```

**练习2**：计算KV缓存大小

```python
# TODO: 给定模型配置,计算KV缓存大小
# batch_size=32, seq_len=2048, num_layers=32
# num_heads=32, head_dim=128, dtype=FP16
```

**练习3**：对比MHA vs GQA参数量

```python
# TODO: 计算MHA和GQA(8 KV头)的参数量差异
# hidden_size=4096, num_heads=32
```

---

#### 高级练习

**练习4**：实现GPTQ量化

```python
# TODO: 实现简化版GPTQ
# 要求:
# 1. 逐块量化权重
# 2. 最小化重构误差
# 3. 支持4-bit量化
```

**练习5**：实现Continuous Batching调度器

```python
# TODO: 实现动态批处理调度器
# 要求:
# 1. 支持不同长度请求
# 2. 请求完成即退出batch
# 3. 动态补充新请求
```

**练习6**：部署FastAPI推理服务

```python
# TODO: 实现完整推理API
# 要求:
# 1. 加载量化模型
# 2. 支持流式输出
# 3. 添加性能监控
# 4. 实现健康检查
```

---

#### 实战项目

**项目1**：压缩LLaMA2-7B到手机端

- 目标：模型<2GB，延迟<500ms
- 技术栈：量化(GGUF) + llama.cpp
- 平台：iOS / Android

**项目2**：构建高吞吐推理集群

- 目标：10000 tokens/s
- 技术栈：vLLM + TensorRT-LLM
- 部署：Kubernetes + 多GPU

**项目3**：蒸馏+量化Pipeline

- 教师：LLaMA2-70B
- 学生：LLaMA2-7B
- 量化：AWQ 4-bit
- 评估：MMLU、GSM8K

---

### 参考资料

#### 核心论文

1. **GPTQ** (Frantar et al., 2022)
   - [https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)

2. **AWQ** (Lin et al., 2023)
   - [https://arxiv.org/abs/2306.00978](https://arxiv.org/abs/2306.00978)

3. **vLLM: PagedAttention** (Kwon et al., 2023)
   - [https://arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180)

4. **GQA: Grouped-Query Attention** (Ainslie et al., 2023)
   - [https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245)

5. **Knowledge Distillation** (Hinton et al., 2015)
   - [https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)

#### 开源项目

1. **vLLM**
   - [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

2. **TensorRT-LLM**
   - [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)

3. **AutoGPTQ**
   - [https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)

4. **llama.cpp**
   - [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

#### 工具与框架

1. **bitsandbytes**：8-bit/4-bit量化
2. **ONNX Runtime**：跨平台推理
3. **Triton Inference Server**：模型服务

---

### 下一章预告

第2章《分布式训练与并行策略》将深入讲解：

- 数据并行 vs 模型并行 vs 流水线并行
- ZeRO优化器（DeepSpeed）
- Tensor Parallelism（Megatron-LM）
- 3D并行组合策略
- 实战：训练70B+模型

**核心问题**：如何用有限GPU训练超大模型？

---

