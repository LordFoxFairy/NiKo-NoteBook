# 第十一篇:生产部署与评估

> 企业级LLM系统的架构设计、评估理论与运维实践

**适合人群**: 架构师、DevOps工程师、算法工程师
**预计时间**: 10-12小时
**前置知识**: 第六篇(RAG)、第十篇(推理加速)

---

## 本篇概览

整合**部署架构**、**评估理论**、**成本优化**、**监控运维**:
- 私有化部署方案与混合云架构
- 评估指标数学推导(BLEU/ROUGE/BERTScore)
- 成本优化策略(缓存/批处理/蒸馏)
- Prometheus监控与告警

---

## 第1章:评估理论基础

### 1.1 生成质量评估

#### 1.1.1 BLEU (Bilingual Evaluation Understudy)

**核心思想**: n-gram精确匹配

**数学定义**:
$$
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right)
$$

其中:
- $p_n$: n-gram精度
- $w_n$: 权重(通常均匀 $w_n = 1/N$)
- $\text{BP}$: 惩罚过短翻译

**n-gram精度**:
$$
p_n = \frac{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram} \in C} \text{Count}_{\text{clip}}(\text{n-gram})}{\sum_{C \in \text{Candidates}} \sum_{\text{n-gram}' \in C} \text{Count}(\text{n-gram}')}
$$

**Count_clip**: 截断计数,最多为参考答案中的出现次数

**示例**:
```
候选: "the the the cat"
参考: "the cat is on the mat"

1-gram: "the"出现3次,clip到2 → precision=2/4=0.5
```

**Brevity Penalty** (长度惩罚):
$$
\text{BP} = \begin{cases}
1 & \text{if } c > r \\
e^{1-r/c} & \text{if } c \leq r
\end{cases}
$$

- $c$: 候选长度
- $r$: 参考长度

**完整公式** (BLEU-4):
$$
\text{BLEU-4} = \text{BP} \cdot \exp\left(\frac{1}{4}\sum_{n=1}^4 \log p_n\right) = \text{BP} \cdot \sqrt[4]{p_1 \cdot p_2 \cdot p_3 \cdot p_4}
$$

**优缺点**:
- ✅ 快速,可解释
- ❌ 只考虑精确匹配,忽略同义词
- ❌ 无法衡量流畅性

#### 1.1.2 ROUGE (Recall-Oriented Understudy)

**与BLEU对比**: BLEU看精确度,ROUGE看召回率

**ROUGE-N**:
$$
\text{ROUGE-N} = \frac{\sum_{S \in \text{Ref}} \sum_{\text{gram}_n \in S} \text{Count}_{\text{match}}(\text{gram}_n)}{\sum_{S \in \text{Ref}} \sum_{\text{gram}_n \in S} \text{Count}(\text{gram}_n)}
$$

**ROUGE-L** (最长公共子序列):
$$
\text{ROUGE-L} = \frac{(1+\beta^2) \cdot R_{\text{lcs}} \cdot P_{\text{lcs}}}{\beta^2 \cdot P_{\text{lcs}} + R_{\text{lcs}}}
$$

其中:
$$
R_{\text{lcs}} = \frac{\text{LCS}(X, Y)}{|Y|}, \quad P_{\text{lcs}} = \frac{\text{LCS}(X, Y)}{|X|}
$$

**示例**:
```
参考: "the cat sat on the mat"
候选: "the cat on the mat"

LCS = "the cat on the mat" (长度5)
R_lcs = 5/6 = 0.833
P_lcs = 5/5 = 1.0
ROUGE-L = 0.909
```

**适用场景**:
- 摘要任务(召回更重要)
- 信息检索

#### 1.1.3 BERTScore

**动机**: 考虑语义相似度,而非字面匹配!

**核心思想**: 用BERT embedding计算token级相似度

**数学定义**:

对于候选 $\hat{x} = \{\hat{x}_i\}_{i=1}^{|\hat{x}|}$ 和参考 $x = \{x_j\}_{j=1}^{|x|}$:

**召回率**:
$$
R_{\text{BERT}} = \frac{1}{|x|} \sum_{x_j \in x} \max_{\hat{x}_i \in \hat{x}} \frac{E(x_j)^T E(\hat{x}_i)}{\|E(x_j)\| \|E(\hat{x}_i)\|}
$$

**精确度**:
$$
P_{\text{BERT}} = \frac{1}{|\hat{x}|} \sum_{\hat{x}_i \in \hat{x}} \max_{x_j \in x} \text{cos}(E(\hat{x}_i), E(x_j))
$$

**F1分数**:
$$
F_{\text{BERT}} = 2 \cdot \frac{P_{\text{BERT}} \cdot R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}}
$$

**重要性加权** (IDF):
$$
R_{\text{BERT}} = \frac{\sum_{x_j \in x} \text{idf}(x_j) \cdot \max_{\hat{x}_i} \text{cos}(...)}{\sum_{x_j \in x} \text{idf}(x_j)}
$$

降低停用词权重!

**优势**:
- 捕获语义相似("car" vs "automobile")
- 对改写鲁棒

**示例**:
```
参考: "The weather is nice today"
候选A: "Today the weather is good" (改写)
候选B: "asdf qwer zxcv" (无意义)

BLEU(A) = 0.4, BLEU(B) = 0.0
BERTScore(A) = 0.92, BERTScore(B) = 0.15
```

BERTScore正确识别语义匹配!

### 1.2 统计显著性检验

#### 1.2.1 为什么需要?

**问题**: 模型A得分80%,模型B得分82%,真的更好吗?

可能只是随机波动!

**解决**: 统计检验

#### 1.2.2 配对t检验

**假设**:
- $H_0$: 两个模型性能无差异
- $H_1$: 模型B优于模型A

**检验统计量**:
$$
t = \frac{\bar{d}}{s_d / \sqrt{n}}
$$

其中:
- $d_i = \text{score}_B(x_i) - \text{score}_A(x_i)$: 样本差异
- $\bar{d} = \frac{1}{n}\sum d_i$: 平均差异
- $s_d = \sqrt{\frac{1}{n-1}\sum (d_i - \bar{d})^2}$: 标准差

**决策规则**:

若 $p\text{-value} < 0.05$,拒绝 $H_0$,认为差异显著!

**计算流程**:

1. **收集成对样本差异**:
   - 对每个测试样本i,计算差值: $d_i = \text{score}_B(x_i) - \text{score}_A(x_i)$

2. **计算统计量**:
   - 平均差异: $\bar{d} = \frac{1}{n}\sum_{i=1}^n d_i$
   - 标准差: $s_d = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (d_i - \bar{d})^2}$
   - t统计量: $t = \frac{\bar{d}}{s_d / \sqrt{n}}$

3. **查t分布表**:
   - 自由度: $df = n-1$
   - 单尾检验,显著性水平 $\alpha=0.05$

4. **做出决策**:
   - 若 $|t| > t_{\alpha, df}$,拒绝原假设,认为差异显著
   - 或计算p值: $p = P(|T| > |t|)$,若 $p < 0.05$则显著

**示例计算**:
```
样本数n=20
模型A平均分: 0.80
模型B平均分: 0.83
差异均值d̄ = 0.03
标准差sd = 0.015

t = 0.03 / (0.015/√20) = 8.94
自由度df=19, t0.05,19 = 1.729

由于8.94 > 1.729, p < 0.001
结论: 模型B显著优于A
```

#### 1.2.3 Bootstrap置信区间

**方法**: 重采样估计分数分布

**算法步骤**:

1. **重采样**:
   - 从原始样本 $\{x_1, ..., x_n\}$ 中有放回抽取n个样本
   - 重复B次(通常B=10,000)

2. **计算统计量**:
   - 对每个重采样样本,计算均值 $\bar{x}_b^*$
   - 得到B个均值的分布

3. **确定置信区间**:
   - 95%置信区间取第2.5和97.5百分位数
   - $\text{CI}_{0.95} = [\hat{\theta}_{0.025}, \hat{\theta}_{0.975}]$

**数学原理**:

Bootstrap依赖**插入原理**(Plug-in Principle):
$$
\hat{F} \text{ (经验分布)} \rightarrow \text{真实分布 } F
$$

当样本量足够大:
$$
P(\theta \in \text{CI}) \approx 1 - \alpha
$$

**示例**:
```
原始样本: [0.80, 0.75, 0.82, 0.79, 0.81]
重采样1: [0.80, 0.80, 0.79, 0.75, 0.81] → 均值0.79
重采样2: [0.82, 0.79, 0.80, 0.80, 0.81] → 均值0.804
...
重采样10000: [0.75, 0.81, 0.82, 0.82, 0.79] → 均值0.798

10000个均值排序:
第250位(2.5%): 0.768
第9750位(97.5%): 0.812

结论: 95% CI为[0.768, 0.812]
```

**解释**: 95%的把握,真实分数在区间内!

### 1.3 人类评估

#### 1.3.1 Elo评分系统

**来源**: 国际象棋评分

**更新规则**:

模型A vs 模型B,若A获胜:
$$
R_A' = R_A + K \cdot (1 - E_A)
$$
$$
R_B' = R_B + K \cdot (0 - E_B)
$$

其中:
$$
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
$$

- $K$: 学习率(通常32)
- $E_A$: A获胜的期望概率

**示例**:
```
初始: R_A = 1500, R_B = 1600
E_A = 1/(1+10^{100/400}) = 0.36

若A获胜:
R_A' = 1500 + 32*(1-0.36) = 1520
R_B' = 1600 + 32*(0-0.64) = 1580

若B获胜:
R_A' = 1500 + 32*(0-0.36) = 1488
R_B' = 1600 + 32*(1-0.64) = 1611
```

**平台**: 
- Chatbot Arena (LMSYS)
- AlpacaEval

#### 1.3.2 Kappa一致性

**衡量**: 标注员之间的一致性

**公式**:
$$
\kappa = \frac{P_o - P_e}{1 - P_e}
$$

- $P_o$: 观察一致率
- $P_e$: 期望一致率(随机)

**示例**:
```
标注员1: [A, A, B, B, A]
标注员2: [A, B, B, B, A]

P_o = 4/5 = 0.8
P_e = (3/5)^2 + (2/5)^2 = 0.52
κ = (0.8-0.52)/(1-0.52) = 0.58
```

**解释标准**:
- κ < 0.2: 差
- 0.2-0.4: 一般
- 0.4-0.6: 中等
- 0.6-0.8: 好
- 0.8-1.0: 极好

### 1.4 标准基准测试(2025年度)

#### 1.4.1 综合能力评估

##### a) MMLU (Massive Multitask Language Understanding)

**数据集规模**: 15,908道选择题,覆盖57个学科领域

**涵盖领域**:
- STEM: 数学、物理、化学、计算机科学
- 人文: 历史、哲学、法律
- 社会科学: 经济学、心理学、社会学
- 其他: 医学、商业、全球事实

**难度分级**: 初等、中等、高等、专业级

**评估指标**:
$$
\text{MMLU Score} = \frac{\sum_{i=1}^{57} \text{Accuracy}_i}{57} \times 100\%
$$

**2025年性能基准**:

| 模型 | MMLU分数 | 备注 |
|-----|---------|------|
| GPT-4o | 88.7% | OpenAI旗舰模型 |
| Claude 3.7 Opus | 90.1% | **当前SOTA** |
| Gemini 2.5 Pro | 89.5% | Google最新 |
| Llama 4-405B | 87.3% | 开源最强 |
| Qwen3-72B | 86.2% | 中文优化 |

**评估流程设计**:

**1. 数据准备**:
- 从HuggingFace下载MMLU数据集(`cais/mmlu`)
- 选择评估科目(单科或全部57科)
- 划分数据集: 训练集(5 few-shot示例) + 验证集 + 测试集

**2. 提示模板设计**:

```
标准Few-shot格式:

[示例1]
Question: [问题1]
A. [选项A]
B. [选项B]
C. [选项C]
D. [选项D]
Answer: [正确答案]

[示例2-5同上]

[测试问题]
Question: {实际问题}
A. {选项A}
B. {选项B}
C. {选项C}
D. {选项D}
Answer:
```

**3. 推理策略**:

| 方法 | 实现 | 准确率影响 |
|-----|------|----------|
| **直接生成** | 生成单个token(A/B/C/D) | 基准 |
| **对数概率比较** | 计算P(A)、P(B)、P(C)、P(D),取最大 | +2-3% |
| **CoT推理** | 先生成推理过程,再输出答案 | +5-8%(STEM科目) |

**4. 答案提取**:
- 解析模型输出的最后一个token
- 映射: A→0, B→1, C→2, D→3
- 与标准答案比对

**5. 分数计算**:
$$
\text{Accuracy} = \frac{\sum_{i=1}^{N} \mathbb{1}[\text{pred}_i = \text{true}_i]}{N}
$$

**6. 分科统计**:
```
总分 = 平均(各科准确率)

示例输出:
├─ STEM科目: 85.2%
│  ├─ 数学: 82.1%
│  ├─ 物理: 87.3%
│  └─ 计算机: 86.2%
├─ 人文科目: 88.7%
└─ 社会科学: 84.5%

总体MMLU: 86.1%
```

**局限性(2025观察)**:
- ⚠️ **基准饱和**: 顶级模型逼近90%,区分度下降
- ⚠️ **数据污染风险**: 训练数据可能包含测试题
- ⚠️ **提示敏感**: 改变prompt格式可导致±5%波动

**替代方案**: MMLU-Pro(更难的变体,5倍选项,推理链更长)

---

##### b) HellaSwag (常识推理)

**任务**: 给定场景,选择最合理的后续

**示例**:
```
场景: "一个女人正在切洋葱..."
选项:
A. 然后她开始飞起来 ❌
B. 然后她擦了擦眼泪 ✓
C. 然后她变成了恐龙 ❌
D. 然后洋葱开始说话 ❌
```

**数据集**: 70,000个场景

**评估指标**:
$$
\text{HellaSwag} = \frac{\text{正确预测数}}{\text{总题数}} \times 100\%
$$

**2025基准**:
- 人类表现: 95.6%
- GPT-4o: 93.2%
- Claude 3.7: 94.1%

**关键洞察**: 测试常识推理而非死记硬背

---

##### c) TruthfulQA (真实性评估)

**目标**: 检测模型是否会输出常见误解和虚假信息

**示例问题**:
```
Q: "人类大脑只使用了10%,是真的吗?"
错误答案: "是的,我们只用了10%大脑" ❌
正确答案: "这是误解,大脑几乎所有区域都在使用" ✓
```

**数据集**: 817个问题,涵盖健康、法律、阴谋论

**评分**:
$$
\text{TruthfulQA} = \frac{\text{真实且有用的答案}}{\text{总问题数}}
$$

**2025结果**:
| 模型 | 真实性 | 信息量 |
|-----|--------|--------|
| GPT-4o | 83.2% | 89.1% |
| Claude 3.7 | 86.5% | 91.3% |
| Llama 4-70B | 78.9% | 85.2% |

**关键发现**: 模型规模 ≠ 真实性(大模型可能更自信地错误)

---

#### 1.4.2 数学推理评估

##### a) GSM8K (小学数学)

**数据集**: 8,500道小学数学应用题

**示例**:
```
Q: "珍妮有15个苹果,她给了汤姆3个,然后买了8个。她现在有多少苹果?"

答案: 15 - 3 + 8 = 20
```

**评估方法论**:

**1. 答案验证策略**:

| 方法 | 准确性 | 复杂度 | 适用场景 |
|-----|--------|--------|---------|
| **精确匹配** | 低(~60%) | O(1) | 答案格式统一 |
| **数值提取** | 中(~85%) | O(n) | 提取最终数字 |
| **表达式归一化** | 高(~95%) | O(n²) | 处理等价形式 |
| **符号验证** | 极高(~99%) | O(n³) | 使用SymPy验证 |

**2. 答案提取正则**:
```
模式1: "答案是 {数字}"
模式2: "#### {数字}"  # GSM8K标准格式
模式3: 提取最后出现的数字
模式4: 括号内的数字: "(...{数字}...)"
```

**3. 评分流程**:

```
输入: 模型输出文本 + 标准答案

步骤1: 提取模型答案
  - 使用正则匹配"####"后的数字
  - 后备: 提取最后一个数字

步骤2: 归一化
  - 去除千分位符号: 1,000 → 1000
  - 统一小数: 2.0 → 2
  - 处理百分比: 50% → 0.5

步骤3: 比较
  - 允许误差范围: |pred - true| < 0.01

步骤4: 统计
  准确率 = 正确数 / 总题数
```

**4. 2025基准参考**:
```
GPT-4o: 94.2%
Claude 3.7: 95.8%
o1-mini: 96.4% (CoT专门优化)
```

**Chain-of-Thought提示**:
```python
prompt = """
Solve the following problem step by step:

Problem: {question}

Let's think step by step:
1. First, identify what we know
2. Then, set up the equation
3. Finally, calculate the answer

Answer: """
```

**CoT提升**: +15-20% accuracy!

**2025更新**: GSM8K Platinum版本(减少标注噪声)

---

##### b) MATH Benchmark (竞赛数学)

**数据集**: 12,500道竞赛级数学题

**难度**: AMC、AIME、IMO级别

**涵盖领域**:
- 代数(Algebra)
- 几何(Geometry)
- 微积分(Calculus)
- 数论(Number Theory)
- 概率(Probability)

**示例**(难度5/5):
```
Q: "证明:对于任意正整数n,存在n个连续合数"

提示: 考虑 (n+1)!+2, (n+1)!+3, ..., (n+1)!+(n+1)
```

**评估指标**:
$$
\text{MATH Score} = \frac{\text{精确匹配答案数}}{\text{总题数}} \times 100\%
$$

**2025性能**:
| 模型 | MATH分数 | 备注 |
|-----|---------|------|
| Claude 3.7 Opus | 71.2% | **领先** |
| GPT-4o | 68.5% | - |
| Gemini 2.5 Pro | 69.8% | - |
| o1 (preview) | 83.3% | **强化学习优化** |
| 人类数学家 | 90%+ | - |

**关键洞察**: 需要深层推理,非简单模式匹配

**数学答案验证流程**:

**1. 答案提取**:

MATH数据集使用LaTeX格式,标准答案在`\boxed{}`中:
```latex
示例输出:
"解: 设x为未知数...
因此 x = \frac{3}{4}
最终答案是 \boxed{\frac{3}{4}}"

提取正则: r'\\boxed\{(.+?)\}'
结果: "\frac{3}{4}"
```

**2. 表达式归一化**:

| 原始形式 | 归一化后 | 方法 |
|---------|---------|------|
| `\frac{3}{4}` | `0.75` | LaTeX分数→小数 |
| `2.0` | `2` | 去除末尾.0 |
| `\sqrt{2}` | `1.41421356` | 符号→数值 |
| `2\pi` | `6.28318531` | 常数替换 |

**3. 等价性判断**:

```
精确匹配(字符串):
  "\frac{1}{2}" == "\frac{1}{2}" ✓
  "\frac{1}{2}" == "0.5" ✗  (假阴性)

数值比较(浮点):
  |0.5 - 0.5| < ε ✓
  |0.333333 - 1/3| < 0.0001 ✓

符号验证(最优):
  SymPy: simplify(expr1 - expr2) == 0
  示例: (x+1)² - (x²+2x+1) → 0 ✓
```

**4. 特殊情况处理**:

- **多答案**: `\boxed{2, 3, 5}` → 集合匹配
- **区间**: `\boxed{(0, 1)}` → 端点比较
- **矩阵**: LaTeX矩阵 → NumPy数组比对
- **逻辑表达式**: "True/False" → 布尔值

---

#### 1.4.3 代码生成评估

##### a) HumanEval (函数生成)

**数据集**: 164道手工编写的编程题

**任务**: 根据函数签名和文档字符串生成函数体

**示例**:
```python
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """
    检查列表中是否存在两个数字,它们之间的距离小于给定阈值

    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    # 模型生成代码
```

**评估指标**: pass@k

**pass@k定义**:
$$
\text{pass@k} = \mathbb{E}_{Problems}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
$$

其中:
- $n$: 生成的代码样本数
- $c$: 通过测试的样本数
- $k$: 选择的样本数

**直观解释**: 生成k个代码,至少1个通过测试的概率

**2025基准** (pass@1):
| 模型 | HumanEval | 备注 |
|-----|-----------|------|
| GPT-4o | 90.2% | - |
| Claude 3.7 Sonnet | 92.0% | **最强** |
| Llama 4-70B | 81.7% | 开源 |
| DeepSeek-Coder-V2 | 85.5% | 代码专用 |

**代码执行评估流程**:

**1. 测试用例设计**:

HumanEval每道题包含:
```python
{
  "task_id": "HumanEval/0",
  "prompt": "def has_close_elements(...):\n    ...",
  "canonical_solution": "正确实现代码",
  "test": "def check(candidate):\n    assert ...",
  "entry_point": "has_close_elements"
}
```

**2. 沙盒执行策略**:

| 方法 | 安全性 | 性能 | 复杂度 |
|-----|--------|------|--------|
| **临时文件执行** | 低 | 高 | 简单 |
| **Docker容器** | 高 | 中 | 中等 |
| **虚拟机** | 极高 | 低 | 复杂 |
| **在线Judge** | 高 | 中 | 简单 |

**推荐方案**: Docker沙盒
```yaml
超时限制: 5秒
内存限制: 256MB
网络: 禁用
文件系统: 只读
```

**3. 测试执行流程**:

```
步骤1: 拼接代码
  完整代码 = prompt + 模型生成 + 测试用例

步骤2: 写入临时文件
  /tmp/eval_xxxxx.py

步骤3: 执行Python解释器
  subprocess.run(['python', file], timeout=5)

步骤4: 判断结果
  - returncode == 0 → 通过
  - returncode != 0 → 失败(断言错误)
  - 超时 → 失败(死循环/超时)
  - 异常 → 失败(语法/运行时错误)
```

**4. pass@k指标计算**:

**数学定义**:
$$
\text{pass@k} = \mathbb{E}_{Problems}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
$$

**直观理解**:
- 生成n个代码样本
- 其中c个通过测试
- 从n个中随机选k个,至少1个通过的概率

**示例**:
```
生成n=100个样本, c=30个通过

pass@1 = 1 - C(70,1)/C(100,1) = 1 - 70/100 = 0.30

pass@10 = 1 - C(70,10)/C(100,10)
        = 1 - 0.4845 = 0.5155

pass@100 = 1.0 (必定包含至少1个正确)
```

**5. 误差来源**:

- 测试用例不完备(边界未覆盖)
- 超时阈值设置(快速错误算法可能超时)
- 随机性(温度>0时多次运行结果不同)

**提示工程影响**:
```python
# 普通提示
prompt1 = "Complete the following function:\n{code}"

# 优化提示(+20% pass@1)
prompt2 = """
You are an expert Python programmer. Complete this function with:
1. Correct logic
2. Edge case handling
3. Efficient algorithm

{code}

Your code:
"""
```

**关键发现(2025)**: JSON格式提示 → 纯文本格式 = +300% GPT-4性能!

---

##### b) LiveCodeBench (实时编程)

**创新点**: 使用最新竞赛题,避免数据污染

**数据源**:
- LeetCode周赛
- AtCoder比赛
- Codeforces比赛

**题目数**: 1,000+ (持续更新)

**时间标记**: 每道题标注发布日期,确保在模型训练之后

**难度分级**:
- Easy: 30%
- Medium: 50%
- Hard: 20%

**评估维度**:
1. 语法正确性
2. 算法效率
3. 边界处理

**2025发现**:
```
HumanEval性能 ≠ LiveCodeBench性能

示例:
模型A: HumanEval 85%, LiveCodeBench 62% (疑似过拟合)
模型B: HumanEval 82%, LiveCodeBench 79% (泛化良好)
```

**评估方法**:

**1. 数据获取**:
- 访问官网: https://livebench.ai/
- API或下载数据集
- 过滤题目: `publish_date > model_cutoff_date`

**2. 难度分级评估**:

```
评估维度:
├─ Easy (30%题目)
│  ├─ 数组遍历
│  ├─ 字符串处理
│  └─ 简单数学
│
├─ Medium (50%题目)
│  ├─ 双指针/滑动窗口
│  ├─ 二分查找
│  └─ 动态规划(基础)
│
└─ Hard (20%题目)
   ├─ 复杂DP
   ├─ 图算法
   └─ 高级数据结构

分数计算:
  每个难度单独统计通过率
```

**3. 结果输出格式**:
```
模型: gpt-4o
数据集: 2025-01-01后题目 (450道)

Easy:   38/135 → 90.4%
Medium: 162/225 → 72.0%
Hard:   41/90 → 45.6%

加权总分: 0.3×90.4% + 0.5×72.0% + 0.2×45.6% = 72.2%
```

---

#### 1.4.4 多模态评估

##### MMMU (Massive Multi-discipline Multimodal Understanding)

**任务**: 结合图像的多学科问题

**示例**:
```
[图像: 化学分子结构图]
Q: "这个分子的IUPAC命名是什么?"
A: "2,3-二甲基丁烷"
```

**数据集**: 11.5K问题,30个学科,涉及OCR、图表理解、科学推理

**2025性能**:
| 模型 | MMMU分数 |
|-----|---------|
| GPT-4V | 56.8% |
| Gemini Ultra | 59.4% |
| Claude 3.5 Sonnet | 60.1% |
| 人类专家 | 88.6% |

**多模态评估流程**:

**1. 输入处理**:

```
视觉编码器:
  图像 → CLIP/ViT → 视觉特征向量

文本编码器:
  问题 + 选项 → Tokenizer → 文本嵌入

特征融合:
  [图像特征; 文本特征] → 多模态Transformer
```

**2. 提示模板设计**:

```
标准格式:

<image>
Given the image above, answer:

Question: {问题描述}

A. {选项A}
B. {选项B}
C. {选项C}
D. {选项D}

Answer: {模型生成}
```

**3. 答案提取策略**:

| 情况 | 处理方法 | 示例 |
|-----|---------|------|
| **直接输出** | 提取A/B/C/D | "Answer: B" → B |
| **文本推理** | 关键词匹配 | "选项B正确" → B |
| **数值答案** | 正则提取 | "答案是42" → 与选项匹配 |
| **歧义输出** | LLM二次解析 | 用GPT判断选哪个 |

**4. 评估指标**:

```
整体准确率 = 正确数 / 总题数

分学科统计:
├─ 物理: 58.2%
├─ 化学: 61.5%
├─ 生物: 64.3%
├─ 数学: 52.1%
└─ 工程: 55.8%

难度分析:
├─ 需要OCR: 准确率-15%
├─ 需要计算: 准确率-20%
└─ 纯图像理解: 准确率+10%
```

---

#### 1.4.5 评估框架对比

| 框架 | 支持基准 | 易用性 | 自定义 | 开源 |
|-----|---------|--------|--------|------|
| **LM Evaluation Harness** | 200+ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ |
| **DeepEval** | 10+ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ✅ |
| **OpenAI Evals** | 内置 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ |
| **HELM** | 50+ | ⭐⭐ | ⭐⭐ | ✅ |

**推荐组合**:

**1. LM Evaluation Harness (推荐)**:
- 支持200+基准
- 统一接口,易于扩展
- HuggingFace/OpenAI模型均支持

```bash
# 安装
pip install lm-eval

# 运行MMLU
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-4-8B \
    --tasks mmlu \
    --batch_size 8 \
    --output_path results/

# 运行GSM8K
lm_eval --model openai-chat-completions \
    --model_args model=gpt-4o \
    --tasks gsm8k
```

**2. DeepEval (易用性强)**:
- 内置LLM辅助评估
- 适合快速原型
- 支持自定义指标

```bash
pip install deepeval

# 编写测试文件test_eval.py
# 运行评估
deepeval test run test_eval.py
```

**3. OpenAI Evals (官方)**:
- OpenAI模型优化
- YAML配置简洁
- 适合GPT系列

**4. HELM (学术标准)**:
- 斯坦福出品
- 全面评估报告
- 适合研究对比

---

#### 1.4.6 基准测试最佳实践(2025)

**1. 防止数据污染**

```python
# ✓ 使用时间戳过滤
def filter_test_set(dataset, model_cutoff_date):
    """只保留模型训练截止日期之后的数据"""
    return [
        example for example in dataset
        if example['publish_date'] > model_cutoff_date
    ]

# ✓ 使用动态基准(LiveCodeBench, LiveBench)
```

**2. 多维度评估**

```python
evaluation_suite = {
    '通用能力': ['MMLU', 'HellaSwag', 'ARC'],
    '数学推理': ['GSM8K', 'MATH'],
    '代码生成': ['HumanEval', 'MBPP', 'LiveCodeBench'],
    '真实性': ['TruthfulQA'],
    '多模态': ['MMMU', 'VQA']
}
```

**3. 提示标准化**

提示词对评估结果影响巨大(±10%波动),必须标准化:

**方法1: 使用框架内置模板**
```python
# LM Evaluation Harness提供标准模板
# 无需手动编写,自动适配任务
```

**方法2: 自定义模板库**
```yaml
# prompts.yaml
mmlu:
  format: "few_shot"
  n_shots: 5
  template: |
    Answer the following question.

    Question: {question}
    A. {choice_a}
    B. {choice_b}
    C. {choice_c}
    D. {choice_d}

    Answer:

gsm8k:
  format: "chain_of_thought"
  template: |
    Solve step by step:
    {question}

    Solution:
```

**关键原则**:
- 同一基准,同一模板
- 记录模板版本
- A/B测试模板优化

**4. 统计显著性检验**

**问题**: 模型A得分85%, 模型B得分87%, 真的更好吗?

**解决方案**: 独立样本t检验

**检验流程**:

1. **收集多次运行数据**:
   - 模型A: 运行100次, 得分 $\{x_1, ..., x_{100}\}$
   - 模型B: 运行100次, 得分 $\{y_1, ..., y_{100}\}$

2. **计算统计量**:
   $$
   t = \frac{\bar{x} - \bar{y}}{\sqrt{\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}}}
   $$

3. **查表/计算p值**:
   - 自由度: $df \approx n_x + n_y - 2$
   - $p = P(|T| > |t|)$

4. **判断**:
   - $p < 0.05$: 显著差异
   - $p \geq 0.05$: 无显著差异

**示例计算**:
```
模型A: 均值=0.85, 标准差=0.03, n=100
模型B: 均值=0.87, 标准差=0.025, n=100

t = (0.87-0.85) / sqrt(0.03²/100 + 0.025²/100)
  = 0.02 / 0.00391
  = 5.12

p < 0.001 (高度显著)
结论: 模型B确实优于A
```

**5. 成本估算**

| 基准 | 样本数 | 平均tokens/样本 | GPT-4成本 |
|-----|-------|----------------|----------|
| MMLU | 14,042 | 500 | $210 |
| HumanEval | 164 | 300 | $2.5 |
| GSM8K | 1,319 | 400 | $16 |
| **总计** | - | - | **~$230** |

**节省策略**:
```python
# 使用采样评估
subset = random.sample(dataset, k=min(1000, len(dataset)))

# 或使用更便宜的模型预筛选
cheap_results = evaluate(model="gpt-3.5-turbo", subset)
if cheap_results > threshold:
    expensive_results = evaluate(model="gpt-4o", full_dataset)
```

---

**关键要点总结**:

1. **MMLU**: 通用能力金标准,但接近饱和(90%+)
2. **GSM8K vs MATH**: 小学数学vs竞赛数学,MATH更能区分强模型
3. **HumanEval**: 代码基准事实标准,但易过拟合(使用LiveCodeBench验证)
4. **pass@k**: 代码评估专用指标,反映生成多样性
5. **数据污染**: 2025年最大挑战,使用时间过滤和动态基准
6. **提示敏感性**: 同一模型,不同提示可导致±10%波动

**推荐阅读**:
- [LM Evaluation Harness](https://www.analyticsvidhya.com/blog/2025/04/what-are-llm-benchmarks/)
- [DeepEval GSM8K文档](https://deepeval.com/docs/benchmarks-gsm8k)
- [LiveCodeBench官网](https://livebench.ai/)
- [MMLU详解](https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond)
- [HumanEval深度解析](https://www.datacamp.com/tutorial/humaneval-benchmark-for-evaluating-llm-code-generation-capabilities)

---

## 第2章:部署架构

### 2.1 混合云路由

**决策树**:
```python
def route_request(query, context):
    if contains_pii(query):
        return "local_model"  # 隐私敏感
    
    complexity = estimate_complexity(query)
    if complexity == "simple":
        return "local_7b"
    elif complexity == "medium":
        return "local_70b"
    else:
        return "cloud_gpt4"  # 复杂任务
```

**复杂度估计**:
```python
def estimate_complexity(query):
    # 启发式规则
    if len(query.split()) < 20:
        return "simple"
    
    # 使用小模型预测
    complexity_score = classifier(query)
    if complexity_score < 0.3:
        return "simple"
    elif complexity_score < 0.7:
        return "medium"
    else:
        return "complex"
```

### 2.2 硬件选型

| 模型规模 | GPU配置 | 内存 | 吞吐 | 成本/月 |
|---------|--------|-----|------|--------|
| 7B | 1×A10(24GB) | 64GB | 50 QPS | $500 |
| 14B | 2×A10 | 128GB | 30 QPS | $1K |
| 70B | 4×A100(80GB) | 512GB | 10 QPS | $8K |

**计算公式**:

显存需求(FP16):
$$
\text{Memory} = 2N + \text{KV Cache} + \text{Activation}
$$

- $N$: 参数量(bytes)
- KV Cache: $2 \times L \times \text{seq\_len} \times d \times \text{batch}$

**示例**(LLaMA-7B, batch=8, seq=2048):
$$
\text{Mem} = 2 \times 7\text{B} + 2 \times 32 \times 2048 \times 4096 \times 8 \times 2 / 10^9 \approx 14 + 17 = 31\text{GB}
$$

需要40GB显卡!

### 2.3 K8s部署

**HPA (Horizontal Pod Autoscaler)**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: gpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## 第3章:成本优化

### 3.1 语义缓存

**原理**: 相似问题直接返回缓存答案

**实现**:
```python
class SemanticCache:
    def __init__(self, threshold=0.95):
        self.cache = []  # [(embedding, answer)]
        self.threshold = threshold
        
    def get(self, query):
        query_emb = embed(query)
        for cached_emb, answer in self.cache:
            sim = cosine(query_emb, cached_emb)
            if sim > self.threshold:
                return answer
        return None
    
    def set(self, query, answer):
        query_emb = embed(query)
        self.cache.append((query_emb, answer))
```

**命中率优化**:

使用**Locality-Sensitive Hashing** (LSH)加速:
$$
h(v) = \text{sign}(v^T r)
$$

相似向量哈希值相同!

**成本节省**: 缓存命中率30% → 节省30%成本!

### 3.2 批处理

**Continuous Batching** (vLLM):

传统批处理:
```
Batch 1: [req1, req2, req3] → 全部完成才处理Batch 2
```

连续批处理:
```
Batch t: [req1, req3, req5] (req1完成,替换为req4)
Batch t+1: [req4, req3, req5] 
```

**吞吐提升**:
$$
\text{Throughput}_{\text{continuous}} \approx 2-4 \times \text{Throughput}_{\text{static}}
$$

---

## 第4章:监控告警

### 4.1 Prometheus配置

**监控指标体系设计**:

**1. 核心指标(Golden Signals)**:

| 指标类型 | 指标名 | 单位 | 阈值 |
|---------|-------|------|------|
| **延迟** | P50延迟 | 秒 | < 1s |
| | P99延迟 | 秒 | < 3s |
| **流量** | QPS | 请求/秒 | - |
| | Tokens/秒 | tokens/s | - |
| **错误** | 错误率 | % | < 1% |
| | 超时率 | % | < 0.1% |
| **饱和度** | GPU利用率 | % | 60-80% |
| | 队列长度 | 个 | < 100 |

**2. Prometheus指标设计**:

**Counter(计数器)**:
```
llm_requests_total{model="gpt-4", status="success"}
llm_requests_total{model="gpt-4", status="error"}
llm_tokens_total{model="gpt-4", direction="input"}
llm_tokens_total{model="gpt-4", direction="output"}

用途: 计算速率(rate)
查询: rate(llm_requests_total[5m])
```

**Histogram(直方图)**:
```
llm_request_duration_seconds{le="0.1"}  # ≤0.1秒的请求数
llm_request_duration_seconds{le="0.5"}
llm_request_duration_seconds{le="1"}
llm_request_duration_seconds{le="2"}
llm_request_duration_seconds{le="5"}
llm_request_duration_seconds{le="10"}
llm_request_duration_seconds{le="+Inf"}

用途: 计算分位数
查询: histogram_quantile(0.99, ...)
```

**Gauge(仪表盘)**:
```
llm_gpu_utilization{gpu="0"}
llm_queue_length
llm_active_connections

用途: 实时值监控
```

**3. 业务指标**:

```
llm_cache_hit_rate  # 缓存命中率
llm_cost_per_request  # 单次成本
llm_user_satisfaction  # 用户满意度(1-5)
```

**PromQL查询技巧**:

**1. 延迟分位数**:
```promql
# P50延迟
histogram_quantile(0.50,
  rate(llm_request_duration_seconds_bucket[5m])
)

# P99延迟
histogram_quantile(0.99,
  rate(llm_request_duration_seconds_bucket[5m])
)

# 按模型分组
histogram_quantile(0.99,
  sum by (model, le) (
    rate(llm_request_duration_seconds_bucket[5m])
  )
)
```

**2. 错误率计算**:
```promql
# 总错误率
rate(llm_requests_total{status="error"}[5m]) /
rate(llm_requests_total[5m])

# 按错误类型分组
sum by (error_type) (
  rate(llm_requests_total{status="error"}[5m])
)
```

**3. GPU利用率**:
```promql
# 平均利用率
avg(nvidia_gpu_utilization_rate)

# 单卡监控
nvidia_gpu_utilization_rate{gpu="0"}

# 内存使用
nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes
```

**4. 吞吐量**:
```promql
# QPS
rate(llm_requests_total[1m])

# Tokens/秒
rate(llm_tokens_total[1m])
```

**5. 成本监控**:
```promql
# 每秒成本
rate(llm_tokens_total{direction="output"}[5m]) * 0.00003  # GPT-4价格

# 缓存节省
llm_cache_hit_rate * rate(llm_requests_total[5m]) * avg_cost_per_request
```

### 4.2 告警策略设计

**告警分级**:

| 级别 | 响应时间 | 通知方式 | 示例 |
|-----|---------|---------|------|
| **P0(严重)** | 立即 | 电话+短信 | 服务完全不可用 |
| **P1(紧急)** | 15分钟 | 短信+企业微信 | 错误率>5% |
| **P2(重要)** | 1小时 | 企业微信 | P99延迟>5s |
| **P3(警告)** | 次日 | 邮件 | GPU利用率低 |

**告警规则配置** (Prometheus格式):

```yaml
groups:
  - name: llm_critical_alerts
    rules:
      # P0: 服务不可用
      - alert: ServiceDown
        expr: up{job="llm-inference"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "LLM推理服务宕机"
          description: "服务{{ $labels.instance }}无响应超过1分钟"

      # P1: 高错误率
      - alert: HighErrorRate
        expr: |
          rate(llm_requests_total{status="error"}[5m]) /
          rate(llm_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "错误率超过5%"

      # P2: 延迟异常
      - alert: HighP99Latency
        expr: |
          histogram_quantile(0.99,
            rate(llm_request_duration_seconds_bucket[5m])
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "P99延迟超过5秒"

      # P3: GPU利用率低(成本优化信号)
      - alert: LowGPUUtilization
        expr: avg(nvidia_gpu_utilization_rate) < 30
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "GPU利用率低于30%,考虑缩容"

  - name: llm_business_alerts
    rules:
      # 缓存命中率低
      - alert: LowCacheHitRate
        expr: llm_cache_hit_rate < 0.20
        for: 20m
        labels:
          severity: warning
        annotations:
          summary: "缓存命中率低于20%"

      # 成本异常
      - alert: HighCostPerRequest
        expr: |
          rate(llm_tokens_total{direction="output"}[1h]) /
          rate(llm_requests_total[1h]) > 2000
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "平均输出tokens超过2000,成本异常"
```

**告警抑制** (避免告警风暴):
```yaml
inhibit_rules:
  # 服务宕机时,忽略延迟告警
  - source_match:
      alertname: ServiceDown
    target_match:
      alertname: HighP99Latency
    equal: ['instance']
```

**告警路由**:
```yaml
route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'default'

  routes:
    - match:
        severity: critical
      receiver: 'pager'
    - match:
        severity: warning
      receiver: 'wechat'
    - match:
        severity: info
      receiver: 'email'

receivers:
  - name: 'pager'
    pagerduty_configs:
      - service_key: '<key>'
  - name: 'wechat'
    wechat_configs:
      - corp_id: '<id>'
  - name: 'email'
    email_configs:
      - to: 'ops@example.com'
```

---

## 总结

### 评估金字塔

```
自动评估(基础):
├─ BLEU/ROUGE (n-gram匹配)
├─ BERTScore (语义相似度)
└─ Perplexity (困惑度)

人类评估(中层):
├─ Elo评分 (两两对比)
├─ Likert量表 (1-5分)
└─ Kappa一致性

业务指标(顶层):
├─ 用户满意度
├─ 任务完成率
└─ ROI
```

### 部署最佳实践

1. **从小到大**: POC(云API) → 试点(小模型) → 规模化(混合云)
2. **监控先行**: 部署前建立监控体系
3. **A/B测试**: 新模型灰度发布,逐步切量
4. **成本可控**: 缓存 + 批处理 + 模型蒸馏

### 关键成功因素

- **安全第一**: 敏感数据本地处理
- **性能可观测**: Prometheus + Grafana
- **成本优化**: 缓存命中率 > 30%
- **持续改进**: 每周A/B测试新Prompt

---

**推荐工具**:
- 评估: RAGAS, DeepEval, LangSmith
- 部署: K8s, Docker, vLLM
- 监控: Prometheus, Grafana, Jaeger
- 追踪: LangFuse, Phoenix
