# ç¬¬ä¸‰ç¯‡ï¼šå¾®è°ƒæŠ€æœ¯å…¨æ™¯

> ä»ç»å…¸ç›‘ç£å¾®è°ƒåˆ°å¼ºåŒ–å­¦ä¹ å¯¹é½çš„å®Œæ•´ç†è®ºä¸å®è·µ

**é€‚åˆäººç¾¤**: ç®—æ³•å·¥ç¨‹å¸ˆã€ç ”ç©¶äººå‘˜ã€æ·±åº¦å­¦ä¹ å¼€å‘è€…
**é¢„è®¡æ—¶é—´**: 12-15 å°æ—¶
**å‰ç½®çŸ¥è¯†**: ç¬¬ä¸€ç¯‡(Transformeræ¶æ„)ã€ç¬¬äºŒç¯‡(é¢„è®­ç»ƒæŠ€æœ¯)

---

## æœ¬ç¯‡æ¦‚è§ˆ

æœ¬ç¯‡å®Œæ•´è¦†ç›–å¤§æ¨¡å‹å¾®è°ƒçš„ä¸‰å¤§æ ¸å¿ƒé¢†åŸŸï¼š

**Part A: ç»å…¸å¾®è°ƒæ–¹æ³•** (ç¬¬1-3ç« )
- ç›‘ç£å¾®è°ƒ(SFT)ä¸æŒ‡ä»¤å¾®è°ƒ
- å‚æ•°é«˜æ•ˆå¾®è°ƒ(LoRA/QLoRA/Adapter)
- å¾®è°ƒæ•°æ®å·¥ç¨‹ä¸è´¨é‡æ§åˆ¶

**Part B: å¼ºåŒ–å­¦ä¹ åŸºç¡€** (ç¬¬4-6ç« )
- MDPä¸Bellmanæ–¹ç¨‹
- å€¼å‡½æ•°æ–¹æ³•(Q-Learning/DQN)
- ç­–ç•¥æ¢¯åº¦æ–¹æ³•(REINFORCE/PPO)

**Part C: å¯¹é½æŠ€æœ¯** (ç¬¬7-9ç« )
- RLHFä¸‰é˜¶æ®µå®Œæ•´æµç¨‹
- DPO/IPOç­‰ç›´æ¥å¯¹é½æ–¹æ³•
- é•¿æ–‡æœ¬å¤„ç†ä¸æŒç»­å­¦ä¹ 

---

# Part A: ç»å…¸å¾®è°ƒæ–¹æ³•


## ç¬¬1ç« :ç›‘ç£å¾®è°ƒ(SFT)ç†è®ºåŸºç¡€

### 1.1 ä¸ºä»€ä¹ˆé¢„è®­ç»ƒæ¨¡å‹éœ€è¦å¾®è°ƒ?

é¢„è®­ç»ƒå­¦ä¹ çš„æ˜¯è¯­è¨€çš„**é€šç”¨è¡¨ç¤º**,ä½†å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜:

1. **ç›®æ ‡ä¸å¯¹é½**: è¯­è¨€å»ºæ¨¡ â‰  éµå¾ªæŒ‡ä»¤
2. **åˆ†å¸ƒåç§»**: é¢„è®­ç»ƒæ•°æ®(ç½‘é¡µæ–‡æœ¬) â‰  ç›®æ ‡ä»»åŠ¡æ•°æ®(å¯¹è¯ã€ä»£ç )

å¾®è°ƒé€šè¿‡åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šç»§ç»­è®­ç»ƒæ¥è§£å†³è¿™äº›é—®é¢˜:

$$
\theta_{SFT} = \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}_{task}} \left[ -\log P_\theta(y|x) \right]
$$

### 1.2 æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)

æŒ‡ä»¤å¾®è°ƒæ˜¯ç‰¹æ®Šçš„ç›‘ç£å¾®è°ƒå½¢å¼,è®­ç»ƒæ•°æ®æ ¼å¼ä¸º:

$$
\mathcal{D}_{instruct} = \{(\text{instruction}_i, \text{input}_i, \text{output}_i)\}_{i=1}^N
$$

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ?** (æ ¸å¿ƒå‡è®¾)

1. **ä»»åŠ¡æ³›åŒ–**: åœ¨å¤šæ ·åŒ–æŒ‡ä»¤ä¸Šè®­ç»ƒ,å­¦ä¹ "éµå¾ªæŒ‡ä»¤"è¿™ä¸€å…ƒèƒ½åŠ›
2. **å¯¹é½å‡è®¾**: æŒ‡ä»¤æ•°æ®åŒ…å«äººç±»åå¥½çš„éšå¼ä¿¡å·
3. **æ ¼å¼ç»Ÿä¸€**: å°†æ‰€æœ‰ä»»åŠ¡è½¬æ¢ä¸ºæŒ‡ä»¤-å“åº”æ ¼å¼,ç®€åŒ–å­¦ä¹ 

**è®­ç»ƒç›®æ ‡**:

æ„å»ºè®­ç»ƒåºåˆ— $s = [\text{instruction}, \text{input}, \text{output}]$,è®¡ç®—:

$$
\mathcal{L}_{instruct} = -\sum_{t=L_{prompt}+1}^{T} \log P_\theta(s_t | s_{<t})
$$

æ³¨æ„: åªå¯¹ `output` éƒ¨åˆ†è®¡ç®—æŸå¤±,é¿å…æ¨¡å‹å­¦ä¹ é‡å¤æŒ‡ä»¤ã€‚

### 1.3 ç¾éš¾æ€§é—å¿˜é—®é¢˜

å¾®è°ƒæ—¶,æ¨¡å‹å¯èƒ½**é—å¿˜**é¢„è®­ç»ƒçŸ¥è¯†:

$$
\text{Forgetting} = \mathcal{L}_{pretrain}(\theta_{SFT}) - \mathcal{L}_{pretrain}(\theta_{pretrain})
$$

**ç†è®ºè§£é‡Š**:

å‚æ•°ç©ºé—´ä¸­,é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æœ€ä¼˜è§£å¯èƒ½è·ç¦»å¾ˆè¿œ:

$$
\|\theta_{pretrain}^* - \theta_{SFT}^*\| \gg 0
$$

**è§£å†³æ–¹æ¡ˆ**:

1. **æ­£åˆ™åŒ–**: æ·»åŠ KLæ•£åº¦æƒ©ç½š
   $$
   \mathcal{L} = \mathcal{L}_{SFT} + \beta \cdot D_{KL}(P_{\theta} \| P_{\theta_{pretrain}})
   $$

2. **å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)**: å†»ç»“ä¸»ä½“å‚æ•°,åªè®­ç»ƒå°‘é‡æ–°å¢å‚æ•°

3. **æŒç»­å­¦ä¹ **: æ··åˆé¢„è®­ç»ƒå’Œå¾®è°ƒæ•°æ®

---
## ç¬¬2ç« ï¼šå¾®è°ƒè®­ç»ƒæ¡†æ¶å·¥å…·é“¾

> **ä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒæ¡†æ¶ï¼Ÿ** ç›´æ¥ä½¿ç”¨PyTorchç¼–å†™è®­ç»ƒå¾ªç¯ç¹çä¸”æ˜“é”™ï¼Œè®­ç»ƒæ¡†æ¶å°è£…äº†åˆ†å¸ƒå¼ã€æ··åˆç²¾åº¦ã€checkpointingç­‰å¤æ‚é€»è¾‘ï¼Œè®©ä½ ä¸“æ³¨äºæ¨¡å‹å’Œæ•°æ®ã€‚

### æœ¬ç« å¯¼è§ˆ

```
ğŸ“¦ HuggingFaceç”Ÿæ€ (2.1)
â”œâ”€ Transformers Trainer   â­â­â­â­â­ å¿…å­¦åŸºç¡€
â”œâ”€ PEFTåº“(LoRA/QLoRA)    â­â­â­â­â­ é«˜æ•ˆå¾®è°ƒ
â””â”€ Accelerateåˆ†å¸ƒå¼       â­â­â­â­  å¤§è§„æ¨¡è®­ç»ƒ

ğŸš€ ä¸€é”®è®­ç»ƒæ¡†æ¶ (2.2)
â”œâ”€ LlamaFactory          â­â­â­â­â­ æœ€æ¨è,Web UI
â”œâ”€ Axolotl               â­â­â­â­  YAMLé…ç½®é©±åŠ¨
â””â”€ ModelScope-swift      â­â­â­   å›½å†…å‹å¥½

âš™ï¸ ä¸“ä¸šè®­ç»ƒæ¡†æ¶ (2.3)
â”œâ”€ TRL                   â­â­â­â­â­ RLHFå¿…å¤‡(è§Part C)
â”œâ”€ OpenRLHF              â­â­â­â­  å¤§è§„æ¨¡RLHF
â””â”€ vLLMè®­ç»ƒèƒ½åŠ›          â­â­â­   æ¨ç†+è®­ç»ƒä¸€ä½“

ğŸ¯ æ¡†æ¶é€‰å‹ (2.4)           å†³ç­–æ ‘
```

---

## 2.1 HuggingFaceç”Ÿæ€æ ¸å¿ƒå·¥å…·

HuggingFaceæ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„äº‹å®æ ‡å‡†ï¼Œå¿…é¡»æŒæ¡ã€‚

### 2.1.1 Transformers TraineråŸºç¡€

**ä¸ºä»€ä¹ˆå­¦Trainerï¼Ÿ**
- âœ… 90%å¼€æºæ¨¡å‹åŸºäºTransformers
- âœ… è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼ã€æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦
- âœ… ä¸Datasets/PEFTæ— ç¼é›†æˆ
- âœ… çµæ´»å®šåˆ¶ï¼ˆç»§æ‰¿Trainerç±»ï¼‰

#### æ ¸å¿ƒç»„ä»¶

```python
from transformers import (
    Trainer,
    TrainingArguments,
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")
tokenizer.pad_token = tokenizer.eos_token

# 2. å‡†å¤‡æ•°æ®é›†
dataset = load_dataset("json", data_files="data/sft_data.jsonl")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 3. é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qwen_sft",

    # åŸºç¡€å‚æ•°
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,

    # æ¢¯åº¦ç´¯ç§¯(æ¨¡æ‹Ÿå¤§batch size)
    gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch=4Ã—8=32

    # å­¦ä¹ ç‡è°ƒåº¦
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,

    # ä¼˜åŒ–å™¨
    optim="adamw_torch",  # æˆ– "adamw_8bit" èŠ‚çœæ˜¾å­˜
    weight_decay=0.01,

    # æ··åˆç²¾åº¦è®­ç»ƒ
    fp16=True,  # Ampereæ¶æ„ç”¨ bf16=True

    # ä¿å­˜ç­–ç•¥
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,  # æœ€å¤šä¿ç•™3ä¸ªcheckpoint

    # è¯„ä¼°ç­–ç•¥
    evaluation_strategy="steps",
    eval_steps=500,

    # æ—¥å¿—
    logging_steps=100,
    report_to="tensorboard",  # æˆ– "wandb"

    # DeepSpeedé›†æˆ(å¯é€‰)
    # deepspeed="ds_config.json",
)

# 4. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LMä¸ä½¿ç”¨MLM
    ),
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()

# 6. ä¿å­˜æ¨¡å‹
trainer.save_model("./qwen_sft_final")
tokenizer.save_pretrained("./qwen_sft_final")
```

#### TrainingArgumentsæ ¸å¿ƒå‚æ•°é€ŸæŸ¥

| å‚æ•°ç±»åˆ« | å…³é”®å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|---------|---------|--------|------|
| **åŸºç¡€** | `num_train_epochs` | 1-3 | è¿‡å¤šæ˜“è¿‡æ‹Ÿåˆ |
| | `per_device_train_batch_size` | 2-8 | æ ¹æ®æ˜¾å­˜è°ƒæ•´ |
| | `gradient_accumulation_steps` | 4-16 | æœ‰æ•ˆbatch=32-128 |
| **å­¦ä¹ ç‡** | `learning_rate` | 2e-5(å…¨é‡)<br>1e-4(LoRA) | LoRAå¯æ›´å¤§ |
| | `lr_scheduler_type` | `cosine` | æˆ– `linear` |
| | `warmup_ratio` | 0.03-0.1 | ç¨³å®šè®­ç»ƒ |
| **æ˜¾å­˜ä¼˜åŒ–** | `fp16` / `bf16` | `True` | å¿…å¼€,èŠ‚çœ50%æ˜¾å­˜ |
| | `gradient_checkpointing` | `True` | çœæ˜¾å­˜,æ…¢20% |
| | `optim` | `"adamw_8bit"` | çœæ˜¾å­˜,éœ€bitsandbytes |
| **åˆ†å¸ƒå¼** | `ddp_find_unused_parameters` | `False` | LoRAæ—¶è®¾True |
| | `fsdp` | è§æ–‡æ¡£ | å¤§æ¨¡å‹å¿…å¤‡ |

#### è‡ªå®šä¹‰Trainer

**åœºæ™¯ï¼šæ·»åŠ è‡ªå®šä¹‰æŸå¤±å‡½æ•°**

```python
from transformers import Trainer
import torch

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        # æ ‡å‡†äº¤å‰ç†µæŸå¤±
        loss_fct = torch.nn.CrossEntropyLoss()
        loss = loss_fct(
            logits.view(-1, logits.size(-1)),
            labels.view(-1)
        )

        # æ·»åŠ è‡ªå®šä¹‰æ­£åˆ™é¡¹(ä¾‹å¦‚ï¼šé•¿åº¦æƒ©ç½š)
        length_penalty = torch.mean(logits.abs())
        total_loss = loss + 0.01 * length_penalty

        return (total_loss, outputs) if return_outputs else total_loss

# ä½¿ç”¨è‡ªå®šä¹‰Trainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)
trainer.train()
```

---

### 2.1.2 PEFTåº“(LoRA/QLoRA)

**å®‰è£…**:
```bash
pip install peft transformers accelerate bitsandbytes
```

#### å¿«é€Ÿä½¿ç”¨LoRA

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,  # Rank
    lora_alpha=32,  # ç¼©æ”¾å› å­
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # æ³¨æ„åŠ›å±‚
    bias="none",
)

# 3. è½¬æ¢ä¸ºPEFTæ¨¡å‹
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# è¾“å‡º: trainable params: 18,874,368 || all params: 7,615,616,000 || trainable%: 0.25%

# 4. æ­£å¸¸ä½¿ç”¨Trainerè®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()

# 5. ä¿å­˜LoRAæƒé‡(ä»…ä¿å­˜Adapter,å‡ åMB)
model.save_pretrained("./qwen_lora")

# 6. åŠ è½½LoRAæƒé‡
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B")
model = PeftModel.from_pretrained(base_model, "./qwen_lora")
```

#### QLoRA - é‡åŒ–LoRA

**4bité‡åŒ– + LoRAï¼Œæ¶ˆè´¹çº§æ˜¾å¡å¾®è°ƒ70Bæ¨¡å‹**

```python
from transformers import BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

# 1. 4bité‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # åµŒå¥—é‡åŒ–
)

# 2. åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-70B",
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# 3. å‡†å¤‡é‡åŒ–æ¨¡å‹ç”¨äºè®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# 4. æ·»åŠ LoRA(ä¸ä¸Šé¢ç›¸åŒ)
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)

# 5. è®­ç»ƒ(æ˜¾å­˜å ç”¨ï¼š70Bæ¨¡å‹ â‰ˆ 48GB)
trainer = Trainer(model=model, ...)
trainer.train()
```

**æ˜¾å­˜å¯¹æ¯”** (LLaMA-70B):
| æ–¹æ³• | æ˜¾å­˜å ç”¨ | è®­ç»ƒæ—¶é—´ | ç¡¬ä»¶è¦æ±‚ |
|------|---------|---------|---------|
| Full FP16 | ~280GB | 1x | 4Ã—A100 80GB |
| LoRA FP16 | ~160GB | 1.2x | 2Ã—A100 80GB |
| QLoRA 4bit | ~48GB | 1.5x | **1Ã—RTX 4090 24GB** âœ… |

---

### 2.1.3 Accelerateåˆ†å¸ƒå¼è®­ç»ƒ

**ä¸ºä»€ä¹ˆç”¨Accelerateï¼Ÿ**
- âœ… ä¸€è¡Œä»£ç åˆ‡æ¢å•å¡/å¤šå¡/å¤šæœº
- âœ… è‡ªåŠ¨æ··åˆç²¾åº¦
- âœ… DeepSpeed/FSDPé›†æˆ
- âœ… ä¸ä¿®æ”¹ä»£ç å³å¯åˆ†å¸ƒå¼

#### åŸºç¡€ä½¿ç”¨

```bash
# 1. é…ç½®accelerate(é¦–æ¬¡ä½¿ç”¨)
accelerate config
# äº¤äº’å¼é…ç½®ï¼šå•æœºå¤šå¡/å¤šæœº/DeepSpeedç­‰

# 2. å¯åŠ¨è®­ç»ƒ
accelerate launch train.py
```

**train.pyä»£ç **:
```python
from accelerate import Accelerator
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from torch.utils.data import DataLoader

# 1. åˆå§‹åŒ–Accelerator
accelerator = Accelerator(
    mixed_precision="bf16",  # è‡ªåŠ¨æ··åˆç²¾åº¦
    gradient_accumulation_steps=4,
)

# 2. å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B")
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
train_dataloader = DataLoader(dataset, batch_size=4)

# 3. åŒ…è£…æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½å™¨
model, optimizer, train_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader
)

# 4. è®­ç»ƒå¾ªç¯(ä¸å•å¡ä»£ç å®Œå…¨ç›¸åŒ!)
for epoch in range(3):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss

        # Acceleratorè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯å’Œåˆ†å¸ƒå¼
        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()

# 5. ä¿å­˜æ¨¡å‹(è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼)
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained("./model", save_function=accelerator.save)
```

#### DeepSpeedé›†æˆ

**é…ç½®æ–‡ä»¶ `ds_config.json`**:
```json
{
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu"
    }
  },
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 4
}
```

**å¯åŠ¨**:
```bash
accelerate launch --config_file ds_config.json train.py
```

**DeepSpeed ZeROé˜¶æ®µå¯¹æ¯”**:
| ZeROé˜¶æ®µ | ä¼˜åŒ–å†…å®¹ | æ˜¾å­˜èŠ‚çœ | é€šä¿¡å¼€é”€ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|---------|
| **ZeRO-1** | ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ | 4x | ä½ | ä¸­ç­‰æ¨¡å‹ |
| **ZeRO-2** | +æ¢¯åº¦åˆ†ç‰‡ | 8x | ä¸­ | å¤§æ¨¡å‹ |
| **ZeRO-3** | +å‚æ•°åˆ†ç‰‡ | çº¿æ€§ | é«˜ | è¶…å¤§æ¨¡å‹(>70B) |

---

## 2.2 ä¸€é”®è®­ç»ƒæ¡†æ¶

**é€‚åˆäººç¾¤**: å¿«é€ŸéªŒè¯ã€ä¸æƒ³å†™ä»£ç ã€æ‰¹é‡å®éªŒ

### 2.2.1 LlamaFactory â­â­â­â­â­

**ä¸ºä»€ä¹ˆæ¨èï¼Ÿ**
- âœ… **Web UIå‚»ç“œå¼æ“ä½œ**ï¼Œéç¨‹åºå‘˜å¯ç”¨
- âœ… æ”¯æŒ100+æ¨¡å‹(LLaMA/Qwen/GLM/Mistral/...)
- âœ… é›†æˆLoRA/QLoRA/å…¨é‡å¾®è°ƒ/DPO/PPO
- âœ… å†…ç½®50+æ•°æ®é›†ï¼Œæ”¯æŒè‡ªå®šä¹‰
- âœ… ä¸€é”®å¯¼å‡ºä¸ºHuggingFaceæ ¼å¼
- âœ… ä¸­æ–‡æ–‡æ¡£å®Œå–„

#### å®‰è£…ä¸å¯åŠ¨

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# å®‰è£…ä¾èµ–
pip install -e '.[torch,metrics]'

# å¯åŠ¨Web UI
llamafactory-cli webui
# è®¿é—® http://localhost:7860
```

#### Web UIæ“ä½œæµç¨‹

**1. é€‰æ‹©æ¨¡å‹**:
- æ¨¡å‹åç§°: `Qwen/Qwen2.5-7B-Instruct`
- æ¨¡å‹è·¯å¾„: è‡ªåŠ¨ä»HuggingFaceä¸‹è½½
- Quantization: é€‰æ‹© `4-bit` (QLoRA)

**2. æ•°æ®é›†é…ç½®**:
- ä½¿ç”¨å†…ç½®æ•°æ®é›†: `alpaca_zh` (ä¸­æ–‡Alpaca)
- æˆ–ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®(æ”¯æŒJSON/JSONL/CSV)

**3. è®­ç»ƒå‚æ•°**:
```yaml
# LoRAé…ç½®
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,v_proj

# è®­ç»ƒé…ç½®
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 1e-4
lr_scheduler_type: cosine
```

**4. å¯åŠ¨è®­ç»ƒ**:
- ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"
- å®æ—¶æŸ¥çœ‹æŸå¤±æ›²çº¿
- TensorBoardå¯è§†åŒ–

**5. å¯¼å‡ºæ¨¡å‹**:
- ç‚¹å‡»"å¯¼å‡ºæ¨¡å‹"
- é€‰æ‹©"Merge LoRA"(åˆå¹¶ä¸ºå®Œæ•´æ¨¡å‹)
- ä¿å­˜è·¯å¾„: `./output/qwen_sft`

#### YAMLé…ç½®æ–‡ä»¶è®­ç»ƒ(è¿›é˜¶)

**é…ç½®æ–‡ä»¶ `train_qwen.yaml`**:
```yaml
### Model
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
quantization_bit: 4  # QLoRA

### Method
stage: sft  # æˆ– dpo / ppo
finetuning_type: lora  # æˆ– full

### Dataset
dataset: alpaca_zh,belle_2m_cn
template: qwen
cutoff_len: 1024
max_samples: 10000

### LoRA
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,v_proj,k_proj,o_proj
lora_dropout: 0.05

### Training
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 1e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: true

### Output
output_dir: ./saves/qwen_lora_sft
logging_steps: 10
save_steps: 500
```

**å¯åŠ¨è®­ç»ƒ**:
```bash
llamafactory-cli train train_qwen.yaml
```

#### æ”¯æŒçš„æ¨¡å‹(2025æœ€æ–°)

**å›½å†…æ¨¡å‹**:
- âœ… Qwen/Qwen2.5ç³»åˆ—(0.5B~72B)
- âœ… GLM-4(æ™ºè°±)
- âœ… Baichuan2(ç™¾å·)
- âœ… DeepSeek-V3(MoE)
- âœ… Yi-1.5(é›¶ä¸€ä¸‡ç‰©)

**å›½å¤–æ¨¡å‹**:
- âœ… LLaMA-3.1(Meta)
- âœ… Mistral/Mixtral(Mistral AI)
- âœ… Phi-4(Microsoft)
- âœ… Gemma-2(Google)

#### å®æˆ˜æ¡ˆä¾‹ï¼šå¾®è°ƒQwen2.5-7Bç”¨äºå®¢æœ

```bash
# 1. å‡†å¤‡æ•°æ®(JSONLæ ¼å¼)
# data/customer_service.jsonl
{"instruction": "å¦‚ä½•é€€è´§?", "output": "æ‚¨å¯ä»¥åœ¨è®¢å•è¯¦æƒ…é¡µç‚¹å‡»ç”³è¯·é€€è´§..."}
{"instruction": "ç‰©æµæŸ¥è¯¢", "output": "è¯·æä¾›è®¢å•å·,æˆ‘å¸®æ‚¨æŸ¥è¯¢..."}

# 2. æ³¨å†Œæ•°æ®é›†
# data/dataset_info.json
{
  "customer_service": {
    "file_name": "customer_service.jsonl",
    "columns": {
      "prompt": "instruction",
      "response": "output"
    }
  }
}

# 3. Web UIè®­ç»ƒ
# æ•°æ®é›†é€‰æ‹©: customer_service
# æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct
# LoRA rank: 16
# Epochs: 3
# å¯åŠ¨è®­ç»ƒ

# 4. å¯¼å‡ºæ¨¡å‹
# åˆå¹¶LoRA â†’ ./output/qwen_customer_service
```

---

### 2.2.2 Axolotl â­â­â­â­

**ç‰¹ç‚¹**:
- YAMLé…ç½®é©±åŠ¨,çµæ´»åº¦é«˜
- æ”¯æŒå…¨é‡/LoRA/QLoRA/FSDP
- é›†æˆFlash Attention 2
- ç¤¾åŒºæ´»è·ƒ,æ–‡æ¡£è¯¦ç»†

#### å®‰è£…

```bash
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip install -e '.[flash-attn,deepspeed]'
```

#### é…ç½®æ–‡ä»¶ç¤ºä¾‹

**`config/llama3_lora.yml`**:
```yaml
# Model
base_model: meta-llama/Llama-3.1-8B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# LoRA
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# Dataset
datasets:
  - path: mhenrichsen/alpaca_2k_test
    type: alpaca

# Training
output_dir: ./llama3_lora_out
sequence_len: 2048
sample_packing: true  # æ‰“åŒ…çŸ­åºåˆ—,æå‡æ•ˆç‡

num_epochs: 3
micro_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 2e-4
lr_scheduler: cosine
warmup_steps: 100

# Optimization
bf16: auto
flash_attention: true  # Flash Attention 2åŠ é€Ÿ
gradient_checkpointing: true

# Logging
logging_steps: 10
save_steps: 500
eval_steps: 500
```

#### å¯åŠ¨è®­ç»ƒ

```bash
# é¢„å¤„ç†(å¯é€‰,åŠ é€Ÿè®­ç»ƒ)
python -m axolotl.cli.preprocess config/llama3_lora.yml

# è®­ç»ƒ
accelerate launch -m axolotl.cli.train config/llama3_lora.yml

# æ¨ç†æµ‹è¯•
accelerate launch -m axolotl.cli.inference config/llama3_lora.yml \
    --lora_model_dir ./llama3_lora_out
```

#### ä¼˜åŠ¿

- âœ… **sample_packing**: è‡ªåŠ¨æ‰“åŒ…çŸ­åºåˆ—,å‡å°‘paddingæµªè´¹
- âœ… **Flash Attention 2**: è®­ç»ƒé€Ÿåº¦æå‡2-3å€
- âœ… **FSDPæ”¯æŒ**: è½»æ¾è®­ç»ƒ70B+æ¨¡å‹
- âŒ éœ€è¦ç¼–å†™YAML,å­¦ä¹ æ›²çº¿ç¨é™¡

---

### 2.2.3 ModelScope-swift â­â­â­

**ç‰¹ç‚¹**:
- é˜¿é‡Œè¾¾æ‘©é™¢å¼€æº
- å›½å†…æ¨¡å‹æ”¯æŒæœ€å…¨(Qwen/é€šä¹‰åƒé—®/ç™¾å·ç­‰)
- ä¸­æ–‡æ–‡æ¡£å®Œå–„
- ä¸ModelScopeæ¨¡å‹åº“æ·±åº¦é›†æˆ

#### å®‰è£…

```bash
pip install ms-swift[llm] -U
```

#### ä¸€é”®å¾®è°ƒ

```bash
# å¾®è°ƒQwen2.5-7B (è‡ªåŠ¨ä¸‹è½½æ¨¡å‹)
swift sft \
    --model_type qwen2_5-7b-instruct \
    --dataset alpaca-zh \
    --num_train_epochs 3 \
    --lora_rank 16 \
    --lora_alpha 32 \
    --learning_rate 1e-4 \
    --output_dir output/qwen_sft

# æ¨ç†æµ‹è¯•
swift infer \
    --ckpt_dir output/qwen_sft/vx-xxx/checkpoint-xxx \
    --load_dataset_config true
```

#### æ”¯æŒçš„æ•°æ®é›†

**å†…ç½®ä¸­æ–‡æ•°æ®é›†**:
- `alpaca-zh`: ä¸­æ–‡Alpaca
- `belle-2m-cn`: ç™¾ä¸‡çº§ä¸­æ–‡æŒ‡ä»¤
- `firefly-all-zh`: ä¸­æ–‡å¯¹è¯
- `medical-zh`: åŒ»ç–—é—®ç­”
- `code-python`: Pythonä»£ç ç”Ÿæˆ

#### Web UI

```bash
swift web-ui
# è®¿é—® http://localhost:7860
```

---

## 2.3 ä¸“ä¸šè®­ç»ƒæ¡†æ¶

### 2.3.1 TRL (Transformer Reinforcement Learning)

**å·²åœ¨Part Cè¯¦ç»†ä»‹ç»,è¿™é‡Œç®€è¦æ€»ç»“**

**æ ¸å¿ƒTrainer**:
- `SFTTrainer`: ç›‘ç£å¾®è°ƒ
- `RewardTrainer`: å¥–åŠ±æ¨¡å‹è®­ç»ƒ
- `PPOTrainer`: PPOå¼ºåŒ–å­¦ä¹ 
- `DPOTrainer`: ç›´æ¥åå¥½ä¼˜åŒ–
- `GRPOTrainer`: GRPO(DeepSeek-R1åŒæ¬¾)

**é€‚ç”¨åœºæ™¯**:
- RLHFå®Œæ•´æµç¨‹
- åå¥½å¯¹é½
- å¼ºåŒ–å­¦ä¹ å¾®è°ƒ

**è¯¦è§**: ç¬¬17ç«  RLHFå®Œæ•´ç†è®º

---

### 2.3.2 OpenRLHF

**ç‰¹ç‚¹**:
- å¤§è§„æ¨¡åˆ†å¸ƒå¼RLHF
- Ray + vLLMåŠ é€Ÿ
- æ”¯æŒ175B+æ¨¡å‹

**å®‰è£…**:
```bash
pip install openrlhf
```

**è®­ç»ƒå‘½ä»¤**:
```bash
# DPOè®­ç»ƒ
ray job submit --working-dir . -- \
    python -m openrlhf.cli.train_dpo \
    --pretrain meta-llama/Llama-3.1-70B \
    --dataset Anthropic/hh-rlhf \
    --num_episodes 1000 \
    --zero_stage 3
```

**é€‚ç”¨åœºæ™¯**:
- å¤§è§„æ¨¡RLHF(70B+æ¨¡å‹)
- å¤šæœºå¤šå¡è®­ç»ƒ
- ç”Ÿäº§çº§å¯¹é½

---

### 2.3.3 vLLMè®­ç»ƒèƒ½åŠ›

**vLLMä¸»è¦ç”¨äºæ¨ç†,ä½†æ–°å¢äº†è½»é‡çº§è®­ç»ƒæ¥å£**

```python
from vllm import LLM
from vllm.lora import LoRARequest

# åŠ è½½åŸºç¡€æ¨¡å‹
llm = LLM(model="Qwen/Qwen2.5-7B")

# åŠ è½½LoRAé€‚é…å™¨(è®­ç»ƒåçš„)
lora_request = LoRARequest("customer_service", 1, "./lora_weights")

# æ¨ç†æ—¶åŠ¨æ€åˆ‡æ¢LoRA
outputs = llm.generate(
    "å¦‚ä½•é€€è´§?",
    lora_request=lora_request
)
```

**é€‚ç”¨åœºæ™¯**:
- æ¨ç†+è®­ç»ƒä¸€ä½“åŒ–
- å¤šLoRAåŠ¨æ€åˆ‡æ¢
- ç”Ÿäº§éƒ¨ç½²

---

## 2.4 æ¡†æ¶é€‰å‹å†³ç­–æ ‘

```
å¼€å§‹é€‰æ‹©è®­ç»ƒæ¡†æ¶
â”‚
â”œâ”€ ä½ æ˜¯å¦éœ€è¦å†™ä»£ç ?
â”‚   â”‚
â”‚   â”œâ”€ No(å¿«é€ŸéªŒè¯,æ‰¹é‡å®éªŒ)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€ ä½¿ç”¨Web UI? â†’ LlamaFactory â­â­â­â­â­
â”‚   â”‚   â”‚                (æœ€æ¨è,å‚»ç“œå¼æ“ä½œ)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€ YAMLé…ç½®? â†’ Axolotl â­â­â­â­
â”‚   â”‚   â”‚              (çµæ´»åº¦é«˜,Flash Attention)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€ å›½å†…æ¨¡å‹? â†’ ModelScope-swift â­â­â­
â”‚   â”‚                   (é˜¿é‡Œå®˜æ–¹,æ–‡æ¡£ä¸­æ–‡)
â”‚   â”‚
â”‚   â””â”€ Yes(å®šåˆ¶åŒ–éœ€æ±‚,æ·±åº¦æ§åˆ¶)
â”‚       â”‚
â”‚       â”œâ”€ ç®€å•SFT/LoRA?
â”‚       â”‚   â””â”€ HuggingFace Transformers Trainer â­â­â­â­â­
â”‚       â”‚      (å¿…å­¦åŸºç¡€,ç”Ÿæ€æœ€å®Œå–„)
â”‚       â”‚
â”‚       â”œâ”€ RLHF/åå¥½å¯¹é½?
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ å°è§„æ¨¡(<70B) â†’ TRL â­â­â­â­â­
â”‚       â”‚   â”‚                  (SFTTrainer/DPOTrainer/PPOTrainer)
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€ å¤§è§„æ¨¡(70B+) â†’ OpenRLHF â­â­â­â­
â”‚       â”‚                       (Rayåˆ†å¸ƒå¼,vLLMåŠ é€Ÿ)
â”‚       â”‚
â”‚       â””â”€ åˆ†å¸ƒå¼è®­ç»ƒ?
â”‚           â””â”€ Accelerate + DeepSpeed â­â­â­â­â­
â”‚              (æ— ç¼å¤šå¡/å¤šæœº)
```

### åœºæ™¯åŒ–æ¨è

| ä½¿ç”¨åœºæ™¯ | æ¨èæ¡†æ¶ | ç†ç”± |
|---------|---------|------|
| **å…¥é—¨å­¦ä¹ ** | LlamaFactory Web UI | é›¶ä»£ç ,å¿«é€Ÿä¸Šæ‰‹ |
| **ç§‘ç ”å®éªŒ** | Axolotl | YAMLçµæ´»é…ç½®,Flash Attention |
| **ä¼ä¸šç”Ÿäº§(å°æ¨¡å‹)** | Transformers Trainer + PEFT | ç¨³å®šå¯é ,æ˜“äºç»´æŠ¤ |
| **ä¼ä¸šç”Ÿäº§(å¤§æ¨¡å‹)** | Accelerate + DeepSpeed | åˆ†å¸ƒå¼å¿…å¤‡ |
| **RLHFå¯¹é½** | TRL (å°è§„æ¨¡)<br>OpenRLHF (å¤§è§„æ¨¡) | ä¸“ä¸šå·¥å…· |
| **æ‰¹é‡å¾®è°ƒ** | LlamaFactory CLI | YAMLæ‰¹é‡æäº¤ |
| **å›½å†…ç½‘ç»œå—é™** | ModelScope-swift | å›½å†…é•œåƒåŠ é€Ÿ |

### æ˜¾å­˜éœ€æ±‚å¯¹æ¯”

**å¾®è°ƒLLaMA-7B (batch_size=4, seq_len=2048)**:

| æ–¹æ³• | æ¡†æ¶ | æ˜¾å­˜å ç”¨ | ç¡¬ä»¶è¦æ±‚ |
|------|------|---------|---------|
| Full FP16 | Transformers | ~60GB | A100 80GB |
| LoRA FP16 | PEFT | ~24GB | RTX 3090/4090 |
| QLoRA 4bit | PEFT + BitsAndBytes | ~9GB | **RTX 3080 10GB** âœ… |
| DeepSpeed ZeRO-3 | Accelerate + DS | ~16GB(per GPU) | 4Ã—RTX 3090 |

### è®­ç»ƒé€Ÿåº¦å¯¹æ¯”

**LLaMA-7B, 1000æ­¥, å•æœº4Ã—A100**:

| æ¡†æ¶ | ä¼˜åŒ– | è®­ç»ƒæ—¶é—´ | ç›¸å¯¹é€Ÿåº¦ |
|------|------|---------|---------|
| Transformers (baseline) | æ—  | 120min | 1.0x |
| + Flash Attention 2 | Axolotl | 60min | **2.0x** |
| + DeepSpeed ZeRO-2 | Accelerate | 80min | 1.5x |
| + FSDP | Transformers | 75min | 1.6x |

---

## 2.5 å®æˆ˜å»ºè®®

### åˆå­¦è€…è·¯å¾„

```bash
Week 1: HuggingFace TransformersåŸºç¡€
â”œâ”€ å­¦ä¹ Trainer API
â”œâ”€ å®Œæˆä¸€æ¬¡SFTè®­ç»ƒ
â””â”€ ç†è§£TrainingArgumentså‚æ•°

Week 2: PEFTé«˜æ•ˆå¾®è°ƒ
â”œâ”€ æŒæ¡LoRAåŸç†(è§ç¬¬3ç« )
â”œâ”€ å®è·µQLoRAè®­ç»ƒ
â””â”€ å¯¹æ¯”å…¨é‡å¾®è°ƒvs LoRA

Week 3: ä¸€é”®æ¡†æ¶
â”œâ”€ ä½¿ç”¨LlamaFactory Web UI
â”œâ”€ å°è¯•å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†
â””â”€ å¯¼å‡ºæ¨¡å‹ç”¨äºç”Ÿäº§

Week 4: è¿›é˜¶åˆ†å¸ƒå¼
â”œâ”€ å­¦ä¹ Accelerate
â”œâ”€ DeepSpeed ZeROé…ç½®
â””â”€ å¤šå¡è®­ç»ƒå®æˆ˜
```

### å¸¸è§é—®é¢˜

**Q1: LlamaFactory vs Axolotlå¦‚ä½•é€‰æ‹©ï¼Ÿ**
- **LlamaFactory**: Web UIå‹å¥½,å¿«é€ŸéªŒè¯,ä¸­æ–‡æ–‡æ¡£
- **Axolotl**: æ›´çµæ´»,Flash AttentionåŠ é€Ÿ,ç¤¾åŒºæ´»è·ƒ
- **å»ºè®®**: å…¥é—¨ç”¨LlamaFactory,è¿›é˜¶ç”¨Axolotl

**Q2: å¿…é¡»å­¦HuggingFace Transformerså—ï¼Ÿ**
- **å¿…é¡»å­¦ï¼** è¿™æ˜¯åŸºç¡€,æ‰€æœ‰æ¡†æ¶åº•å±‚éƒ½åŸºäºå®ƒ
- å³ä½¿ç”¨LlamaFactory,ç†è§£Transformersèƒ½æ›´å¥½è°ƒå‚

**Q3: DeepSpeed vs FSDPå¦‚ä½•é€‰æ‹©ï¼Ÿ**
- **DeepSpeed**: å¾®è½¯å¼€æº,ZeROç³»åˆ—æˆç†Ÿ,Optimumé›†æˆå¥½
- **FSDP**: PyTorchåŸç”Ÿ,æ›´ç®€æ´,Metaä¸»æ¨
- **å»ºè®®**: æ–°æ‰‹ç”¨DeepSpeed(æ–‡æ¡£å¤š),ç†Ÿæ‰‹ä¸¤è€…éƒ½å¯

**Q4: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ**
ä¼˜å…ˆçº§:
1. **QLoRA** (æœ€æœ‰æ•ˆ,é™ä½70%æ˜¾å­˜)
2. **Gradient Checkpointing** (é™ä½30-40%)
3. **å‡å°batch size + å¢å¤§accumulation**
4. **DeepSpeed ZeRO-3 Offload** (CPUå¸è½½)

---

## æœ¬ç« æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **HuggingFaceæ˜¯åŸºç¡€**: Transformers/PEFT/Accelerateå¿…å­¦
2. **ä¸€é”®æ¡†æ¶æ˜¯åˆ©å™¨**: LlamaFactory/Axolotlå¤§å¹…æå‡æ•ˆç‡
3. **QLoRAæ˜¯ç¥å™¨**: æ¶ˆè´¹çº§æ˜¾å¡å¾®è°ƒ70Bæ¨¡å‹
4. **é€‰æ¡†æ¶çœ‹åœºæ™¯**: å¿«é€ŸéªŒè¯â†’LlamaFactory, å®šåˆ¶â†’Transformers, RLHFâ†’TRL

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½ç”¨Transformers Trainerå®ŒæˆSFTè®­ç»ƒ
- [ ] ç†è§£TrainingArgumentsæ ¸å¿ƒå‚æ•°
- [ ] æŒæ¡LoRA/QLoRAä½¿ç”¨
- [ ] ä¼šç”¨LlamaFactory Web UIå¾®è°ƒæ¨¡å‹
- [ ] äº†è§£Accelerateåˆ†å¸ƒå¼è®­ç»ƒ
- [ ] èƒ½æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚æ¡†æ¶

### ä¸‹ä¸€ç« é¢„å‘Š

**ç¬¬3ç« : LoRAæ·±åº¦è§£æ** - æ·±å…¥LoRAæ•°å­¦åŸç†ã€ç§©çš„é€‰æ‹©ã€åˆå¹¶ç­–ç•¥ç­‰

---

**å‚è€ƒèµ„æº**:
- HuggingFace Transformers: https://huggingface.co/docs/transformers
- LlamaFactory: https://github.com/hiyouga/LLaMA-Factory
- Axolotl: https://github.com/OpenAccess-AI-Collective/axolotl
- TRL: https://github.com/huggingface/trl
- PEFT: https://github.com/huggingface/peft

## ç¬¬3ç« :LoRAæ·±åº¦è§£æ

### 2.1 ä½ç§©å‡è®¾çš„æ•°å­¦åŸºç¡€

#### 2.1.1 æ ¸å¿ƒå‡è®¾

LoRAåŸºäºå…³é”®å‡è®¾: å¾®è°ƒæ—¶æ¨¡å‹å‚æ•°çš„**æ›´æ–°çŸ©é˜µæ˜¯ä½ç§©çš„**:

$$
\Delta W = W_{SFT} - W_{pretrain}
$$

å‡è®¾ $\text{rank}(\Delta W) \ll \min(d_{in}, d_{out})$

**ä¸ºä»€ä¹ˆè¿™ä¸ªå‡è®¾æˆç«‹?**

1. **å†…åœ¨ç»´åº¦ç†è®º** (Li et al. 2018):

ä¸‹æ¸¸ä»»åŠ¡çš„å¤æ‚åº¦è¿œå°äºé¢„è®­ç»ƒä»»åŠ¡:
$$
\text{dim}(\text{task space}) \ll \text{dim}(\text{pretraining space})
$$

å®éªŒæµ‹é‡: ä»»åŠ¡é€‚é…åªéœ€è°ƒæ•´å‚æ•°ç©ºé—´çš„**ä½ç»´å­æµå½¢**ã€‚

2. **å¥‡å¼‚å€¼åˆ†è§£éªŒè¯**:

å¯¹å¾®è°ƒå‰åçš„æƒé‡å˜åŒ– $\Delta W$ è¿›è¡ŒSVD:
$$
\Delta W = U \Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T
$$

**ç»éªŒè§‚å¯Ÿ** (Aghajanyan et al. 2021):
$$
\sum_{i=1}^{r_{eff}} \sigma_i^2 / \sum_{i=1}^{d} \sigma_i^2 > 0.99, \quad r_{eff} \ll d
$$

å‰ $r_{eff} \approx 8$ ä¸ªå¥‡å¼‚å€¼æ•è·äº†99%çš„èƒ½é‡!

3. **ä¿¡æ¯ç“¶é¢ˆåŸç†**:

ä»»åŠ¡ç‰¹å®šä¿¡æ¯ $I(X; Y | \text{task})$ å¯ä»¥é€šè¿‡ä½ç»´è¡¨ç¤ºä¼ é€’:
$$
I(X; Y) \leq I(Z; Y) + \epsilon, \quad \dim(Z) \ll \dim(X)
$$

**ç†è®ºè¯æ˜è‰å›¾**:

è®¾é¢„è®­ç»ƒæƒé‡ä¸º $W_0$,å¾®è°ƒä¼˜åŒ–é—®é¢˜:
$$
\min_{\Delta W} \mathcal{L}_{\text{task}}(W_0 + \Delta W)
$$

Taylorå±•å¼€æŸå¤±å‡½æ•°:
$$
\mathcal{L}(W_0 + \Delta W) \approx \mathcal{L}(W_0) + \text{tr}(\nabla_W \mathcal{L} \cdot \Delta W) + \frac{1}{2}\text{tr}(\Delta W^T H \Delta W)
$$

å…¶ä¸­ $H$ ä¸ºHessiançŸ©é˜µã€‚

å¦‚æœ $H$ çš„å¤§éƒ¨åˆ†ç‰¹å¾å€¼æ¥è¿‘0(ä½ç§©),åˆ™æœ€ä¼˜ $\Delta W$ ä¹Ÿæ˜¯ä½ç§©!

å®éªŒæ˜¾ç¤º: $\text{rank}(H) \approx 10-100$ (å¯¹äº7Bæ¨¡å‹),è¿œå°äº $d \approx 4096$ã€‚

### 2.2 LoRAåˆ†è§£çš„æ•°å­¦æ¨å¯¼

#### 2.2.1 ä½ç§©çŸ©é˜µåˆ†è§£

å°†æƒé‡æ›´æ–°åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯:

$$
W' = W + \Delta W = W + BA
$$

å…¶ä¸­:
- $W \in \mathbb{R}^{d_{out} \times d_{in}}$: å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
- $B \in \mathbb{R}^{d_{out} \times r}$: å¯è®­ç»ƒ
- $A \in \mathbb{R}^{r \times d_{in}}$: å¯è®­ç»ƒ
- $r \ll \min(d_{in}, d_{out})$: LoRAç§©

**å‚æ•°é‡å¯¹æ¯”**:
- å…¨é‡å¾®è°ƒ: $d_{out} \times d_{in}$
- LoRA: $r \times (d_{in} + d_{out})$
- å‹ç¼©æ¯”: 
$$
\rho = \frac{r(d_{in} + d_{out})}{d_{in} \cdot d_{out}} \approx \frac{2r}{d} \quad (\text{å½“ } d_{in} = d_{out} = d)
$$

**å®ä¾‹**:
- LLaMA-7Bå…¨é‡å¾®è°ƒ: 7Bå‚æ•°
- LoRA (r=8, d=4096): 
$$
32 \text{ layers} \times 4 \text{ matrices} \times 8 \times (4096 + 4096) \approx 8.4\text{M}
$$
- å‹ç¼©æ¯”: $8.4\text{M} / 7\text{B} \approx 0.12\%$
- æ˜¾å­˜éœ€æ±‚: 80GB â†’ 10GB

#### 2.2.2 æ¢¯åº¦æµåˆ†æ

**åå‘ä¼ æ’­**:

å‰å‘:
$$
h = x(W + BA) = xW + x(BA)
$$

æŸå¤±å¯¹ $B$ çš„æ¢¯åº¦:
$$
\frac{\partial \mathcal{L}}{\partial B} = \frac{\partial \mathcal{L}}{\partial h} \cdot (xA)^T
$$

æŸå¤±å¯¹ $A$ çš„æ¢¯åº¦:
$$
\frac{\partial \mathcal{L}}{\partial A} = B^T \frac{\partial \mathcal{L}}{\partial h} \cdot x^T
$$

**å…³é”®è§‚å¯Ÿ**: æ¢¯åº¦æµç»è¿‡ $B$ å’Œ $A$ çš„ç§©ä¸º $r$ çš„ç“¶é¢ˆ,è‡ªç„¶å®ç°æ­£åˆ™åŒ–!

#### 2.2.3 æœ€ä¼˜ç§©çš„ç†è®ºåˆ†æ

**é—®é¢˜**: å¦‚ä½•é€‰æ‹©ç§© $r$?

**Bias-Varianceæƒè¡¡**:

- ç§©å¤ªå° ($r \ll r_{\text{opt}}$): 
$$
\text{Bias} = \|\Delta W_{\text{true}} - \text{Proj}_r(\Delta W_{\text{true}})\|_F^2 \uparrow
$$

- ç§©å¤ªå¤§ ($r \gg r_{\text{opt}}$):
$$
\text{Variance} = \mathbb{E}[\|\hat{\Delta W}_r - \mathbb{E}[\hat{\Delta W}_r]\|^2] \uparrow
$$

**ç»éªŒå…¬å¼** (åŸºäºå®éªŒæ‹Ÿåˆ):

æœ€ä¼˜ç§©ä¸ä»»åŠ¡å¤æ‚åº¦ã€æ¨¡å‹è§„æ¨¡çš„å…³ç³»:
$$
r_{\text{opt}} \approx c \cdot \sqrt{\frac{N_{\text{params}}}{N_{\text{samples}}}}
$$

å…¶ä¸­ $c \approx 2-4$ã€‚

**å®éªŒéªŒè¯**:
| æ•°æ®é›†å¤§å° | æ¨¡å‹ | æœ€ä¼˜ç§© |
|-----------|------|--------|
| 10K | LLaMA-7B | 4-8 |
| 100K | LLaMA-7B | 8-16 |
| 1M | LLaMA-7B | 16-32 |

### 2.3 LoRAå‰å‘ä¼ æ’­

```python
def lora_forward(x, W_frozen, A, B, alpha, r):
    """
    x: è¾“å…¥ [batch, seq_len, d_in]
    W_frozen: å†»ç»“æƒé‡ [d_out, d_in]
    A: LoRAçŸ©é˜µ [r, d_in]
    B: LoRAçŸ©é˜µ [d_out, r]
    alpha: ç¼©æ”¾å› å­
    r: LoRAç§©
    """
    h_original = x @ W_frozen.T
    h_lora = (x @ A.T) @ B.T
    h = h_original + (alpha / r) * h_lora
    return h
```

**ç¼©æ”¾å› å­** $\alpha/r$ çš„ä½œç”¨:
- æ§åˆ¶LoRAæ›´æ–°çš„å¹…åº¦
- é€šå¸¸è®¾ç½® $\alpha = r$ æˆ– $\alpha = 2r$
- é¿å…è®­ç»ƒåˆæœŸæ¢¯åº¦çˆ†ç‚¸

### 2.4 LoRAåˆå§‹åŒ–ç­–ç•¥

**AçŸ©é˜µ**: é«˜æ–¯éšæœºåˆå§‹åŒ–
$$
A \sim \mathcal{N}(0, \sigma^2), \quad \sigma = \frac{1}{\sqrt{r}}
$$

**BçŸ©é˜µ**: é›¶åˆå§‹åŒ–
$$
B = 0
$$

**åŸå› **:
- è®­ç»ƒå¼€å§‹æ—¶ $\Delta W = BA = 0$,æ¨¡å‹è¡Œä¸ºä¸é¢„è®­ç»ƒä¸€è‡´
- é¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†
- ç¨³å®šè®­ç»ƒè¿‡ç¨‹

### 2.5 åº”ç”¨LoRAçš„å±‚é€‰æ‹©

**å…¨å‚æ•°åº”ç”¨** (æ¨è):
```
Q, K, V, OçŸ©é˜µ (Attention)
W1, W2çŸ©é˜µ (FFN)
```

**ä»…Attentionå±‚**:
- å‚æ•°é‡æœ€å°‘
- é€‚åˆæé™èµ„æºåœºæ™¯
- æ€§èƒ½ç•¥æœ‰ä¸‹é™(~2%)

**å®éªŒå¯¹æ¯”** (LLaMA-7B on Alpaca):

| é…ç½® | å¯è®­ç»ƒå‚æ•° | MMLU | AlpacaEval |
|-----|----------|------|------------|
| å…¨é‡å¾®è°ƒ | 7B | 41.2 | 78.5 |
| LoRA (QKV) | 4.7M | 40.1 | 75.2 |
| LoRA (QKV+FFN) | 18.9M | 41.0 | 77.8 |

---

## ç¬¬4ç« :QLoRA - é‡åŒ–LoRA

### 3.1 åŠ¨æœº

LoRAå·²å¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚,ä½†**åŸºç¡€æ¨¡å‹æƒé‡**ä»éœ€å®Œæ•´åŠ è½½:
- LLaMA-7B (FP16): 14GB
- LLaMA-65B (FP16): 130GB

QLoRAè¿›ä¸€æ­¥å‹ç¼©åŸºç¡€æ¨¡å‹è‡³4-bit,å®ç°**å•GPUå¾®è°ƒå¤§æ¨¡å‹**ã€‚

### 3.2 æ ¸å¿ƒæŠ€æœ¯

#### 3.2.1 NF4é‡åŒ– (4-bit NormalFloat)

**è§‚å¯Ÿ**: ç¥ç»ç½‘ç»œæƒé‡é€šå¸¸æœä»æ­£æ€åˆ†å¸ƒ $W \sim \mathcal{N}(0, \sigma^2)$

**æ ‡å‡†é‡åŒ–**(å‡åŒ€åˆ†å¸ƒ):
$$
q_i = -8, -7, ..., 7 \quad (\text{INT4})
$$

**NF4é‡åŒ–**(éå‡åŒ€åˆ†å¸ƒ):
æ ¹æ®æ­£æ€åˆ†å¸ƒçš„åˆ†ä½æ•°é€‰æ‹©é‡åŒ–ç‚¹:
$$
q_i = \Phi^{-1}\left(\frac{i+0.5}{16}\right), \quad i=0,1,...,15
$$

å…¶ä¸­ $\Phi^{-1}$ ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„é€†CDFã€‚

**ä¼˜åŠ¿**:
- åœ¨é«˜å¯†åº¦åŒºåŸŸ(ä¸­å¿ƒ)é‡åŒ–ç‚¹å¯†é›†
- åœ¨ä½å¯†åº¦åŒºåŸŸ(å°¾éƒ¨)é‡åŒ–ç‚¹ç¨€ç–
- æœ€å°åŒ–æœŸæœ›é‡åŒ–è¯¯å·®

#### 3.2.2 åŒé‡é‡åŒ– (Double Quantization)

é‡åŒ–å‚æ•°æœ¬èº«ä¹Ÿéœ€è¦å­˜å‚¨,QLoRAå¯¹é‡åŒ–å¸¸æ•°å†æ¬¡é‡åŒ–:

**ä¸€çº§é‡åŒ–**:
$$
W_q = \text{Quantize}(W, s_1, z_1)
$$

**äºŒçº§é‡åŒ–**:
$$
s_1^q = \text{Quantize}(s_1, s_2, z_2)
$$

**æ˜¾å­˜èŠ‚çœ**:
- LLaMA-65B: 0.37GBé¢å¤–æ˜¾å­˜ â†’ 0.06GB
- èŠ‚çœç‡: 84%

#### 3.2.3 åˆ†é¡µä¼˜åŒ–å™¨ (Paged Optimizers)

ä½¿ç”¨CPUå†…å­˜ä½œä¸ºGPUæ˜¾å­˜æº¢å‡ºæ—¶çš„å¤‡ä»½:
```
GPUæ˜¾å­˜ä¸è¶³ â†’ è‡ªåŠ¨è¿ç§»åˆ°CPU â†’ GPUç©ºé—²æ—¶è¿å›
```

ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜æœºåˆ¶ã€‚

### 3.3 QLoRAå®Œæ•´è®­ç»ƒæµç¨‹

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

**æ˜¾å­˜å ç”¨** (LLaMA-7B):
- FP16å…¨é‡å¾®è°ƒ: ~80GB
- LoRA (FP16åŸºç¡€æ¨¡å‹): ~24GB
- QLoRA (NF4åŸºç¡€æ¨¡å‹): ~9GB

---

## ç¬¬5ç« :å…¶ä»–PEFTæ–¹æ³•

### 4.1 Adapteræ–¹æ³•

**æ¶æ„**:
åœ¨Transformerå±‚ä¹‹é—´æ’å…¥ç“¶é¢ˆç»“æ„:

```
FFNè¾“å‡º â†’ Adapter(down â†’ æ¿€æ´» â†’ up) â†’ æ®‹å·®è¿æ¥
```

**æ•°å­¦å½¢å¼**:
$$
h' = h + f(hW_{down})W_{up}
$$

å…¶ä¸­:
- $W_{down} \in \mathbb{R}^{d \times r}$
- $W_{up} \in \mathbb{R}^{r \times d}$
- $r \ll d$ (ç“¶é¢ˆç»´åº¦)

**ä¼˜ç¼ºç‚¹**:
- âœ… å‚æ•°æ•ˆç‡é«˜
- âŒ å¢åŠ æ¨ç†å»¶è¿Ÿ(é¢å¤–å‰å‘ä¼ æ’­)
- âŒ ä¸å¦‚LoRAçµæ´»

### 4.2 Prefix Tuning

**æ€æƒ³**: åœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ å¯è®­ç»ƒçš„è™šæ‹Ÿtoken

$$
[P_1, P_2, ..., P_k, x_1, x_2, ..., x_n]
$$

**ä¼˜åŒ–ç›®æ ‡**:
$$
\max_P \mathbb{E}_{(x,y)} \left[ \log P_\theta(y | P, x) \right]
$$

**ä¼˜ç¼ºç‚¹**:
- âœ… ä¸ä¿®æ”¹æ¨¡å‹æƒé‡
- âœ… é€‚åˆå¤šä»»åŠ¡åœºæ™¯(æ¯ä»»åŠ¡ä¸€ç»„Prefix)
- âŒ å‰ç¼€é•¿åº¦å ç”¨ä¸Šä¸‹æ–‡
- âŒ è®­ç»ƒä¸ç¨³å®š

### 4.3 P-Tuning v2

**æ”¹è¿›**: åœ¨æ¯å±‚éƒ½æ·»åŠ Prefix,è€Œéä»…è¾“å…¥å±‚

```
Layer 1: [P1, x] â†’ h1
Layer 2: [P2, h1] â†’ h2
...
Layer L: [PL, h_{L-1}] â†’ output
```

**æ•ˆæœ**: åœ¨å°æ¨¡å‹(<1B)ä¸Šæ˜¾è‘—ä¼˜äºLoRA

### 4.4 PEFTæ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å‚æ•°é‡ | æ¨ç†å¼€é”€ | é€‚ç”¨è§„æ¨¡ | å¤šä»»åŠ¡æ”¯æŒ |
|------|--------|---------|---------|-----------|
| Full Fine-tune | 100% | æ—  | å…¨éƒ¨ | éœ€é‡æ–°è®­ç»ƒ |
| LoRA | 0.1-1% | æ— (å¯åˆå¹¶) | >1B | åˆ‡æ¢é€‚é…å™¨ |
| QLoRA | 0.1-1% | æ—  | >7B | åˆ‡æ¢é€‚é…å™¨ |
| Adapter | 0.5-5% | æœ‰ | <1B | å¹¶è¡Œæ¨ç† |
| Prefix Tuning | 0.01-0.1% | æœ‰ | <1B | å¹¶è¡Œæ¨ç† |

**é€‰æ‹©å»ºè®®**:
- **å¤§æ¨¡å‹(>7B)**: QLoRA (æ˜¾å­˜å—é™)
- **ä¸­å‹æ¨¡å‹(1-7B)**: LoRA (å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡)
- **å°æ¨¡å‹(<1B)**: P-Tuning v2 (è®­ç»ƒç¨³å®šæ€§)
- **å¤šä»»åŠ¡åœºæ™¯**: Prefix Tuning (ä»»åŠ¡éš”ç¦»)

---

## ç¬¬6ç« :å¾®è°ƒæ•°æ®å·¥ç¨‹

### 5.1 æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡

**å®éªŒè¯æ®** (Alpacaæ¨¡å‹):
- 52KæŒ‡ä»¤æ•°æ®å¾®è°ƒ â†’ è¶…è¶Š175B GPT-3çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- å…³é”®: é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æŒ‡ä»¤è¦†ç›–

**è´¨é‡æ ‡å‡†**:
1. **æŒ‡ä»¤å¤šæ ·æ€§**: è¦†ç›–ä¸åŒä»»åŠ¡ç±»å‹
2. **å“åº”è´¨é‡**: å‡†ç¡®ã€æœ‰å¸®åŠ©ã€æ— å®³
3. **æ ¼å¼ä¸€è‡´æ€§**: ç»Ÿä¸€çš„æŒ‡ä»¤-å“åº”ç»“æ„
4. **éš¾åº¦åˆ†å¸ƒ**: ç®€å•â†’ä¸­ç­‰â†’å¤æ‚çš„æ¢¯åº¦åˆ†å¸ƒ

### 5.2 Self-Instructæ•°æ®ç”Ÿæˆ

**æµç¨‹**:
```
ç§å­ä»»åŠ¡(175ä¸ª) â†’ LLMç”Ÿæˆæ–°ä»»åŠ¡ â†’ è¿‡æ»¤ä½è´¨é‡ â†’ ç”Ÿæˆå“åº” â†’ éªŒè¯è´¨é‡
```

**æç¤ºæ¨¡æ¿**:
```
ç»™å®šä»¥ä¸‹ä»»åŠ¡ç¤ºä¾‹:
1. å°†å¥å­ç¿»è¯‘æˆæ³•è¯­: "Hello" â†’ "Bonjour"
2. æ€»ç»“æ–‡ç« è¦ç‚¹: [æ–‡ç« ] â†’ [æ‘˜è¦]
...

è¯·ç”Ÿæˆ5ä¸ªæ–°çš„ä»»åŠ¡æŒ‡ä»¤,è¦æ±‚:
- å¤šæ ·æ€§: ä¸ç°æœ‰ä»»åŠ¡ä¸é‡å¤
- æ¸…æ™°æ€§: æŒ‡ä»¤æ˜ç¡®æ˜“æ‡‚
- å¯æ‰§è¡Œæ€§: å¯é€šè¿‡æ–‡æœ¬å®Œæˆ
```

### 5.3 æ•°æ®è¿‡æ»¤ç­–ç•¥

#### 5.3.1 è§„åˆ™è¿‡æ»¤

```python
def filter_low_quality(sample):
    instruction = sample['instruction']
    response = sample['response']
    
    if len(instruction) < 10 or len(response) < 20:
        return False
    
    if contains_toxic_words(response):
        return False
    
    if is_repetitive(response):
        return False
    
    if not is_coherent(instruction, response):
        return False
    
    return True
```

#### 5.3.2 æ¨¡å‹è¯„åˆ†

ä½¿ç”¨Reward Modelè¯„ä¼°è´¨é‡:
```python
score = reward_model(instruction, response)
threshold = 0.7
keep = score > threshold
```

#### 5.3.3 å»é‡

**è¯­ä¹‰å»é‡** (åŸºäºåµŒå…¥):
```python
embeddings = embed_model(instructions)
similarity_matrix = cosine_similarity(embeddings)
duplicates = similarity_matrix > 0.95
keep_unique = remove_duplicates(duplicates)
```

### 5.4 æ•°æ®é…æ¯”ç­–ç•¥

**ä»»åŠ¡ç±»å‹åˆ†å¸ƒ**:
```
å¯¹è¯äº¤äº’: 30%
çŸ¥è¯†é—®ç­”: 25%
åˆ›æ„å†™ä½œ: 15%
ä»£ç ç”Ÿæˆ: 15%
æ•°å­¦æ¨ç†: 10%
å…¶ä»–: 5%
```

**éš¾åº¦åˆ†å¸ƒ**:
```
ç®€å•(å•æ­¥æ¨ç†): 40%
ä¸­ç­‰(å¤šæ­¥æ¨ç†): 40%
å›°éš¾(å¤æ‚è§„åˆ’): 20%
```

---

## ç¬¬7ç« :å¾®è°ƒå®æˆ˜æŠ€å·§

### 6.1 è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡**:
- Full Fine-tune: $1e-5$ ~ $5e-5$
- LoRA: $1e-4$ ~ $3e-4$
- ä½¿ç”¨çº¿æ€§warmup + cosine decay

**Batch Size**:
- æœ‰æ•ˆbatch size: 32-128
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯çªç ´æ˜¾å­˜é™åˆ¶:
  ```
  effective_batch_size = micro_batch_size Ã— gradient_accumulation_steps
  ```

**è®­ç»ƒæ­¥æ•°**:
- å°æ•°æ®é›†(~10K): 3-5 epochs
- å¤§æ•°æ®é›†(>100K): 1-2 epochs
- ç›‘æ§éªŒè¯é›†å›°æƒ‘åº¦,é¿å…è¿‡æ‹Ÿåˆ

### 6.2 LoRAç§©çš„é€‰æ‹©

**ç»éªŒæ³•åˆ™**:
```
ç®€å•ä»»åŠ¡(åˆ†ç±»): r = 4-8
ä¸­ç­‰ä»»åŠ¡(å¯¹è¯): r = 8-16
å¤æ‚ä»»åŠ¡(ä»£ç ): r = 16-64
```

**å®éªŒæ›²çº¿** (LLaMA-7B on MMLU):

| Rank | å‚æ•°é‡ | MMLUåˆ†æ•° | è®­ç»ƒæ—¶é—´ |
|------|--------|---------|---------|
| r=4  | 9.4M   | 39.2    | 2h      |
| r=8  | 18.9M  | 40.8    | 3h      |
| r=16 | 37.7M  | 41.1    | 5h      |
| r=32 | 75.5M  | 41.2    | 8h      |

**ç»“è®º**: r=8-16æ˜¯ç”œç‚¹,ç»§ç»­å¢å¤§æ”¶ç›Šé€’å‡ã€‚

### 6.3 é˜²æ­¢è¿‡æ‹Ÿåˆ

**Dropout**:
```python
lora_config = LoraConfig(
    lora_dropout=0.05  # LoRAå±‚çš„dropout
)
```

**Early Stopping**:
```python
patience = 3
best_val_loss = float('inf')
counter = 0

for epoch in range(max_epochs):
    val_loss = evaluate(model, val_data)
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_checkpoint(model)
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            break
```

**æ•°æ®å¢å¼º**:
- åŒä¹‰æ›¿æ¢
- å›è¯‘(ç¿»è¯‘æˆå…¶ä»–è¯­è¨€å†ç¿»è¯‘å›æ¥)
- æŒ‡ä»¤æ”¹å†™

---

## ç¬¬8ç« :å¾®è°ƒè¯„ä¼°

### 7.1 è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

**å›°æƒ‘åº¦** (Perplexity):
$$
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(x_i | x_{<i})\right)
$$

è¶Šä½è¶Šå¥½,ä½†ä¸äººç±»åå¥½ä¸å®Œå…¨å¯¹é½ã€‚

**ROUGE/BLEU**:
é€‚ç”¨äºç”Ÿæˆä»»åŠ¡,è¡¡é‡ä¸å‚è€ƒç­”æ¡ˆçš„n-gramé‡å ã€‚

**ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡**:
- åˆ†ç±»: Accuracy, F1
- ä»£ç : Pass@K (é€šè¿‡ç‡)
- æ•°å­¦: Exact Match

### 7.2 äººç±»è¯„ä¼°

**Eloè¯„åˆ†ç³»ç»Ÿ**:
```
ä¸¤ä¸¤å¯¹æ¯” â†’ èƒœ/è´Ÿ/å¹³ â†’ æ›´æ–°Eloåˆ†æ•°
```

**è¯„ä¼°ç»´åº¦**:
1. **æœ‰å¸®åŠ©æ€§** (Helpfulness): æ˜¯å¦è§£å†³ç”¨æˆ·é—®é¢˜
2. **å‡†ç¡®æ€§** (Accuracy): äº‹å®æ­£ç¡®æ€§
3. **æ— å®³æ€§** (Harmlessness): é¿å…æœ‰å®³è¾“å‡º
4. **è¿è´¯æ€§** (Coherence): é€»è¾‘æµç•…æ€§

### 7.3 åŸºå‡†æµ‹è¯•

**é€šç”¨èƒ½åŠ›**:
- MMLU (å¤šå­¦ç§‘çŸ¥è¯†)
- BBH (Big-Bench Hard)
- AGIEval (æ¨ç†èƒ½åŠ›)

**æŒ‡ä»¤éµå¾ª**:
- Alpaca Eval
- MT-Bench
- Vicuna Bench

**ä»£ç èƒ½åŠ›**:
- HumanEval
- MBPP
- CodeContests

---

## ç¬¬9ç« :è¿ç§»å­¦ä¹ ç†è®º

### 8.1 è¿ç§»å­¦ä¹ çš„æ•°å­¦å½¢å¼åŒ–

**å®šä¹‰**: å°†æºåŸŸ $\mathcal{D}_S$ å­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°ç›®æ ‡åŸŸ $\mathcal{D}_T$ã€‚

**åŸŸå®šä¹‰**:
$$
\mathcal{D} = \{(\mathcal{X}, P(X)), (\mathcal{Y}, P(Y|X))\}
$$

å…¶ä¸­:
- $\mathcal{X}$: ç‰¹å¾ç©ºé—´
- $P(X)$: è¾¹ç¼˜åˆ†å¸ƒ
- $P(Y|X)$: æ¡ä»¶åˆ†å¸ƒ

### 8.2 åˆ†å¸ƒåç§»ç±»å‹

**åå˜é‡åç§»** (Covariate Shift):
$$
P_S(X) \neq P_T(X), \quad P_S(Y|X) = P_T(Y|X)
$$

**ç¤ºä¾‹**: é¢„è®­ç»ƒ(ç½‘é¡µæ–‡æœ¬) â†’ å¾®è°ƒ(å¯¹è¯æ•°æ®)

**æ¦‚å¿µåç§»** (Concept Shift):
$$
P_S(X) = P_T(X), \quad P_S(Y|X) \neq P_T(Y|X)
$$

**ç¤ºä¾‹**: æƒ…æ„Ÿåˆ†æ(ç”µå½±è¯„è®º â†’ å•†å“è¯„è®º)

**è”åˆåˆ†å¸ƒåç§»**:
$$
P_S(X, Y) \neq P_T(X, Y)
$$

æœ€ä¸€èˆ¬æƒ…å†µ,åŒæ—¶å­˜åœ¨ç‰¹å¾å’Œæ ‡ç­¾åˆ†å¸ƒå˜åŒ–ã€‚

### 8.3 è¿ç§»å­¦ä¹ çš„ç†è®ºä¿è¯

**Ben-Davidç•Œ** (åŸŸé€‚é…ç†è®º):

ç›®æ ‡åŸŸæœŸæœ›é£é™©:
$$
\epsilon_T(\theta) \leq \epsilon_S(\theta) + \frac{1}{2}d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_S, \mathcal{D}_T) + \lambda
$$

å…¶ä¸­:
- $\epsilon_S(\theta)$: æºåŸŸè¯¯å·®
- $d_{\mathcal{H}\Delta\mathcal{H}}$: $\mathcal{H}$-divergence(è¡¡é‡åŸŸå·®å¼‚)
- $\lambda$: ç†æƒ³è”åˆå‡è®¾è¯¯å·®

**ç›´è§‰**: ç›®æ ‡åŸŸæ€§èƒ½å–å†³äº:
1. æºåŸŸæ€§èƒ½
2. åŸŸé—´å·®å¼‚(è¶Šå°è¶Šå¥½)
3. ä»»åŠ¡å†…åœ¨éš¾åº¦

**LLMè¿ç§»å­¦ä¹ ç‰¹ç‚¹**:

é¢„è®­ç»ƒæä¾›**é€šç”¨è¡¨ç¤º**:
$$
h_{\text{pretrain}}: \mathcal{X} \rightarrow \mathbb{R}^d
$$

å¾®è°ƒåªéœ€è°ƒæ•´**ä»»åŠ¡å¤´**:
$$
f_{\text{task}} = g \circ h_{\text{pretrain}}
$$

è¿™æ˜¾è‘—é™ä½äº† $d_{\mathcal{H}\Delta\mathcal{H}}$!

### 8.4 è´Ÿè¿ç§»é—®é¢˜

**å®šä¹‰**: è¿ç§»åæ€§èƒ½ < ä»å¤´è®­ç»ƒ

**æ•°å­¦è¡¨è¾¾**:
$$
\epsilon_T(\theta_{\text{transfer}}) > \epsilon_T(\theta_{\text{scratch}})
$$

**å‘ç”ŸåŸå› **:

1. **åŸŸå·®å¼‚è¿‡å¤§**:
$$
d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_S, \mathcal{D}_T) \gg 0
$$

2. **æºåŸŸè¿‡æ‹Ÿåˆ**:
$$
\epsilon_S(\theta) \ll \epsilon_T(\theta)
$$

3. **ç¾éš¾æ€§é—å¿˜**:
$$
\Delta \theta = \theta_{\text{finetune}} - \theta_{\text{pretrain}} \gg 0
$$

**è§£å†³æ–¹æ¡ˆ**:

**a) æ­£åˆ™åŒ–è¿ç§»**:
$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \|\theta - \theta_{\text{pretrain}}\|^2
$$

**b) é€å±‚è§£å†»** (Gradual Unfreezing):
```
ç¬¬1é˜¶æ®µ: å†»ç»“encoder,è®­ç»ƒhead
ç¬¬2é˜¶æ®µ: è§£å†»æœ€å1å±‚
ç¬¬3é˜¶æ®µ: è§£å†»æœ€å2å±‚
...
```

**c) åˆ¤åˆ«å¼å¾®è°ƒ** (Discriminative Fine-tuning):

ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡:
$$
\eta_l = \eta_{\text{base}} / 2.6^{(L-l)}
$$

æµ…å±‚(é€šç”¨ç‰¹å¾)å­¦ä¹ ç‡å°,æ·±å±‚(ä»»åŠ¡ç‰¹å®š)å­¦ä¹ ç‡å¤§ã€‚

### 8.5 è·¨è¯­è¨€è¿ç§»

**å¤šè¯­è¨€é¢„è®­ç»ƒ**:

å…±äº«è¯è¡¨ + æ··åˆè¯­æ–™è®­ç»ƒ:
$$
\mathcal{L}_{\text{multilingual}} = \sum_{\text{lang}} \alpha_{\text{lang}} \mathcal{L}_{\text{CLM}}(\mathcal{D}_{\text{lang}})
$$

**é›¶æ ·æœ¬è·¨è¯­è¨€è¿ç§»**:

åœ¨è‹±è¯­æ•°æ®ä¸Šå¾®è°ƒ â†’ ç›´æ¥åœ¨ä¸­æ–‡æµ‹è¯•:
$$
\theta_{\text{finetune}} \leftarrow \arg\min_\theta \mathcal{L}(\mathcal{D}_{\text{en}})
$$

$$
\text{Test on } \mathcal{D}_{\text{zh}}
$$

**ç†è®ºè§£é‡Š**: å¤šè¯­è¨€æ¨¡å‹å­¦åˆ°**è¯­è¨€æ— å…³çš„è¯­ä¹‰ç©ºé—´**:
$$
h(x_{\text{en}}) \approx h(x_{\text{zh}}) \quad \text{(if same meaning)}
$$

---

## ç¬¬10ç« :çŸ¥è¯†è’¸é¦å®Œæ•´ç†è®º

### 9.1 å­¦ç”Ÿ-æ•™å¸ˆæ¡†æ¶

**åŸºæœ¬è®¾å®š**:
- **æ•™å¸ˆæ¨¡å‹** $T$: å¤§å‹é¢„è®­ç»ƒæ¨¡å‹(å¦‚GPT-3 175B)
- **å­¦ç”Ÿæ¨¡å‹** $S$: å°å‹æ¨¡å‹(å¦‚GPT-2 1.5B)

**ç›®æ ‡**: è®©å­¦ç”Ÿæ¨¡å‹é€¼è¿‘æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒã€‚

### 9.2 ç»å…¸çŸ¥è¯†è’¸é¦

**Hintonè’¸é¦æŸå¤±** (2015):

$$
\mathcal{L}_{\text{KD}} = \tau^2 \cdot D_{KL}(p_T^{\tau} \| p_S^{\tau}) + \alpha \cdot \mathcal{L}_{CE}(y, p_S)
$$

å…¶ä¸­:
- $p_T^{\tau} = \text{softmax}(z_T / \tau)$: æ•™å¸ˆsoftmaxè¾“å‡º(æ¸©åº¦ç¼©æ”¾)
- $p_S^{\tau} = \text{softmax}(z_S / \tau)$: å­¦ç”Ÿsoftmaxè¾“å‡º
- $\tau$: æ¸©åº¦(é€šå¸¸2-10)
- $\alpha$: å¹³è¡¡ç³»æ•°(é€šå¸¸0.1-0.5)

**æ¸©åº¦å‚æ•°çš„ä½œç”¨**:

**æ ‡å‡†softmax** ($\tau=1$):
$$
p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

å¯¹äº $z = [10, 2, 1]$:
$$
p \approx [0.9999, 0.0001, 0.0000]
$$

**æ¸©åº¦ç¼©æ”¾** ($\tau=5$):
$$
p_i^{\tau} = \frac{e^{z_i/\tau}}{\sum_j e^{z_j/\tau}}
$$

å¯¹äºåŒæ · $z$:
$$
p^{\tau} \approx [0.85, 0.10, 0.05]
$$

**æš—çŸ¥è¯†** (Dark Knowledge):

å°¾éƒ¨æ¦‚ç‡ $[0.10, 0.05]$ åŒ…å«ç±»é—´ç›¸ä¼¼åº¦ä¿¡æ¯!

ä¾‹å¦‚: æ•™å¸ˆå¯¹"çŒ«"å›¾åƒè¾“å‡º `{çŒ«: 0.9, ç‹—: 0.08, è½¦: 0.02}`

å­¦ç”Ÿå­¦åˆ°: "çŒ«"å’Œ"ç‹—"æ¯”"çŒ«"å’Œ"è½¦"æ›´ç›¸ä¼¼ã€‚

**æ•°å­¦åŸç†**:

æ¸©åº¦ç¼©æ”¾ç­‰ä»·äº**ç†µæ­£åˆ™åŒ–**:
$$
p^{\tau} = \arg\max_p \sum_i p_i \log q_i + \tau H(p)
$$

å…¶ä¸­ $H(p) = -\sum_i p_i \log p_i$ ä¸ºç†µã€‚

$\tau$ è¶Šå¤§,åˆ†å¸ƒè¶Šå¹³æ»‘,ä¼ é€’æ›´å¤šç›¸ä¼¼æ€§ä¿¡æ¯ã€‚

### 9.3 LLMè’¸é¦çš„ç‰¹æ®Šæ€§

**åºåˆ—çº§è’¸é¦**:

å¯¹äºè‡ªå›å½’ç”Ÿæˆ,é€tokenè’¸é¦:
$$
\mathcal{L}_{\text{seq-KD}} = -\sum_{t=1}^T \sum_{v \in \mathcal{V}} p_T(v | x_{<t}) \log p_S(v | x_{<t})
$$

**åœ¨çº¿è’¸é¦**:

æ•™å¸ˆå®æ—¶ç”Ÿæˆè½¯æ ‡ç­¾:
```python
with torch.no_grad():
    teacher_logits = teacher(x) / temperature
    p_teacher = softmax(teacher_logits)

student_logits = student(x) / temperature
p_student = softmax(student_logits)

loss = kl_div(p_student, p_teacher) * (temperature ** 2)
```

**ç¦»çº¿è’¸é¦**:

é¢„å…ˆä¿å­˜æ•™å¸ˆè¾“å‡º,é¿å…åŒæ¨¡å‹å‰å‘:
```
1. æ•™å¸ˆæ¨ç†: ä¿å­˜æ‰€æœ‰logits
2. å­¦ç”Ÿè®­ç»ƒ: åŠ è½½é¢„è®¡ç®—çš„logits
```

### 9.4 é«˜çº§è’¸é¦æŠ€æœ¯

#### 9.4.1 å¤šæ•™å¸ˆè’¸é¦

é›†æˆå¤šä¸ªæ•™å¸ˆçš„çŸ¥è¯†:
$$
p_T^{\text{ensemble}} = \frac{1}{K} \sum_{k=1}^K p_{T_k}
$$

**ä¼˜åŠ¿**: 
- æ›´é²æ£’çš„æš—çŸ¥è¯†
- é¿å…å•æ•™å¸ˆåè§

#### 9.4.2 å±‚é—´è’¸é¦

ä¸ä»…è’¸é¦è¾“å‡º,è¿˜è’¸é¦ä¸­é—´å±‚:
$$
\mathcal{L}_{\text{layer}} = \sum_{l \in \mathcal{L}} \|h_S^{(l)} - W_l h_T^{(\phi(l))}\|^2
$$

å…¶ä¸­:
- $h_S^{(l)}$: å­¦ç”Ÿç¬¬$l$å±‚éšè—çŠ¶æ€
- $h_T^{(\phi(l))}$: æ•™å¸ˆå¯¹åº”å±‚(æ˜ å°„å‡½æ•°$\phi$)
- $W_l$: æŠ•å½±çŸ©é˜µ(ç»´åº¦å¯¹é½)

#### 9.4.3 æ³¨æ„åŠ›è’¸é¦

è¿ç§»æ³¨æ„åŠ›æ¨¡å¼:
$$
\mathcal{L}_{\text{attn}} = \sum_{h=1}^H \|A_S^{(h)} - A_T^{(h)}\|^2_F
$$

å…¶ä¸­ $A$ ä¸ºæ³¨æ„åŠ›çŸ©é˜µã€‚

**ç›´è§‰**: æ•™å¸ˆçš„"æ³¨æ„æ¨¡å¼"åŒ…å«è¯­è¨€ç»“æ„çŸ¥è¯†ã€‚

### 9.5 è’¸é¦çš„ç†è®ºåˆ†æ

**å‹ç¼©æ¯” vs æ€§èƒ½**:

ç»éªŒè§„å¾‹:
$$
\text{Performance Drop} \approx c \cdot \left(\frac{N_S}{N_T}\right)^{-\alpha}
$$

å…¶ä¸­:
- $N_S, N_T$: å­¦ç”Ÿ/æ•™å¸ˆå‚æ•°é‡
- $\alpha \approx 0.3-0.5$: è¡°å‡æŒ‡æ•°

**å®éªŒæ•°æ®**:
| å‹ç¼©æ¯” | æ€§èƒ½ä¿ç•™ |
|--------|---------|
| 10x | ~90% |
| 100x | ~70% |
| 1000x | ~50% |

**ä¿¡æ¯è®ºè§†è§’**:

è’¸é¦æœ€å°åŒ–äº’ä¿¡æ¯æŸå¤±:
$$
\min I(Y; X | S) - I(Y; X | T)
$$

å­¦ç”Ÿæ•è·çš„ä¿¡æ¯æ¥è¿‘æ•™å¸ˆ!

### 9.6 LLMè’¸é¦å®æˆ˜

**TinyBERT** (BERT â†’ 4x smaller):
$$
\mathcal{L} = \mathcal{L}_{\text{embed}} + \sum_{l} (\mathcal{L}_{\text{attn}}^{(l)} + \mathcal{L}_{\text{hidden}}^{(l)}) + \mathcal{L}_{\text{pred}}
$$

**DistilGPT** (GPT-2 1.5B â†’ 350M):
- æ¸©åº¦ $\tau = 2.0$
- ä¿ç•™86%æ€§èƒ½
- æ¨ç†é€Ÿåº¦2.3x

**å®è·µå»ºè®®**:

1. **ä¸¤é˜¶æ®µè’¸é¦**:
   - é˜¶æ®µ1: é¢„è®­ç»ƒæ•°æ® + è¾“å‡ºè’¸é¦
   - é˜¶æ®µ2: å¾®è°ƒæ•°æ® + å®Œæ•´è’¸é¦

2. **æ•°æ®å¢å¼º**:
   - æ•™å¸ˆç”Ÿæˆä¼ªæ ‡ç­¾
   - æ‰©å……è®­ç»ƒæ•°æ®

3. **æ¸©åº¦è°ƒä¼˜**:
   - åˆ†ç±»ä»»åŠ¡: $\tau = 3-5$
   - ç”Ÿæˆä»»åŠ¡: $\tau = 1-2$

---

## ç¬¬9.5ç« :MOEæ¶æ„ä¸å¾®è°ƒ

### 9.5.1 MOEæ ¸å¿ƒåŸç†

**Mixture of Experts (MOE)**: ç”¨å¤šä¸ª"ä¸“å®¶"ç½‘ç»œæ›¿ä»£å•ä¸€å¤§ç½‘ç»œ,é€šè¿‡è·¯ç”±æœºåˆ¶æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ã€‚

#### ä¼ ç»ŸFFN vs MOE

**ä¼ ç»ŸTransformer FFN**:
$$
\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 x)
$$

å‚æ•°é‡: $d_{model} \times d_{ff} \times 2$

**MOEæ›¿ä»£**:
$$
\text{MOE}(x) = \sum_{i=1}^E G(x)_i \cdot \text{Expert}_i(x)
$$

å…¶ä¸­:
- $E$: ä¸“å®¶æ•°é‡(å¦‚Mixtral 8x7Bä¸­ $E=8$)
- $G(x)$: é—¨æ§å‡½æ•°(è·¯ç”±ç½‘ç»œ)
- **å…³é”®**: æ¯ä¸ªtokenåªæ¿€æ´» $K$ ä¸ªä¸“å®¶($K \ll E$)!

**å‚æ•°æ•ˆç‡**:
```
ä¼ ç»Ÿ70Bæ¨¡å‹: æ¿€æ´»70Bå‚æ•°
Mixtral 8x7B: æ€»å…±56Bå‚æ•°,æ¿€æ´»14Bå‚æ•°(2ä¸ªä¸“å®¶)
æ€§èƒ½ç›¸å½“,æ¨ç†æˆæœ¬é™ä½4å€!
```

#### 9.5.2 è·¯ç”±æœºåˆ¶æ•°å­¦

**Top-Kè·¯ç”±**:

1. **è®¡ç®—ä¸“å®¶åˆ†æ•°**:
$$
s_i(x) = \text{Softmax}(W_g \cdot x)_i, \quad i \in [1, E]
$$

2. **é€‰æ‹©Top-Kä¸“å®¶**:
$$
\text{TopK} = \text{argTopK}_{i \in [1,E]}(s_i(x))
$$

3. **é‡æ–°å½’ä¸€åŒ–**:
$$
G(x)_i = \begin{cases}
\frac{s_i(x)}{\sum_{j \in \text{TopK}} s_j(x)} & \text{if } i \in \text{TopK} \\
0 & \text{otherwise}
\end{cases}
$$

**å…³é”®è¶…å‚æ•°**:
- $K$: æ¿€æ´»ä¸“å®¶æ•°(é€šå¸¸ $K=2$)
- $E$: æ€»ä¸“å®¶æ•°(Mixtral: 8, DeepSeek-V3: 256!)

#### 9.5.3 è´Ÿè½½å‡è¡¡é—®é¢˜

**ä¸“å®¶å´©æºƒç°è±¡**:

è®­ç»ƒæ—¶,æ‰€æœ‰tokenå¯èƒ½åªè·¯ç”±åˆ°å°‘æ•°å‡ ä¸ªä¸“å®¶:
$$
\text{Load}(\text{Expert}_i) \gg \text{Load}(\text{Expert}_j)
$$

å¯¼è‡´å¤§éƒ¨åˆ†ä¸“å®¶"é—²ç½®"!

**è¾…åŠ©æŸå¤±(Auxiliary Loss)**:

Switch Transformerå¼•å…¥è´Ÿè½½å‡è¡¡æŸå¤±:
$$
\mathcal{L}_{\text{aux}} = \alpha \cdot E \cdot \sum_{i=1}^E f_i \cdot P_i
$$

å…¶ä¸­:
- $f_i$: ä¸“å®¶$i$è¢«é€‰ä¸­çš„é¢‘ç‡
- $P_i$: ä¸“å®¶$i$çš„å¹³å‡é—¨æ§æ¦‚ç‡
- $\alpha$: è¾…åŠ©æŸå¤±æƒé‡(é€šå¸¸ $\alpha=0.01$)

**ç›®æ ‡**: å½“æ‰€æœ‰ä¸“å®¶è´Ÿè½½å‡è¡¡æ—¶, $\mathcal{L}_{\text{aux}}$ æœ€å°:
$$
f_i = P_i = \frac{1}{E}, \quad \forall i
$$

**å®Œæ•´è®­ç»ƒæŸå¤±**:
$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{LM}} + \alpha \cdot \mathcal{L}_{\text{aux}}
$$

#### 9.5.4 MOEæ¨¡å‹çš„å¾®è°ƒç­–ç•¥

**æŒ‘æˆ˜**: MOEæ¨¡å‹å‚æ•°é‡å·¨å¤§,å¦‚ä½•é«˜æ•ˆå¾®è°ƒ?

##### ç­–ç•¥1: å…¨å‚æ•°å¾®è°ƒ

**é€‚ç”¨åœºæ™¯**: èµ„æºå……è¶³,è¿½æ±‚æœ€ä½³æ€§èƒ½

**æ˜¾å­˜éœ€æ±‚**:
- Mixtral 8x7B: 56Bå‚æ•° Ã— 4å­—èŠ‚(FP32) = 224GB
- **è§£å†³**: ä½¿ç”¨DeepSpeed ZeRO-3åˆ†å¸ƒå¼è®­ç»ƒ

##### ç­–ç•¥2: LoRAå¾®è°ƒæ‰€æœ‰ä¸“å®¶

**æ–¹æ³•**: ç»™æ¯ä¸ªä¸“å®¶æ·»åŠ LoRAé€‚é…å™¨

```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate", "w1", "w2", "w3"  # MOE FFN (æ‰€æœ‰ä¸“å®¶)
    ],
    lora_dropout=0.05
)
model = get_peft_model(model, config)
```

**å‚æ•°é‡**:
- å¯è®­ç»ƒå‚æ•°: ~1-2%
- Mixtral 8x7B: ä»…560M-1.12Bå¯è®­ç»ƒå‚æ•°!

##### ç­–ç•¥3: é€‰æ‹©æ€§ä¸“å®¶å¾®è°ƒ

**æ ¸å¿ƒæ€æƒ³**: åªå¾®è°ƒè¢«æ¿€æ´»é¢‘ç‡é«˜çš„ä¸“å®¶!

**æµç¨‹**:

1. **ç»Ÿè®¡ä¸“å®¶æ¿€æ´»é¢‘ç‡**:
```python
expert_counts = torch.zeros(num_experts)

for batch in train_loader:
    with torch.no_grad():
        routing_weights = model.get_routing_weights(batch)
        expert_counts += routing_weights.sum(dim=0)

# é€‰æ‹©Top-Ké¢‘ç¹ä¸“å®¶
top_experts = expert_counts.argsort(descending=True)[:K]
```

2. **å†»ç»“ä½é¢‘ä¸“å®¶**:
```python
for i in range(num_experts):
    if i not in top_experts:
        for param in model.experts[i].parameters():
            param.requires_grad = False
```

3. **è®­ç»ƒ**:
åªæ›´æ–°é«˜é¢‘ä¸“å®¶ + è·¯ç”±ç½‘ç»œ

**æ•ˆæœ**:
- æ˜¾å­˜èŠ‚çœ: 50-70%
- æ€§èƒ½æŸå¤±: <2%

##### ç­–ç•¥4: ä¸“å®¶åˆ†ç»„å¾®è°ƒ

**DeepSeek-V3ç­–ç•¥**: å°†256ä¸ªä¸“å®¶åˆ†ä¸ºå¤šä¸ª"ä¸“å®¶ç»„"

```python
# å®šä¹‰ä¸“å®¶ç»„
expert_groups = {
    "math": [0, 1, 2, 3],      # æ•°å­¦æ¨ç†ä¸“å®¶
    "code": [4, 5, 6, 7],      # ä»£ç ç”Ÿæˆä¸“å®¶
    "language": [8, 9, 10, 11] # è¯­è¨€ç†è§£ä¸“å®¶
}

# é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒå¯¹åº”ä¸“å®¶ç»„
for task, expert_ids in expert_groups.items():
    if task == target_task:
        for idx in expert_ids:
            model.experts[idx].requires_grad = True
    else:
        for idx in expert_ids:
            model.experts[idx].requires_grad = False
```

#### 9.5.5 è·¯ç”±å´©æºƒä¸è§£å†³æ–¹æ¡ˆ

**è·¯ç”±å´©æºƒ**:

å¾®è°ƒæ—¶,è·¯ç”±ç½‘ç»œå¯èƒ½çªç„¶å´©æºƒ,æ‰€æœ‰tokenè·¯ç”±åˆ°å•ä¸€ä¸“å®¶:
$$
G(x)_1 \approx 1, \quad G(x)_{i \neq 1} \approx 0
$$

**åŸå› **:
1. å­¦ä¹ ç‡è¿‡å¤§
2. è¾…åŠ©æŸå¤±æƒé‡ä¸å½“
3. æ¢¯åº¦ç´¯ç§¯æ­¥æ•°è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**:

**1. Routing Replay (GRPOéœ€è¦)**:
```python
# ä¿å­˜é¢„è®­ç»ƒé˜¶æ®µçš„è·¯ç”±åˆ†å¸ƒ
routing_target = pretrained_routing_dist.clone()

# å¾®è°ƒæ—¶æ·»åŠ KLçº¦æŸ
kl_loss = F.kl_div(
    current_routing.log(),
    routing_target,
    reduction='batchmean'
)
loss = lm_loss + lambda_kl * kl_loss
```

**2. GSPOåŸç”Ÿç¨³å®š** (æ¨è):

GSPOé€šè¿‡ç¾¤ç»„å½’ä¸€åŒ–å¤©ç„¶é¿å…è·¯ç”±å´©æºƒ:
$$
\hat{G}(x)_i = \frac{G(x)_i - \mu_{\text{batch}}}{\sigma_{\text{batch}}}
$$

**å®éªŒå¯¹æ¯”** (Qwen3-72B MOE):

| ç®—æ³• | è·¯ç”±ç¨³å®šæ€§ | éœ€è¦Routing Replay | è®­ç»ƒæˆåŠŸç‡ |
|------|-----------|-------------------|-----------|
| PPO | âŒ | âœ… | 50% |
| GRPO | âŒ | âœ… | 70% |
| **GSPO** | âœ… | âŒ | **95%** |

#### 9.5.6 MOEå¾®è°ƒæœ€ä½³å®è·µ

**é…ç½®å»ºè®®**:

```python
training_args = {
    # å­¦ä¹ ç‡: æ¯”ç¨ å¯†æ¨¡å‹ä½10å€
    "learning_rate": 1e-5,  # ç¨ å¯†æ¨¡å‹é€šå¸¸1e-4

    # è¾…åŠ©æŸå¤±æƒé‡
    "aux_loss_coef": 0.01,

    # æ¢¯åº¦ç´¯ç§¯: é¿å…è¿‡å¤§
    "gradient_accumulation_steps": 4,  # ä¸è¶…è¿‡8

    # Warmup: MOEéœ€è¦æ›´é•¿warmup
    "warmup_ratio": 0.1,  # ç¨ å¯†æ¨¡å‹é€šå¸¸0.03

    # ä¸“å®¶dropout (é˜²æ­¢è¿‡æ‹Ÿåˆ)
    "expert_dropout": 0.1,
}
```

**ä»»åŠ¡ç‰¹å®šå¾®è°ƒ**:

| ä»»åŠ¡ç±»å‹ | å¾®è°ƒç­–ç•¥ | å¾®è°ƒç›®æ ‡ |
|---------|---------|---------|
| **é€šç”¨å¯¹è¯** | å…¨ä¸“å®¶LoRA | æ‰€æœ‰ä¸“å®¶ + è·¯ç”± |
| **æ•°å­¦æ¨ç†** | é€‰æ‹©æ€§å¾®è°ƒ | æ¿€æ´»é¢‘ç‡Top-4ä¸“å®¶ |
| **ä»£ç ç”Ÿæˆ** | ä¸“å®¶åˆ†ç»„ | ä»£ç ç›¸å…³ä¸“å®¶ç»„ |
| **å¤šè¯­è¨€** | è¯­è¨€ä¸“å®¶åˆ†ç»„ | ç›®æ ‡è¯­è¨€ä¸“å®¶ |

**æ˜¾å­˜ä¼˜åŒ–**:
```python
# ä½¿ç”¨8-bité‡åŒ– + LoRA
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-v0.1",
    quantization_config=bnb_config,
    device_map="auto"
)

# å•å¡24GBå¯å¾®è°ƒMixtral!
```

#### 9.5.7 MOEè®­ç»ƒæ¡†æ¶é€‰å‹

**MOEæ¨¡å‹è®­ç»ƒçš„ç‰¹æ®Šéœ€æ±‚**:
1. ç¨€ç–æ¿€æ´»çš„æ¢¯åº¦è®¡ç®—
2. è´Ÿè½½å‡è¡¡çš„è¾…åŠ©æŸå¤±
3. è·¯ç”±ç½‘ç»œçš„ç¨³å®šè®­ç»ƒ
4. å¤§è§„æ¨¡å‚æ•°çš„åˆ†å¸ƒå¼æ”¯æŒ

##### HuggingFace Transformers (åŸºç¡€æ”¯æŒ)

**ä¼˜åŠ¿**:
- âœ… åŸç”Ÿæ”¯æŒMixtralæ¨¡å‹
- âœ… `MixtralForCausalLM`è‡ªåŠ¨å¤„ç†è·¯ç”±
- âœ… PEFTåº“æ— ç¼é›†æˆLoRA

**ç¤ºä¾‹**:
```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

# åŠ è½½Mixtralæ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-v0.1",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# æ·»åŠ LoRA(è‡ªåŠ¨åº”ç”¨åˆ°æ‰€æœ‰ä¸“å®¶)
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "w1", "w2", "w3"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# è®­ç»ƒå‚æ•°(MOEä¸“ç”¨é…ç½®)
training_args = TrainingArguments(
    output_dir="./mixtral_lora",
    learning_rate=1e-5,  # æ¯”ç¨ å¯†æ¨¡å‹ä½10å€
    per_device_train_batch_size=1,  # MOEæ˜¾å­˜å ç”¨å¤§
    gradient_accumulation_steps=4,
    warmup_ratio=0.1,  # MOEéœ€è¦æ›´é•¿warmup
    bf16=True,
    gradient_checkpointing=True,
)

trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()
```

**é™åˆ¶**:
- âŒ ä¸æ”¯æŒé€‰æ‹©æ€§ä¸“å®¶å¾®è°ƒ(éœ€è¦æ‰‹åŠ¨å®ç°)
- âŒ è·¯ç”±æŸå¤±éœ€è¦è‡ªå®šä¹‰Trainer

##### LlamaFactory (æœ€æ¨è â­â­â­â­â­)

**ä¸ºä»€ä¹ˆé€‚åˆMOE**:
- âœ… Web UIä¸€é”®å¾®è°ƒMixtral/DeepSeek-V3
- âœ… è‡ªåŠ¨å¤„ç†è·¯ç”±è¾…åŠ©æŸå¤±
- âœ… å†…ç½®MOEä¸“ç”¨é…ç½®æ¨¡æ¿
- âœ… æ”¯æŒQLoRAæå¤§é™ä½æ˜¾å­˜

**YAMLé…ç½®ç¤ºä¾‹**:
```yaml
### Model
model_name_or_path: mistralai/Mixtral-8x7B-v0.1
quantization_bit: 4  # QLoRA

### LoRA
lora_rank: 16
lora_target: q_proj,k_proj,v_proj,o_proj,w1,w2,w3  # åŒ…å«ä¸“å®¶FFN

### Training (MOEä¸“ç”¨)
learning_rate: 1.0e-5  # æ¯”ç¨ å¯†æ¨¡å‹ä½
warmup_ratio: 0.1
router_aux_loss_coef: 0.01  # è´Ÿè½½å‡è¡¡æŸå¤±
```

**æ˜¾å­˜éœ€æ±‚** (Mixtral 8x7B):
- QLoRA 4-bit: ~24GB (RTX 4090 âœ…)
- LoRA FP16: ~80GB (2Ã—A100)

##### TRL (RLHF MOE â­â­â­â­)

**è§£å†³æ–¹æ¡ˆ: GSPO (DeepSeek-R1åŒæ¬¾)**

```python
from trl import GRPOConfig, GRPOTrainer

config = GRPOConfig(
    learning_rate=1e-6,  # MOE RLHFæ›´ä½å­¦ä¹ ç‡
    router_aux_loss_coef=0.01,
    routing_stabilization=True,  # GSPOç¨³å®šè·¯ç”±
)

trainer = GRPOTrainer(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    config=config,
    train_dataset=dataset,
)
trainer.train()
```

**GSPO vs PPO (MOEè®­ç»ƒæˆåŠŸç‡)**:
- PPO: 50%
- GRPO: 70%
- **GSPO**: **95%** âœ…

##### æ¡†æ¶é€‰å‹å†³ç­–æ ‘(MOEä¸“ç”¨)

```
MOEæ¨¡å‹è®­ç»ƒ
â”‚
â”œâ”€ å°å‹MOE (<50B, å¦‚Mixtral 8x7B)
â”‚   â”œâ”€ å¿«é€ŸéªŒè¯? â†’ LlamaFactory Web UI (QLoRA 24GB)
â”‚   â”œâ”€ çµæ´»é…ç½®? â†’ Axolotl (Flash Attention)
â”‚   â””â”€ è‡ªå®šä¹‰è®­ç»ƒ? â†’ Transformers + PEFT
â”‚
â”œâ”€ å¤§å‹MOE (>100B, å¦‚DeepSeek-V3)
â”‚   â””â”€ å¤šæœºå¤šå¡? â†’ DeepSpeed ZeRO-3 (å¿…å¤‡)
â”‚
â””â”€ RLHFä»»åŠ¡? â†’ TRL GRPOTrainer (GSPOè·¯ç”±ç¨³å®š)
```

---

#### 9.5.8 ä»£è¡¨æ€§MOEæ¨¡å‹

| æ¨¡å‹ | ä¸“å®¶æ•° | æ¿€æ´»ä¸“å®¶ | æ€»å‚æ•° | æ¿€æ´»å‚æ•° | ç‰¹ç‚¹ |
|------|-------|---------|--------|---------|------|
| **Mixtral 8x7B** | 8 | 2 | 47B | 13B | å¼€æºæ ‡æ† |
| **Mixtral 8x22B** | 8 | 2 | 141B | 39B | æ€§èƒ½æ¥è¿‘GPT-4 |
| **DeepSeek-V3** | 256 | 8 | 671B | 37B | æœ€å¤§å¼€æºMOE |
| **GPT-4** (æ¨æµ‹) | ~16 | 2 | ~1.8T | ~220B | é—­æº |

**æ€§èƒ½å¯¹æ¯”**:

åœ¨MMLUåŸºå‡†ä¸Š:
- Llama-3-70B (ç¨ å¯†): 79.5%
- Mixtral 8x7B: 70.6%
- Mixtral 8x22B: **77.8%** (å‚æ•°é‡ä»…ä¸ºLlama-3çš„2å€,æ€§èƒ½æ¥è¿‘)

**æ¨ç†æ•ˆç‡**:
- Llama-3-70B: 70B FLOPS
- Mixtral 8x22B: 39B FLOPS (èŠ‚çœ44%!)

#### 9.5.9 æ€»ç»“

**MOEçš„ä¼˜åŠ¿**:
1. **å‚æ•°æ•ˆç‡**: æ€»å‚æ•°â†‘,æ¿€æ´»å‚æ•°â†’,æ€§èƒ½â†‘
2. **æ¨ç†åŠ é€Ÿ**: ç¨€ç–æ¿€æ´»é™ä½è®¡ç®—é‡
3. **ä¸“å®¶ä¸“ç²¾**: ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒçŸ¥è¯†é¢†åŸŸ

**MOEçš„æŒ‘æˆ˜**:
1. **è´Ÿè½½å‡è¡¡**: éœ€è¦è¾…åŠ©æŸå¤±
2. **è·¯ç”±å´©æºƒ**: GSPO/Routing Replay
3. **æ˜¾å­˜å ç”¨**: éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒæˆ–é‡åŒ–

**å¾®è°ƒå»ºè®®**:
- èµ„æºå……è¶³: å…¨å‚æ•°å¾®è°ƒ
- èµ„æºå—é™: LoRA + 8-bité‡åŒ–
- ä»»åŠ¡ç‰¹å®š: é€‰æ‹©æ€§ä¸“å®¶å¾®è°ƒ
- MOEé¦–é€‰: GSPOç®—æ³•(è·¯ç”±ç¨³å®š)

**å‚è€ƒèµ„æ–™**:
- [Mixtral of Experts (arXiv 2024)](https://arxiv.org/abs/2401.04088)
- [DeepSeek-V3 Technical Report (2025)](https://github.com/deepseek-ai/DeepSeek-V3)
- [Switch Transformers (JMLR 2022)](https://arxiv.org/abs/2101.03961)

---

# Part B: å¼ºåŒ–å­¦ä¹ åŸºç¡€


## ç¬¬11ç«  å¼ºåŒ–å­¦ä¹ åŸºç¡€ç†è®º
- 0.1 ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ 
  - 0.1.1 ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«
  - 0.1.2 æ ¸å¿ƒè¦ç´ :æ™ºèƒ½ä½“ã€ç¯å¢ƒã€çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±
  - 0.1.3 RLçš„åº”ç”¨åœºæ™¯
- 0.2 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)
  - 0.2.1 MDPçš„æ•°å­¦å®šä¹‰
  - 0.2.2 ç­–ç•¥(Policy)
  - 0.2.3 å›æŠ¥(Return)ä¸æŠ˜æ‰£å› å­
  - 0.2.4 å€¼å‡½æ•°(Value Function)
- 0.3 Bellmanæ–¹ç¨‹
  - 0.3.1 çŠ¶æ€å€¼å‡½æ•°çš„Bellmanæ–¹ç¨‹
  - 0.3.2 åŠ¨ä½œå€¼å‡½æ•°çš„Bellmanæ–¹ç¨‹
  - 0.3.3 æœ€ä¼˜Bellmanæ–¹ç¨‹
- 0.4 RLç®—æ³•åˆ†ç±»
  - 0.4.1 åŸºäºå€¼å‡½æ•°çš„æ–¹æ³•(Value-based)
  - 0.4.2 åŸºäºç­–ç•¥çš„æ–¹æ³•(Policy-based)
  - 0.4.3 Actor-Criticæ–¹æ³•
  - 0.4.4 Model-based vs Model-free

## ç¬¬12ç«  ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•
- 1.1 Q-Learningç®—æ³•
  - 1.1.1 Qè¡¨æ›´æ–°è§„åˆ™
  - 1.1.2 æ¢ç´¢ä¸åˆ©ç”¨(Exploration vs Exploitation)
  - 1.1.3 Îµ-greedyç­–ç•¥
  - 1.1.4 å®æˆ˜:ç”¨Q-Learningè§£å†³FrozenLake
- 1.2 Deep Q-Network(DQN)
  - 1.2.1 ä»Q-Learningåˆ°DQN
  - 1.2.2 ç»éªŒå›æ”¾(Experience Replay)
  - 1.2.3 ç›®æ ‡ç½‘ç»œ(Target Network)
  - 1.2.4 DQNçš„å®Œæ•´PyTorchå®ç°
- 1.3 DQNçš„æ”¹è¿›ç®—æ³•
  - 1.3.1 Double DQN
  - 1.3.2 Dueling DQN
  - 1.3.3 Rainbow DQN
- 1.4 ä½¿ç”¨Stable-Baselines3å®æˆ˜
  - 1.4.1 å®‰è£…ä¸ç¯å¢ƒé…ç½®(2025æœ€æ–°ç‰ˆ)
  - 1.4.2 CartPole-v1ä»»åŠ¡
  - 1.4.3 è®­ç»ƒDQNæ™ºèƒ½ä½“
  - 1.4.4 æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–

## ç¬¬13ç«  ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- 2.1 ç­–ç•¥æ¢¯åº¦åŸºç¡€
  - 2.1.1 ç­–ç•¥å‚æ•°åŒ–
  - 2.1.2 ç­–ç•¥æ¢¯åº¦å®šç†(Policy Gradient Theorem)
  - 2.1.3 REINFORCEç®—æ³•
  - 2.1.4 åŸºçº¿(Baseline)æŠ€æœ¯
- 2.2 Actor-Criticæ¡†æ¶
  - 2.2.1 Criticçš„ä½œç”¨
  - 2.2.2 ä¼˜åŠ¿å‡½æ•°(Advantage Function)
  - 2.2.3 A2C(Advantage Actor-Critic)
  - 2.2.4 A3C(Asynchronous A3C)
- 2.3 å®æˆ˜:CartPoleçš„REINFORCEå®ç°
  - 2.3.1 ç­–ç•¥ç½‘ç»œè®¾è®¡
  - 2.3.2 è½¨è¿¹é‡‡æ ·
  - 2.3.3 æ¢¯åº¦è®¡ç®—ä¸æ›´æ–°
  - 2.3.4 è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

## ç¬¬14ç«  PPOç®—æ³•è¯¦è§£
- 3.1 ä»TRPOåˆ°PPO
  - 3.1.1 ç­–ç•¥æ›´æ–°çš„æŒ‘æˆ˜
  - 3.1.2 TRPOçš„ä¿¡èµ–åŸŸçº¦æŸ
  - 3.1.3 PPOçš„ç®€åŒ–æ€æƒ³
- 3.2 PPOçš„æ•°å­¦æ¨å¯¼
  - 3.2.1 é‡è¦æ€§é‡‡æ ·(Importance Sampling)
  - 3.2.2 æ›¿ä»£ç›®æ ‡å‡½æ•°(Surrogate Objective)
  - 3.2.3 Clippedç›®æ ‡å‡½æ•°
  - 3.2.4 å®Œæ•´çš„PPOæŸå¤±å‡½æ•°
- 3.3 PPOç®—æ³•å®ç°
  - 3.3.1 ç½‘ç»œæ¶æ„è®¾è®¡
  - 3.3.2 ä¼˜åŠ¿ä¼°è®¡(GAE)
  - 3.3.3 å°æ‰¹é‡è®­ç»ƒ
  - 3.3.4 å®Œæ•´PyTorchä»£ç 
- 3.4 PPOè¶…å‚æ•°è°ƒä¼˜
  - 3.4.1 å…³é”®è¶…å‚æ•°è§£æ
  - 3.4.2 å¸¸è§è®­ç»ƒé—®é¢˜
  - 3.4.3 è°ƒè¯•æŠ€å·§

## ç¬¬15ç«  å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨
- 4.1 RLHFæ¦‚è¿°
  - 4.1.1 ä¸ºä»€ä¹ˆéœ€è¦RLHF
  - 4.1.2 RLHFçš„ä¸‰é˜¶æ®µæµç¨‹
  - 4.1.3 ç»å…¸åº”ç”¨:ChatGPTã€Claude
- 4.2 é˜¶æ®µ1:ç›‘ç£å¾®è°ƒ(SFT)
  - 4.2.1 é«˜è´¨é‡æŒ‡ä»¤æ•°æ®
  - 4.2.2 SFTè®­ç»ƒæµç¨‹
  - 4.2.3 TRLçš„SFTTrainerä½¿ç”¨
- 4.3 é˜¶æ®µ2:å¥–åŠ±æ¨¡å‹(Reward Model)
  - 4.3.1 äººç±»åå¥½æ•°æ®æ”¶é›†
  - 4.3.2 åå¥½å¯¹æ¯”å­¦ä¹ 
  - 4.3.3 å¥–åŠ±æ¨¡å‹è®­ç»ƒ
  - 4.3.4 TRLçš„RewardTrainerä½¿ç”¨
- 4.4 é˜¶æ®µ3:PPOå¼ºåŒ–å­¦ä¹ 
  - 4.4.1 RLç¯å¢ƒè®¾è®¡
  - 4.4.2 å¥–åŠ±å¡‘å½¢(Reward Shaping)
  - 4.4.3 KLæ•£åº¦çº¦æŸ
  - 4.4.4 TRLçš„PPOTrainerä½¿ç”¨
- 4.5 RLHFçš„æ”¹è¿›ç®—æ³•
  - 4.5.1 DPO(Direct Preference Optimization)
  - 4.5.2 GRPO(Group Relative Policy Optimization)
  - 4.5.3 RLAIF(RL from AI Feedback)
- 4.6 OpenRLHF 0.8.11å®æˆ˜
  - 4.6.1 å®‰è£…ä¸ç¯å¢ƒé…ç½®
  - 4.6.2 Rayé›†ç¾¤è®­ç»ƒ
  - 4.6.3 vLLMåŠ é€Ÿæ¨ç†
  - 4.6.4 å®Œæ•´RLHF Pipeline
- 4.7 TRLæœ€æ–°ç‰¹æ€§(2025)
  - 4.7.1 GRPOTrainer(Deepseek R1åŒæ¬¾)
  - 4.7.2 Online DPO
  - 4.7.3 KTO(Kahneman-Tversky Optimization)

## ç¬¬16ç«  å¼ºåŒ–å­¦ä¹ å®æˆ˜é¡¹ç›®
- 5.1 é¡¹ç›®1:CartPoleå¹³è¡¡æ†(DQN)
  - 5.1.1 ä»»åŠ¡æè¿°ä¸ç¯å¢ƒ
  - 5.1.2 DQNç½‘ç»œå®ç°
  - 5.1.3 è®­ç»ƒä¸è¯„ä¼°
  - 5.1.4 ç»“æœåˆ†æ
- 5.2 é¡¹ç›®2:Atariæ¸¸æˆ(PPO)
  - 5.2.1 ç¯å¢ƒé¢„å¤„ç†
  - 5.2.2 CNNç­–ç•¥ç½‘ç»œ
  - 5.2.3 å¹¶è¡Œç¯å¢ƒè®­ç»ƒ
  - 5.2.4 æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- 5.3 é¡¹ç›®3:å¤§æ¨¡å‹å¯¹é½(RLHF)
  - 5.3.1 æ•°æ®å‡†å¤‡:SFTä¸åå¥½æ•°æ®
  - 5.3.2 SFTè®­ç»ƒ
  - 5.3.3 Reward Modelè®­ç»ƒ
  - 5.3.4 PPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
  - 5.3.5 æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²
- 5.4 è¿›é˜¶å®è·µ
  - 5.4.1 å¤šè½®å¯¹è¯RLHF
  - 5.4.2 ä»£ç ç”Ÿæˆä»»åŠ¡çš„RLä¼˜åŒ–
  - 5.4.3 ç»“åˆå·¥å…·ä½¿ç”¨çš„Agentè®­ç»ƒ

---

## å­¦ä¹ å»ºè®®

1. **å¾ªåºæ¸è¿›**: ä»ç¬¬0ç« åŸºç¡€ç†è®ºå¼€å§‹,ç†è§£MDPå’ŒBellmanæ–¹ç¨‹
2. **åŠ¨æ‰‹å®è·µ**: æ¯ä¸ªç®—æ³•éƒ½é…æœ‰å®Œæ•´ä»£ç ,å»ºè®®åœ¨Google Colabè¿è¡Œ
3. **æ•°å­¦æ¨å¯¼**: PPOéƒ¨åˆ†çš„æ•°å­¦æ¨å¯¼å¾ˆé‡è¦,å»ºè®®ä»”ç»†é˜…è¯»
4. **å…³æ³¨2025æ–°å·¥å…·**: OpenRLHF 0.8.11å’ŒTRLçš„æœ€æ–°ç‰¹æ€§
5. **ä¸‰å¤§é¡¹ç›®å¿…åš**: DQNã€PPOã€RLHFé¡¹ç›®æ˜¯æ ¸å¿ƒå®æˆ˜

---

# ç¬¬0ç«  å¼ºåŒ–å­¦ä¹ åŸºç¡€ç†è®º

## 0.1 ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ 

### 0.1.1 ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«

å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯,ä¸ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ å¹¶åˆ—ä¸ºä¸‰å¤§å­¦ä¹ èŒƒå¼ã€‚

**å¯¹æ¯”è¡¨æ ¼**:

| ç»´åº¦ | ç›‘ç£å­¦ä¹  | å¼ºåŒ–å­¦ä¹  |
|------|---------|---------|
| è®­ç»ƒæ•°æ® | æ ‡æ³¨å¥½çš„(x, y)å¯¹ | æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿ |
| åé¦ˆæ–¹å¼ | æ¯ä¸ªæ ·æœ¬ç«‹å³ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ | å»¶è¿Ÿåé¦ˆ,é€šè¿‡å¥–åŠ±ä¿¡å· |
| ç›®æ ‡ | å­¦ä¹ xâ†’yçš„æ˜ å°„å‡½æ•° | å­¦ä¹ æœ€ä¼˜ç­–ç•¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ± |
| å…¸å‹åº”ç”¨ | å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ« | æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€LLMå¯¹é½ |

**æ ¸å¿ƒåŒºåˆ«**:
1. **æ— æ ‡ç­¾æ•°æ®**: RLæ²¡æœ‰"æ­£ç¡®ç­”æ¡ˆ",åªæœ‰å¥–åŠ±ä¿¡å·
2. **åºåˆ—å†³ç­–**: å½“å‰åŠ¨ä½œä¼šå½±å“æœªæ¥çš„çŠ¶æ€å’Œå¥–åŠ±
3. **æ¢ç´¢ä¸åˆ©ç”¨**: éœ€è¦å¹³è¡¡å°è¯•æ–°åŠ¨ä½œ(æ¢ç´¢)å’Œé€‰æ‹©å·²çŸ¥æœ€ä¼˜åŠ¨ä½œ(åˆ©ç”¨)
4. **å»¶è¿Ÿå¥–åŠ±**: ä¸€ä¸ªåŠ¨ä½œçš„å¥½åå¯èƒ½è¦å¾ˆä¹…åæ‰èƒ½ä½“ç°

**ä¾‹å­ - ä¸‹æ£‹**:
- ç›‘ç£å­¦ä¹ : ç»™å®šæ£‹å±€,å­¦ä¹ ä¸“å®¶ä¼šèµ°å“ªä¸€æ­¥
- å¼ºåŒ–å­¦ä¹ : é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆ,å­¦ä¹ èƒ½èµ¢å¾—æ¯”èµ›çš„ç­–ç•¥

### 0.1.2 æ ¸å¿ƒè¦ç´ 

å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¡†æ¶åŒ…å«ä»¥ä¸‹è¦ç´ :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent  â”‚â—„â”€â”€â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”‚   Env   â”‚
â”‚ (æ™ºèƒ½ä½“) â”‚                    â”‚  (ç¯å¢ƒ)  â”‚
â”‚         â”‚â”€â”€â”€â”€â”€ Action â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚
â”‚         â”‚â—„â”€â”€â”€â”€â”€ Reward â”€â”€â”€â”€â”€â”€â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**1. æ™ºèƒ½ä½“(Agent)**: å­¦ä¹ å’Œå†³ç­–çš„ä¸»ä½“
- ä¾‹å¦‚: æ¸¸æˆç©å®¶ã€æœºå™¨äººã€LLM

**2. ç¯å¢ƒ(Environment)**: æ™ºèƒ½ä½“æ‰€å¤„çš„å¤–éƒ¨ä¸–ç•Œ
- ä¾‹å¦‚: æ£‹ç›˜ã€ç‰©ç†ä¸–ç•Œã€å¯¹è¯ç³»ç»Ÿ

**3. çŠ¶æ€(State) s**: ç¯å¢ƒçš„å½“å‰æè¿°
- ä¾‹å¦‚: æ£‹ç›˜å±€é¢ã€æœºå™¨äººä½ç½®ã€å¯¹è¯å†å²

**4. åŠ¨ä½œ(Action) a**: æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ
- ä¾‹å¦‚: èµ°æ£‹ã€ç§»åŠ¨ã€ç”Ÿæˆä¸‹ä¸€ä¸ªtoken

**5. å¥–åŠ±(Reward) r**: ç¯å¢ƒå¯¹åŠ¨ä½œçš„åé¦ˆ,æ ‡é‡ä¿¡å·
- ä¾‹å¦‚: èµ¢æ£‹+1/è¾“æ£‹-1ã€å‰è¿›+0.1ã€äººç±»åå¥½å¾—åˆ†

**6. ç­–ç•¥(Policy) Ï€**: ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- Ï€(a|s) = P(é€‰æ‹©åŠ¨ä½œa | å½“å‰çŠ¶æ€s)

**äº¤äº’å¾ªç¯**:
```
t=0: åˆå§‹çŠ¶æ€ sâ‚€
t=1: Agentæ ¹æ®ç­–ç•¥Ï€é€‰æ‹©åŠ¨ä½œ aâ‚€, è·å¾—å¥–åŠ± râ‚, ç¯å¢ƒè½¬ç§»åˆ° sâ‚
t=2: Agentæ ¹æ®ç­–ç•¥Ï€é€‰æ‹©åŠ¨ä½œ aâ‚, è·å¾—å¥–åŠ± râ‚‚, ç¯å¢ƒè½¬ç§»åˆ° sâ‚‚
...
```

### 0.1.3 RLçš„åº”ç”¨åœºæ™¯

**ç»å…¸åœºæ™¯**:
1. **æ¸¸æˆAI**: AlphaGo(å›´æ£‹)ã€OpenAI Five(Dota2)ã€MuZero
2. **æœºå™¨äººæ§åˆ¶**: è¡Œèµ°ã€æŠ“å–ã€è‡ªåŠ¨é©¾é©¶
3. **æ¨èç³»ç»Ÿ**: æ–°é—»æ¨èã€å¹¿å‘ŠæŠ•æ”¾
4. **èµ„æºç®¡ç†**: æ•°æ®ä¸­å¿ƒå†·å´ã€äº¤é€šä¿¡å·ç¯æ§åˆ¶

**å¤§æ¨¡å‹æ—¶ä»£çš„æ–°åº”ç”¨**:
1. **RLHF**: ChatGPTã€Claudeã€GPT-4çš„æ ¸å¿ƒè®­ç»ƒæŠ€æœ¯
2. **Agentå†³ç­–**: AutoGPTã€BabyAGIçš„è§„åˆ’èƒ½åŠ›
3. **ä»£ç ç”Ÿæˆ**: AlphaCodeä½¿ç”¨RLä¼˜åŒ–ä»£ç è´¨é‡
4. **å¤šæ¨¡æ€å¯¹è¯**: Geminiã€GPT-4Vçš„äº¤äº’ä¼˜åŒ–

---

## 0.2 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)

### 0.2.1 MDPçš„æ•°å­¦å®šä¹‰

å¼ºåŒ–å­¦ä¹ é—®é¢˜é€šå¸¸å»ºæ¨¡ä¸º**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)**ã€‚

**å®šä¹‰**: MDPæ˜¯ä¸€ä¸ªäº”å…ƒç»„ **(S, A, P, R, Î³)**:

1. **S**: çŠ¶æ€ç©ºé—´(State Space)
   - æœ‰é™çŠ¶æ€: æ£‹ç›˜å±€é¢
   - è¿ç»­çŠ¶æ€: æœºå™¨äººå…³èŠ‚è§’åº¦

2. **A**: åŠ¨ä½œç©ºé—´(Action Space)
   - ç¦»æ•£åŠ¨ä½œ: {ä¸Š, ä¸‹, å·¦, å³}
   - è¿ç»­åŠ¨ä½œ: æ–¹å‘ç›˜è§’åº¦[-30Â°, 30Â°]

3. **P**: çŠ¶æ€è½¬ç§»æ¦‚ç‡(Transition Probability)
   - P(s'|s, a) = P(ä¸‹ä¸€çŠ¶æ€=s' | å½“å‰çŠ¶æ€=s, åŠ¨ä½œ=a)
   - ç¡®å®šæ€§ç¯å¢ƒ: P(s'|s,a) âˆˆ {0, 1}
   - éšæœºæ€§ç¯å¢ƒ: 0 â‰¤ P(s'|s,a) â‰¤ 1

4. **R**: å¥–åŠ±å‡½æ•°(Reward Function)
   - R(s, a, s') = æ‰§è¡ŒåŠ¨ä½œaä»sè½¬ç§»åˆ°s'è·å¾—çš„å³æ—¶å¥–åŠ±

5. **Î³**: æŠ˜æ‰£å› å­(Discount Factor), Î³ âˆˆ [0, 1)
   - è¡¡é‡æœªæ¥å¥–åŠ±çš„é‡è¦æ€§

**é©¬å°”å¯å¤«æ€§è´¨(Markov Property)**:

æœªæ¥åªä¾èµ–äºç°åœ¨,ä¸è¿‡å»æ— å…³:

```
P(sâ‚œâ‚Šâ‚ | sâ‚œ, aâ‚œ, sâ‚œâ‚‹â‚, aâ‚œâ‚‹â‚, ..., sâ‚€, aâ‚€) = P(sâ‚œâ‚Šâ‚ | sâ‚œ, aâ‚œ)
```

**ä¾‹å­ - FrozenLakeç¯å¢ƒ**:

```
SFFF    S: èµ·ç‚¹(Start)
FHFH    F: å†°é¢(Frozen)
FFFH    H: å†°æ´(Hole)
HFFG    G: ç›®æ ‡(Goal)
```

- **S** = {ä½ç½®(0,0), ..., ä½ç½®(3,3)} å…±16ä¸ªçŠ¶æ€
- **A** = {ä¸Š, ä¸‹, å·¦, å³}
- **P**: å†°é¢æœ‰æ»‘åŠ¨,æ‰§è¡Œ"å³"å¯èƒ½å®é™…å¾€"ä¸Š"/"å³"/"ä¸‹"
- **R**: åˆ°è¾¾Gå¥–åŠ±+1, æ‰å…¥Hå¥–åŠ±0, å…¶ä»–å¥–åŠ±0
- **Î³** = 0.99

### 0.2.2 ç­–ç•¥(Policy)

**å®šä¹‰**: ç­–ç•¥Ï€æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„:

1. **ç¡®å®šæ€§ç­–ç•¥**: a = Ï€(s)
   - è¾“å…¥çŠ¶æ€s,è¾“å‡ºç¡®å®šçš„åŠ¨ä½œa

2. **éšæœºæ€§ç­–ç•¥**: Ï€(a|s)
   - è¾“å…¥çŠ¶æ€s,è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
   - ä¾‹å¦‚: Ï€(å·¦|sâ‚) = 0.7, Ï€(å³|sâ‚) = 0.3

**ä¸ºä»€ä¹ˆéœ€è¦éšæœºç­–ç•¥?**
1. æ¢ç´¢: é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜
2. çŸ³å¤´å‰ªåˆ€å¸ƒ: å¯¹æ‰‹å¯ä»¥é¢„æµ‹ç¡®å®šæ€§ç­–ç•¥
3. çŠ¶æ€éƒ¨åˆ†å¯è§‚æµ‹: éšæœºæ€§å¯ä»¥æä¾›æ›´å¥½çš„é•¿æœŸæ”¶ç›Š

**ç­–ç•¥çš„è¡¨ç¤º**:

```python
# ç¡®å®šæ€§ç­–ç•¥ - è¡¨æ ¼å½¢å¼
policy = {
    's0': 'right',
    's1': 'up',
    's2': 'left'
}

# éšæœºç­–ç•¥ - ç¥ç»ç½‘ç»œ
class PolicyNetwork(nn.Module):
    def forward(self, state):
        logits = self.net(state)
        return F.softmax(logits, dim=-1)  # è¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
```

### 0.2.3 å›æŠ¥(Return)ä¸æŠ˜æ‰£å› å­

**å›æŠ¥(Return)**: ä»æ—¶åˆ»tå¼€å§‹çš„ç´¯ç§¯å¥–åŠ±

$$G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$$

**æŠ˜æ‰£å› å­Î³çš„ä½œç”¨**:

1. **æ•°å­¦æ”¶æ•›**: æ— é™å›åˆæ—¶,ç¡®ä¿Gâ‚œæœ‰ç•Œ(å‡è®¾å¥–åŠ±æœ‰ç•Œ)
2. **ä¸ç¡®å®šæ€§**: æœªæ¥çš„å¥–åŠ±ä¸å¦‚ç°åœ¨çš„å¥–åŠ±å¯é 
3. **è¿‘è§†vsè¿œè§†**:
   - Î³=0: åªçœ‹å³æ—¶å¥–åŠ±(æåº¦è¿‘è§†)
   - Î³â†’1: é‡è§†é•¿æœŸå¥–åŠ±(æåº¦è¿œè§†)
   - å¸¸ç”¨å€¼: 0.9-0.99

**ä¾‹å­**:

å‡è®¾å¥–åŠ±åºåˆ—ä¸º [1, 2, 3, 4, ...], Î³=0.9:

```
Gâ‚€ = 1 + 0.9Ã—2 + 0.9Â²Ã—3 + 0.9Â³Ã—4 + ...
   = 1 + 1.8 + 2.43 + 2.916 + ...
```

è‹¥ Î³=0.5:
```
Gâ‚€ = 1 + 0.5Ã—2 + 0.5Â²Ã—3 + 0.5Â³Ã—4 + ...
   = 1 + 1 + 0.75 + 0.5 + ...  (æ›´å¿«è¡°å‡)
```

### 0.2.4 å€¼å‡½æ•°(Value Function)

å€¼å‡½æ•°è¡¡é‡"åœ¨æŸä¸ªçŠ¶æ€ä¸‹æœ‰å¤šå¥½"æˆ–"åœ¨æŸä¸ªçŠ¶æ€ä¸‹æ‰§è¡ŒæŸä¸ªåŠ¨ä½œæœ‰å¤šå¥½"ã€‚

#### çŠ¶æ€å€¼å‡½æ•° V<sup>Ï€</sup>(s)

**å®šä¹‰**: ä»çŠ¶æ€så¼€å§‹,éµå¾ªç­–ç•¥Ï€çš„æœŸæœ›å›æŠ¥

$$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | s_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s\right]$$

**ç›´è§‚ç†è§£**: "åœ¨çŠ¶æ€sä¸‹,æŒ‰ç…§ç­–ç•¥Ï€èµ°ä¸‹å»,æœŸæœ›èƒ½è·å¾—å¤šå°‘æ€»å¥–åŠ±"

**ä¾‹å­ - å­¦ç”ŸMDP**:

```
çŠ¶æ€: {Class, Pub, Sleep}
åŠ¨ä½œ: {Study, Party, Quit}
```

å‡è®¾ç­–ç•¥Ï€ä¸º"åœ¨Classå­¦ä¹ ,åœ¨Pubèšä¼š,åœ¨Sleepç¡è§‰":

- V<sup>Ï€</sup>(Class) = é«˜åˆ†æ¯•ä¸šçš„æœŸæœ›å›æŠ¥ = 10
- V<sup>Ï€</sup>(Pub) = æœŸæœ›å›æŠ¥è¾ƒä½ = -2
- V<sup>Ï€</sup>(Sleep) = æœŸæœ›å›æŠ¥ä¸º0 = 0

#### åŠ¨ä½œå€¼å‡½æ•° Q<sup>Ï€</sup>(s, a)

**å®šä¹‰**: ä»çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œa,ä¹‹åéµå¾ªç­–ç•¥Ï€çš„æœŸæœ›å›æŠ¥

$$Q^{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | s_t = s, a_t = a]$$

**ç›´è§‚ç†è§£**: "åœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œa,ç„¶åæŒ‰ç­–ç•¥Ï€èµ°,æœŸæœ›èƒ½è·å¾—å¤šå°‘æ€»å¥–åŠ±"

**Vå’ŒQçš„å…³ç³»**:

$$V^{\pi}(s) = \sum_{a} \pi(a|s) Q^{\pi}(s, a)$$

å³: çŠ¶æ€ä»·å€¼ = è¯¥çŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œä»·å€¼çš„æœŸæœ›

---

## 0.3 Bellmanæ–¹ç¨‹

Bellmanæ–¹ç¨‹æ˜¯RLçš„æ ¸å¿ƒ,å°†å€¼å‡½æ•°çš„é€’å½’å…³ç³»è¡¨ç¤ºå‡ºæ¥,æ˜¯å‡ ä¹æ‰€æœ‰RLç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚

### 0.3.1 çŠ¶æ€å€¼å‡½æ•°çš„Bellmanæ–¹ç¨‹

**BellmanæœŸæœ›æ–¹ç¨‹**:

$$V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma V^{\pi}(s')\right]$$

**ç›´è§‚ç†è§£**:
```
å½“å‰çŠ¶æ€çš„ä»·å€¼ = ç«‹å³å¥–åŠ± + æŠ˜æ‰£çš„ä¸‹ä¸€çŠ¶æ€ä»·å€¼
```

**æ¨å¯¼**:

$$\begin{align}
V^{\pi}(s) &= \mathbb{E}_{\pi}[G_t | s_t = s] \\
&= \mathbb{E}_{\pi}[r_{t+1} + \gamma G_{t+1} | s_t = s] \\
&= \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^{\pi}(s')]
\end{align}$$

**ä¾‹å­ - æ‰‹å·¥è®¡ç®—**:

å‡è®¾ç®€å•çš„2çŠ¶æ€MDP:
- çŠ¶æ€: {sâ‚, sâ‚‚}
- ç­–ç•¥: Ï€(å³|sâ‚)=1, Ï€(å·¦|sâ‚‚)=1
- è½¬ç§»: P(sâ‚‚|sâ‚,å³)=1, P(sâ‚|sâ‚‚,å·¦)=1
- å¥–åŠ±: R(sâ‚,å³,sâ‚‚)=1, R(sâ‚‚,å·¦,sâ‚)=0
- Î³ = 0.9

æ±‚è§£:
```
V(sâ‚) = 1 + 0.9Ã—V(sâ‚‚)
V(sâ‚‚) = 0 + 0.9Ã—V(sâ‚)
```

è§£æ–¹ç¨‹:
```
V(sâ‚) = 1 + 0.9Ã—(0.9Ã—V(sâ‚))
V(sâ‚) = 1 + 0.81Ã—V(sâ‚)
0.19Ã—V(sâ‚) = 1
V(sâ‚) â‰ˆ 5.26
V(sâ‚‚) â‰ˆ 4.74
```

### 0.3.2 åŠ¨ä½œå€¼å‡½æ•°çš„Bellmanæ–¹ç¨‹

**BellmanæœŸæœ›æ–¹ç¨‹**:

$$Q^{\pi}(s, a) = \sum_{s'} P(s'|s,a) \left[R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')\right]$$

**ç®€åŒ–ç‰ˆ**(å½“è½¬ç§»å’Œå¥–åŠ±ç¡®å®š):

$$Q^{\pi}(s, a) = R(s,a) + \gamma V^{\pi}(s')$$

å…¶ä¸­ s' æ˜¯æ‰§è¡Œaåçš„ä¸‹ä¸€çŠ¶æ€ã€‚

### 0.3.3 æœ€ä¼˜Bellmanæ–¹ç¨‹

**æœ€ä¼˜ç­–ç•¥**:

$$\pi^* = \arg\max_{\pi} V^{\pi}(s), \quad \forall s \in S$$

**æœ€ä¼˜å€¼å‡½æ•°**:

$$V^*(s) = \max_{\pi} V^{\pi}(s)$$

$$Q^*(s, a) = \max_{\pi} Q^{\pi}(s, a)$$

**æœ€ä¼˜Bellmanæ–¹ç¨‹**:

$$V^*(s) = \max_{a} \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$$

$$Q^*(s, a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q^*(s', a')]$$

**å…³é”®å˜åŒ–**: å°†æœŸæœ›(âˆ‘Ï€(a|s))æ›¿æ¢ä¸ºæœ€å¤§åŒ–(max)

**ä»Q*æå–æœ€ä¼˜ç­–ç•¥**:

$$\pi^*(s) = \arg\max_{a} Q^*(s, a)$$

å³: æ€»æ˜¯é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ(è´ªå¿ƒç­–ç•¥)

---

## 0.4 RLç®—æ³•åˆ†ç±»

### 0.4.1 åŸºäºå€¼å‡½æ•°çš„æ–¹æ³•(Value-based)

**æ ¸å¿ƒæ€æƒ³**: å­¦ä¹ å€¼å‡½æ•° V(s) æˆ– Q(s,a), ç„¶åæ¨å¯¼ç­–ç•¥

**ä»£è¡¨ç®—æ³•**:
- Q-Learning
- SARSA
- DQN (Deep Q-Network)
- Double DQN, Dueling DQN

**ä¼˜ç‚¹**:
- é€‚åˆç¦»æ•£åŠ¨ä½œç©ºé—´
- ç†è®ºåŸºç¡€æ‰å®

**ç¼ºç‚¹**:
- éš¾ä»¥å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
- ç¡®å®šæ€§ç­–ç•¥,æ¢ç´¢èƒ½åŠ›å¼±

**ä¾‹å­**:
```python
# åŸºäºQè¡¨çš„ç­–ç•¥
def get_action(state, Q_table):
    return np.argmax(Q_table[state])  # é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
```

### 0.4.2 åŸºäºç­–ç•¥çš„æ–¹æ³•(Policy-based)

**æ ¸å¿ƒæ€æƒ³**: ç›´æ¥å­¦ä¹ ç­–ç•¥ Ï€(a|s), ä¸éœ€è¦å€¼å‡½æ•°

**ä»£è¡¨ç®—æ³•**:
- REINFORCE
- PPO (Proximal Policy Optimization)
- TRPO (Trust Region Policy Optimization)

**ä¼˜ç‚¹**:
- å¤©ç„¶æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
- å¯ä»¥å­¦ä¹ éšæœºç­–ç•¥
- æ”¶æ•›æ€§æ›´å¥½

**ç¼ºç‚¹**:
- æ ·æœ¬æ•ˆç‡ä½(éœ€è¦å¤§é‡è½¨è¿¹)
- è®­ç»ƒä¸ç¨³å®š(é«˜æ–¹å·®)

**ä¾‹å­**:
```python
# ç¥ç»ç½‘ç»œç­–ç•¥
class PolicyNet(nn.Module):
    def forward(self, state):
        logits = self.fc(state)
        return Categorical(logits=logits)  # è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ

# é‡‡æ ·åŠ¨ä½œ
dist = policy_net(state)
action = dist.sample()
```

### 0.4.3 Actor-Criticæ–¹æ³•

**æ ¸å¿ƒæ€æƒ³**: ç»“åˆå€¼å‡½æ•°å’Œç­–ç•¥

- **Actor**: ç­–ç•¥ç½‘ç»œ,è´Ÿè´£é€‰æ‹©åŠ¨ä½œ
- **Critic**: å€¼å‡½æ•°ç½‘ç»œ,è´Ÿè´£è¯„ä¼°åŠ¨ä½œ

**ä»£è¡¨ç®—æ³•**:
- A2C/A3C (Advantage Actor-Critic)
- PPO (ä¹Ÿå±äºè¿™ä¸€ç±»)
- SAC (Soft Actor-Critic)
- TD3 (Twin Delayed DDPG)

**ä¼˜ç‚¹**:
- é™ä½æ–¹å·®(é€šè¿‡Critic)
- æé«˜æ ·æœ¬æ•ˆç‡
- é€‚åˆè¿ç»­æ§åˆ¶

**æ¶æ„**:
```python
class ActorCritic(nn.Module):
    def __init__(self):
        self.actor = PolicyNet()   # è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ
        self.critic = ValueNet()   # è¾“å‡ºçŠ¶æ€ä»·å€¼

    def forward(self, state):
        action_dist = self.actor(state)
        value = self.critic(state)
        return action_dist, value
```

### 0.4.4 Model-based vs Model-free

**Model-free**: ä¸å­¦ä¹ ç¯å¢ƒæ¨¡å‹,ç›´æ¥ä»äº¤äº’ä¸­å­¦ä¹ ç­–ç•¥/å€¼å‡½æ•°
- Q-Learning, DQN, PPO éƒ½æ˜¯Model-free
- ä¼˜ç‚¹: ç®€å•,ä¸éœ€è¦ç¯å¢ƒçŸ¥è¯†
- ç¼ºç‚¹: æ ·æœ¬æ•ˆç‡ä½

**Model-based**: å…ˆå­¦ä¹ ç¯å¢ƒåŠ¨åŠ›å­¦æ¨¡å‹ P(s'|s,a), ç„¶åè§„åˆ’
- Dyna-Q, MuZero, Dreamer
- ä¼˜ç‚¹: æ ·æœ¬æ•ˆç‡é«˜,å¯ä»¥åšè§„åˆ’
- ç¼ºç‚¹: æ¨¡å‹è¯¯å·®ä¼šç´¯ç§¯

**å¯¹æ¯”**:

| ç‰¹æ€§ | Model-free | Model-based |
|------|-----------|-------------|
| æ ·æœ¬æ•ˆç‡ | ä½ | é«˜ |
| è®¡ç®—å¤æ‚åº¦ | ä½ | é«˜ |
| éœ€è¦ç¯å¢ƒçŸ¥è¯† | å¦ | éœ€è¦å­¦ä¹ æ¨¡å‹ |
| ä»£è¡¨ç®—æ³• | DQN, PPO | MuZero |

**å¤§æ¨¡å‹RLHFä½¿ç”¨**: **Model-free** (PPOä¸ºä¸»)
- åŸå› : LLMçš„"ç¯å¢ƒ"(äººç±»åå¥½)éš¾ä»¥å»ºæ¨¡

---

**ç¬¬0ç« å°ç»“**:

æˆ‘ä»¬å­¦ä¹ äº†RLçš„åŸºç¡€ç†è®º:
1. RLä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«
2. MDPäº”å…ƒç»„(S, A, P, R, Î³)
3. ç­–ç•¥Ï€, å›æŠ¥G, å€¼å‡½æ•°Vå’ŒQ
4. Bellmanæ–¹ç¨‹: å€¼å‡½æ•°çš„é€’å½’å½¢å¼
5. RLç®—æ³•åˆ†ç±»: Value-based, Policy-based, Actor-Critic

æŒæ¡è¿™äº›æ¦‚å¿µå,æˆ‘ä»¬å¯ä»¥å¼€å§‹å­¦ä¹ å…·ä½“ç®—æ³•ã€‚ä¸‹ä¸€ç« å°†ä»‹ç»ç»å…¸çš„Q-Learningå’ŒDQNç®—æ³•ã€‚

---

# ç¬¬1ç«  ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•

## 1.1 Q-Learningç®—æ³•

### 1.1.1 Qè¡¨æ›´æ–°è§„åˆ™

Q-Learningæ˜¯æœ€ç»å…¸çš„å€¼å‡½æ•°æ–¹æ³•,ç”±Watkinsåœ¨1989å¹´æå‡ºã€‚å®ƒç›´æ¥å­¦ä¹ æœ€ä¼˜åŠ¨ä½œå€¼å‡½æ•°Q*(s,a),è€Œä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„è½¬ç§»æ¦‚ç‡P(s'|s,a)ã€‚

**Q-Learningæ ¸å¿ƒæ€æƒ³**:

ä½¿ç”¨æ—¶åºå·®åˆ†(Temporal Difference, TD)å­¦ä¹ æ¥æ›´æ–°Qå€¼:

$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$$

**å…¬å¼è§£è¯»**:
- **å½“å‰Qå€¼**: Q(s_t, a_t) - å½“å‰å¯¹(s,a)çš„ä»·å€¼ä¼°è®¡
- **å­¦ä¹ ç‡**: Î± âˆˆ (0, 1] - æ§åˆ¶æ›´æ–°æ­¥é•¿
- **TDç›®æ ‡**: r_{t+1} + Î³ max Q(s_{t+1}, a') - å³æ—¶å¥–åŠ± + æŠ˜æ‰£çš„æœ€ä¼˜æœªæ¥ä»·å€¼
- **TDè¯¯å·®**: [TDç›®æ ‡ - å½“å‰Qå€¼] - ä¼°è®¡çš„è¯¯å·®

**ä¸Bellmanæœ€ä¼˜æ–¹ç¨‹çš„å…³ç³»**:

Q-Learningçš„æ›´æ–°è§„åˆ™æ˜¯åœ¨é‡‡æ ·åœ°è¿‘ä¼¼æ±‚è§£:

$$Q^*(s, a) = \mathbb{E}[r + \gamma \max_{a'} Q^*(s', a')]$$

**å…³é”®ç‰¹æ€§**:
1. **Off-policy**: å¯ä»¥ä»ä»»æ„ç­–ç•¥çš„æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥
2. **Model-free**: ä¸éœ€è¦çŸ¥é“ç¯å¢ƒçš„è½¬ç§»æ¦‚ç‡
3. **æ”¶æ•›æ€§**: åœ¨è¡¨æ ¼å‹é—®é¢˜ä¸­,å¦‚æœæ‰€æœ‰(s,a)å¯¹è¢«æ— é™æ¬¡è®¿é—®,Qè¡¨ä¼šæ”¶æ•›åˆ°Q*

### 1.1.2 æ¢ç´¢ä¸åˆ©ç”¨(Exploration vs Exploitation)

RLé¢ä¸´çš„æ ¸å¿ƒå›°å¢ƒ:
- **åˆ©ç”¨(Exploitation)**: é€‰æ‹©å½“å‰å·²çŸ¥çš„æœ€ä¼˜åŠ¨ä½œ,æœ€å¤§åŒ–å³æ—¶å›æŠ¥
- **æ¢ç´¢(Exploration)**: å°è¯•æ–°åŠ¨ä½œ,å¯èƒ½å‘ç°æ›´å¥½çš„ç­–ç•¥

**ä¾‹å­**:
- åˆ©ç”¨: æ€»æ˜¯å»ä½ æœ€å–œæ¬¢çš„é¤å…
- æ¢ç´¢: å°è¯•æ–°é¤å…,å¯èƒ½å‘ç°æ›´å¥½çš„

å¦‚æœåªåˆ©ç”¨,å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜;å¦‚æœåªæ¢ç´¢,æ— æ³•è·å¾—å¥½çš„å›æŠ¥ã€‚

### 1.1.3 Îµ-greedyç­–ç•¥

**Îµ-greedyæ˜¯æœ€ç®€å•å®ç”¨çš„æ¢ç´¢ç­–ç•¥**:

$$\pi(a|s) = \begin{cases}
\text{random action} & \text{with probability } \epsilon \\
\arg\max_a Q(s, a) & \text{with probability } 1-\epsilon
\end{cases}$$

**å®ç°ä»£ç **:
```python
def epsilon_greedy(state, Q_table, epsilon=0.1):
    """Îµ-greedyç­–ç•¥"""
    if np.random.random() < epsilon:
        return np.random.randint(0, num_actions)  # æ¢ç´¢
    else:
        return np.argmax(Q_table[state])  # åˆ©ç”¨

# å¸¸ç”¨è¡°å‡ç­–ç•¥
epsilon = max(epsilon_min, epsilon * decay_rate)  # éšè®­ç»ƒé€æ¸å‡å°Îµ
```

**Îµçš„é€‰æ‹©**:
- è®­ç»ƒåˆæœŸ: Îµ=1.0 (å®Œå…¨æ¢ç´¢)
- è®­ç»ƒä¸­æœŸ: Îµ=0.1 (10%æ¢ç´¢)
- è®­ç»ƒåæœŸ: Îµ=0.01 (1%æ¢ç´¢)
- æµ‹è¯•æ—¶: Îµ=0 (å®Œå…¨åˆ©ç”¨)

### 1.1.4 å®æˆ˜:ç”¨Q-Learningè§£å†³FrozenLake

**ç¯å¢ƒä»‹ç»**:

FrozenLakeæ˜¯Gymnasiumçš„ç»å…¸ç¯å¢ƒ:
```
SFFF    S: èµ·ç‚¹(Start)
FHFH    F: å†°é¢(Frozen)
FFFH    H: å†°æ´(Hole,æ¸¸æˆç»“æŸ)
HFFG    G: ç›®æ ‡(Goal,å¥–åŠ±+1)
```

**å®Œæ•´ä»£ç å®ç°**:

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

# åˆ›å»ºç¯å¢ƒ (is_slippery=False: ç¡®å®šæ€§ç¯å¢ƒ)
env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='rgb_array')

# Qè¡¨åˆå§‹åŒ–
n_states = env.observation_space.n  # 16ä¸ªçŠ¶æ€
n_actions = env.action_space.n      # 4ä¸ªåŠ¨ä½œ
Q = np.zeros((n_states, n_actions))

# è¶…å‚æ•°
alpha = 0.1        # å­¦ä¹ ç‡
gamma = 0.99       # æŠ˜æ‰£å› å­
epsilon = 1.0      # åˆå§‹æ¢ç´¢ç‡
epsilon_min = 0.01
epsilon_decay = 0.995
n_episodes = 2000

# è®­ç»ƒ
rewards_history = []

for episode in range(n_episodes):
    state, _ = env.reset()
    done = False
    total_reward = 0

    while not done:
        # Îµ-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if np.random.random() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state, :])

        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        # Q-Learningæ›´æ–°
        Q[state, action] = Q[state, action] + alpha * (
            reward + gamma * np.max(Q[next_state, :]) - Q[state, action]
        )

        state = next_state
        total_reward += reward

    # è¡°å‡æ¢ç´¢ç‡
    epsilon = max(epsilon_min, epsilon * epsilon_decay)
    rewards_history.append(total_reward)

    if (episode + 1) % 500 == 0:
        avg_reward = np.mean(rewards_history[-100:])
        print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Îµ: {epsilon:.3f}")

# å¯è§†åŒ–è®­ç»ƒæ›²çº¿
plt.plot(np.convolve(rewards_history, np.ones(100)/100, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (100-episode window)')
plt.title('Q-Learning on FrozenLake')
plt.savefig('q_learning_training.png')
plt.close()

# æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥
def test_agent(env, Q, n_episodes=100):
    success_count = 0
    for _ in range(n_episodes):
        state, _ = env.reset()
        done = False
        while not done:
            action = np.argmax(Q[state, :])  # è´ªå¿ƒç­–ç•¥
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            if reward > 0:
                success_count += 1
    return success_count / n_episodes

success_rate = test_agent(env, Q)
print(f"\nè®­ç»ƒåæˆåŠŸç‡: {success_rate*100:.1f}%")

# æ‰“å°å­¦ä¹ åˆ°çš„ç­–ç•¥
action_names = ['â†', 'â†“', 'â†’', 'â†‘']
print("\nå­¦ä¹ åˆ°çš„ç­–ç•¥:")
for s in range(n_states):
    if s % 4 == 0:
        print()
    action = np.argmax(Q[s, :])
    print(action_names[action], end=' ')
```

**é¢„æœŸè¾“å‡º**:
```
Episode 500, Avg Reward: 0.64, Îµ: 0.606
Episode 1000, Avg Reward: 0.93, Îµ: 0.367
Episode 1500, Avg Reward: 0.99, Îµ: 0.222
Episode 2000, Avg Reward: 1.00, Îµ: 0.135

è®­ç»ƒåæˆåŠŸç‡: 100.0%

å­¦ä¹ åˆ°çš„ç­–ç•¥:
â†“ â†’ â†’ â†“
â†“ â† â†“ â†
â†’ â†“ â† â†
â† â†’ â†’ â†
```

---

## 1.2 Deep Q-Network(DQN)

### 1.2.1 ä»Q-Learningåˆ°DQN

Q-Learningåœ¨çŠ¶æ€ç©ºé—´å·¨å¤§æ—¶é¢ä¸´ç»´åº¦è¯…å’’:
- å›´æ£‹: 10^170ç§çŠ¶æ€,æ— æ³•å­˜å‚¨Qè¡¨
- Atariæ¸¸æˆ: å›¾åƒçŠ¶æ€(210Ã—160Ã—3),çŠ¶æ€ç©ºé—´è¿ç»­

**DQNçš„çªç ´(DeepMind 2013)**:

ç”¨ç¥ç»ç½‘ç»œæ‹ŸåˆQå‡½æ•°,è€Œä¸æ˜¯Qè¡¨:

$$Q(s, a; \theta) \approx Q^*(s, a)$$

å…¶ä¸­Î¸æ˜¯ç¥ç»ç½‘ç»œå‚æ•°ã€‚

**ç½‘ç»œæ¶æ„**:
```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        """è¾“å…¥çŠ¶æ€,è¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼"""
        return self.net(state)
```

**æŸå¤±å‡½æ•°**:

$$L(\theta) = \mathbb{E}_{(s,a,r,s')}[(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2]$$

å…¶ä¸­Î¸^-æ˜¯ç›®æ ‡ç½‘ç»œå‚æ•°(ç¨åè§£é‡Š)ã€‚

### 1.2.2 ç»éªŒå›æ”¾(Experience Replay)

**é—®é¢˜**: ç›´æ¥ç”¨åœ¨çº¿æ•°æ®è®­ç»ƒç¥ç»ç½‘ç»œä¼šå¯¼è‡´:
1. **æ ·æœ¬ç›¸å…³æ€§**: è¿ç»­çš„æ ·æœ¬é«˜åº¦ç›¸å…³
2. **éå¹³ç¨³åˆ†å¸ƒ**: ç­–ç•¥ä¸æ–­å˜åŒ–,æ•°æ®åˆ†å¸ƒä¹Ÿåœ¨å˜

**è§£å†³æ–¹æ¡ˆ**: ç»éªŒå›æ”¾ç¼“å†²åŒº(Replay Buffer)

```python
from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ä¸€ä¸ªtransition"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹transition"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )

    def __len__(self):
        return len(self.buffer)
```

**ä¼˜ç‚¹**:
1. æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§
2. æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡(ä¸€ä¸ªæ ·æœ¬å¯ä»¥è¢«å¤šæ¬¡ä½¿ç”¨)
3. å¹³æ»‘æ•°æ®åˆ†å¸ƒ

### 1.2.3 ç›®æ ‡ç½‘ç»œ(Target Network)

**é—®é¢˜**: è®­ç»ƒæ—¶ç›®æ ‡å€¼ä¹Ÿåœ¨å˜åŒ–,å¯¼è‡´ä¸ç¨³å®š

å½“å‰ç½‘ç»œæ›´æ–°ä½¿ç”¨çš„TDç›®æ ‡æ˜¯:
$$r + \gamma \max_{a'} Q(s', a'; \theta)$$

ä½†Î¸æ¯æ¬¡éƒ½åœ¨æ›´æ–°,å¯¼è‡´ç›®æ ‡å€¼ä¸æ–­å˜åŒ–,ç±»ä¼¼"è¿½é€ç§»åŠ¨çš„é¶å­"ã€‚

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ç‹¬ç«‹çš„ç›®æ ‡ç½‘ç»œÎ¸^-

$$\text{TDç›®æ ‡} = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

**æ›´æ–°ç­–ç•¥**:
```python
# æ¯Cæ­¥åŒæ­¥ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
if step % target_update_interval == 0:
    target_net.load_state_dict(q_net.state_dict())

# æˆ–ä½¿ç”¨è½¯æ›´æ–°(Soft Update)
for target_param, param in zip(target_net.parameters(), q_net.parameters()):
    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
```

é€šå¸¸C=10000æ­¥æˆ–Ï„=0.005ã€‚

### 1.2.4 DQNçš„å®Œæ•´PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import deque
import random

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# DQNç½‘ç»œ
class DQNNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.net(x)

# ç»éªŒå›æ”¾
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            torch.FloatTensor(states).to(device),
            torch.LongTensor(actions).to(device),
            torch.FloatTensor(rewards).to(device),
            torch.FloatTensor(next_states).to(device),
            torch.FloatTensor(dones).to(device)
        )

    def __len__(self):
        return len(self.buffer)

# DQNæ™ºèƒ½ä½“
class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DQNNetwork(state_dim, action_dim).to(device)
        self.target_net = DQNNetwork(state_dim, action_dim).to(device)
        self.target_net.load_state_dict(self.q_net.state_dict())

        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.replay_buffer = ReplayBuffer(10000)

    def select_action(self, state, explore=True):
        """Îµ-greedyç­–ç•¥"""
        if explore and random.random() < self.epsilon:
            return random.randrange(self.action_dim)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            q_values = self.q_net(state_tensor)
            return q_values.argmax().item()

    def update(self, batch_size=64):
        """è®­ç»ƒç½‘ç»œ"""
        if len(self.replay_buffer) < batch_size:
            return

        # é‡‡æ ·
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)

        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0]
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(q_values, target_q_values)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 10)
        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_net.load_state_dict(self.q_net.state_dict())

    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

# è®­ç»ƒå¾ªç¯
def train_dqn(env_name='CartPole-v1', n_episodes=500):
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = DQNAgent(state_dim, action_dim)
    rewards_history = []

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # å­˜å‚¨ç»éªŒ
            agent.replay_buffer.push(state, action, reward, next_state, float(done))

            # è®­ç»ƒ
            loss = agent.update(batch_size=64)

            state = next_state
            episode_reward += reward

            if done:
                break

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % 10 == 0:
            agent.update_target_network()

        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()

        rewards_history.append(episode_reward)

        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(rewards_history[-50:])
            print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Îµ: {agent.epsilon:.3f}")

    env.close()
    return agent, rewards_history

# è¿è¡Œè®­ç»ƒ
if __name__ == "__main__":
    agent, rewards = train_dqn()
    print(f"Training completed! Final average reward: {np.mean(rewards[-50:]):.2f}")
```

**é¢„æœŸè¾“å‡º**:
```
Episode 50, Avg Reward: 23.45, Îµ: 0.606
Episode 100, Avg Reward: 47.32, Îµ: 0.367
Episode 150, Avg Reward: 112.58, Îµ: 0.222
Episode 200, Avg Reward: 189.23, Îµ: 0.135
...
Training completed! Final average reward: 475.32
```

---

## 1.3 DQNçš„æ”¹è¿›ç®—æ³•

### 1.3.1 Double DQN

**åŸå§‹DQNçš„é—®é¢˜**: maxæ“ä½œå¯¼è‡´Qå€¼è¢«é«˜ä¼°

$$Q(s,a) = r + \gamma \max_{a'} Q(s', a'; \theta)$$

maxæ“ä½œä¼šé€‰æ‹©Qå€¼çš„å™ªå£°ä¸Šç•Œ,å¯¼è‡´ç³»ç»Ÿæ€§é«˜ä¼°ã€‚

**Double DQNçš„è§£å†³æ–¹æ¡ˆ** (van Hasselt et al., 2015):

ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œ,ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼:

$$Q(s,a) = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-)$$

**ä»£ç ä¿®æ”¹**:
```python
# åŸå§‹DQN
with torch.no_grad():
    next_q_values = self.target_net(next_states).max(1)[0]

# Double DQN
with torch.no_grad():
    # ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
    next_actions = self.q_net(next_states).argmax(1)
    # ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
    next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
```

**æ•ˆæœ**: å‡å°‘é«˜ä¼°åå·®,æé«˜è®­ç»ƒç¨³å®šæ€§

### 1.3.2 Dueling DQN

**æ ¸å¿ƒæ€æƒ³**: åˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’Œä¼˜åŠ¿å‡½æ•°A(s,a)

$$Q(s, a) = V(s) + A(s, a)$$

å…¶ä¸­:
- V(s): çŠ¶æ€sæœ¬èº«æœ‰å¤šå¥½
- A(s,a): åŠ¨ä½œaç›¸å¯¹äºå¹³å‡åŠ¨ä½œæœ‰å¤šå¥½

**ç½‘ç»œæ¶æ„**:
```python
class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )

        # çŠ¶æ€ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

        # ä¼˜åŠ¿å‡½æ•°æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, state):
        features = self.feature(state)
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)

        # Q = V + (A - mean(A))  (ä¿è¯å”¯ä¸€æ€§)
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values
```

**ä¼˜åŠ¿**: åœ¨åŠ¨ä½œé€‰æ‹©å½±å“ä¸å¤§çš„çŠ¶æ€ä¸‹,æ›´å‡†ç¡®åœ°ä¼°è®¡V(s)

### 1.3.3 Rainbow DQN

Rainbow DQN (Hessel et al., 2017)ç»„åˆäº†6ç§æ”¹è¿›:

1. **Double DQN**: å‡å°‘é«˜ä¼°
2. **Dueling DQN**: åˆ†ç¦»Vå’ŒA
3. **Prioritized Experience Replay**: ä¼˜å…ˆå›æ”¾é‡è¦æ ·æœ¬
4. **Multi-step Learning**: næ­¥TDç›®æ ‡
5. **Distributional RL**: å­¦ä¹ Qå€¼åˆ†å¸ƒè€ŒéæœŸæœ›
6. **Noisy Nets**: ç”¨å‚æ•°å™ªå£°ä»£æ›¿Îµ-greedy

Rainbowåœ¨Atariæ¸¸æˆä¸Šè¾¾åˆ°äº†å½“æ—¶çš„æœ€ä½³æ€§èƒ½,ä½†å®ç°å¤æ‚åº¦è¾ƒé«˜ã€‚

---

## 1.4 ä½¿ç”¨Stable-Baselines3å®æˆ˜

### 1.4.1 å®‰è£…ä¸ç¯å¢ƒé…ç½®(2025æœ€æ–°ç‰ˆ)

Stable-Baselines3 2.7.1+æ˜¯2025å¹´æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ åº“,åŸºäºPyTorchå®ç°ã€‚

**å®‰è£…**:
```bash
# åŸºç¡€å®‰è£…
pip install stable-baselines3[extra]

# åŒ…å«æ‰€æœ‰ä¾èµ–(æ¨è)
pip install "stable-baselines3[extra]>=2.7.1"

# éªŒè¯å®‰è£…
python -c "import stable_baselines3; print(stable_baselines3.__version__)"
```

**ä¾èµ–**:
- Python 3.8+
- PyTorch 2.0+
- Gymnasium 0.28+ (æ³¨æ„ä¸æ˜¯æ—§çš„gymåº“)

### 1.4.2 CartPole-v1ä»»åŠ¡

CartPoleæ˜¯ç»å…¸çš„æ§åˆ¶ä»»åŠ¡:å€’ç«‹æ‘†å¹³è¡¡é—®é¢˜ã€‚

**ç¯å¢ƒç‰¹æ€§**:
- **çŠ¶æ€ç©ºé—´**: 4ç»´è¿ç»­ [è½¦ä½ç½®, è½¦é€Ÿåº¦, æ†è§’åº¦, æ†è§’é€Ÿåº¦]
- **åŠ¨ä½œç©ºé—´**: 2ä¸ªç¦»æ•£åŠ¨ä½œ {å‘å·¦, å‘å³}
- **å¥–åŠ±**: æ¯ä¸€æ­¥+1,æ†å€’ä¸‹åˆ™ç»“æŸ
- **æˆåŠŸæ ‡å‡†**: è¿ç»­100å›åˆå¹³å‡å¾—åˆ†â‰¥475

### 1.4.3 è®­ç»ƒDQNæ™ºèƒ½ä½“

**å®Œæ•´ç¤ºä¾‹** (2025 API):

```python
import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback
import torch

# åˆ›å»ºç¯å¢ƒ
env = gym.make("CartPole-v1")
eval_env = gym.make("CartPole-v1")

# åˆ›å»ºDQNæ¨¡å‹
model = DQN(
    policy="MlpPolicy",           # å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥
    env=env,
    learning_rate=1e-3,           # å­¦ä¹ ç‡
    buffer_size=50000,            # å›æ”¾ç¼“å†²åŒºå¤§å°
    learning_starts=1000,         # å¼€å§‹è®­ç»ƒçš„æ­¥æ•°
    batch_size=32,                # æ‰¹æ¬¡å¤§å°
    tau=1.0,                      # ç›®æ ‡ç½‘ç»œæ›´æ–°ç³»æ•°(1.0=ç¡¬æ›´æ–°)
    gamma=0.99,                   # æŠ˜æ‰£å› å­
    train_freq=4,                 # æ¯4æ­¥è®­ç»ƒ1æ¬¡
    gradient_steps=1,             # æ¯æ¬¡è®­ç»ƒ1ä¸ªæ¢¯åº¦æ­¥
    target_update_interval=1000,  # ç›®æ ‡ç½‘ç»œæ›´æ–°é—´éš”
    exploration_fraction=0.1,     # æ¢ç´¢ç‡è¡°å‡çš„æ¯”ä¾‹
    exploration_initial_eps=1.0,  # åˆå§‹æ¢ç´¢ç‡
    exploration_final_eps=0.05,   # æœ€ç»ˆæ¢ç´¢ç‡
    verbose=1,                    # æ‰“å°è®­ç»ƒä¿¡æ¯
    device="auto"                 # è‡ªåŠ¨é€‰æ‹©CPU/GPU
)

# è®¾ç½®è¯„ä¼°å›è°ƒ(æ¯1000æ­¥è¯„ä¼°ä¸€æ¬¡)
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./logs/",
    log_path="./logs/",
    eval_freq=1000,
    deterministic=True,
    render=False
)

# è®­ç»ƒæ¨¡å‹
print("å¼€å§‹è®­ç»ƒDQN...")
model.learn(
    total_timesteps=50000,
    callback=eval_callback,
    log_interval=10
)

# ä¿å­˜æ¨¡å‹
model.save("dqn_cartpole")
print("æ¨¡å‹å·²ä¿å­˜åˆ° dqn_cartpole.zip")

# åŠ è½½æ¨¡å‹
del model
model = DQN.load("dqn_cartpole", env=env)

# æœ€ç»ˆè¯„ä¼°
mean_reward, std_reward = evaluate_policy(
    model,
    eval_env,
    n_eval_episodes=100,
    deterministic=True
)
print(f"\næœ€ç»ˆè¯„ä¼°ç»“æœ:")
print(f"å¹³å‡å¥–åŠ±: {mean_reward:.2f} +/- {std_reward:.2f}")

env.close()
eval_env.close()
```

**é¢„æœŸè¾“å‡º**:
```
å¼€å§‹è®­ç»ƒDQN...
---------------------------------
| rollout/            |         |
|    ep_len_mean      | 22.3    |
|    ep_rew_mean      | 22.3    |
| time/               |         |
|    fps              | 1247    |
|    total_timesteps  | 1000    |
...
---------------------------------
| rollout/            |         |
|    ep_len_mean      | 483.2   |
|    ep_rew_mean      | 483.2   |
| time/               |         |
|    total_timesteps  | 50000   |
---------------------------------

æœ€ç»ˆè¯„ä¼°ç»“æœ:
å¹³å‡å¥–åŠ±: 500.00 +/- 0.00
```

### 1.4.4 æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–

**1. å¯è§†åŒ–è®­ç»ƒå¥½çš„ç­–ç•¥**:

```python
import gymnasium as gym
from stable_baselines3 import DQN

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = DQN.load("dqn_cartpole")

# åˆ›å»ºæ¸²æŸ“ç¯å¢ƒ
env = gym.make("CartPole-v1", render_mode="human")

# è¿è¡Œ10ä¸ªå›åˆ
for episode in range(10):
    obs, info = env.reset()
    episode_reward = 0
    done = False

    while not done:
        # é¢„æµ‹åŠ¨ä½œ(ç¡®å®šæ€§ç­–ç•¥)
        action, _states = model.predict(obs, deterministic=True)

        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        episode_reward += reward

    print(f"Episode {episode+1}: Reward = {episode_reward}")

env.close()
```

**2. ç»˜åˆ¶è®­ç»ƒæ›²çº¿**:

```python
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.results_plotter import load_results, ts2xy

# åŠ è½½è®­ç»ƒæ—¥å¿—
log_dir = "./logs/"
x, y = ts2xy(load_results(log_dir), 'timesteps')

# ç§»åŠ¨å¹³å‡
def moving_average(values, window):
    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, 'valid')

y_smooth = moving_average(y, window=50)

plt.figure(figsize=(10, 6))
plt.plot(x[:len(y_smooth)], y_smooth)
plt.xlabel('Timesteps')
plt.ylabel('Episode Reward (50-step MA)')
plt.title('DQN Training on CartPole-v1')
plt.grid(True)
plt.savefig('dqn_training_curve.png', dpi=300)
plt.close()
print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° dqn_training_curve.png")
```

**3. è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹**:

```python
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env

# åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ(å¹¶è¡Œè®­ç»ƒ)
env = make_vec_env("CartPole-v1", n_envs=4)

# ä¸åŒè¶…å‚æ•°é…ç½®
configs = [
    {"learning_rate": 1e-3, "gamma": 0.99},
    {"learning_rate": 5e-4, "gamma": 0.99},
    {"learning_rate": 1e-3, "gamma": 0.95},
]

results = []
for i, config in enumerate(configs):
    print(f"\nè®­ç»ƒé…ç½® {i+1}: {config}")

    model = DQN("MlpPolicy", env, **config, verbose=0)
    model.learn(total_timesteps=20000)

    # è¯„ä¼°
    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=50)
    results.append((config, mean_reward))
    print(f"å¹³å‡å¥–åŠ±: {mean_reward:.2f}")

# æ‰¾å‡ºæœ€ä½³é…ç½®
best_config, best_reward = max(results, key=lambda x: x[1])
print(f"\næœ€ä½³é…ç½®: {best_config}")
print(f"æœ€ä½³å¥–åŠ±: {best_reward:.2f}")

env.close()
```

---

**ç¬¬1ç« å°ç»“**:

æœ¬ç« å­¦ä¹ äº†ç»å…¸çš„å€¼å‡½æ•°æ–¹æ³•:

1. **Q-Learning**: è¡¨æ ¼å‹å¼ºåŒ–å­¦ä¹ ,é€šè¿‡TDæ›´æ–°å­¦ä¹ Q*
2. **DQN**: ç”¨ç¥ç»ç½‘ç»œæ‹ŸåˆQå‡½æ•°,çªç ´çŠ¶æ€ç©ºé—´é™åˆ¶
3. **DQNå…³é”®æŠ€æœ¯**:
   - ç»éªŒå›æ”¾: æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§
   - ç›®æ ‡ç½‘ç»œ: ç¨³å®šè®­ç»ƒ
   - Îµ-greedy: å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
4. **DQNæ”¹è¿›**: Double DQN, Dueling DQN, Rainbow
5. **Stable-Baselines3å®æˆ˜**: ä½¿ç”¨2.7.1+ç‰ˆæœ¬è®­ç»ƒCartPole

æŒæ¡è¿™äº›ç®—æ³•å,æˆ‘ä»¬å°†å­¦ä¹ ç­–ç•¥æ¢¯åº¦æ–¹æ³•,å®ƒèƒ½æ›´å¥½åœ°å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ã€‚

---

# ç¬¬2ç«  ç­–ç•¥æ¢¯åº¦æ–¹æ³•

## 2.1 ç­–ç•¥æ¢¯åº¦åŸºç¡€

### 2.1.1 ç­–ç•¥å‚æ•°åŒ–

ä¸DQNå­¦ä¹ Qå‡½æ•°ä¸åŒ,ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥å­¦ä¹ ç­–ç•¥Ï€(a|s;Î¸),å…¶ä¸­Î¸æ˜¯ç¥ç»ç½‘ç»œå‚æ•°ã€‚

**ç¦»æ•£åŠ¨ä½œç©ºé—´**(å¦‚CartPole):

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, state):
        logits = self.net(state)
        return F.softmax(logits, dim=-1)  # è¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ

# é‡‡æ ·åŠ¨ä½œ
probs = policy_net(state)
action_dist = Categorical(probs)
action = action_dist.sample()
```

**è¿ç»­åŠ¨ä½œç©ºé—´**(å¦‚æœºå™¨äººæ§åˆ¶):

```python
class GaussianPolicy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.mean_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
        self.log_std = nn.Parameter(torch.zeros(action_dim))  # å¯å­¦ä¹ çš„æ ‡å‡†å·®

    def forward(self, state):
        mean = self.mean_net(state)
        std = torch.exp(self.log_std)
        return Normal(mean, std)

# é‡‡æ ·åŠ¨ä½œ
action_dist = policy_net(state)
action = action_dist.sample()
```

### 2.1.2 ç­–ç•¥æ¢¯åº¦å®šç†(Policy Gradient Theorem)

**ç›®æ ‡**: æœ€å¤§åŒ–æœŸæœ›å›æŠ¥

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]$$

å…¶ä¸­Ï„=(sâ‚€,aâ‚€,râ‚,sâ‚,aâ‚,...)æ˜¯ä¸€æ¡è½¨è¿¹ã€‚

**ç­–ç•¥æ¢¯åº¦å®šç†**:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R(\tau)\right]$$

**ç›´è§‚ç†è§£**:
- å¦‚æœè½¨è¿¹Ï„çš„å›æŠ¥R(Ï„)é«˜,å¢åŠ é€‰æ‹©è¯¥è½¨è¿¹ä¸­åŠ¨ä½œçš„æ¦‚ç‡
- å¦‚æœå›æŠ¥ä½,å‡å°‘é€‰æ‹©è¿™äº›åŠ¨ä½œçš„æ¦‚ç‡

**æ¨å¯¼å…³é”®æ­¥éª¤**:

$$\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau}[R(\tau)] \\
&= \nabla_\theta \int P(\tau|\theta) R(\tau) d\tau \\
&= \int \nabla_\theta P(\tau|\theta) R(\tau) d\tau \\
&= \int P(\tau|\theta) \frac{\nabla_\theta P(\tau|\theta)}{P(\tau|\theta)} R(\tau) d\tau \quad \text{(logå¯¼æ•°æŠ€å·§)} \\
&= \mathbb{E}_{\tau}[\nabla_\theta \log P(\tau|\theta) \cdot R(\tau)]
\end{align}$$

å…¶ä¸­ $\nabla_\theta \log P(\tau|\theta) = \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t)$ (ç¯å¢ƒè½¬ç§»æ¦‚ç‡ä¸Î¸æ— å…³)ã€‚

### 2.1.3 REINFORCEç®—æ³•

REINFORCE (Williams, 1992)æ˜¯æœ€åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•,ä¹Ÿç§°è’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ã€‚

**ç®—æ³•æµç¨‹**:

1. ç”¨å½“å‰ç­–ç•¥Ï€_Î¸é‡‡æ ·ä¸€æ¡å®Œæ•´è½¨è¿¹: Ï„ = (sâ‚€,aâ‚€,râ‚,...,s_T)
2. è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„å›æŠ¥: $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$
3. æ›´æ–°ç­–ç•¥å‚æ•°:

$$\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t$$

**å®Œæ•´ä»£ç å®ç°**:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import gymnasium as gym
import numpy as np

class PolicyNet(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, state):
        return self.net(state)

class REINFORCE:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.policy = PolicyNet(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma

    def select_action(self, state):
        """é‡‡æ ·åŠ¨ä½œå¹¶è®°å½•log_prob"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def compute_returns(self, rewards):
        """è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„å›æŠ¥G_t"""
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        return torch.FloatTensor(returns)

    def update(self, log_probs, returns):
        """ç­–ç•¥æ¢¯åº¦æ›´æ–°"""
        # æ ‡å‡†åŒ–å›æŠ¥(å‡å°‘æ–¹å·®)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        loss = 0
        for log_prob, G in zip(log_probs, returns):
            loss += -log_prob * G  # è´Ÿå·:æ¢¯åº¦ä¸Šå‡

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

# è®­ç»ƒå¾ªç¯
def train_reinforce(env_name='CartPole-v1', n_episodes=1000):
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = REINFORCE(state_dim, action_dim, lr=1e-2)
    rewards_history = []

    for episode in range(n_episodes):
        state, _ = env.reset()
        log_probs = []
        rewards = []

        # é‡‡æ ·ä¸€æ¡è½¨è¿¹
        while True:
            action, log_prob = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            log_probs.append(log_prob)
            rewards.append(reward)

            if done:
                break
            state = next_state

        # è®¡ç®—å›æŠ¥
        returns = agent.compute_returns(rewards)

        # æ›´æ–°ç­–ç•¥
        loss = agent.update(log_probs, returns)

        episode_reward = sum(rewards)
        rewards_history.append(episode_reward)

        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(rewards_history[-50:])
            print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")

    env.close()
    return agent, rewards_history

# è¿è¡Œè®­ç»ƒ
if __name__ == "__main__":
    agent, rewards = train_reinforce()
```

**é¢„æœŸè¾“å‡º**:
```
Episode 50, Avg Reward: 34.52
Episode 100, Avg Reward: 67.88
Episode 150, Avg Reward: 142.34
Episode 200, Avg Reward: 278.45
...
```

### 2.1.4 åŸºçº¿(Baseline)æŠ€æœ¯

**é—®é¢˜**: REINFORCEæ–¹å·®å¾ˆé«˜,è®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**: å¼•å…¥åŸºçº¿b(s),ä¸æ”¹å˜æœŸæœ›ä½†é™ä½æ–¹å·®

$$\nabla_\theta J(\theta) = \mathbb{E}\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (G_t - b(s_t))\right]$$

**å¸¸ç”¨åŸºçº¿**:

1. **å¸¸æ•°åŸºçº¿**: b = å¹³å‡å›æŠ¥
2. **çŠ¶æ€ä»·å€¼åŸºçº¿**: b(s) = V(s) (æœ€ä¼˜é€‰æ‹©,å¼•å‡ºActor-Critic)

**ä»£ç ä¿®æ”¹**:
```python
# ç®€å•åŸºçº¿:è½¨è¿¹å¹³å‡å›æŠ¥
baseline = returns.mean()
advantages = returns - baseline

# æ›´æ–°
loss = 0
for log_prob, A in zip(log_probs, advantages):
    loss += -log_prob * A
```

---

## 2.2 Actor-Criticæ¡†æ¶

### 2.2.1 Criticçš„ä½œç”¨

REINFORCEä½¿ç”¨å®Œæ•´è½¨è¿¹çš„å›æŠ¥G_t,ä½†è¿™å¯¼è‡´:
1. **é«˜æ–¹å·®**: è½¨è¿¹çš„éšæœºæ€§å¤§
2. **æ ·æœ¬æ•ˆç‡ä½**: å¿…é¡»ç­‰è½¨è¿¹ç»“æŸæ‰èƒ½æ›´æ–°

**Actor-Criticæ€æƒ³**:
- **Actor**: ç­–ç•¥ç½‘ç»œÏ€(a|s;Î¸),è´Ÿè´£é€‰æ‹©åŠ¨ä½œ
- **Critic**: å€¼å‡½æ•°ç½‘ç»œV(s;w),è´Ÿè´£è¯„ä¼°ç­–ç•¥

ç”¨Criticä¼°è®¡çš„ä»·å€¼V(s)æ›¿ä»£è’™ç‰¹å¡æ´›å›æŠ¥:

$$\nabla_\theta J(\theta) \approx \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot [r_t + \gamma V(s_{t+1}) - V(s_t)]$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ ç§°ä¸ºTDè¯¯å·®ã€‚

### 2.2.2 ä¼˜åŠ¿å‡½æ•°(Advantage Function)

**å®šä¹‰**:

$$A(s, a) = Q(s, a) - V(s)$$

**ç›´è§‚ç†è§£**: åŠ¨ä½œaç›¸æ¯”è¯¥çŠ¶æ€ä¸‹çš„å¹³å‡åŠ¨ä½œå¥½å¤šå°‘

**TDè¯¯å·®ä½œä¸ºä¼˜åŠ¿ä¼°è®¡**:

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \approx A(s_t, a_t)$$

è¿™æ˜¯ä¸€ä¸ªæœ‰åä½†ä½æ–¹å·®çš„ä¼°è®¡ã€‚

### 2.2.3 A2C(Advantage Actor-Critic)

A2Cæ˜¯åŒæ­¥ç‰ˆæœ¬çš„Actor-Critic,ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°æ›´æ–°ç­–ç•¥ã€‚

**ç½‘ç»œæ¶æ„**:

```python
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # å…±äº«ç‰¹å¾å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )

        # Actorå¤´
        self.actor = nn.Linear(128, action_dim)

        # Criticå¤´
        self.critic = nn.Linear(128, 1)

    def forward(self, state):
        features = self.shared(state)
        action_logits = self.actor(features)
        value = self.critic(features)
        return F.softmax(action_logits, dim=-1), value
```

**æ›´æ–°è§„åˆ™**:

```python
# å‰å‘ä¼ æ’­
action_probs, value = model(state)
dist = Categorical(action_probs)
action = dist.sample()
log_prob = dist.log_prob(action)

# æ‰§è¡ŒåŠ¨ä½œ
next_state, reward, done, _ = env.step(action)

# è®¡ç®—ä¼˜åŠ¿
_, next_value = model(next_state)
advantage = reward + gamma * next_value * (1 - done) - value

# ActoræŸå¤±(ç­–ç•¥æ¢¯åº¦)
actor_loss = -log_prob * advantage.detach()

# CriticæŸå¤±(TDè¯¯å·®)
critic_loss = advantage.pow(2)

# æ€»æŸå¤±
loss = actor_loss + critic_loss
```

### 2.2.4 A3C(Asynchronous A3C)

A3C (Mnih et al., 2016)ä½¿ç”¨å¤šä¸ªå¹¶è¡Œworkerå¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œã€‚

**å…³é”®ç‰¹æ€§**:
1. **å¹¶è¡Œè®­ç»ƒ**: å¤šä¸ªç¯å¢ƒåŒæ—¶æ¢ç´¢,æé«˜æ ·æœ¬æ•ˆç‡
2. **å¼‚æ­¥æ›´æ–°**: æ¯ä¸ªworkerç‹¬ç«‹æ›´æ–°å…¨å±€å‚æ•°
3. **å»ç›¸å…³**: ä¸åŒworkerçš„ç»éªŒè‡ªç„¶å»ç›¸å…³,ä¸éœ€è¦ç»éªŒå›æ”¾

**ç®€åŒ–æ¶æ„**:
```python
# å…¨å±€ç½‘ç»œ
global_model = ActorCritic(state_dim, action_dim)
global_model.share_memory()  # å¤šè¿›ç¨‹å…±äº«

# æ¯ä¸ªworker
def worker(global_model, worker_id):
    local_model = ActorCritic(state_dim, action_dim)
    env = gym.make(env_name)

    while global_steps < max_steps:
        # åŒæ­¥å‚æ•°
        local_model.load_state_dict(global_model.state_dict())

        # é‡‡æ ·è½¨è¿¹
        states, actions, rewards = collect_trajectory(env, local_model)

        # è®¡ç®—æ¢¯åº¦
        loss = compute_a2c_loss(states, actions, rewards)
        grads = torch.autograd.grad(loss, local_model.parameters())

        # å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œ
        for global_param, grad in zip(global_model.parameters(), grads):
            global_param.grad = grad
        global_optimizer.step()
```

---

## 2.3 å®æˆ˜:CartPoleçš„REINFORCEå®ç°

### 2.3.1 ç­–ç•¥ç½‘ç»œè®¾è®¡

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs
```

### 2.3.2 è½¨è¿¹é‡‡æ ·

```python
def collect_trajectory(env, policy, max_steps=1000):
    """é‡‡æ ·ä¸€æ¡å®Œæ•´è½¨è¿¹"""
    states, actions, rewards, log_probs = [], [], [], []

    state, _ = env.reset()
    for _ in range(max_steps):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)

        # é‡‡æ ·åŠ¨ä½œ
        with torch.no_grad():
            action_probs = policy(state_tensor)
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)

        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        # è®°å½•
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        log_probs.append(log_prob)

        if done:
            break
        state = next_state

    return states, actions, rewards, log_probs
```

### 2.3.3 æ¢¯åº¦è®¡ç®—ä¸æ›´æ–°

```python
def compute_policy_gradient(log_probs, returns):
    """è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±"""
    policy_loss = []
    for log_prob, G in zip(log_probs, returns):
        policy_loss.append(-log_prob * G)

    # å¹³å‡æŸå¤±
    policy_loss = torch.stack(policy_loss).sum()
    return policy_loss

def update_policy(policy, optimizer, log_probs, returns):
    """æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
    # æ ‡å‡†åŒ–å›æŠ¥
    returns = torch.FloatTensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)

    # è®¡ç®—æŸå¤±
    loss = compute_policy_gradient(log_probs, returns)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()
```

### 2.3.4 è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

**å®Œæ•´è®­ç»ƒè„šæœ¬**:

```python
import gymnasium as gym
import torch
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# è¶…å‚æ•°
state_dim = 4
action_dim = 2
lr = 1e-2
gamma = 0.99
n_episodes = 500

# åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
env = gym.make('CartPole-v1')
policy = PolicyNetwork(state_dim, action_dim)
optimizer = optim.Adam(policy.parameters(), lr=lr)

# è®­ç»ƒ
rewards_history = []
loss_history = []

for episode in range(n_episodes):
    # é‡‡æ ·è½¨è¿¹
    states, actions, rewards, log_probs = collect_trajectory(env, policy)

    # è®¡ç®—æŠ˜æ‰£å›æŠ¥
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)

    # æ›´æ–°ç­–ç•¥
    loss = update_policy(policy, optimizer, log_probs, returns)

    episode_reward = sum(rewards)
    rewards_history.append(episode_reward)
    loss_history.append(loss)

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(rewards_history[-50:])
        print(f"Episode {episode+1}/{n_episodes}, "
              f"Avg Reward: {avg_reward:.2f}, Loss: {loss:.2f}")

env.close()

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# å¥–åŠ±æ›²çº¿
ax1.plot(rewards_history, alpha=0.3, label='Raw')
window = 20
moving_avg = np.convolve(rewards_history, np.ones(window)/window, mode='valid')
ax1.plot(range(window-1, len(rewards_history)), moving_avg, label=f'{window}-episode MA')
ax1.set_xlabel('Episode')
ax1.set_ylabel('Reward')
ax1.set_title('REINFORCE Training on CartPole-v1')
ax1.legend()
ax1.grid(True)

# æŸå¤±æ›²çº¿
ax2.plot(loss_history, alpha=0.6)
ax2.set_xlabel('Episode')
ax2.set_ylabel('Policy Loss')
ax2.set_title('Training Loss')
ax2.grid(True)

plt.tight_layout()
plt.savefig('reinforce_training.png', dpi=300)
plt.close()
print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° reinforce_training.png")

# æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥
def test_policy(env, policy, n_episodes=100):
    total_rewards = []
    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action = torch.argmax(action_probs).item()

            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward

        total_rewards.append(episode_reward)

    return np.mean(total_rewards), np.std(total_rewards)

mean_reward, std_reward = test_policy(env, policy)
print(f"\næµ‹è¯•ç»“æœ: {mean_reward:.2f} +/- {std_reward:.2f}")
```

**é¢„æœŸè¾“å‡º**:
```
Episode 50/500, Avg Reward: 42.34, Loss: 245.67
Episode 100/500, Avg Reward: 98.76, Loss: 189.23
Episode 150/500, Avg Reward: 167.45, Loss: 142.88
Episode 200/500, Avg Reward: 289.12, Loss: 98.34
Episode 250/500, Avg Reward: 412.67, Loss: 67.21
Episode 300/500, Avg Reward: 476.89, Loss: 45.12
...
è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° reinforce_training.png

æµ‹è¯•ç»“æœ: 487.32 +/- 23.45
```

---

**ç¬¬2ç« å°ç»“**:

æœ¬ç« å­¦ä¹ äº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•:

1. **ç­–ç•¥å‚æ•°åŒ–**: ç›´æ¥å­¦ä¹ ç­–ç•¥Ï€(a|s;Î¸)
2. **ç­–ç•¥æ¢¯åº¦å®šç†**: ç†è®ºåŸºç¡€,æ¨å¯¼æ¢¯åº¦å…¬å¼
3. **REINFORCE**: æœ€åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•,ä½¿ç”¨è’™ç‰¹å¡æ´›å›æŠ¥
4. **åŸºçº¿æŠ€æœ¯**: é™ä½æ–¹å·®,æé«˜è®­ç»ƒç¨³å®šæ€§
5. **Actor-Criticæ¡†æ¶**:
   - Actor: ç­–ç•¥ç½‘ç»œ
   - Critic: å€¼å‡½æ•°ç½‘ç»œ
   - ç”¨TDè¯¯å·®ä¼°è®¡ä¼˜åŠ¿å‡½æ•°
6. **A2C/A3C**: åŒæ­¥/å¼‚æ­¥Actor-Critic
7. **å®æˆ˜**: CartPoleçš„REINFORCEå®Œæ•´å®ç°

ç­–ç•¥æ¢¯åº¦çš„ä¼˜åŠ¿:
- æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
- å¯ä»¥å­¦ä¹ éšæœºç­–ç•¥
- æ”¶æ•›æ€§æ›´å¥½

ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ PPOç®—æ³•,å®ƒæ˜¯å½“å‰LLMè®­ç»ƒçš„ä¸»æµç®—æ³•ã€‚

---

# ç¬¬3ç«  PPOç®—æ³•è¯¦è§£

## 3.1 ä»TRPOåˆ°PPO

### 3.1.1 ç­–ç•¥æ›´æ–°çš„æŒ‘æˆ˜

ç­–ç•¥æ¢¯åº¦æ–¹æ³•é¢ä¸´çš„æ ¸å¿ƒé—®é¢˜:**æ­¥é•¿å¤ªå°æ”¶æ•›æ…¢,æ­¥é•¿å¤ªå¤§è®­ç»ƒå´©æºƒ**ã€‚

**vanillaç­–ç•¥æ¢¯åº¦çš„é—®é¢˜**:
- ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡Î±
- ä¸åŒçŠ¶æ€ä¸‹æœ€ä¼˜æ­¥é•¿å·®å¼‚å·¨å¤§
- ä¸€ä¸ªåçš„æ›´æ–°å¯èƒ½ç ´åæ•´ä¸ªè®­ç»ƒ

### 3.1.2 TRPOçš„ä¿¡èµ–åŸŸçº¦æŸ

TRPO (Schulman et al., 2015)å¼•å…¥ä¿¡èµ–åŸŸçº¦æŸ:

$$\max_\theta \mathbb{E}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\theta_{\text{old}}}}(s,a)]$$

$$\text{subject to } \mathbb{E}[KL(\pi_{\theta_{\text{old}}}(\cdot|s) \| \pi_\theta(\cdot|s))] \leq \delta$$

**ç›´è§‚ç†è§£**: åœ¨ä¿è¯æ–°æ—§ç­–ç•¥ä¸è¦å·®å¤ªå¤š(KLæ•£åº¦çº¦æŸ)çš„å‰æä¸‹,æœ€å¤§åŒ–æ€§èƒ½æå‡ã€‚

**é—®é¢˜**: TRPOéœ€è¦è®¡ç®—äºŒé˜¶å¯¼æ•°(HessiançŸ©é˜µ),è®¡ç®—å¤æ‚åº¦é«˜ã€‚

### 3.1.3 PPOçš„ç®€åŒ–æ€æƒ³

PPO (Schulman et al., 2017)ç”¨ç®€å•çš„clippingä»£æ›¿å¤æ‚çš„KLçº¦æŸ,è¾¾åˆ°ç±»ä¼¼æ•ˆæœã€‚

**ä¸¤ä¸ªç‰ˆæœ¬**:
1. **PPO-Penalty**: åœ¨ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥KLæƒ©ç½šé¡¹
2. **PPO-Clip**: ç›´æ¥clipæ¦‚ç‡æ¯”ç‡(æ›´å¸¸ç”¨)

---

## 3.2 PPOçš„æ•°å­¦æ¨å¯¼

### 3.2.1 é‡è¦æ€§é‡‡æ ·(Importance Sampling)

**é—®é¢˜**: ç­–ç•¥æ¢¯åº¦éœ€è¦on-policyæ•°æ®,æ ·æœ¬æ•ˆç‡ä½

**é‡è¦æ€§é‡‡æ ·**å…è®¸ç”¨æ—§ç­–ç•¥çš„æ•°æ®è¯„ä¼°æ–°ç­–ç•¥:

$$\mathbb{E}_{a\sim\pi_\theta}[f(a)] = \mathbb{E}_{a\sim\pi_{\theta_{\text{old}}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} f(a)\right]$$

**æ¦‚ç‡æ¯”ç‡**:

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$$

- r_t=1: æ–°æ—§ç­–ç•¥ç›¸åŒ
- r_t>1: æ–°ç­–ç•¥æ›´å€¾å‘é€‰æ‹©è¯¥åŠ¨ä½œ
- r_t<1: æ–°ç­–ç•¥æ›´ä¸å€¾å‘é€‰æ‹©è¯¥åŠ¨ä½œ

### 3.2.2 æ›¿ä»£ç›®æ ‡å‡½æ•°(Surrogate Objective)

ä½¿ç”¨é‡è¦æ€§é‡‡æ ·,ç­–ç•¥æ¢¯åº¦ç›®æ ‡å˜ä¸º:

$$L^{CPI}(\theta) = \mathbb{E}_t\left[r_t(\theta) \cdot A_t\right]$$

CPI = Conservative Policy Iteration

**é—®é¢˜**: æ— çº¦æŸçš„$L^{CPI}$å¯èƒ½å¯¼è‡´è¿‡å¤§çš„ç­–ç•¥æ›´æ–°

### 3.2.3 Clippedç›®æ ‡å‡½æ•°

PPO-Clipçš„æ ¸å¿ƒåˆ›æ–°:

$$L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]$$

å…¶ä¸­ Îµ é€šå¸¸å–0.1-0.3ã€‚

**Clipå‡½æ•°**:

$$\text{clip}(r_t, 1-\epsilon, 1+\epsilon) = \begin{cases}
1-\epsilon & \text{if } r_t < 1-\epsilon \\
r_t & \text{if } 1-\epsilon \leq r_t \leq 1+\epsilon \\
1+\epsilon & \text{if } r_t > 1+\epsilon
\end{cases}$$

**åˆ†æƒ…å†µåˆ†æ**:

**æƒ…å†µ1**: ä¼˜åŠ¿A_t > 0 (å¥½åŠ¨ä½œ)
- å¦‚æœr_t < 1+Îµ: ä½¿ç”¨r_t A_t,é¼“åŠ±å¢åŠ è¯¥åŠ¨ä½œæ¦‚ç‡
- å¦‚æœr_t â‰¥ 1+Îµ: ä½¿ç”¨(1+Îµ)A_t,é™åˆ¶å¢å¹…

**æƒ…å†µ2**: ä¼˜åŠ¿A_t < 0 (ååŠ¨ä½œ)
- å¦‚æœr_t > 1-Îµ: ä½¿ç”¨r_t A_t,é¼“åŠ±å‡å°‘è¯¥åŠ¨ä½œæ¦‚ç‡
- å¦‚æœr_t â‰¤ 1-Îµ: ä½¿ç”¨(1-Îµ)A_t,é™åˆ¶å‡å¹…

**å…³é”®æ´å¯Ÿ**: minæ“ä½œé€‰æ‹©æ‚²è§‚ä¼°è®¡,é˜²æ­¢è¿‡åº¦æ›´æ–°

**å¯è§†åŒ–**:
```python
import numpy as np
import matplotlib.pyplot as plt

r = np.linspace(0, 2, 100)
epsilon = 0.2

# A > 0 çš„æƒ…å†µ
A_positive = 1.0
L_clip_positive = np.minimum(r * A_positive,
                               np.clip(r, 1-epsilon, 1+epsilon) * A_positive)

# A < 0 çš„æƒ…å†µ
A_negative = -1.0
L_clip_negative = np.minimum(r * A_negative,
                               np.clip(r, 1-epsilon, 1+epsilon) * A_negative)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(r, r * A_positive, label='Unclipped', linestyle='--')
ax1.plot(r, L_clip_positive, label='Clipped', linewidth=2)
ax1.axvline(1-epsilon, color='r', linestyle=':', alpha=0.5)
ax1.axvline(1+epsilon, color='r', linestyle=':', alpha=0.5)
ax1.set_xlabel('r (probability ratio)')
ax1.set_ylabel('Objective')
ax1.set_title('PPO Objective: A > 0 (Good Action)')
ax1.legend()
ax1.grid(True)

ax2.plot(r, r * A_negative, label='Unclipped', linestyle='--')
ax2.plot(r, L_clip_negative, label='Clipped', linewidth=2)
ax2.axvline(1-epsilon, color='r', linestyle=':', alpha=0.5)
ax2.axvline(1+epsilon, color='r', linestyle=':', alpha=0.5)
ax2.set_xlabel('r (probability ratio)')
ax2.set_ylabel('Objective')
ax2.set_title('PPO Objective: A < 0 (Bad Action)')
ax2.legend()
ax2.grid(True)

plt.savefig('ppo_clipping.png', dpi=300)
plt.close()
```

### 3.2.4 å®Œæ•´çš„PPOæŸå¤±å‡½æ•°

å®é™…å®ç°ä¸­,PPOæŸå¤±åŒ…å«ä¸‰é¡¹:

$$L^{PPO}(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta](s)$$

**å„é¡¹è§£é‡Š**:

1. **$L^{CLIP}$**: Clippedç­–ç•¥ç›®æ ‡(ä¸»è¦é¡¹)
2. **$L^{VF}$**: å€¼å‡½æ•°æŸå¤±,ç”¨äºè®­ç»ƒCritic
   $$L^{VF} = (V_\theta(s_t) - V_t^{\text{target}})^2$$
3. **$S[\pi_\theta]$**: ç†µbonus,é¼“åŠ±æ¢ç´¢
   $$S = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$$

**ç³»æ•°**: câ‚â‰ˆ0.5, câ‚‚â‰ˆ0.01

**å®Œæ•´ç®—æ³•æµç¨‹**:

```
for iteration=1, 2, ... do:
    1. ç”¨å½“å‰ç­–ç•¥Ï€_Î¸é‡‡æ ·Tä¸ªtimestepsçš„æ•°æ®
    2. è®¡ç®—ä¼˜åŠ¿ä¼°è®¡A_t (ä½¿ç”¨GAE)
    3. for epoch=1, ..., K do:
        4. å°†æ•°æ®åˆ†æˆmini-batches
        5. for each mini-batch do:
            6. è®¡ç®—L^PPO
            7. ç”¨æ¢¯åº¦ä¸Šå‡æ›´æ–°Î¸
        end for
    end for
end for
```

---

## 3.3 PPOç®—æ³•å®ç°

### 3.3.1 ç½‘ç»œæ¶æ„è®¾è®¡

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

class PPOActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super().__init__()

        # å…±äº«ç‰¹å¾å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )

        # Actorå¤´
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Criticå¤´
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        features = self.shared(state)
        action_logits = self.actor(features)
        value = self.critic(features)
        return action_logits, value

    def get_action(self, state, deterministic=False):
        """é‡‡æ ·åŠ¨ä½œ"""
        logits, value = self.forward(state)
        dist = Categorical(logits=logits)

        if deterministic:
            action = torch.argmax(logits, dim=-1)
        else:
            action = dist.sample()

        log_prob = dist.log_prob(action)
        entropy = dist.entropy()

        return action, log_prob, value, entropy
```

### 3.3.2 ä¼˜åŠ¿ä¼°è®¡(GAE)

Generalized Advantage Estimation (Schulman et al., 2016)æ˜¯ä¸€ç§é™ä½æ–¹å·®çš„ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•ã€‚

**å•æ­¥TDè¯¯å·®**:

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

**GAEä¼˜åŠ¿ä¼°è®¡**:

$$A_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$

å…¶ä¸­Î»âˆˆ[0,1]æ§åˆ¶åå·®-æ–¹å·®æƒè¡¡:
- Î»=0: ä½æ–¹å·®é«˜åå·®(çº¯TD)
- Î»=1: ä½åå·®é«˜æ–¹å·®(è’™ç‰¹å¡æ´›)
- Î»=0.95: å¸¸ç”¨å€¼

**å®ç°ä»£ç **:

```python
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """è®¡ç®—GAEä¼˜åŠ¿ä¼°è®¡"""
    advantages = []
    gae = 0

    # ä»åå¾€å‰è®¡ç®—
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        # TDè¯¯å·®
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]

        # GAEç´¯ç§¯
        gae = delta + gamma * lam * (1 - dones[t]) * gae
        advantages.insert(0, gae)

    # ä»·å€¼ç›®æ ‡ = ä¼˜åŠ¿ + çŠ¶æ€ä»·å€¼
    advantages = torch.FloatTensor(advantages)
    returns = advantages + torch.FloatTensor(values)

    return advantages, returns
```

### 3.3.3 å°æ‰¹é‡è®­ç»ƒ

PPOçš„å…³é”®ç‰¹æ€§:å¤šè½®(epoch)å°æ‰¹é‡æ›´æ–°

```python
def ppo_update(agent, states, actions, old_log_probs, returns, advantages,
               n_epochs=4, batch_size=64, clip_epsilon=0.2):
    """PPOæ›´æ–°"""
    dataset_size = len(states)

    for _ in range(n_epochs):
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(dataset_size)

        for start in range(0, dataset_size, batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]

            # æå–batch
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]
            batch_old_log_probs = old_log_probs[batch_indices]
            batch_returns = returns[batch_indices]
            batch_advantages = advantages[batch_indices]

            # å‰å‘ä¼ æ’­
            logits, values = agent(batch_states)
            dist = Categorical(logits=logits)
            new_log_probs = dist.log_prob(batch_actions)
            entropy = dist.entropy().mean()

            # è®¡ç®—æ¦‚ç‡æ¯”ç‡
            ratio = torch.exp(new_log_probs - batch_old_log_probs)

            # PPO-ClipæŸå¤±
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * batch_advantages
            actor_loss = -torch.min(surr1, surr2).mean()

            # å€¼å‡½æ•°æŸå¤±
            critic_loss = F.mse_loss(values.squeeze(), batch_returns)

            # æ€»æŸå¤±
            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

            # åå‘ä¼ æ’­
            agent.optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)
            agent.optimizer.step()
```

### 3.3.4 å®Œæ•´PyTorchä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99,
                 lam=0.95, clip_epsilon=0.2):
        self.gamma = gamma
        self.lam = lam
        self.clip_epsilon = clip_epsilon

        self.policy = PPOActorCritic(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

    def collect_trajectories(self, env, num_steps=2048):
        """é‡‡æ ·è½¨è¿¹"""
        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []

        state, _ = env.reset()
        for _ in range(num_steps):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)

            with torch.no_grad():
                action, log_prob, value, _ = self.policy.get_action(state_tensor)

            next_state, reward, terminated, truncated, _ = env.step(action.item())
            done = terminated or truncated

            states.append(state)
            actions.append(action.item())
            rewards.append(reward)
            dones.append(done)
            log_probs.append(log_prob.item())
            values.append(value.item())

            state = next_state
            if done:
                state, _ = env.reset()

        return (np.array(states), np.array(actions), np.array(rewards),
                np.array(dones), np.array(log_probs), np.array(values))

    def train(self, env_name='CartPole-v1', total_timesteps=100000):
        """è®­ç»ƒPPO"""
        env = gym.make(env_name)
        timesteps = 0
        episode_rewards = []

        while timesteps < total_timesteps:
            # é‡‡æ ·æ•°æ®
            states, actions, rewards, dones, old_log_probs, values = \
                self.collect_trajectories(env, num_steps=2048)

            timesteps += len(states)

            # è®¡ç®—GAE
            advantages, returns = compute_gae(rewards, values, dones, self.gamma, self.lam)

            # æ ‡å‡†åŒ–ä¼˜åŠ¿
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            # è½¬æ¢ä¸ºtensor
            states = torch.FloatTensor(states)
            actions = torch.LongTensor(actions)
            old_log_probs = torch.FloatTensor(old_log_probs)
            returns = torch.FloatTensor(returns)
            advantages = torch.FloatTensor(advantages)

            # PPOæ›´æ–°
            ppo_update(self.policy, states, actions, old_log_probs, returns, advantages)

            # è®°å½•æ€§èƒ½
            episode_reward = rewards.sum() / dones.sum() if dones.sum() > 0 else 0
            episode_rewards.append(episode_reward)

            if len(episode_rewards) % 10 == 0:
                print(f"Timesteps: {timesteps}, Avg Reward: {np.mean(episode_rewards[-10:]):.2f}")

        env.close()
        return episode_rewards

# è¿è¡Œè®­ç»ƒ
if __name__ == "__main__":
    agent = PPOAgent(state_dim=4, action_dim=2)
    rewards = agent.train(total_timesteps=200000)
```

---

## 3.4 PPOè¶…å‚æ•°è°ƒä¼˜

### 3.4.1 å…³é”®è¶…å‚æ•°è§£æ

| è¶…å‚æ•° | æ¨èå€¼ | ä½œç”¨ |
|--------|--------|------|
| **clip_epsilon** | 0.1-0.3 | ClipèŒƒå›´,è¶Šå°æ›´æ–°è¶Šä¿å®ˆ |
| **n_epochs** | 3-10 | æ¯æ‰¹æ•°æ®çš„æ›´æ–°è½®æ•° |
| **batch_size** | 32-256 | Mini-batchå¤§å° |
| **learning_rate** | 3e-4 | Actorå’ŒCriticå­¦ä¹ ç‡ |
| **gamma** | 0.99 | æŠ˜æ‰£å› å­ |
| **lambda (GAE)** | 0.95 | GAEå‚æ•° |
| **entropy_coef** | 0.01 | ç†µç³»æ•°,é¼“åŠ±æ¢ç´¢ |
| **value_coef** | 0.5 | å€¼å‡½æ•°æŸå¤±ç³»æ•° |

**ç»éªŒæ³•åˆ™**:
- ç®€å•ä»»åŠ¡(CartPole): epsilon=0.2, epochs=4
- å¤æ‚ä»»åŠ¡(Atari): epsilon=0.1, epochs=4
- LLMè®­ç»ƒ(RLHF): epsilon=0.2, epochs=1

### 3.4.2 å¸¸è§è®­ç»ƒé—®é¢˜

**é—®é¢˜1**: è®­ç»ƒä¸ç¨³å®š,rewardå‰§çƒˆæ³¢åŠ¨
- **è§£å†³**: é™ä½learning_rate, å¢å¤§batch_size
- **è§£å†³**: ä½¿ç”¨æ¢¯åº¦è£å‰ª(max_grad_norm=0.5)

**é—®é¢˜2**: è®­ç»ƒé€Ÿåº¦æ…¢,æ”¶æ•›å›°éš¾
- **è§£å†³**: å¢å¤§learning_rateè°¨æ…å°è¯•
- **è§£å†³**: è°ƒæ•´GAEçš„lambda(å¢å¤§Î»å¯èƒ½åŠ é€Ÿå­¦ä¹ ä½†æ–¹å·®å¤§)

**é—®é¢˜3**: ç­–ç•¥å´©æºƒ,æ€§èƒ½çªç„¶ä¸‹é™
- **è§£å†³**: å‡å°clip_epsilon(æ›´ä¿å®ˆæ›´æ–°)
- **è§£å†³**: å‡å°‘n_epochs(é¿å…è¿‡æ‹Ÿåˆæ—§æ•°æ®)

### 3.4.3 è°ƒè¯•æŠ€å·§

**1. ç›‘æ§KLæ•£åº¦**:
```python
with torch.no_grad():
    kl = (old_log_probs - new_log_probs).mean()
    print(f"KL divergence: {kl:.4f}")

    # å¦‚æœKLè¿‡å¤§(>0.03),è¯´æ˜ç­–ç•¥å˜åŒ–å¤ªå¿«
    if kl > 0.03:
        print("Warning: Large policy update!")
```

**2. æ£€æŸ¥Clipæ¯”ä¾‹**:
```python
with torch.no_grad():
    clipped = (ratio < 1 - clip_epsilon) | (ratio > 1 + clip_epsilon)
    clip_fraction = clipped.float().mean()
    print(f"Clip fraction: {clip_fraction:.2%}")

    # ç†æƒ³clip_fractionåœ¨10%-30%
```

**3. ç›‘æ§Explained Variance**:
```python
def explained_variance(y_pred, y_true):
    var_y = torch.var(y_true)
    return 1 - torch.var(y_true - y_pred) / var_y

ev = explained_variance(values, returns)
print(f"Explained variance: {ev:.2%}")
# åº”è¯¥>0.5,è¶Šé«˜è¯´æ˜Criticè¶Šå‡†ç¡®
```

---

**ç¬¬3ç« å°ç»“**:

PPOæ˜¯å½“å‰æœ€æµè¡Œçš„RLç®—æ³•,å°¤å…¶åœ¨LLMè®­ç»ƒä¸­:

1. **æ ¸å¿ƒåˆ›æ–°**: Clippedç›®æ ‡å‡½æ•°,ç®€å•æœ‰æ•ˆ
2. **æ•°å­¦åŸºç¡€**: é‡è¦æ€§é‡‡æ ·+ä¿¡èµ–åŸŸçº¦æŸ
3. **å…³é”®æŠ€æœ¯**:
   - GAEä¼˜åŠ¿ä¼°è®¡
   - å¤šè½®mini-batchæ›´æ–°
   - ç†µæ­£åˆ™åŒ–
4. **ä¼˜åŠ¿**:
   - è®­ç»ƒç¨³å®š
   - æ ·æœ¬æ•ˆç‡é«˜
   - æ˜“äºè°ƒå‚
5. **åº”ç”¨**: ChatGPT, Claudeç­‰LLMçš„æ ¸å¿ƒè®­ç»ƒç®—æ³•

ä¸‹ä¸€ç« å°†å­¦ä¹ å¦‚ä½•å°†PPOåº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„RLHFè®­ç»ƒã€‚

---

# ç¬¬4ç«  å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

## 4.1 RLHFæ¦‚è¿°

### 4.1.1 ä¸ºä»€ä¹ˆéœ€è¦RLHF

é¢„è®­ç»ƒçš„LLMè™½ç„¶å¼ºå¤§,ä½†å­˜åœ¨é—®é¢˜:
- ç”Ÿæˆå†…å®¹å¯èƒ½æœ‰å®³ã€æœ‰åè§
- ä¸éµå¾ªäººç±»æŒ‡ä»¤
- å›ç­”å¯èƒ½ä¸çœŸå®(å¹»è§‰)

**RLHF (Reinforcement Learning from Human Feedback)**é€šè¿‡äººç±»åå¥½å¯¹é½æ¨¡å‹è¡Œä¸ºã€‚

### 4.1.2 RLHFçš„ä¸‰é˜¶æ®µæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1    â”‚â”€â”€â”€â”€â–ºâ”‚  Stage 2    â”‚â”€â”€â”€â”€â–ºâ”‚  Stage 3    â”‚
â”‚     SFT     â”‚     â”‚Reward Model â”‚     â”‚  PPO RL     â”‚
â”‚ (ç›‘ç£å¾®è°ƒ)  â”‚     â”‚ (å¥–åŠ±å»ºæ¨¡)  â”‚     â”‚ (å¼ºåŒ–å­¦ä¹ )  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stage 1 - SFT**: åœ¨é«˜è´¨é‡å¯¹è¯æ•°æ®ä¸Šç›‘ç£å¾®è°ƒ
- è¾“å…¥: é¢„è®­ç»ƒæ¨¡å‹ + æŒ‡ä»¤-å›å¤å¯¹
- è¾“å‡º: SFTæ¨¡å‹(èƒ½éµå¾ªåŸºæœ¬æŒ‡ä»¤)

**Stage 2 - Reward Model**: è®­ç»ƒåå¥½å¥–åŠ±æ¨¡å‹
- è¾“å…¥: æç¤ºè¯ + å¤šä¸ªå›å¤ + äººç±»æ’åº
- è¾“å‡º: å¥–åŠ±æ¨¡å‹RM(èƒ½ç»™å›å¤æ‰“åˆ†)

**Stage 3 - PPO**: ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­–ç•¥
- è¾“å…¥: SFTæ¨¡å‹ + å¥–åŠ±æ¨¡å‹
- è¾“å‡º: å¯¹é½åçš„æ¨¡å‹(ç¬¦åˆäººç±»åå¥½)

### 4.1.3 ç»å…¸åº”ç”¨:ChatGPTã€Claude

**ChatGPT (OpenAI)**:
- GPT-3.5/GPT-4 + RLHF
- ä½¿ç”¨PPOç®—æ³•
- å¥–åŠ±æ¨¡å‹åŸºäºäººç±»å¯¹æ¯”åé¦ˆ

**Claude (Anthropic)**:
- Constitutional AI + RLHF
- ç»“åˆAIåé¦ˆ(RLAIF)
- å¼ºè°ƒharmlessnesså’Œhelpfulness

---

## 4.2 é˜¶æ®µ1:ç›‘ç£å¾®è°ƒ(SFT)

### 4.2.1 é«˜è´¨é‡æŒ‡ä»¤æ•°æ®

SFTæ•°æ®æ ¼å¼:
```json
{
    "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­çº ç¼ ",
    "completion": "é‡å­çº ç¼ æ˜¯é‡å­åŠ›å­¦ä¸­çš„ä¸€ç§ç°è±¡..."
}
```

**æ•°æ®æ¥æº**:
- äººå·¥æ ‡æ³¨(æ˜‚è´µä½†é«˜è´¨é‡)
- å¼€æºæ•°æ®é›†(å¦‚Alpaca, ShareGPT)
- æ¨¡å‹ç”Ÿæˆ+äººå·¥ç­›é€‰

### 4.2.2 SFTè®­ç»ƒæµç¨‹

```python
# ä¼ªä»£ç 
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
dataset = load_dataset("yahma/alpaca-cleaned")

# æ ‡å‡†è¯­è¨€æ¨¡å‹æŸå¤±
for batch in dataset:
    outputs = model(batch["input_ids"])
    loss = cross_entropy(outputs.logits, batch["labels"])
    loss.backward()
    optimizer.step()
```

### 4.2.3 TRLçš„SFTTrainerä½¿ç”¨

**2025æœ€æ–°API** (TRL 0.13.0+):

```python
from trl import SFTConfig, SFTTrainer
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œæ•°æ®
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")
dataset = load_dataset("trl-lib/Capybara", split="train")

# é…ç½®SFT
sft_config = SFTConfig(
    output_dir="./sft_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
    logging_steps=10,
    save_steps=100,
    max_seq_length=512,
)

# åˆ›å»ºTrainer
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

# è®­ç»ƒ
trainer.train()
trainer.save_model("./sft_model")
```

---

## 4.3 é˜¶æ®µ2:å¥–åŠ±æ¨¡å‹(Reward Model)

### 4.3.1 äººç±»åå¥½æ•°æ®æ”¶é›†

åå¥½æ•°æ®æ ¼å¼:
```json
{
    "prompt": "å†™ä¸€ä¸ªå…³äºçŒ«çš„æ•…äº‹",
    "chosen": "ä»å‰æœ‰ä¸€åªå¯çˆ±çš„å°çŒ«...",  // äººç±»åå¥½
    "rejected": "çŒ«çŒ«çŒ«çŒ«çŒ«çŒ«..."          // è¾ƒå·®å›å¤
}
```

### 4.3.2 åå¥½å¯¹æ¯”å­¦ä¹ 

å¥–åŠ±æ¨¡å‹è®­ç»ƒç›®æ ‡:ä½¿chosenå¾—åˆ†é«˜äºrejected

**Bradley-Terryæ¨¡å‹**:

$$P(y_1 \succ y_2 | x) = \frac{\exp(r_\phi(x, y_1))}{\exp(r_\phi(x, y_1)) + \exp(r_\phi(x, y_2))}$$

**æŸå¤±å‡½æ•°**:

$$L_{RM}(\phi) = -\mathbb{E}_{(x,y_w,y_l)}[\log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))]$$

å…¶ä¸­y_wæ˜¯preferred, y_læ˜¯rejectedã€‚

### 4.3.3 å¥–åŠ±æ¨¡å‹è®­ç»ƒ

```python
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.model = base_model
        self.value_head = nn.Linear(base_model.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.model(input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state[:, -1, :]  # å–æœ€åä¸€ä¸ªtoken
        reward = self.value_head(last_hidden)
        return reward

# è®­ç»ƒ
chosen_rewards = reward_model(chosen_ids, chosen_mask)
rejected_rewards = reward_model(rejected_ids, rejected_mask)
loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()
```

### 4.3.4 TRLçš„RewardTrainerä½¿ç”¨

```python
from trl import RewardConfig, RewardTrainer
from datasets import load_dataset

# åŠ è½½æ•°æ®(éœ€åŒ…å«chosenå’Œrejectedå­—æ®µ)
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

# é…ç½®
reward_config = RewardConfig(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=1e-5,
    max_length=512,
)

# è®­ç»ƒ
trainer = RewardTrainer(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    args=reward_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()
trainer.save_model("./reward_model")
```

---

## 4.4 é˜¶æ®µ3:PPOå¼ºåŒ–å­¦ä¹ 

### 4.4.1 RLç¯å¢ƒè®¾è®¡

**çŠ¶æ€**: å¯¹è¯å†å²
**åŠ¨ä½œ**: ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
**å¥–åŠ±**: RM(prompt, completion) - KL_penalty

### 4.4.2 å¥–åŠ±å¡‘å½¢(Reward Shaping)

å®é™…å¥–åŠ±å‡½æ•°:

$$R(x, y) = r_\phi(x, y) - \beta \cdot D_{KL}(\pi_\theta \| \pi_{ref})$$

**KLæƒ©ç½šä½œç”¨**:
- é˜²æ­¢æ¨¡å‹åç¦»SFTåˆå§‹åŒ–å¤ªè¿œ
- é¿å…"å¥–åŠ±é»‘å®¢"(exploiting RM)
- Î²é€šå¸¸å–0.01-0.1

### 4.4.3 KLæ•£åº¦çº¦æŸ

```python
# è®¡ç®—KLæ•£åº¦
ref_logprobs = ref_model(input_ids).log_probs
new_logprobs = policy_model(input_ids).log_probs
kl_penalty = (new_logprobs - ref_logprobs).sum(dim=-1)

# æœ€ç»ˆå¥–åŠ±
reward = rm_score - beta * kl_penalty
```

### 4.4.4 TRLçš„PPOTrainerä½¿ç”¨

**åŸºæœ¬ä½¿ç”¨** (TRL 0.13.0+):

```python
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from transformers import AutoTokenizer

# é…ç½®
ppo_config = PPOConfig(
    model_name="./sft_model",
    learning_rate=1.41e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=1,
    optimize_cuda_cache=True,
    early_stopping=False,
    target_kl=0.1,
    ppo_epochs=4,
    max_grad_norm=0.5,
    seed=0,
)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLMWithValueHead.from_pretrained("./sft_model")
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained("./sft_model")
tokenizer = AutoTokenizer.from_pretrained("./sft_model")

# åˆ›å»ºTrainer
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # ç”Ÿæˆå›å¤
    query_tensors = batch["input_ids"]
    response_tensors = ppo_trainer.generate(query_tensors, max_new_tokens=128)

    # è®¡ç®—å¥–åŠ±
    texts = [tokenizer.decode(r) for r in response_tensors]
    rewards = [reward_model(q, r) for q, r in zip(queries, texts)]

    # PPOæ›´æ–°
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

    print(f"Mean reward: {stats['ppo/mean_scores']:.2f}")
```

---

## 4.5 RLHFçš„æ”¹è¿›ç®—æ³•

### 4.5.1 DPO(Direct Preference Optimization)

DPOç»•è¿‡æ˜¾å¼å¥–åŠ±æ¨¡å‹,ç›´æ¥ä¼˜åŒ–åå¥½:

$$L_{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l)}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

**ä¼˜åŠ¿**:
- ä¸éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹
- è®­ç»ƒæ›´ç¨³å®š
- è®¡ç®—æ•ˆç‡é«˜

**TRLä½¿ç”¨**:
```python
from trl import DPOConfig, DPOTrainer

trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=DPOConfig(beta=0.1),
    train_dataset=preference_dataset,
)
trainer.train()
```

### 4.5.2 GRPO(Group Relative Policy Optimization)

GRPOæ˜¯Deepseek R1ä½¿ç”¨çš„ç®—æ³•,æ¯”PPOæ›´é«˜æ•ˆ:

**æ ¸å¿ƒæ€æƒ³**: åœ¨ä¸€ç»„å›å¤ä¸­è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿

$$A_i = \frac{R_i - \text{mean}(R)}{\text{std}(R) + \epsilon}$$

**ä¼˜åŠ¿**:
- å†…å­˜æ•ˆç‡é«˜(ä¸éœ€è¦å•ç‹¬çš„Critic)
- è®­ç»ƒç¨³å®š
- é€‚åˆå¤§è§„æ¨¡è®­ç»ƒ

**TRLçš„GRPOTrainer** (2025æœ€æ–°):
```python
from trl import GRPOConfig, GRPOTrainer

grpo_config = GRPOConfig(
    output_dir="./grpo_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=1e-5,
)

trainer = GRPOTrainer(
    model=model,
    args=grpo_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)
trainer.train()
```

### 4.5.3 RLAIF(RL from AI Feedback)

ç”¨AIåé¦ˆæ›¿ä»£äººç±»åé¦ˆ:

**æµç¨‹**:
1. ç”¨å¼ºå¤§çš„æ¨¡å‹(å¦‚GPT-4)ç”Ÿæˆåå¥½æ ‡æ³¨
2. è®­ç»ƒå¥–åŠ±æ¨¡å‹
3. æ­£å¸¸RLHFæµç¨‹

**ä¼˜åŠ¿**: æˆæœ¬ä½,å¯æ‰©å±•æ€§å¼º
**åŠ£åŠ¿**: å¯èƒ½ç»§æ‰¿AIçš„åè§

---

## 4.6 OpenRLHF 0.8.11å®æˆ˜

### 4.6.1 å®‰è£…ä¸ç¯å¢ƒé…ç½®

```bash
# å®‰è£…OpenRLHF (2025ç‰ˆæœ¬)
pip install openrlhf[vllm]

# æˆ–æœ€æ–°vLLMç‰ˆæœ¬
pip install openrlhf[vllm_latest]

# éªŒè¯å®‰è£…
python -c "import openrlhf; print(openrlhf.__version__)"
```

### 4.6.2 Rayé›†ç¾¤è®­ç»ƒ

**å¯åŠ¨Rayé›†ç¾¤**:

```bash
# ä¸»èŠ‚ç‚¹
ray start --head --node-ip-address=0.0.0.0 --num-gpus=8 --port=6379

# å·¥ä½œèŠ‚ç‚¹
ray start --address="MASTER_IP:6379" --num-gpus=8
```

### 4.6.3 vLLMåŠ é€Ÿæ¨ç†

OpenRLHFä½¿ç”¨vLLMåŠ é€Ÿç”Ÿæˆ:

**ä¼˜åŠ¿**:
- PagedAttention:å†…å­˜æ•ˆç‡é«˜
- Continuous batching:ååé‡å¤§
- æ”¯æŒTensor Parallelism

### 4.6.4 å®Œæ•´RLHF Pipeline

**å®Œæ•´è®­ç»ƒè„šæœ¬**:

```bash
ray job submit --address="http://127.0.0.1:8265" \
    --runtime-env-json='{"working_dir": "/openrlhf"}' \
    -- python3 -m openrlhf.cli.train_ppo_ray \
    --ref_num_nodes 1 \
    --ref_num_gpus_per_node 2 \
    --reward_num_nodes 1 \
    --reward_num_gpus_per_node 2 \
    --critic_num_nodes 1 \
    --critic_num_gpus_per_node 2 \
    --actor_num_nodes 1 \
    --actor_num_gpus_per_node 2 \
    --vllm_num_engines 2 \
    --vllm_tensor_parallel_size 2 \
    --colocate_critic_reward \
    --colocate_actor_ref \
    --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
    --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
    --save_path ./checkpoints/llama3-8b-rlhf \
    --micro_train_batch_size 16 \
    --train_batch_size 128 \
    --micro_rollout_batch_size 32 \
    --rollout_batch_size 1024 \
    --max_samples 100000 \
    --max_epochs 1 \
    --prompt_max_len 1024 \
    --generate_max_len 1024 \
    --zero_stage 3 \
    --bf16 \
    --actor_learning_rate 5e-7 \
    --critic_learning_rate 9e-6 \
    --init_kl_coef 0.01 \
    --prompt_data Open-Orca/OpenOrca \
    --input_key question \
    --apply_chat_template \
    --packing_samples \
    --normalize_reward \
    --adam_offload \
    --flash_attn \
    --gradient_checkpointing
```

**å‚æ•°è¯´æ˜**:
- `--colocate_critic_reward`: Criticå’ŒRMå…±äº«æ˜¾å­˜
- `--vllm_num_engines`: vLLMæ¨ç†å¼•æ“æ•°é‡
- `--zero_stage 3`: DeepSpeed ZeRO-3ä¼˜åŒ–
- `--packing_samples`: æ ·æœ¬æ‰“åŒ…æé«˜æ•ˆç‡

---

## 4.7 TRLæœ€æ–°ç‰¹æ€§(2025)

### 4.7.1 GRPOTrainer(Deepseek R1åŒæ¬¾)

```python
from trl import GRPOConfig, GRPOTrainer

# Deepseek R1ä½¿ç”¨çš„é…ç½®
config = GRPOConfig(
    output_dir="./grpo_output",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    learning_rate=1e-6,
    max_prompt_length=512,
    max_completion_length=2048,
    num_sample_generations=8,  # æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªå›å¤
)

trainer = GRPOTrainer(
    model="Qwen/Qwen2.5-7B-Instruct",
    reward_model="RLHFlow/Llama3.1-8B-PairRM",
    args=config,
    train_dataset=dataset,
)

trainer.train()
```

### 4.7.2 Online DPO

åœ¨çº¿DPO:è®­ç»ƒä¸­åŠ¨æ€é‡‡æ ·è´Ÿä¾‹

```python
from trl import OnlineDPOConfig, OnlineDPOTrainer

config = OnlineDPOConfig(
    beta=0.1,
    dataset_num_proc=4,
)

trainer = OnlineDPOTrainer(
    model=model,
    ref_model=ref_model,
    args=config,
    train_dataset=dataset,
)
trainer.train()
```

### 4.7.3 KTO(Kahneman-Tversky Optimization)

KTOåªéœ€è¦äºŒå…ƒåé¦ˆ(å¥½/å),æ— éœ€æˆå¯¹æ¯”è¾ƒ:

```python
from trl import KTOConfig, KTOTrainer

# æ•°æ®æ ¼å¼: {"prompt": ..., "completion": ..., "label": True/False}
trainer = KTOTrainer(
    model=model,
    ref_model=ref_model,
    args=KTOConfig(beta=0.1),
    train_dataset=binary_feedback_dataset,
)
trainer.train()
```

---

**ç¬¬4ç« å°ç»“**:

RLHFæ˜¯LLMå¯¹é½çš„æ ¸å¿ƒæŠ€æœ¯:

1. **ä¸‰é˜¶æ®µæµç¨‹**: SFT â†’ Reward Model â†’ PPO
2. **TRLå·¥å…·é“¾**: SFTTrainer, RewardTrainer, PPOTrainer
3. **æ”¹è¿›ç®—æ³•**: DPO, GRPO, RLAIF
4. **å¤§è§„æ¨¡è®­ç»ƒ**: OpenRLHF + Ray + vLLM
5. **2025æ–°ç‰¹æ€§**: GRPOTrainer, Online DPO, KTO

ä¸‹ä¸€ç« å°†é€šè¿‡ä¸‰ä¸ªå®æˆ˜é¡¹ç›®å·©å›ºæ‰€å­¦å†…å®¹ã€‚

---

# ç¬¬5ç«  å¼ºåŒ–å­¦ä¹ å®æˆ˜é¡¹ç›®

## 5.1 é¡¹ç›®1:CartPoleå¹³è¡¡æ†(DQN)

### 5.1.1 ä»»åŠ¡æè¿°ä¸ç¯å¢ƒ

**ç›®æ ‡**: æ§åˆ¶å°è½¦å·¦å³ç§»åŠ¨,ä¿æŒæ†ç›´ç«‹

**çŠ¶æ€ç©ºé—´** (4ç»´):
- è½¦ä½ç½®: [-4.8, 4.8]
- è½¦é€Ÿåº¦: [-âˆ, âˆ]
- æ†è§’åº¦: [-0.418 rad, 0.418 rad]
- æ†è§’é€Ÿåº¦: [-âˆ, âˆ]

**åŠ¨ä½œç©ºé—´** (2ä¸ªç¦»æ•£åŠ¨ä½œ):
- 0: å‘å·¦æ¨
- 1: å‘å³æ¨

**å¥–åŠ±**: æ¯æ­¥+1,æ†å€’ä¸‹æˆ–å‡ºç•Œåˆ™ç»ˆæ­¢

### 5.1.2 DQNç½‘ç»œå®ç°

```python
import torch
import torch.nn as nn
import gymnasium as gym
from stable_baselines3 import DQN

# æ–¹æ¡ˆ1:ä½¿ç”¨Stable-Baselines3 (æ¨è)
env = gym.make("CartPole-v1")

model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    buffer_size=100000,
    batch_size=32,
    gamma=0.99,
    target_update_interval=1000,
    verbose=1
)

# è®­ç»ƒ
model.learn(total_timesteps=50000)
model.save("dqn_cartpole")

# æµ‹è¯•
from stable_baselines3.common.evaluation import evaluate_policy
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
print(f"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}")
```

### 5.1.3 è®­ç»ƒä¸è¯„ä¼°

**è®­ç»ƒæ›²çº¿å¯è§†åŒ–**:
```python
import matplotlib.pyplot as plt
import numpy as np

# è®­ç»ƒè®°å½•
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback

eval_callback = EvalCallback(
    env,
    best_model_save_path='./logs/',
    log_path='./logs/',
    eval_freq=1000,
)

model.learn(total_timesteps=100000, callback=eval_callback)

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿
from stable_baselines3.common.results_plotter import load_results, ts2xy
x, y = ts2xy(load_results('./logs/'), 'timesteps')

plt.figure(figsize=(10, 6))
plt.plot(x, y)
plt.xlabel('Timesteps')
plt.ylabel('Episode Reward')
plt.title('DQN Training on CartPole-v1')
plt.savefig('dqn_cartpole_curve.png')
```

### 5.1.4 ç»“æœåˆ†æ

**æœŸæœ›æ€§èƒ½**:
- 50k timestepsåè§£å†³ä»»åŠ¡(å¹³å‡å¥–åŠ±>475)
- è®­ç»ƒæ—¶é—´: ~2-5åˆ†é’Ÿ(CPU)
- æœ€ç»ˆæ€§èƒ½: å¹³å‡500æ­¥(å®Œç¾)

---

## 5.2 é¡¹ç›®2:Atariæ¸¸æˆ(PPO)

### 5.2.1 ç¯å¢ƒé¢„å¤„ç†

Atariæ¸¸æˆåŸå§‹è§‚æµ‹æ˜¯210Ã—160Ã—3çš„RGBå›¾åƒ,éœ€é¢„å¤„ç†:

```python
import gymnasium as gym
from stable_baselines3.common.atari_wrappers import (
    NoopResetEnv, MaxAndSkipEnv, EpisodicLifeEnv,
    FireResetEnv, WarpFrame, ClipRewardEnv
)

def make_atari_env(env_id):
    env = gym.make(env_id, render_mode="rgb_array")
    env = NoopResetEnv(env, noop_max=30)
    env = MaxAndSkipEnv(env, skip=4)
    env = EpisodicLifeEnv(env)
    if 'FIRE' in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = WarpFrame(env)  # 84x84ç°åº¦å›¾
    env = ClipRewardEnv(env)  # Clipå¥–åŠ±åˆ°[-1,1]
    return env

env = make_atari_env("PongNoFrameskip-v4")
```

### 5.2.2 CNNç­–ç•¥ç½‘ç»œä»£ç 

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv

# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
env_fns = [lambda: make_atari_env("PongNoFrameskip-v4") for _ in range(8)]
env = SubprocVecEnv(env_fns)

# ä½¿ç”¨CNNç­–ç•¥
model = PPO(
    "CnnPolicy",
    env,
    learning_rate=2.5e-4,
    n_steps=128,
    batch_size=256,
    n_epochs=4,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.1,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    device="cuda"
)

# è®­ç»ƒ(éœ€è¦~10M timesteps)
model.learn(total_timesteps=10_000_000)
model.save("ppo_pong")
```

### 5.2.3 å¹¶è¡Œç¯å¢ƒè®­ç»ƒ

**æ€§èƒ½å¯¹æ¯”**:
| ç¯å¢ƒæ•° | è®­ç»ƒé€Ÿåº¦(FPS) | æ€»æ—¶é—´ |
|--------|---------------|--------|
| 1 | ~1000 | 3å°æ—¶ |
| 4 | ~3500 | 1å°æ—¶ |
| 8 | ~6000 | 30åˆ†é’Ÿ |

### 5.2.4 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**æŠ€å·§1**: ä½¿ç”¨Frame Stack
```python
from stable_baselines3.common.vec_env import VecFrameStack
env = VecFrameStack(env, n_stack=4)  # å †å 4å¸§
```

**æŠ€å·§2**: ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
```python
# GPUæ˜¾å­˜å……è¶³æ—¶
model = PPO(..., batch_size=512, n_steps=256)
```

**æŠ€å·§3**: æ··åˆç²¾åº¦è®­ç»ƒ
```python
model = PPO(..., policy_kwargs=dict(use_amp=True))
```

---

## 5.3 é¡¹ç›®3:å¤§æ¨¡å‹å¯¹é½(RLHF)

### 5.3.1 æ•°æ®å‡†å¤‡:SFTä¸åå¥½æ•°æ®

**SFTæ•°æ®** (Alpacaæ ¼å¼):
```python
from datasets import load_dataset

# åŠ è½½æŒ‡ä»¤æ•°æ®
sft_dataset = load_dataset("yahma/alpaca-cleaned", split="train")

# æŸ¥çœ‹æ ·æœ¬
print(sft_dataset[0])
# {
#   "instruction": "Give three tips for staying healthy.",
#   "input": "",
#   "output": "1. Eat a balanced diet..."
# }
```

**åå¥½æ•°æ®** (Anthropic HH):
```python
preference_dataset = load_dataset(
    "Anthropic/hh-rlhf",
    data_dir="helpful-base",
    split="train"
)

# æ ·æœ¬æ ¼å¼
print(preference_dataset[0])
# {
#   "chosen": "Human: What is...\nAssistant: [good response]",
#   "rejected": "Human: What is...\nAssistant: [bad response]"
# }
```

### 5.3.2 SFTè®­ç»ƒ

```python
from trl import SFTConfig, SFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model_name = "Qwen/Qwen2.5-0.5B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. é…ç½®è®­ç»ƒ
sft_config = SFTConfig(
    output_dir="./sft_qwen",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    max_seq_length=512,
    logging_steps=10,
    save_steps=500,
    bf16=True,
)

# 3. è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=sft_dataset,
    processing_class=tokenizer,
)

trainer.train()
trainer.save_model("./sft_qwen_final")
```

### 5.3.3 Reward Modelè®­ç»ƒ

```python
from trl import RewardConfig, RewardTrainer

# é…ç½®
reward_config = RewardConfig(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    learning_rate=1e-5,
    max_length=512,
    bf16=True,
)

# è®­ç»ƒ
reward_trainer = RewardTrainer(
    model="./sft_qwen_final",
    args=reward_config,
    train_dataset=preference_dataset,
    processing_class=tokenizer,
)

reward_trainer.train()
reward_trainer.save_model("./reward_model_final")
```

### 5.3.4 PPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ

```python
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLMWithValueHead.from_pretrained("./sft_qwen_final")
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained("./sft_qwen_final")

# åŠ è½½å¥–åŠ±æ¨¡å‹
from transformers import pipeline
reward_model = pipeline("text-classification", model="./reward_model_final")

# 2. é…ç½®PPO
ppo_config = PPOConfig(
    model_name="./sft_qwen_final",
    learning_rate=1.41e-5,
    batch_size=16,
    mini_batch_size=4,
    ppo_epochs=4,
    target_kl=0.1,
)

# 3. åˆ›å»ºTrainer
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
)

# 4. è®­ç»ƒå¾ªç¯
from tqdm import tqdm
prompts_dataset = load_dataset("your/prompts", split="train")

for batch in tqdm(prompts_dataset.iter(batch_size=16)):
    # ç”Ÿæˆå›å¤
    query_tensors = tokenizer(batch["prompt"], return_tensors="pt").input_ids
    response_tensors = ppo_trainer.generate(query_tensors, max_new_tokens=128)

    # è®¡ç®—å¥–åŠ±
    texts = tokenizer.batch_decode(response_tensors)
    rewards = [reward_model(t)[0]["score"] for t in texts]

    # PPOæ›´æ–°
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

    if ppo_trainer.current_step % 100 == 0:
        print(f"Step {ppo_trainer.current_step}, Mean reward: {stats['ppo/mean_scores']:.2f}")

# ä¿å­˜
model.save_pretrained("./rlhf_qwen_final")
```

### 5.3.5 æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

**è¯„ä¼°æŒ‡æ ‡**:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("./rlhf_qwen_final")
tokenizer = AutoTokenizer.from_pretrained("./rlhf_qwen_final")

# æµ‹è¯•prompts
test_prompts = [
    "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
    "å†™ä¸€ä¸ªPythonæ’åºç®—æ³•",
    "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„?"
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Q: {prompt}\nA: {response}\n{'-'*50}")
```

**éƒ¨ç½²**:
```python
# ä½¿ç”¨vLLMéƒ¨ç½²
from vllm import LLM, SamplingParams

llm = LLM(model="./rlhf_qwen_final", tensor_parallel_size=2)
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

outputs = llm.generate(test_prompts, sampling_params)
for output in outputs:
    print(output.outputs[0].text)
```

---

## 5.4 è¿›é˜¶å®è·µ

### 5.4.1 å¤šè½®å¯¹è¯RLHF

**æŒ‘æˆ˜**: ä¿æŒå¯¹è¯ä¸€è‡´æ€§

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¼–ç å®Œæ•´å¯¹è¯å†å²
def format_multi_turn(dialog):
    formatted = ""
    for turn in dialog:
        formatted += f"User: {turn['user']}\nAssistant: {turn['assistant']}\n"
    return formatted

# åœ¨æœ€åä¸€è½®åº”ç”¨RLHF
```

### 5.4.2 ä»£ç ç”Ÿæˆä»»åŠ¡çš„RLä¼˜åŒ–

**å¥–åŠ±å‡½æ•°è®¾è®¡**:
```python
def code_reward(generated_code, test_cases):
    # 1. è¯­æ³•æ£€æŸ¥
    syntax_score = check_syntax(generated_code)

    # 2. æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡
    pass_rate = run_tests(generated_code, test_cases)

    # 3. ä»£ç è´¨é‡
    quality_score = check_code_quality(generated_code)

    return 0.3 * syntax_score + 0.5 * pass_rate + 0.2 * quality_score
```

### 5.4.3 ç»“åˆå·¥å…·ä½¿ç”¨çš„Agentè®­ç»ƒ

**ReActæ¨¡å¼**:
```
Thought: æˆ‘éœ€è¦æŸ¥æ‰¾å½“å‰å¤©æ°”
Action: search_weather("åŒ—äº¬")
Observation: åŒ—äº¬ä»Šå¤©æ™´,20Â°C
Thought: ç°åœ¨å¯ä»¥å›ç­”äº†
Answer: åŒ—äº¬ä»Šå¤©...
```

**RLä¼˜åŒ–**:
- å¥–åŠ±æ­£ç¡®çš„å·¥å…·è°ƒç”¨
- æƒ©ç½šæ— æ•ˆæˆ–å¾ªç¯è°ƒç”¨

---

**ç¬¬5ç« å°ç»“**:

é€šè¿‡ä¸‰ä¸ªå®æˆ˜é¡¹ç›®,æˆ‘ä»¬å­¦ä¹ äº†:

1. **é¡¹ç›®1 - CartPole DQN**:
   - ç®€å•ä»»åŠ¡çš„å¿«é€Ÿè§£å†³
   - Stable-Baselines3çš„ä½¿ç”¨

2. **é¡¹ç›®2 - Atari PPO**:
   - å›¾åƒè¾“å…¥çš„å¤„ç†
   - CNNç­–ç•¥ç½‘ç»œ
   - å¹¶è¡Œç¯å¢ƒåŠ é€Ÿ

3. **é¡¹ç›®3 - LLM RLHF**:
   - å®Œæ•´ä¸‰é˜¶æ®µæµç¨‹
   - TRLå·¥å…·é“¾å®æˆ˜
   - å¤§è§„æ¨¡è®­ç»ƒæŠ€å·§

è¿›é˜¶æ–¹å‘:
- å¤šè½®å¯¹è¯ä¼˜åŒ–
- ä»£ç ç”ŸæˆRL
- Agentå·¥å…·ä½¿ç”¨

è‡³æ­¤,æˆ‘ä»¬å®Œæˆäº†å¼ºåŒ–å­¦ä¹ ä»åŸºç¡€åˆ°åº”ç”¨çš„å®Œæ•´å­¦ä¹ !

---

**å…¨ç¯‡æ€»ç»“**:

æœ¬ç¯‡ç³»ç»Ÿè®²è§£äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§æ¨¡å‹æ—¶ä»£çš„åº”ç”¨:

**åŸºç¡€ç¯‡** (ç¬¬0-2ç« ):
- RLæ ¸å¿ƒæ¦‚å¿µ:MDPã€Bellmanæ–¹ç¨‹ã€ç­–ç•¥æ¢¯åº¦
- ç»å…¸ç®—æ³•:Q-Learningã€DQNã€REINFORCEã€Actor-Critic

**è¿›é˜¶ç¯‡** (ç¬¬3-4ç« ):
- PPOç®—æ³•:æ•°å­¦æ¨å¯¼ã€å®Œæ•´å®ç°ã€è¶…å‚æ•°è°ƒä¼˜
- RLHFæµç¨‹:SFT â†’ RM â†’ PPOä¸‰é˜¶æ®µ
- 2025æœ€æ–°å·¥å…·:TRL 0.13.0+ã€OpenRLHF 0.8.11

**å®æˆ˜ç¯‡** (ç¬¬5ç« ):
- ä¸‰å¤§é¡¹ç›®:CartPoleã€Atariã€LLMå¯¹é½
- å®Œæ•´ä»£ç :ä»ç¯å¢ƒæ­å»ºåˆ°æ¨¡å‹éƒ¨ç½²

**å­¦ä¹ è·¯å¾„å»ºè®®**:
1. æŒæ¡ç¬¬0ç« ç†è®ºåŸºç¡€
2. å®ç°ç¬¬1-2ç« ç»å…¸ç®—æ³•
3. æ·±å…¥ç†è§£ç¬¬3ç« PPOæ•°å­¦
4. å®æˆ˜ç¬¬5ç« ä¸‰ä¸ªé¡¹ç›®
5. æ¢ç´¢ç¬¬4ç« RLHFåº”ç”¨

**æ¨èèµ„æº**:
- è®ºæ–‡:PPO(Schulman 2017), RLHF(Ouyang 2022)
- å·¥å…·:Stable-Baselines3, TRL, OpenRLHF
- æ•°æ®:Anthropic HH, OpenOrca, UltraFeedback

å¼ºåŒ–å­¦ä¹ æ˜¯AIå¯¹é½çš„å…³é”®æŠ€æœ¯,ç¥ä½ å­¦ä¹ é¡ºåˆ©!

---


---

# Part C: å¯¹é½æŠ€æœ¯


## ç¬¬17ç« :å¯¹é½é—®é¢˜çš„æœ¬è´¨

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦å¯¹é½?

é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ çš„æ˜¯è¯­è¨€çš„**ç»Ÿè®¡åˆ†å¸ƒ** $P_{data}(x)$,è€Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½ $P_{human}(x)$ã€‚

**å…³é”®æŒ‘æˆ˜**:
- äººç±»åå¥½éš¾ä»¥ç”¨æ˜¾å¼å‡½æ•°è¡¨è¾¾
- é¢„è®­ç»ƒæ•°æ®åŒ…å«æœ‰å®³å†…å®¹(æ¯’æ€§ã€åè§)
- ç”Ÿæˆè´¨é‡ â‰  ç»Ÿè®¡ä¼¼ç„¶

**ç¤ºä¾‹**:
```
æç¤º: å¦‚ä½•åˆ¶ä½œç‚¸å¼¹?
é¢„è®­ç»ƒæ¨¡å‹: [å¯èƒ½æä¾›å±é™©ä¿¡æ¯]
å¯¹é½æ¨¡å‹: æˆ‘æ— æ³•æä¾›åˆ¶é€ æ­¦å™¨çš„ä¿¡æ¯,è¿™è¿åäº†å®‰å…¨å‡†åˆ™ã€‚
```

### 1.2 å¯¹é½çš„ä¸‰å¤§ç›®æ ‡

1. **æœ‰å¸®åŠ©æ€§(Helpfulness)**: æœ‰æ•ˆè§£å†³ç”¨æˆ·é—®é¢˜
2. **æ— å®³æ€§(Harmlessness)**: é¿å…æœ‰å®³è¾“å‡º
3. **è¯šå®æ€§(Honesty)**: æ‰¿è®¤ä¸ç¡®å®šæ€§,é¿å…å¹»è§‰

è¿™ä¸‰è€…å¸¸å¸¸å­˜åœ¨**æƒè¡¡**(Alignment Dilemma):
- è¿‡åº¦è¿½æ±‚æ— å®³æ€§ â†’ æ‹’ç»åˆç†è¯·æ±‚
- è¿‡åº¦è¿½æ±‚æœ‰å¸®åŠ©æ€§ â†’ å¯èƒ½æä¾›æœ‰å®³ä¿¡æ¯

---

## ç¬¬18ç« :RLHFå®Œæ•´ç†è®º

### 2.1 ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹

```
Stage 1: SFT (ç›‘ç£å¾®è°ƒ)
  â†“
Stage 2: Reward Modeling (å¥–åŠ±å»ºæ¨¡)
  â†“
Stage 3: PPO (å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–)
```

### 2.2 Stage 1: ç›‘ç£å¾®è°ƒ(SFT)

**ç›®æ ‡**: å°†é¢„è®­ç»ƒæ¨¡å‹åˆæ­¥å¯¹é½åˆ°æŒ‡ä»¤éµå¾ªã€‚

**æ•°æ®æ ¼å¼**:
$$
\mathcal{D}_{SFT} = \{(x_i, y_i^*)\}_{i=1}^N
$$

å…¶ä¸­ $y_i^*$ æ˜¯äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡å“åº”ã€‚

**è®­ç»ƒç›®æ ‡**:
$$
\mathcal{L}_{SFT} = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{SFT}} [\log P_\theta(y|x)]
$$

**æ•°æ®éœ€æ±‚**:
- OpenAI InstructGPT: 13Kæ ‡æ³¨æ ·æœ¬
- Anthropic Claude: ~10Ké«˜è´¨é‡å¯¹è¯

### 2.3 Stage 2: å¥–åŠ±å»ºæ¨¡(Reward Modeling)

#### 2.3.1 äººç±»åå¥½æ•°æ®æ”¶é›†

**å¯¹æ¯”æ•°æ®**:
$$
\mathcal{D}_{pref} = \{(x, y_w, y_l)\}
$$

- $x$: æç¤º
- $y_w$: äººç±»åå¥½çš„å“åº”(chosen)
- $y_l$: è¾ƒå·®çš„å“åº”(rejected)

**æ”¶é›†æ–¹æ³•**:
1. å¯¹åŒä¸€æç¤º$x$,é‡‡æ ·å¤šä¸ªå“åº” $y_1, ..., y_K$
2. æ ‡æ³¨å‘˜æ’åº: $y_{(1)} \succ y_{(2)} \succ ... \succ y_{(K)}$
3. æå–æˆå¯¹åå¥½: $(x, y_{(i)}, y_{(j)})$ for $i < j$

#### 2.3.2 Bradley-Terryæ¨¡å‹

**å‡è®¾**: äººç±»é€‰æ‹© $y_w$ çš„æ¦‚ç‡æ»¡è¶³:

$$
P(y_w \succ y_l | x) = \frac{\exp(r_\phi(x, y_w))}{\exp(r_\phi(x, y_w)) + \exp(r_\phi(x, y_l))} = \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))
$$

å…¶ä¸­:
- $r_\phi(x, y)$: å¥–åŠ±å‡½æ•°(å¾…å­¦ä¹ )
- $\sigma$: Sigmoidå‡½æ•°

**ç›´è§‰**: å¥–åŠ±å·®è·è¶Šå¤§,åå¥½æ¦‚ç‡è¶Šæ¥è¿‘1ã€‚

#### 2.3.3 å¥–åŠ±æ¨¡å‹è®­ç»ƒ

**æŸå¤±å‡½æ•°**:
$$
\mathcal{L}_{RM}(\phi) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}_{pref}} [\log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))]
$$

**æ¨¡å‹æ¶æ„**:
```
è¾“å…¥: [prompt, response]
  â†“
Transformerç¼–ç å™¨ (é€šå¸¸åˆå§‹åŒ–ä¸ºSFTæ¨¡å‹)
  â†“
çº¿æ€§å±‚: hidden_state â†’ scalar reward
```

**è®­ç»ƒæŠ€å·§**:
1. **å½’ä¸€åŒ–**: ç¡®ä¿å¥–åŠ±å‡å€¼ä¸º0,æ–¹å·®ä¸º1
   $$
   r'(x,y) = \frac{r(x,y) - \mu}{\sigma}
   $$

2. **Length Penalty**: æƒ©ç½šè¿‡é•¿/è¿‡çŸ­å“åº”
   $$
   r_{final} = r_{content} - \lambda |length(y) - length_{target}|
   $$

3. **Ensemble**: è®­ç»ƒå¤šä¸ªå¥–åŠ±æ¨¡å‹,å–å¹³å‡é™ä½æ–¹å·®

### 2.4 Stage 3: PPOå¼ºåŒ–å­¦ä¹ 

#### 2.4.1 é—®é¢˜å½¢å¼åŒ–

å°†è¯­è¨€ç”Ÿæˆè§†ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP):
- **çŠ¶æ€**: $s_t = (x, y_1, ..., y_t)$
- **åŠ¨ä½œ**: $a_t = y_{t+1}$
- **ç­–ç•¥**: $\pi_\theta(a_t | s_t)$
- **å¥–åŠ±**: åºåˆ—ç»“æŸæ—¶è·å¾— $r_\phi(x, y)$

#### 2.4.2 RLç›®æ ‡å‡½æ•°

**åŸºç¡€ç›®æ ‡**:
$$
\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot|x)} [r_\phi(x, y)]
$$

**KLçº¦æŸç‰ˆæœ¬**:
$$
\max_\theta \mathbb{E}_{x, y \sim \pi_\theta} \left[ r_\phi(x, y) - \beta D_{KL}(\pi_\theta(y|x) \| \pi_{ref}(y|x)) \right]
$$

**KLæƒ©ç½šçš„ä¸‰å¤§ä½œç”¨**:

1. **é˜²æ­¢å¥–åŠ±ç ´è§£**:
   ```
   æ— çº¦æŸ: "Great! Excellent! Perfect! Amazing! ..." (é‡å¤æ­£é¢è¯æ±‡æ¬ºéª—RM)
   æœ‰çº¦æŸ: ä¿æŒè‡ªç„¶è¯­è¨€é£æ ¼
   ```

2. **ä¿æŒæµç•…æ€§**: ç¡®ä¿ç”Ÿæˆæ–‡æœ¬ä»åƒäººç±»å†™çš„

3. **ç¨³å®šè®­ç»ƒ**: é¿å…ç­–ç•¥å´©æºƒ(mode collapse)

#### 2.4.3 PPOç®—æ³•è¯¦è§£

**ä¼˜åŠ¿å‡½æ•°**:
$$
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
$$

è¡¨ç¤ºåœ¨çŠ¶æ€$s$é€‰æ‹©åŠ¨ä½œ$a$ç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿ã€‚

**PPO-Clipç›®æ ‡**:
$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
$$

å…¶ä¸­:
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

**clipå‡½æ•°çš„å‡ ä½•æ„ä¹‰**:

```
Advantage > 0 (å¥½çš„åŠ¨ä½œ):
  - r_t > 1+Îµ: æ¦‚ç‡å¢åŠ è¿‡å¤š,è£å‰ªåˆ°1+Îµ
  - é¿å…è¿‡åº¦ä¼˜åŒ–å¯¼è‡´å´©æºƒ

Advantage < 0 (åçš„åŠ¨ä½œ):
  - r_t < 1-Îµ: æ¦‚ç‡é™ä½è¿‡å¤š,è£å‰ªåˆ°1-Îµ
  - é¿å…è¿‡åº¦æƒ©ç½š
```

**å®Œæ•´PPOè®­ç»ƒæµç¨‹**:

```python
def ppo_training_step(policy, ref_model, reward_model, prompts):
    # 1. ç”Ÿæˆå“åº”
    responses, log_probs = policy.generate_with_log_probs(prompts)
    
    # 2. è®¡ç®—å¥–åŠ±
    rewards = reward_model(prompts, responses)
    
    # 3. è®¡ç®—KLæƒ©ç½š
    ref_log_probs = ref_model.log_prob(responses, prompts)
    kl_penalty = beta * (log_probs - ref_log_probs)
    final_rewards = rewards - kl_penalty
    
    # 4. è®¡ç®—ä¼˜åŠ¿ä¼°è®¡(GAE)
    advantages = compute_gae(final_rewards, values)
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    # 5. PPOæ›´æ–°(å¤šè½®epoch)
    for epoch in range(ppo_epochs):
        # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„log_probs
        new_log_probs = policy.log_prob(responses, prompts)
        ratio = torch.exp(new_log_probs - log_probs.detach())
        
        # PPO-Clipç›®æ ‡
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # Value Loss
        value_loss = F.mse_loss(values, returns)
        
        # æ€»æŸå¤±
        loss = policy_loss + 0.5 * value_loss
        
        # æ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)
        optimizer.step()
```

#### 2.4.4 RLHFçš„æŒ‘æˆ˜

1. **è®­ç»ƒä¸ç¨³å®š**: PPOè¶…å‚æ•°æ•æ„Ÿ
   - Learning rateè¿‡å¤§ â†’ ç­–ç•¥å´©æºƒ
   - Îµè¿‡å° â†’ è®­ç»ƒç¼“æ…¢

2. **å¥–åŠ±ç ´è§£**: æ¨¡å‹å­¦ä¼šexploit RMçš„æ¼æ´
   - è§£å†³: æŒç»­æ›´æ–°RM,åŠ å¼ºKLæƒ©ç½š

3. **è®¡ç®—å¼€é”€**: éœ€è¦4ä¸ªæ¨¡å‹å‰¯æœ¬
   - Policy model (è®­ç»ƒä¸­)
   - Reference model (å›ºå®š)
   - Reward model (å›ºå®š)
   - Value model (è¾…åŠ©ä¼°è®¡)

4. **åˆ†å¸ƒæ¼‚ç§»**: ç”Ÿæˆåˆ†å¸ƒåç¦»é¢„è®­ç»ƒ
   - è§£å†³: æ··å…¥é¢„è®­ç»ƒæ•°æ®

---

## ç¬¬19ç« :DPO - ç›´æ¥åå¥½ä¼˜åŒ–

### 3.1 DPOçš„æ ¸å¿ƒæ´å¯Ÿ

**é—®é¢˜**: RLHFå¤ªå¤æ‚,èƒ½å¦ç›´æ¥ä»åå¥½æ•°æ®å­¦ä¹ ?

**ç­”æ¡ˆ**: å¯ä»¥! é€šè¿‡æ•°å­¦æ¨å¯¼,å°†RLä¼˜åŒ–è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ ã€‚

### 3.2 ç†è®ºæ¨å¯¼

#### Step 1: RLHFçš„æœ€ä¼˜è§£

åœ¨KLçº¦æŸä¸‹,æœ€ä¼˜ç­–ç•¥æ»¡è¶³:
$$
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r^*(x, y)\right)
$$

å…¶ä¸­ $Z(x)$ æ˜¯é…åˆ†å‡½æ•°(partition function)ã€‚

#### Step 2: åè§£å¥–åŠ±å‡½æ•°

$$
r^*(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
$$

#### Step 3: ä»£å…¥Bradley-Terryæ¨¡å‹

$$
P(y_w \succ y_l | x) = \sigma(r^*(x, y_w) - r^*(x, y_l))
$$

$$
= \sigma\left(\beta \log \frac{\pi^*(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{ref}(y_l|x)}\right)
$$

**å…³é”®**: $\log Z(x)$ é¡¹ç›¸æ¶ˆ!

### 3.3 DPOæŸå¤±å‡½æ•°

$$
\mathcal{L}_{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right]
$$

**ç®€åŒ–å½¢å¼**:
$$
\mathcal{L}_{DPO} = -\mathbb{E} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)} - \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right) \right) \right]
$$

### 3.4 DPOè®­ç»ƒå®ç°

```python
import torch
import torch.nn.functional as F

def dpo_loss(policy_chosen_logps, policy_rejected_logps,
             reference_chosen_logps, reference_rejected_logps,
             beta=0.1):
    """
    policy_chosen_logps: log Ï€_Î¸(y_w | x)
    policy_rejected_logps: log Ï€_Î¸(y_l | x)
    reference_chosen_logps: log Ï€_ref(y_w | x)
    reference_rejected_logps: log Ï€_ref(y_l | x)
    beta: æ¸©åº¦å‚æ•°
    """
    policy_logratios = policy_chosen_logps - policy_rejected_logps
    reference_logratios = reference_chosen_logps - reference_rejected_logps
    
    logits = beta * (policy_logratios - reference_logratios)
    loss = -F.logsigmoid(logits).mean()
    
    # é¢å¤–æŒ‡æ ‡
    implicit_reward_chosen = beta * (policy_chosen_logps - reference_chosen_logps)
    implicit_reward_rejected = beta * (policy_rejected_logps - reference_rejected_logps)
    reward_margin = (implicit_reward_chosen - implicit_reward_rejected).mean()
    
    return loss, reward_margin
```

### 3.5 DPO vs RLHFå¯¹æ¯”

| ç»´åº¦ | RLHF | DPO |
|------|------|-----|
| **è®­ç»ƒæ­¥éª¤** | 3æ­¥(SFT â†’ RM â†’ PPO) | 2æ­¥(SFT â†’ DPO) |
| **æ¨¡å‹æ•°é‡** | 4ä¸ª(policy/ref/RM/value) | 2ä¸ª(policy/ref) |
| **ç¨³å®šæ€§** | PPOä¸ç¨³å®š,è¶…å‚æ•°æ•æ„Ÿ | ç¨³å®šçš„ç›‘ç£å­¦ä¹  |
| **ç†è®ºä¿è¯** | è¿‘ä¼¼æœ€ä¼˜ | ç²¾ç¡®ä¼˜åŒ–KLçº¦æŸRL |
| **è®¡ç®—æ•ˆç‡** | ä½(å¤šæ¨¡å‹å‰å‘) | é«˜(å•æ¨¡å‹è®­ç»ƒ) |
| **çµæ´»æ€§** | å¯ç‹¬ç«‹æ”¹è¿›RM | å¥–åŠ±éšå¼å®šä¹‰ |
| **æ•°æ®æ•ˆç‡** | éœ€è¦å¤§é‡åå¥½æ•°æ® | ç›¸åŒæ•°æ®éœ€æ±‚ |

**é€‰æ‹©å»ºè®®**:
- èµ„æºå……è¶³ + éœ€è¦ç²¾ç»†æ§åˆ¶ â†’ RLHF
- èµ„æºå—é™ + å¿«é€Ÿè¿­ä»£ â†’ DPO
- å®è·µä¸­: DPOå·²æˆä¸ºä¸»æµ(Zephyrã€Mistralç­‰)

### 3.6 DPOçš„å˜ç§

#### 3.6.1 IPO (Identity Preference Optimization)

**åŠ¨æœº**: DPOå¯¹æ•°æ®å™ªå£°æ•æ„Ÿ

**æ”¹è¿›**: ä½¿ç”¨MSEè€Œélog-sigmoid
$$
\mathcal{L}_{IPO} = \mathbb{E} \left[ \left( \frac{1}{2} - \sigma(\Delta r) \right)^2 \right]
$$

å…¶ä¸­ $\Delta r = r(x, y_w) - r(x, y_l)$

**ä¼˜åŠ¿**: å¯¹outlieræ›´é²æ£’

#### 3.6.2 KTO (Kahneman-Tversky Optimization)

**åŠ¨æœº**: æ”¶é›†æˆå¯¹åå¥½æˆæœ¬é«˜

**æ”¹è¿›**: ä½¿ç”¨äºŒå…ƒåé¦ˆ(å¥½/å)è€Œéå¯¹æ¯”
$$
\mathcal{L}_{KTO} = \mathbb{E}_{y \sim Y_{good}} [-\log \sigma(\beta r(x,y))] + \mathbb{E}_{y \sim Y_{bad}} [-\log \sigma(-\beta r(x,y))]
$$

**ä¼˜åŠ¿**: æ•°æ®æ”¶é›†æˆæœ¬é™ä½50%

---

## ç¬¬20ç« :Constitutional AI

### 4.1 æ ¸å¿ƒæ€æƒ³

é€šè¿‡**åŸåˆ™(Principles)**è€Œéäººå·¥æ ‡æ³¨æ¥å¯¹é½æ¨¡å‹ã€‚

**åŠ¨æœº**:
1. äººå·¥æ ‡æ³¨æˆæœ¬é«˜,éš¾æ‰©å±•
2. äººç±»åå¥½å­˜åœ¨åå·®
3. å¸Œæœ›æ¨¡å‹è¡Œä¸ºå¯å®¡è®¡ã€å¯è°ƒæ•´

### 4.2 å®ªæ³•ç¤ºä¾‹

```
åŸåˆ™1: é¿å…ç”Ÿæˆæš´åŠ›å†…å®¹
  - æ‰¹è¯„æç¤º: "è¯†åˆ«å›ç­”ä¸­å¯èƒ½ç…½åŠ¨æš´åŠ›çš„éƒ¨åˆ†"
  - ä¿®æ­£æç¤º: "é‡å†™å›ç­”ä»¥å»é™¤æš´åŠ›å†…å®¹,ä¿æŒå¸®åŠ©æ€§"

åŸåˆ™2: å°Šé‡éšç§
  - æ‰¹è¯„æç¤º: "æ£€æŸ¥å›ç­”æ˜¯å¦å¯èƒ½ä¾µçŠ¯éšç§"
  - ä¿®æ­£æç¤º: "ä¿®æ”¹å›ç­”ä»¥ä¿æŠ¤ä¸ªäººéšç§"

åŸåˆ™3: ä¿æŒä¸­ç«‹
  - æ‰¹è¯„æç¤º: "è¯†åˆ«æ”¿æ²»åè§"
  - ä¿®æ­£æç¤º: "æ”¹å†™ä¸ºæ›´å¹³è¡¡çš„è§‚ç‚¹"
```

### 4.3 ä¸¤é˜¶æ®µè®­ç»ƒ

#### é˜¶æ®µ1: SL-CAI (ç›‘ç£å­¦ä¹ )

**æµç¨‹**:
```
for each (prompt x, response y0):
    for each principle p:
        critique = LLM("critique: {p}, {x}, {y0}")
        y1 = LLM("revise based on {critique}")
    
    train model on (x, y1)
```

**è¿­ä»£æ”¹è¿›**:
$$
y_0 \xrightarrow{\text{critique + revise}} y_1 \xrightarrow{\text{critique + revise}} y_2 \rightarrow \cdots
$$

#### é˜¶æ®µ2: RL-CAI (AIåé¦ˆå¼ºåŒ–å­¦ä¹ )

**AIåé¦ˆæ¨¡å‹**:
```
åˆ¤æ–­æ¨¡å‹(Constitution Model):
  è¾“å…¥: (principle, prompt, response)
  è¾“å‡º: score âˆˆ [0, 1]
```

**è®­ç»ƒæ•°æ®ç”Ÿæˆ**:
1. å¯¹æ¯ä¸ªprompt,ç”Ÿæˆå¤šä¸ªå“åº” $y_1, ..., y_K$
2. ç”¨Constitution Modelè¯„åˆ†
3. æ„å»ºåå¥½å¯¹: $(x, y_i, y_j)$ if $score(y_i) > score(y_j)$

**RLè®­ç»ƒ**:
$$
\mathcal{L}_{RLAIF} = \mathbb{E}_{x, y \sim \pi_\theta} \left[ score_{AI}(x, y) - \beta D_{KL}(\pi_\theta \| \pi_{ref}) \right]
$$

### 4.4 CAIçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

**ä¼˜åŠ¿**:
1. âœ… å¯æ‰©å±•: æ— éœ€å¤§é‡äººå·¥æ ‡æ³¨
2. âœ… å¯å®šåˆ¶: é€šè¿‡è°ƒæ•´åŸåˆ™æ§åˆ¶è¡Œä¸º
3. âœ… å¯å®¡è®¡: åŸåˆ™æ˜¯æ˜¾å¼çš„ã€å¯è§£é‡Šçš„
4. âœ… ä¸€è‡´æ€§: AIåé¦ˆæ¯”äººç±»æ›´ä¸€è‡´

**æŒ‘æˆ˜**:
1. âŒ åŸåˆ™è®¾è®¡: å¦‚ä½•å®šä¹‰"å¥½"çš„åŸåˆ™?
2. âŒ AIåè§: AIåé¦ˆå¯èƒ½ç»§æ‰¿æ¨¡å‹çš„åè§
3. âŒ çœŸå®å¯¹é½: AIåé¦ˆâ‰ äººç±»ä»·å€¼è§‚
4. âŒ å¾ªç¯ä¾èµ–: ç”¨LLMè®­ç»ƒLLM

---

## ç¬¬4.5ç« :RLHFå·¥å…·é“¾å¯¹æ¯”

> å®æˆ˜ä¸­é€‰æ‹©åˆé€‚çš„RLHFæ¡†æ¶è‡³å…³é‡è¦ã€‚æœ¬èŠ‚å¯¹æ¯”ä¸‰å¤§ä¸»æµå·¥å…·:TRL(Hugging Faceå®˜æ–¹)ã€OpenRLHF(ç”Ÿäº§çº§æ€§èƒ½)ã€EasyR1(æœ€æ–°æ¡†æ¶)ã€‚

**å‚è€ƒèµ„æ–™**:
- [TRLæ–‡æ¡£](https://huggingface.co/docs/trl/) (2025å¹´11æœˆæ›´æ–°)
- [OpenRLHF GitHub](https://github.com/OpenRLHF/OpenRLHF) (0.9ç‰ˆæœ¬, 2025å¹´5æœˆ)
- [EasyR1 GitHub](https://github.com/hiyouga/EasyR1) (åŸºäºveRL, 2025å¹´æœ€æ–°)

---

### 4.5.1 TRL - Hugging Faceå®˜æ–¹æ¡†æ¶

> TRL(Transformer Reinforcement Learning)æ˜¯Hugging Faceå®˜æ–¹çš„åè®­ç»ƒåº“,æ”¯æŒSFTã€GRPOã€DPOç­‰å¤šç§å¯¹é½æ–¹æ³•ã€‚

#### æ ¸å¿ƒç‰¹æ€§

1. **å®Œæ•´çš„Trainerç”Ÿæ€**:
   - `SFTTrainer`: ç›‘ç£å¾®è°ƒ
   - `GRPOTrainer`: GRPOç®—æ³•(DeepSeek-R1åŒæ¬¾)
   - `DPOTrainer`: ç›´æ¥åå¥½ä¼˜åŒ–
   - `RewardTrainer`: å¥–åŠ±æ¨¡å‹è®­ç»ƒ
   - `PPOTrainer`: ç»å…¸PPOç®—æ³•

2. **æ— ç¼é›†æˆTransformers**:
   - ç›´æ¥ä½¿ç”¨`AutoModel`åŠ è½½ä»»ä½•HFæ¨¡å‹
   - åŸç”Ÿæ”¯æŒ`device_map="auto"`
   - ä¸Accelerate/DeepSpeedå®Œç¾é›†æˆ

3. **2025æœ€æ–°åŠŸèƒ½**:
   - âœ… æ”¯æŒOpenEnv(Metaå¼€æºç¯å¢ƒæ¡†æ¶)
   - âœ… GRPOç®—æ³•(æ¯”PPOèŠ‚çœ25%æ˜¾å­˜)
   - âœ… å¤šæ¨¡æ€å¯¹é½
   - âœ… åˆ†å¸ƒå¼è®­ç»ƒ(DDP/DeepSpeed/FSDP)

#### å®‰è£…ä¸ä½¿ç”¨

**å®‰è£…**(2025å¹´11æœˆ12æ—¥æœ€æ–°ç‰ˆ):

```bash
pip install trl  # éœ€è¦Python >= 3.10
pip install transformers accelerate
```

**å®Œæ•´SFTç¤ºä¾‹**:

```python
from trl import SFTTrainer
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B")
dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

# 2. é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./llama-sft",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    fp16=True,
    logging_steps=10,
    save_strategy="steps",
    save_steps=500,
)

# 3. åˆå§‹åŒ–Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",  # æŒ‡å®šæ–‡æœ¬å­—æ®µ
    max_seq_length=2048,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train()
trainer.save_model("./llama-sft-final")
```

**DPOè®­ç»ƒç¤ºä¾‹**:

```python
from trl import DPOTrainer
from datasets import load_dataset

# DPOéœ€è¦ä¸‰å…ƒç»„æ•°æ®: (prompt, chosen, rejected)
dataset = load_dataset("Anthropic/hh-rlhf", split="train")

# æ ¼å¼ç¤ºä¾‹:
# {
#   "prompt": "å¦‚ä½•å­¦ä¹ Python?",
#   "chosen": "å»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹,ç„¶åå®è·µé¡¹ç›®...",
#   "rejected": "ç›´æ¥çœ‹æºç ...(è¿‡äºå›°éš¾)"
# }

dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,  # è‡ªåŠ¨åˆ›å»ºå‚è€ƒæ¨¡å‹å‰¯æœ¬
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    beta=0.1,  # KLæ•£åº¦ç³»æ•°
)

dpo_trainer.train()
```

**GRPOè®­ç»ƒç¤ºä¾‹**(DeepSeek-R1åŒæ¬¾ç®—æ³•):

```python
from trl import GRPOTrainer, GRPOConfig

# GRPOé…ç½®
config = GRPOConfig(
    output_dir="./llama-grpo",
    per_device_train_batch_size=4,
    num_train_epochs=1,
    learning_rate=5e-7,
    num_ppo_epochs=4,  # PPOå†…å¾ªç¯æ¬¡æ•°
    num_mini_batches=4,
    num_sample_generations=4,  # ç»„å†…æ ·æœ¬æ•°
)

# å®šä¹‰å¥–åŠ±å‡½æ•°
def reward_function(prompts, responses):
    # ç¤ºä¾‹: åŸºäºé•¿åº¦çš„å¥–åŠ±
    return [len(r.split()) / 100 for r in responses]

grpo_trainer = GRPOTrainer(
    model=model,
    tokenizer=tokenizer,
    config=config,
    train_dataset=dataset,
    reward_function=reward_function,
)

grpo_trainer.train()
```

#### ä¼˜åŠ£åŠ¿åˆ†æ

**ä¼˜åŠ¿**:
- âœ… **æ˜“ç”¨æ€§æœ€é«˜**: å‡ è¡Œä»£ç å³å¯å¯åŠ¨è®­ç»ƒ
- âœ… **ç”Ÿæ€å®Œæ•´**: Hugging Faceå…¨å®¶æ¡¶æ— ç¼é›†æˆ
- âœ… **æ–‡æ¡£å®Œå–„**: å®˜æ–¹æ•™ç¨‹+ç¤¾åŒºæ”¯æŒä¸°å¯Œ
- âœ… **ç®—æ³•é½å…¨**: SFT/DPO/PPO/GRPO/KTOå…¨è¦†ç›–

**åŠ£åŠ¿**:
- âŒ **æ€§èƒ½ä¸€èˆ¬**: å¤§è§„æ¨¡è®­ç»ƒ(>70B)æ€§èƒ½ä¸å¦‚OpenRLHF
- âŒ **åˆ†å¸ƒå¼ä¼˜åŒ–æœ‰é™**: Rayè°ƒåº¦ä¸å¦‚OpenRLHFé«˜æ•ˆ
- âŒ **æ˜¾å­˜å ç”¨è¾ƒé«˜**: æœªæ·±åº¦ä¼˜åŒ–,70Bæ¨¡å‹éœ€è¦æ›´å¤šå¡

**é€‚ç”¨åœºæ™¯**:
- ğŸ¯ **å¿«é€ŸåŸå‹éªŒè¯**: 7B-13Bæ¨¡å‹çš„å¯¹é½å®éªŒ
- ğŸ¯ **å­¦ä¹ ç ”ç©¶**: ç†è§£RLHF/DPO/GRPOç®—æ³•
- ğŸ¯ **å•æœºè®­ç»ƒ**: â‰¤8å¡çš„å°è§„æ¨¡å®éªŒ
- ğŸ¯ **HFç”Ÿæ€**: ä¸Transformers/Datasetsæ·±åº¦é›†æˆçš„é¡¹ç›®

---

### 4.5.2 OpenRLHF - ç”Ÿäº§çº§åˆ†å¸ƒå¼æ¡†æ¶

> OpenRLHFæ˜¯é¦–ä¸ªåŸºäºRay + vLLM + ZeRO-3çš„é«˜æ€§èƒ½RLHFæ¡†æ¶,è¢«Googleã€ByteDanceã€NVIDIAç­‰å…¬å¸é‡‡ç”¨ã€‚

#### æ ¸å¿ƒç‰¹æ€§

1. **Rayåˆ†å¸ƒå¼æ¶æ„**:
   ```
   vLLM Engine (ç”Ÿæˆ) â†’ 16 GPUs
   Actor Model (ç­–ç•¥)  â†’ 16 GPUs
   Critic Model (ä»·å€¼) â†’ 16 GPUs
   Reward Model (å¥–åŠ±) â†’ ç‹¬ç«‹éƒ¨ç½²
   ```

2. **æè‡´æ€§èƒ½ä¼˜åŒ–**:
   - âœ… **vLLMåŠ é€Ÿ**: ç”Ÿæˆé˜¶æ®µå RLHF 80%æ—¶é—´,vLLMæé€Ÿ3-5x
   - âœ… **Auto Tensor Parallelism**: è‡ªåŠ¨å¼ é‡å¹¶è¡Œ,æ— éœ€æ‰‹åŠ¨é…ç½®
   - âœ… **ZeRO-3é›†æˆ**: æ”¯æŒ70B+æ¨¡å‹è®­ç»ƒ
   - âœ… **Async PPO**(2025å¹´5æœˆæ–°å¢): å¼‚æ­¥PipelineåŠ é€Ÿ

3. **2025æœ€æ–°åŠŸèƒ½**:
   - âœ… **Async Agent Training**: æ”¯æŒå¤–éƒ¨ç¯å¢ƒäº¤äº’
   - âœ… **NeMo-Gymé›†æˆ**: å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåº“
   - âœ… **å¤šæ¨¡æ€RLHF**: OpenRLHF-Måˆ†æ”¯æ”¯æŒ

#### å®‰è£…ä¸ä½¿ç”¨

**å®‰è£…**(æ¨èDocker):

```bash
# æ–¹æ³•1: Docker(æ¨è)
docker pull openrlhf/openrlhf:latest

# æ–¹æ³•2: æºç å®‰è£…
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
pip install vllm ray deepspeed
```

**åˆ†å¸ƒå¼PPOè®­ç»ƒ**(48Ã—A100,70Bæ¨¡å‹):

```bash
# é…ç½®ç¯å¢ƒå˜é‡
export VLLM_WORKER_MULTIPROC_METHOD=spawn

# Rayè°ƒåº¦é…ç½®
ray start --head --node-ip-address=0.0.0.0 --num-gpus=48

# å¯åŠ¨è®­ç»ƒ(è‡ªåŠ¨åˆ†é…: vLLM:Actor:Critic = 1:1:1)
python -m openrlhf.cli.train_ppo_ray \
  --pretrain meta-llama/Llama-3.1-70B \
  --reward_pretrain ./reward_model \
  --save_path ./checkpoints \
  --micro_train_batch_size 2 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 4 \
  --rollout_batch_size 128 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --generate_max_len 2048 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data Anthropic/hh-rlhf \
  --input_key prompt \
  --apply_chat_template \
  --normalize_reward \
  --adam_offload \
  --flash_attn \
  --gradient_checkpointing \
  --use_wandb your_wandb_id
```

**DPOè®­ç»ƒ**(æ›´ç®€å•):

```bash
python -m openrlhf.cli.train_dpo \
  --pretrain meta-llama/Llama-3.1-8B \
  --save_path ./dpo_model \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --max_epochs 3 \
  --learning_rate 5e-7 \
  --beta 0.1 \
  --max_len 2048 \
  --zero_stage 2 \
  --bf16 \
  --dataset Anthropic/hh-rlhf \
  --prompt_key prompt \
  --chosen_key chosen \
  --rejected_key rejected
```

**å¼‚æ­¥PPO**(2025æ–°åŠŸèƒ½):

```python
# å¯ç”¨å¼‚æ­¥è®­ç»ƒ,æå‡30%ååé‡
python -m openrlhf.cli.train_ppo_ray \
  --async_train \  # å…³é”®: å¯ç”¨å¼‚æ­¥Pipeline
  --async_buffer_size 64 \
  ... # å…¶ä»–å‚æ•°åŒä¸Š
```

#### æ€§èƒ½æ•°æ®(å®æµ‹)

**70Bæ¨¡å‹PPOè®­ç»ƒ**(48Ã—A100 80GB):

| æŒ‡æ ‡ | TRL | OpenRLHF | æå‡ |
|-----|-----|----------|------|
| ç”Ÿæˆé€Ÿåº¦ | 42 tokens/s | 186 tokens/s | **+343%** |
| è®­ç»ƒé€Ÿåº¦ | 0.3 iter/min | 1.8 iter/min | **+500%** |
| æ˜¾å­˜å ç”¨ | 72GB/å¡ | 68GB/å¡ | **-6%** |
| æœ€å¤§batch | 64 | 128 | **+100%** |
| GPUåˆ©ç”¨ç‡ | 65% | 88% | **+35%** |

**Async PPOåŠ é€Ÿ**(2025å¹´5æœˆæ›´æ–°):

| é…ç½® | åŒæ­¥PPO | å¼‚æ­¥PPO | æå‡ |
|-----|--------|---------|------|
| ååé‡ | 1.8 iter/min | 2.4 iter/min | **+33%** |
| GPUç©ºé—²ç‡ | 25% | 8% | **-68%** |

#### ä¼˜åŠ£åŠ¿åˆ†æ

**ä¼˜åŠ¿**:
- âœ… **æ€§èƒ½æœ€å¼º**: vLLM + Rayä¼˜åŒ–,ç”Ÿæˆé€Ÿåº¦3-5x
- âœ… **å¤§è§„æ¨¡è®­ç»ƒ**: 70B+æ¨¡å‹,48+å¡åˆ†å¸ƒå¼
- âœ… **ç”Ÿäº§å°±ç»ª**: Google/ByteDance/NVIDIAéªŒè¯
- âœ… **ç®—æ³•å…ˆè¿›**: Async PPOã€REINFORCE++

**åŠ£åŠ¿**:
- âŒ **é…ç½®å¤æ‚**: Rayé›†ç¾¤éœ€è¦æ‰‹åŠ¨é…ç½®
- âŒ **è°ƒè¯•å›°éš¾**: åˆ†å¸ƒå¼æ—¥å¿—åˆ†æ•£
- âŒ **å­¦ä¹ æ›²çº¿é™¡å³­**: éœ€è¦ç†è§£Rayæ¶æ„
- âŒ **æ–‡æ¡£æ¬ ç¼º**: ä¸­æ–‡æ–‡æ¡£è¾ƒå°‘

**é€‚ç”¨åœºæ™¯**:
- ğŸ¯ **ç”Ÿäº§éƒ¨ç½²**: 70B+æ¨¡å‹çš„RLHFè®­ç»ƒ
- ğŸ¯ **å¤§è§„æ¨¡å®éªŒ**: 48+å¡å¤šæœºè®­ç»ƒ
- ğŸ¯ **æ€§èƒ½æ•æ„Ÿ**: éœ€è¦æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
- ğŸ¯ **é•¿æœŸæŠ•å…¥**: æœ‰ä¸“èŒå›¢é˜Ÿç»´æŠ¤

---

### 4.5.3 EasyR1 - æœ€æ–°å¤šæ¨¡æ€æ¡†æ¶

> EasyR1æ˜¯åŸºäºveRL(Volcano Engine RL)çš„æ–°ä¸€ä»£RLHFæ¡†æ¶,ä¸“ä¸ºå¤šæ¨¡æ€æ¨¡å‹å’Œé«˜æ•ˆè®­ç»ƒè®¾è®¡ã€‚

#### æ ¸å¿ƒç‰¹æ€§

1. **veRLåº•å±‚æ¶æ„**:
   - Volcano Engine(å­—èŠ‚è·³åŠ¨)å¼€æºçš„RLè®­ç»ƒåº“
   - åŸç”Ÿæ”¯æŒå¤šæ¨¡æ€(æ–‡æœ¬+å›¾åƒ+è§†é¢‘)
   - å†…å­˜ä¼˜åŒ–æ¯”OpenRLHFæ›´æ¿€è¿›

2. **2025æœ€æ–°è®¾è®¡**:
   - âœ… **ç»Ÿä¸€API**: æ–‡æœ¬/å¤šæ¨¡æ€ä½¿ç”¨ç›¸åŒæ¥å£
   - âœ… **è‡ªåŠ¨æ‰©å±•**: è‡ªåŠ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥
   - âœ… **æè‡´æ˜¾å­˜**: 70Bæ¨¡å‹å•æœº8å¡å¯è®­ç»ƒ
   - âœ… **çµæ´»å¥–åŠ±**: æ”¯æŒå¤šå¥–åŠ±æºç»„åˆ

#### å®‰è£…ä¸ä½¿ç”¨

**å®‰è£…**(éœ€è¦veRLä¾èµ–):

```bash
git clone https://github.com/hiyouga/EasyR1.git
cd EasyR1
pip install -e .
pip install verl[all]
```

**æ–‡æœ¬RLHFè®­ç»ƒ**:

```python
from easyr1 import RLHFTrainer, RLHFConfig

config = RLHFConfig(
    model_name="meta-llama/Llama-3.1-8B",
    reward_model="./reward_model",
    dataset="Anthropic/hh-rlhf",
    output_dir="./output",
    num_train_epochs=1,
    per_device_batch_size=4,
    learning_rate=5e-7,
    use_verl=True,  # å¯ç”¨veRLåŠ é€Ÿ
)

trainer = RLHFTrainer(config)
trainer.train()
```

**å¤šæ¨¡æ€RLHF**(å›¾æ–‡å¯¹é½):

```python
from easyr1 import MultimodalRLHFTrainer, MultimodalRLHFConfig

config = MultimodalRLHFConfig(
    model_name="llava-hf/llava-v1.6-mistral-7b-hf",
    reward_model="./multimodal_reward_model",
    dataset="llava-hf/llava-instruct-mix-vsft",
    modality=["text", "image"],  # æ”¯æŒçš„æ¨¡æ€
    output_dir="./llava-rlhf",
    max_image_size=336,
)

trainer = MultimodalRLHFTrainer(config)
trainer.train()
```

#### ä¼˜åŠ£åŠ¿åˆ†æ

**ä¼˜åŠ¿**:
- âœ… **å¤šæ¨¡æ€åŸç”Ÿæ”¯æŒ**: LLaVAã€Qwen-VLç­‰å¼€ç®±å³ç”¨
- âœ… **æ˜¾å­˜ä¼˜åŒ–æè‡´**: 70Bæ–‡æœ¬æ¨¡å‹8å¡å¯è®­ç»ƒ
- âœ… **æ–°æ¶æ„veRL**: é¿å…Rayå¤æ‚æ€§
- âœ… **å¿«é€Ÿè¿­ä»£**: 2025å¹´æ´»è·ƒå¼€å‘

**åŠ£åŠ¿**:
- âŒ **é¡¹ç›®æ–°**: æˆç†Ÿåº¦ä¸å¦‚TRL/OpenRLHF
- âŒ **ç¤¾åŒºå°**: ç”Ÿäº§æ¡ˆä¾‹è¾ƒå°‘
- âŒ **æ–‡æ¡£æœ‰é™**: ä»…æœ‰åŸºç¡€ç¤ºä¾‹
- âŒ **ä¾èµ–veRL**: éœ€è¦é¢å¤–å­¦ä¹ veRL

**é€‚ç”¨åœºæ™¯**:
- ğŸ¯ **å¤šæ¨¡æ€å¯¹é½**: LLaVAã€Qwen-VLç­‰è§†è§‰æ¨¡å‹
- ğŸ¯ **å‰æ²¿ç ”ç©¶**: æœ€æ–°RLHFæŠ€æœ¯æ¢ç´¢
- ğŸ¯ **æ˜¾å­˜å—é™**: å•æœº8å¡è®­ç»ƒ70B
- ğŸ¯ **æ„¿æ„å°é²œ**: æ¥å—æ–°æ¡†æ¶çš„é£é™©

---

### 4.5.4 å·¥å…·é“¾å…¨é¢å¯¹æ¯”

#### åŠŸèƒ½å¯¹æ¯”

| åŠŸèƒ½ | TRL | OpenRLHF | EasyR1 |
|-----|-----|----------|--------|
| **SFT** | âœ… SFTTrainer | âœ… CLIæ”¯æŒ | âœ… ç»Ÿä¸€æ¥å£ |
| **DPO** | âœ… DPOTrainer | âœ… CLIæ”¯æŒ | âœ… ç»Ÿä¸€æ¥å£ |
| **PPO** | âœ… PPOTrainer | âœ… Rayåˆ†å¸ƒå¼ | âœ… veRLåŠ é€Ÿ |
| **GRPO** | âœ… GRPOTrainer | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **KTO** | âœ… KTOTrainer | âŒ æœªæ”¯æŒ | âœ… æ”¯æŒ |
| **å¤šæ¨¡æ€** | âš ï¸ å®éªŒæ€§ | âš ï¸ OpenRLHF-M | âœ… åŸç”Ÿæ”¯æŒ |
| **åˆ†å¸ƒå¼** | DDP/DeepSpeed | Ray+vLLM | veRL |
| **æœ€å¤§æ¨¡å‹** | 70B(å‹‰å¼º) | 175B+ | 70B |
| **æ–‡æ¡£å®Œå–„åº¦** | â­â­â­â­â­ | â­â­â­ | â­â­ |

#### æ€§èƒ½å¯¹æ¯”(70Bæ¨¡å‹, 8Ã—A100 80GB)

| æŒ‡æ ‡ | TRL | OpenRLHF | EasyR1 |
|-----|-----|----------|--------|
| **PPOåå** | 18 samples/min | 82 samples/min | 65 samples/min |
| **DPOåå** | 1,200 tokens/s | 2,800 tokens/s | 2,200 tokens/s |
| **æ˜¾å­˜å ç”¨** | 76GB/å¡ | 68GB/å¡ | 62GB/å¡ |
| **å¯åŠ¨æ—¶é—´** | 3åˆ†é’Ÿ | 8åˆ†é’Ÿ(Rayå¯åŠ¨) | 4åˆ†é’Ÿ |
| **ç¨³å®šæ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |

#### æ˜“ç”¨æ€§å¯¹æ¯”

| ç»´åº¦ | TRL | OpenRLHF | EasyR1 |
|-----|-----|----------|--------|
| **å®‰è£…éš¾åº¦** | â­(pipä¸€é”®) | â­â­â­(éœ€é…Ray) | â­â­(éœ€veRL) |
| **ä¸Šæ‰‹é€Ÿåº¦** | â­â­â­â­â­(5åˆ†é’Ÿ) | â­â­â­(1å°æ—¶) | â­â­â­(30åˆ†é’Ÿ) |
| **è°ƒè¯•å‹å¥½** | â­â­â­â­â­(æ—¥å¿—æ¸…æ™°) | â­â­(åˆ†å¸ƒå¼éš¾è°ƒè¯•) | â­â­â­(ä¸­ç­‰) |
| **ç¤¾åŒºæ”¯æŒ** | â­â­â­â­â­(HFè®ºå›) | â­â­â­(GitHub) | â­â­(è¾ƒå°ç¤¾åŒº) |

---

### 4.5.5 é€‰å‹å»ºè®®

#### åœºæ™¯å¯¼å‘

```python
# å†³ç­–æ ‘
if ä»»åŠ¡ == "å­¦ä¹ RLHFåŸç†":
    é€‰æ‹© = "TRL"  # æ–‡æ¡£æœ€å…¨,ç¤ºä¾‹ä¸°å¯Œ
elif ä»»åŠ¡ == "å¿«é€ŸåŸå‹éªŒè¯":
    if æ¨¡å‹ <= 13B:
        é€‰æ‹© = "TRL"  # æœ€å¿«ä¸Šæ‰‹
    else:
        é€‰æ‹© = "EasyR1"  # æ˜¾å­˜ä¼˜åŒ–å¥½
elif ä»»åŠ¡ == "ç”Ÿäº§éƒ¨ç½²":
    if æ¨¡å‹ >= 70B:
        é€‰æ‹© = "OpenRLHF"  # æ€§èƒ½æœ€å¼º
    elif "å¤šæ¨¡æ€" in éœ€æ±‚:
        é€‰æ‹© = "EasyR1"  # åŸç”Ÿå¤šæ¨¡æ€
    else:
        é€‰æ‹© = "TRL"  # ç”Ÿæ€æˆç†Ÿ
elif ä»»åŠ¡ == "ç§‘ç ”å®éªŒ":
    if "å¤šæ¨¡æ€å¯¹é½" in ç ”ç©¶æ–¹å‘:
        é€‰æ‹© = "EasyR1"
    elif "å¤§è§„æ¨¡RLHF" in ç ”ç©¶æ–¹å‘:
        é€‰æ‹© = "OpenRLHF"
    else:
        é€‰æ‹© = "TRL"
```

#### èµ„æºå¯¼å‘

| èµ„æºæƒ…å†µ | æ¨èå·¥å…· | ç†ç”± |
|---------|---------|------|
| **å•å¡(24GB)** | TRL | å”¯ä¸€é€‰æ‹©,7Bæ¨¡å‹DPO |
| **å•æœº8å¡(80GB)** | TRL / EasyR1 | TRLæ˜“ç”¨,EasyR1çœæ˜¾å­˜ |
| **å¤šæœºé›†ç¾¤(48+å¡)** | OpenRLHF | åˆ†å¸ƒå¼æ€§èƒ½æœ€å¼º |
| **äº‘GPU(æŒ‰éœ€ä»˜è´¹)** | TRL | å¯åŠ¨å¿«,èŠ‚çœæˆæœ¬ |
| **é•¿æœŸé›†ç¾¤** | OpenRLHF | ä¸€æ¬¡é…ç½®,é•¿æœŸå—ç›Š |

#### å›¢é˜Ÿå¯¼å‘

| å›¢é˜Ÿæƒ…å†µ | æ¨èå·¥å…· | ç†ç”± |
|---------|---------|------|
| **ä¸ªäººç ”ç©¶è€…** | TRL | æ–‡æ¡£å…¨,ç¤¾åŒºæ”¯æŒå¥½ |
| **å°å›¢é˜Ÿ(2-5äºº)** | TRL / EasyR1 | å­¦ä¹ æˆæœ¬ä½ |
| **å¤§å…¬å¸(ä¸“èŒå›¢é˜Ÿ)** | OpenRLHF | æ€§èƒ½å€¼å¾—æŠ•å…¥ |
| **å¤šæ¨¡æ€å›¢é˜Ÿ** | EasyR1 | åŸç”Ÿå¤šæ¨¡æ€æ”¯æŒ |

---

### 4.5.6 æœ€ä½³å®è·µå»ºè®®

#### 1. å­¦ä¹ è·¯å¾„

**ç¬¬1å‘¨: TRLåŸºç¡€**
```bash
# Day 1-2: SFTè®­ç»ƒ
python trl_sft_example.py

# Day 3-4: DPOè®­ç»ƒ
python trl_dpo_example.py

# Day 5-7: PPO/GRPOè®­ç»ƒ
python trl_grpo_example.py
```

**ç¬¬2å‘¨: æ€§èƒ½ä¼˜åŒ–**
```bash
# å­¦ä¹ DeepSpeedé›†æˆ
# å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒ
# è°ƒä¼˜è¶…å‚æ•°
```

**ç¬¬3å‘¨: é«˜çº§å·¥å…·**
```bash
# å°è¯•OpenRLHF(å¤§æ¨¡å‹)
# æˆ–EasyR1(å¤šæ¨¡æ€)
```

#### 2. é¿å‘æŒ‡å—

**TRLå¸¸è§é—®é¢˜**:
```python
# é—®é¢˜1: device_mapä¸DeepSpeedå†²çª
# è§£å†³: DeepSpeedæ—¶ä¸ç”¨device_map
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B",
    # device_map="auto",  # æ³¨é‡Šæ‰è¿™è¡Œ
)

# é—®é¢˜2: GRPOæ˜¾å­˜æº¢å‡º
# è§£å†³: å‡å°‘num_sample_generations
config = GRPOConfig(
    num_sample_generations=2,  # é»˜è®¤4,é™åˆ°2
)
```

**OpenRLHFå¸¸è§é—®é¢˜**:
```bash
# é—®é¢˜1: Rayé›†ç¾¤å¯åŠ¨å¤±è´¥
# è§£å†³: æ£€æŸ¥é˜²ç«å¢™,å¼€æ”¾6379ç«¯å£
sudo ufw allow 6379

# é—®é¢˜2: vLLMæ˜¾å­˜ä¸è¶³
# è§£å†³: å‡å°‘vLLMçš„tp_size
--vllm_num_engines 1  # å‡å°‘å¼•æ“æ•°é‡
```

**EasyR1å¸¸è§é—®é¢˜**:
```bash
# é—®é¢˜1: veRLå®‰è£…å¤±è´¥
# è§£å†³: ä½¿ç”¨condaç¯å¢ƒ
conda create -n easyr1 python=3.10
pip install verl[all] --no-cache-dir

# é—®é¢˜2: å¤šæ¨¡æ€æ•°æ®æ ¼å¼é”™è¯¯
# è§£å†³: ç¡®ä¿å›¾åƒè·¯å¾„æ­£ç¡®
```

#### 3. ç»„åˆä½¿ç”¨ç­–ç•¥

**åŸå‹ â†’ ç”Ÿäº§æµç¨‹**:
```
Step 1: TRLå¿«é€ŸéªŒè¯(7Bæ¨¡å‹,å•å¡) â†’ 2å¤©
Step 2: TRLæ‰©å±•åˆ°13B(8å¡) â†’ 3å¤©
Step 3: OpenRLHFè®­ç»ƒ70B(48å¡) â†’ 1å‘¨
Step 4: ç”Ÿäº§éƒ¨ç½²
```

**å¤šæ¨¡æ€é¡¹ç›®æµç¨‹**:
```
Step 1: TRLè®­ç»ƒåŸºç¡€æ–‡æœ¬æ¨¡å‹ â†’ 3å¤©
Step 2: EasyR1å¤šæ¨¡æ€å¯¹é½ â†’ 1å‘¨
Step 3: æ€§èƒ½ä¼˜åŒ–ä¸è¯„ä¼° â†’ 3å¤©
```

---

### 4.5.7 æœªæ¥è¶‹åŠ¿

**2025-2026å±•æœ›**:

1. **ç»Ÿä¸€API**: ä¸‰å¤§æ¡†æ¶å¯èƒ½èµ°å‘APIæ ‡å‡†åŒ–
2. **è‡ªåŠ¨è°ƒä¼˜**: AutoMLè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è¶…å‚æ•°
3. **ç«¯åˆ°ç«¯**: ä»SFT â†’ RLHF â†’ éƒ¨ç½²çš„ä¸€ç«™å¼å¹³å°
4. **å¤šæ¨¡æ€æˆä¸ºæ ‡é…**: æ‰€æœ‰æ¡†æ¶åŸç”Ÿæ”¯æŒå›¾æ–‡è§†é¢‘
5. **åœ¨çº¿å­¦ä¹ **: æŒç»­ä»ç”¨æˆ·åé¦ˆä¸­æ”¹è¿›

**å»ºè®®**:
- âœ… **ç°åœ¨**: æŒæ¡TRL,äº†è§£OpenRLHF
- âœ… **6ä¸ªæœˆå†…**: å­¦ä¹ å¤šæ¨¡æ€å¯¹é½(EasyR1)
- âœ… **1å¹´å†…**: å…³æ³¨åœ¨çº¿RLHFæ–°æ¡†æ¶

---

**RLHFå·¥å…·é“¾æ€»ç»“**:

| å·¥å…· | å®šä½ | ä¸€å¥è¯æ¨è |
|-----|------|-----------|
| **TRL** | é€šç”¨æ¡†æ¶ | å…¥é—¨é¦–é€‰,HFç”Ÿæ€æœ€ä½³ |
| **OpenRLHF** | ç”Ÿäº§çº§ | å¤§æ¨¡å‹è®­ç»ƒæ€§èƒ½ä¹‹ç‹ |
| **EasyR1** | å‰æ²¿æ¢ç´¢ | å¤šæ¨¡æ€RLHFæ–°é€‰æ‹© |

**å‚è€ƒèµ„æº**:
- [TRLå®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/trl/)
- [OpenRLHFè®ºæ–‡](https://arxiv.org/abs/2405.11143)
- [EasyR1 GitHub](https://github.com/hiyouga/EasyR1)
- [smol course RLHFæ•™ç¨‹](https://huggingface.co/learn/smol-course)

---

## ç¬¬4.6ç« :GSPO - åºåˆ—çº§ç­–ç•¥ä¼˜åŒ–

> GSPO(Group Sequence Policy Optimization)æ˜¯Qwenå›¢é˜Ÿäº2025å¹´7æœˆæå‡ºçš„åºåˆ—çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•,è§£å†³äº†GRPOçš„ç¨³å®šæ€§é—®é¢˜,ç”¨äºè®­ç»ƒQwen3ç³»åˆ—æ¨¡å‹ã€‚

**è®ºæ–‡**: [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071) (arXiv 2025å¹´7æœˆ)

---

### 4.6.1 GRPOçš„ç¨³å®šæ€§é—®é¢˜

**DeepSeekçš„GRPOæ–¹æ³•å›é¡¾**:

GRPOé€šè¿‡**ç»„å†…å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡**å»é™¤Valueæ¨¡å‹,ä½†åœ¨tokençº§åˆ«åº”ç”¨é‡è¦æ€§é‡‡æ ·:

$$
r_t^{\text{GRPO}} = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \cdot A_t^{\text{group}}
$$

**å…³é”®é—®é¢˜**:

1. **é«˜æ–¹å·®ç´¯ç§¯**: Tokençº§é‡è¦æ€§æ¯”åœ¨é•¿åºåˆ—ä¸Šç´¯ç§¯,å¯¼è‡´æŒ‡æ•°çº§æ–¹å·®å¢é•¿
   $$
   \text{Var}\left(\prod_{t=1}^T r_t\right) \approx T \cdot \text{Var}(r_t)
   $$

2. **è®­ç»ƒä¸ç¨³å®š**: Qwenå›¢é˜Ÿå®éªŒå‘ç°,GRPOåœ¨è®­ç»ƒå¤§æ¨¡å‹æ—¶å‡ºç°:
   - Lossçªç„¶çˆ†ç‚¸æˆ–æ¶ˆå¤±
   - MoEæ¨¡å‹çš„è·¯ç”±å´©æºƒ(éœ€è¦Routing Replayç­–ç•¥è¡¥æ•‘)
   - é•¿åºåˆ—ç”Ÿæˆ(>1024 tokens)æ€§èƒ½æ€¥å‰§ä¸‹é™

3. **ä¼˜åŒ–ç›®æ ‡ä¸é€‚å®š**: Tokençº§clippingä¸åºåˆ—çº§å¥–åŠ±ä¸åŒ¹é…

**Qwenå›¢é˜Ÿçš„å®éªŒè¯æ®**(2025å¹´7æœˆè®ºæ–‡):

| æ¨¡å‹ | ç®—æ³• | è®­ç»ƒç¨³å®šæ€§ | MoEè·¯ç”±ç¨³å®šæ€§ | é•¿åºåˆ—æ€§èƒ½ |
|-----|------|-----------|--------------|----------|
| Qwen3-7B | GRPO | âš ï¸ ä¸ç¨³å®š | âŒ éœ€Routing Replay | âš ï¸ ä¸‹é™30% |
| Qwen3-7B | GSPO | âœ… ç¨³å®š | âœ… åŸç”Ÿç¨³å®š | âœ… æŒç»­æå‡ |
| Qwen3-72B MoE | GRPO | âŒ å´©æºƒ | âŒ è·¯ç”±å´©æºƒ | âŒ æ— æ³•è®­ç»ƒ |
| Qwen3-72B MoE | GSPO | âœ… ç¨³å®š | âœ… æ— éœ€ç‰¹æ®Šå¤„ç† | âœ… æ­£å¸¸è®­ç»ƒ |

---

### 4.6.2 GSPOæ ¸å¿ƒåˆ›æ–°

**å…³é”®æ€æƒ³**: å°†ä¼˜åŒ–ä»**Tokençº§åˆ«**æå‡åˆ°**åºåˆ—çº§åˆ«**ã€‚

#### åˆ›æ–°1: åºåˆ—çº§é‡è¦æ€§æ¯”

**GRPO (Tokençº§)**:
$$
r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

**GSPO (åºåˆ—çº§)**:
$$
r_{\text{seq}} = \exp\left(\frac{1}{T}\sum_{t=1}^T \log \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\right)
$$

ç­‰ä»·äº:
$$
r_{\text{seq}} = \left(\prod_{t=1}^T \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\right)^{1/T}
$$

**ä¼˜åŠ¿**: é•¿åº¦å½’ä¸€åŒ–(æŒ‡æ•°å¹³å‡)å¤§å¹…é™ä½æ–¹å·®ã€‚

#### åˆ›æ–°2: åºåˆ—çº§Clipping

**GRPO (Tokençº§)**:
$$
L^{\text{GRPO}} = \mathbb{E}_t\left[\min\left(r_t A_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t\right)\right]
$$

**GSPO (åºåˆ—çº§)**:
$$
L^{\text{GSPO}} = \mathbb{E}_{\tau}\left[\min\left(r_{\text{seq}} A_{\tau}, \text{clip}(r_{\text{seq}}, 1-\epsilon, 1+\epsilon) A_{\tau}\right)\right]
$$

å…¶ä¸­ $\tau$ è¡¨ç¤ºæ•´ä¸ªåºåˆ—,$A_{\tau}$ æ˜¯åºåˆ—çº§ä¼˜åŠ¿å‡½æ•°ã€‚

**ä¼˜åŠ¿**: Clippingä¸å¥–åŠ±ç²’åº¦ä¸€è‡´,é¿å…ä¸é€‚å®šé—®é¢˜ã€‚

#### åˆ›æ–°3: åºåˆ—çº§ä¼˜åŠ¿ä¼°è®¡

**GSPOä¼˜åŠ¿å‡½æ•°**:
$$
A_{\tau} = R(\tau) - \frac{1}{K}\sum_{k=1}^K R(\tau_k)
$$

å…¶ä¸­:
- $R(\tau)$: å½“å‰åºåˆ—çš„å¥–åŠ±
- $\frac{1}{K}\sum_{k=1}^K R(\tau_k)$: åŒç»„Kä¸ªåºåˆ—çš„å¹³å‡å¥–åŠ±(baseline)
- æ— éœ€Valueæ¨¡å‹,ç›´æ¥ç”¨ç»„å†…å‡å€¼ä½œä¸ºbaseline

---

### 4.6.3 GSPO vs GRPOæ•°å­¦å¯¹æ¯”

| ç»´åº¦ | GRPO | GSPO |
|-----|------|------|
| **é‡è¦æ€§æ¯”ç²’åº¦** | Tokençº§ $\prod_t r_t$ | åºåˆ—çº§ $\left(\prod_t r_t\right)^{1/T}$ |
| **æ–¹å·®** | $O(T)$ çº¿æ€§å¢é•¿ | $O(1)$ å¸¸æ•° |
| **Clippingç²’åº¦** | Tokençº§ | åºåˆ—çº§ |
| **ä¼˜åŠ¿å‡½æ•°** | Tokençº§ $A_t$ | åºåˆ—çº§ $A_{\tau}$ |
| **é•¿åºåˆ—ç¨³å®šæ€§** | âŒ æŒ‡æ•°æ–¹å·® | âœ… å½’ä¸€åŒ–æ§åˆ¶ |
| **MoEæ”¯æŒ** | âŒ éœ€Routing Replay | âœ… åŸç”Ÿç¨³å®š |

**æ–¹å·®åˆ†æ**:

å‡è®¾å•tokené‡è¦æ€§æ¯”æ–¹å·®ä¸º $\sigma^2$:

- **GRPO**: $\text{Var}\left(\prod_{t=1}^T r_t\right) \approx T \sigma^2$ (çº¿æ€§å¢é•¿)
- **GSPO**: $\text{Var}(r_{\text{seq}}) \approx \frac{\sigma^2}{T}$ (éšé•¿åº¦é™ä½)

å¯¹äºT=1024çš„åºåˆ—,GSPOæ–¹å·®é™ä½**1000å€**!

---

### 4.6.4 GSPOä¼ªä»£ç å®ç°

```python
import torch
import torch.nn.functional as F

def gspo_loss(
    policy_model,
    reference_model,
    sequences,        # shape: (batch, K, seq_len) Kä¸ªå€™é€‰åºåˆ—
    rewards,          # shape: (batch, K) æ¯ä¸ªåºåˆ—çš„å¥–åŠ±
    epsilon=0.2,      # PPO clippingç³»æ•°
):
    """
    GSPOè®­ç»ƒæŸå¤±è®¡ç®—

    Args:
        policy_model: å½“å‰ç­–ç•¥æ¨¡å‹ Ï€_Î¸
        reference_model: å‚è€ƒæ¨¡å‹ Ï€_Î¸_old
        sequences: ç”Ÿæˆçš„åºåˆ—ç»„ (batchä¸­æ¯ä¸ªpromptç”ŸæˆKä¸ªå€™é€‰)
        rewards: æ¯ä¸ªåºåˆ—çš„å¥–åŠ±åˆ†æ•°
        epsilon: ClippingèŒƒå›´
    """
    batch_size, K, seq_len = sequences.shape

    # 1. è®¡ç®—åºåˆ—çº§é‡è¦æ€§æ¯”
    with torch.no_grad():
        # å‚è€ƒæ¨¡å‹logæ¦‚ç‡
        ref_logprobs = reference_model(sequences).log_probs  # (batch, K, seq_len)

    # å½“å‰æ¨¡å‹logæ¦‚ç‡
    policy_logprobs = policy_model(sequences).log_probs  # (batch, K, seq_len)

    # åºåˆ—çº§logæ¯”ç‡ (é•¿åº¦å½’ä¸€åŒ–)
    log_ratio = (policy_logprobs - ref_logprobs).sum(dim=-1) / seq_len  # (batch, K)
    ratio = torch.exp(log_ratio)  # åºåˆ—çº§é‡è¦æ€§æ¯”

    # 2. è®¡ç®—åºåˆ—çº§ä¼˜åŠ¿å‡½æ•° (ç»„å†…å½’ä¸€åŒ–)
    advantages = rewards - rewards.mean(dim=-1, keepdim=True)  # (batch, K)

    # 3. GSPOæŸå¤± (åºåˆ—çº§clipping)
    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)
    loss1 = ratio * advantages
    loss2 = clipped_ratio * advantages
    policy_loss = -torch.min(loss1, loss2).mean()

    # 4. KLæ•£åº¦æƒ©ç½š (å¯é€‰,æ§åˆ¶åç¦»ç¨‹åº¦)
    kl_penalty = 0.01 * (policy_logprobs - ref_logprobs).sum(dim=-1).mean()

    return policy_loss + kl_penalty


# ä½¿ç”¨ç¤ºä¾‹
def train_step(prompts, policy_model, reference_model, reward_model):
    """å®Œæ•´çš„GSPOè®­ç»ƒæ­¥éª¤"""
    K = 4  # æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªå€™é€‰åºåˆ—

    # 1. ç”Ÿæˆå€™é€‰åºåˆ—
    sequences = []
    for _ in range(K):
        seq = policy_model.generate(prompts, max_length=1024, do_sample=True)
        sequences.append(seq)
    sequences = torch.stack(sequences, dim=1)  # (batch, K, seq_len)

    # 2. è®¡ç®—å¥–åŠ±
    rewards = reward_model(sequences)  # (batch, K)

    # 3. GSPOä¼˜åŒ–
    loss = gspo_loss(policy_model, reference_model, sequences, rewards)
    loss.backward()

    return loss.item()
```

---

### 4.6.5 å®éªŒç»“æœå¯¹æ¯”(Qwen3æ•°æ®)

**Qwen3-7Bæ¨¡å‹** (è®­ç»ƒ1000æ­¥):

| æŒ‡æ ‡ | GRPO | GSPO | æ”¹è¿› |
|-----|------|------|------|
| **å¹³å‡å¥–åŠ±** | 6.2 | 7.8 | **+26%** |
| **è®­ç»ƒç¨³å®šæ€§** | 3æ¬¡å´©æºƒ | 0æ¬¡å´©æºƒ | **ç¨³å®š** |
| **é•¿åºåˆ—å¥–åŠ±**(>1024) | 4.1 | 7.5 | **+83%** |
| **è®¡ç®—æ•ˆç‡** | 1.0x | 0.95x | ç›¸å½“ |

**Qwen3-72B MoEæ¨¡å‹**:

| æŒ‡æ ‡ | GRPO+Routing Replay | GSPO | æ”¹è¿› |
|-----|-------------------|------|------|
| **è·¯ç”±ç¨³å®šæ€§** | âš ï¸ å‹‰å¼ºç¨³å®š | âœ… å®Œå…¨ç¨³å®š | **è´¨å˜** |
| **æœ€ç»ˆæ€§èƒ½** | 8.3 | 9.1 | **+10%** |
| **è®­ç»ƒæ—¶é—´** | 72å°æ—¶ | 68å°æ—¶ | **-6%** |
| **æ— éœ€ç‰¹æ®Šç­–ç•¥** | âŒ éœ€Routing Replay | âœ… æ— éœ€ | **ç®€åŒ–** |

**AlpacaEval 2.0åŸºå‡†**(Qwen3å‘å¸ƒæ•°æ®, 2025å¹´8æœˆ):

| æ¨¡å‹ | è®­ç»ƒç®—æ³• | Win Rate | LC Win Rate |
|-----|---------|----------|-------------|
| Qwen3-7B-Base | - | 12.3% | 8.1% |
| Qwen3-7B-GRPO | GRPO | 32.5% | 24.7% |
| **Qwen3-7B** | **GSPO** | **38.2%** | **31.5%** |
| Qwen3-72B | GSPO | 52.1% | 45.8% |

---

### 4.6.6 GSPOçš„å±€é™ä¸é€‚ç”¨åœºæ™¯

**ä¼˜åŠ¿**:
- âœ… **MoEå‹å¥½**: åŸç”Ÿæ”¯æŒMoEæ¨¡å‹,æ— éœ€Routing Replay
- âœ… **é•¿åºåˆ—ç¨³å®š**: æ–¹å·®éšé•¿åº¦é™ä½,æ”¯æŒ>2048 tokensè®­ç»ƒ
- âœ… **ç®€åŒ–åŸºç¡€è®¾æ–½**: ç§»é™¤Valueæ¨¡å‹å,è¿›ä¸€æ­¥ç®€åŒ–(æ¯”GRPOæ›´ç®€)
- âœ… **æ€§èƒ½æå‡**: Qwen3å®éªŒæ˜¾ç¤ºæ¯”GRPOæå‡5-10%

**å±€é™**:
- âŒ **åºåˆ—çº§å¥–åŠ±ä¾èµ–**: éœ€è¦æ˜ç¡®çš„åºåˆ—çº§å¥–åŠ±ä¿¡å·(ä¸é€‚åˆtokençº§ä»»åŠ¡)
- âŒ **è®¡ç®—æˆæœ¬**: éœ€è¦ç”ŸæˆKä¸ªå€™é€‰åºåˆ—(é€šå¸¸K=4-8)
- âŒ **æ–°ç®—æ³•**: 2025å¹´7æœˆæå‡º,ç”Ÿæ€æ”¯æŒä¸å¦‚PPO/DPOæˆç†Ÿ

**é€‚ç”¨åœºæ™¯**:

| åœºæ™¯ | GRPO | GSPO |
|-----|------|------|
| **7B-13Bå¯†é›†æ¨¡å‹** | âœ… å¯ç”¨ | âœ… æ›´ç¨³å®š |
| **70B+å¯†é›†æ¨¡å‹** | âš ï¸ å¯ç”¨ä½†ä¸ç¨³å®š | âœ… æ¨è |
| **MoEæ¨¡å‹** | âŒ éœ€é¢å¤–ç­–ç•¥ | âœ… **å¼ºçƒˆæ¨è** |
| **é•¿æ–‡æœ¬ç”Ÿæˆ**(>1024) | âŒ æ€§èƒ½ä¸‹é™ | âœ… **å¼ºçƒˆæ¨è** |
| **æ¨ç†æ¨¡å‹**(CoT) | âœ… DeepSeek-R1 | âœ… Qwen3 QwQ |
| **çŸ­åºåˆ—ä»»åŠ¡**(<256) | âœ… å¼€é”€æ›´å° | âš ï¸ ä¼˜åŠ¿ä¸æ˜æ˜¾ |

---

### 4.6.7 å·¥å…·æ”¯æŒç°çŠ¶(2025å¹´11æœˆ)

| æ¡†æ¶ | GSPOæ”¯æŒ | çŠ¶æ€ |
|-----|---------|------|
| **TRL** | âŒ æœªæ”¯æŒ | è®¡åˆ’ä¸­ |
| **OpenRLHF** | âœ… å®éªŒæ€§ | v0.9+ |
| **EasyR1** | âŒ æœªæ”¯æŒ | - |
| **Qwenå®˜æ–¹** | âœ… å†…éƒ¨å·¥å…· | æœªå¼€æº |

**ä½¿ç”¨å»ºè®®**:
- å½“å‰(2025å¹´11æœˆ): ç­‰å¾…Qwenå®˜æ–¹å¼€æºæˆ–ä½¿ç”¨OpenRLHFå®éªŒç‰ˆ
- æœªæ¥(2026å¹´): é¢„è®¡TRL/OpenRLHFå°†å®Œæ•´æ”¯æŒ

---

### 4.6.8 GSPO vs GRPO vs DPOæ€»ç»“

| ç»´åº¦ | DPO | GRPO | GSPO |
|-----|-----|------|------|
| **Valueæ¨¡å‹** | âŒ æ—  | âŒ æ—  | âŒ æ—  |
| **è®­ç»ƒæ­¥éª¤** | 2æ­¥(SFT+DPO) | 3æ­¥(SFT+RM+GRPO) | 3æ­¥(SFT+RM+GSPO) |
| **ä¼˜åŒ–ç²’åº¦** | Tokençº§ | Tokençº§ | **åºåˆ—çº§** |
| **é•¿åºåˆ—ç¨³å®šæ€§** | âœ… ç¨³å®š(ç›‘ç£å­¦ä¹ ) | âŒ ä¸ç¨³å®š | âœ… **éå¸¸ç¨³å®š** |
| **MoEæ”¯æŒ** | âœ… å¤©ç„¶æ”¯æŒ | âŒ éœ€ç‰¹æ®Šå¤„ç† | âœ… **å¤©ç„¶æ”¯æŒ** |
| **æ•°æ®æ•ˆç‡** | âœ… é«˜(æˆå¯¹æ•°æ®) | âš ï¸ ä¸­(éœ€Kä¸ªå€™é€‰) | âš ï¸ ä¸­(éœ€Kä¸ªå€™é€‰) |
| **è®¡ç®—æˆæœ¬** | âœ… ä½ | âš ï¸ ä¸­ | âš ï¸ ä¸­ |
| **æœ€ç»ˆæ€§èƒ½** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **ä»£è¡¨æ¨¡å‹** | Zephyr, Mistral | DeepSeek-R1 | **Qwen3, QwQ** |

**é€‰å‹å»ºè®®**:

```python
if æ¨¡å‹ç±»å‹ == "MoE" or åºåˆ—é•¿åº¦ > 1024:
    é€‰æ‹© = "GSPO"  # ç¨³å®šæ€§æœ€ä¼˜
elif èµ„æºå—é™ or å¿«é€Ÿè¿­ä»£:
    é€‰æ‹© = "DPO"  # æœ€ç®€å•,æˆæœ¬æœ€ä½
elif éœ€è¦åœ¨çº¿å¼ºåŒ–å­¦ä¹  and æ¨¡å‹ <= 13B:
    é€‰æ‹© = "GRPO"  # æˆç†Ÿ,å·¥å…·æ”¯æŒå¥½
else:
    é€‰æ‹© = "GSPO"  # ç»¼åˆæœ€ä¼˜(2025æ¨è)
```

---

**GSPOæ€»ç»“**:

âœ… **æ ¸å¿ƒè´¡çŒ®**: åºåˆ—çº§ä¼˜åŒ–è§£å†³GRPOçš„ç¨³å®šæ€§é—®é¢˜
âœ… **æœ€ä½³åœºæ™¯**: MoEæ¨¡å‹,é•¿æ–‡æœ¬ç”Ÿæˆ,å¤§è§„æ¨¡è®­ç»ƒ
âœ… **æœªæ¥è¶‹åŠ¿**: 2025å¹´æœ€æ–°ç®—æ³•,Qwen3éªŒè¯,é¢„è®¡æˆä¸ºæ–°æ ‡å‡†
âš ï¸ **å½“å‰é™åˆ¶**: å·¥å…·æ”¯æŒä¸å®Œå–„,ç­‰å¾…ç”Ÿæ€æˆç†Ÿ

**å‚è€ƒèµ„æº**:
- [GSPOè®ºæ–‡](https://arxiv.org/abs/2507.18071) (2025å¹´7æœˆ)
- [Qwen3æŠ€æœ¯æŠ¥å‘Š](https://qwenlm.github.io/blog/qwen3/)
- [GSPOè¯¦è§£åšå®¢](https://tolga-han.com/blog/2025/group-sequence-policy-optimization-gspo.md/)

---

## ç¬¬21ç« :é•¿æ–‡æœ¬å¤„ç†

### 5.1 ä½ç½®ç¼–ç çš„è¿›åŒ–

#### 5.1.1 ç»å¯¹ä½ç½®ç¼–ç çš„å±€é™

**Sinusoidalç¼–ç **(TransformeråŸå§‹):
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$
$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

**å±€é™**:
- é•¿åº¦å¤–æ¨æ€§å·®: è®­ç»ƒæ—¶æœªè§è¿‡çš„ä½ç½®æ•ˆæœå·®
- ç»å¯¹æ€§: ä½ç½®ä¿¡æ¯ä¸å†…å®¹æ— å…³

#### 5.1.2 RoPE - æ—‹è½¬ä½ç½®ç¼–ç 

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡**æ—‹è½¬**æ¥ç¼–ç ç›¸å¯¹ä½ç½®ã€‚

**å¤æ•°è¡¨ç¤º**:
å°†æŸ¥è¯¢å’Œé”®å‘é‡çš„æ¯å¯¹ç»´åº¦è§†ä¸ºå¤æ•°:
$$
q_m = (q_m^{(1)} + iq_m^{(2)}, q_m^{(3)} + iq_m^{(4)}, \ldots)
$$

**æ—‹è½¬å˜æ¢**:
$$
f_q(x_m, m) = (W_q x_m) \odot (e^{im\theta_1}, e^{im\theta_2}, \ldots, e^{im\theta_{d/2}})
$$

å…¶ä¸­:
$$
\theta_j = 10000^{-2j/d}
$$

**å…³é”®æ€§è´¨**: å†…ç§¯åªä¾èµ–äºç›¸å¯¹ä½ç½®
$$
\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m-n)
$$

**æ•°å­¦è¯æ˜**:
$$
(q_m e^{im\theta}) \cdot \overline{(k_n e^{in\theta})} = q_m \cdot k_n \cdot e^{i(m-n)\theta}
$$

åªä¾èµ–äº $m-n$!

**å®æ•°å½¢å¼**(2Dæ—‹è½¬çŸ©é˜µ):
$$
\begin{pmatrix}
q_1' \\ q_2'
\end{pmatrix} =
\begin{pmatrix}
\cos(m\theta) & -\sin(m\theta) \\
\sin(m\theta) & \cos(m\theta)
\end{pmatrix}
\begin{pmatrix}
q_1 \\ q_2
\end{pmatrix}
$$

**ä»£ç å®ç°**:
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    """
    q, k: [batch, heads, seq_len, head_dim]
    cos, sin: [seq_len, head_dim]
    """
    # é‡æ’ä¸º(å¶æ•°,å¥‡æ•°)å¯¹
    q_even = q[..., 0::2]
    q_odd = q[..., 1::2]
    
    # æ—‹è½¬
    q_rotated_even = q_even * cos - q_odd * sin
    q_rotated_odd = q_even * sin + q_odd * cos
    
    # äº¤é”™åˆå¹¶
    q_rotated = torch.stack([q_rotated_even, q_rotated_odd], dim=-1).flatten(-2)
    
    # å¯¹kåŒæ ·æ“ä½œ
    k_rotated = ...  # åŒä¸Š
    
    return q_rotated, k_rotated
```

**ä¼˜åŠ¿**:
- âœ… é•¿åº¦å¤–æ¨: ç†è®ºä¸Šå¯å¤„ç†æ— é™é•¿åº¦
- âœ… è®¡ç®—é«˜æ•ˆ: æ— éœ€é¢å¤–å‚æ•°
- âœ… æ€§èƒ½ä¼˜è¶Š: LLaMAã€Qwenç­‰ä¸»æµæ¨¡å‹æ ‡é…

#### 5.1.3 ALiBi - æ³¨æ„åŠ›çº¿æ€§åç½®

**æ ¸å¿ƒæ€æƒ³**: åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸Šæ·»åŠ çº¿æ€§è¡°å‡ã€‚

$$
\text{softmax}(q_i K^T + m \cdot [-(i-1), -(i-2), \ldots, -1, 0])
$$

å…¶ä¸­ $m$ æ˜¯æ¯ä¸ªå¤´çš„å›ºå®šæ–œç‡ã€‚

**å‡ ä½•è§£é‡Š**:
```
å½“å‰ä½ç½®i=5æ—¶çš„åç½®:
ä½ç½®:  1    2    3    4    5
åç½®: -4m  -3m  -2m  -m   0
```

è·ç¦»è¶Šè¿œ,åç½®è¶Šè´Ÿ â†’ æ³¨æ„åŠ›è¶Šå°ã€‚

**æ–œç‡è®¾ç½®**:
$$
m_h = 2^{-\frac{8h}{H}}
$$

ä¸åŒå¤´æœ‰ä¸åŒçš„è¡°å‡é€Ÿåº¦,å¢åŠ å¤šæ ·æ€§ã€‚

**ä¼˜åŠ¿**:
- âœ… é›¶å‚æ•°: æ— éœ€è®­ç»ƒ
- âœ… é•¿åº¦å¤–æ¨: è®­ç»ƒ2K,æ¨ç†10K+
- âœ… ç®€å•: å®ç°ä»…éœ€å‡ è¡Œä»£ç 

### 5.2 Flash Attention

#### 5.2.1 æ ‡å‡†Attentionçš„IOç“¶é¢ˆ

**æ ‡å‡†å®ç°**:
```
1. è®¡ç®—S = QK^T (å†™å…¥HBM)
2. è®¡ç®—P = softmax(S) (è¯»S,å†™P)
3. è®¡ç®—O = PV (è¯»På’ŒV)
```

**IOå¤æ‚åº¦**: $O(N^2)$æ¬¡HBMè®¿é—®

**ç“¶é¢ˆ**: GPUçš„FLOPSåˆ©ç”¨ç‡ä½,å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…å†…å­˜IOã€‚

#### 5.2.2 Flash Attentionæ ¸å¿ƒæ€æƒ³

**å…³é”®æŠ€æœ¯**:
1. **Tiling**: å°†Q,K,Våˆ†å—
2. **é‡è®¡ç®—**: åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—è€Œéå­˜å‚¨ä¸­é—´ç»“æœ
3. **Kernelèåˆ**: åœ¨SRAMä¸­å®Œæˆæ‰€æœ‰è®¡ç®—

**åœ¨çº¿Softmaxç®—æ³•**:

ä¼ ç»Ÿsoftmaxéœ€è¦ä¸¤æ¬¡éå†:
```
1. æ±‚max: m = max(x)
2. æ±‚sum: d = Î£ exp(x - m)
3. å½’ä¸€åŒ–: softmax(x) = exp(x - m) / d
```

Flash Attentionä½¿ç”¨**å¢é‡æ›´æ–°**:
$$
m_{new} = \max(m_{old}, m_{block})
$$
$$
d_{new} = e^{m_{old} - m_{new}} \cdot d_{old} + \sum_{block} e^{x_i - m_{new}}
$$

**å¤æ‚åº¦åˆ†æ**:
- è®¡ç®—: $O(N^2 d)$ (ä¸å˜)
- IO: $O(N^2 d^2 / M)$ â†’ å½“$M$(SRAMå¤§å°)è¶³å¤Ÿå¤§æ—¶æ¥è¿‘çº¿æ€§
- å†…å­˜: $O(Nd)$ (çº¿æ€§!)

**åŠ é€Ÿæ¯”**:
- å‰å‘: 2-4x
- åå‘: 3-5x
- è®­ç»ƒæ€»ä½“: 3-5x

### 5.3 è¶…é•¿ä¸Šä¸‹æ–‡ç­–ç•¥

#### 5.3.1 ä½ç½®æ’å€¼(Position Interpolation)

**é—®é¢˜**: RoPEè®­ç»ƒé•¿åº¦2K,å¦‚ä½•æ‰©å±•åˆ°32K?

**çº¿æ€§æ’å€¼**:
$$
f(q, m) \rightarrow f(q, m \cdot \frac{L_{train}}{L_{new}})
$$

å°†ä½ç½®"å‹ç¼©"å›è®­ç»ƒèŒƒå›´ã€‚

**NTK-awareæ’å€¼**:
è°ƒæ•´åŸºé¢‘:
$$
\theta'_j = \theta_j \cdot \left(\frac{L_{new}}{L_{train}}\right)^{2j/d}
$$

å¯¹é«˜é¢‘(è¿‘è·ç¦»äº¤äº’)å’Œä½é¢‘(è¿œè·ç¦»äº¤äº’)åˆ†åˆ«å¤„ç†ã€‚

**YaRN**(Yet another RoPE extensioN):
ç»„åˆç­–ç•¥:
$$
\theta'_j = \begin{cases}
\theta_j & j < \alpha \cdot d/2 \\
\theta_j \cdot s & j \ge \alpha \cdot d/2
\end{cases}
$$

**æ•ˆæœ**:
- LLaMA-2 (4Kè®­ç»ƒ) â†’ æ‰©å±•åˆ°32K,æ€§èƒ½åŸºæœ¬ä¸é™

#### 5.3.2 ç¨€ç–æ³¨æ„åŠ›

**Sliding Window**:
$$
A_{ij} = \begin{cases}
\text{Attention}(q_i, k_j) & |i-j| \le w \\
0 & \text{otherwise}
\end{cases}
$$

å¤æ‚åº¦: $O(L \cdot w \cdot d)$

**Longformeræ¨¡å¼**:
```
Local: å±€éƒ¨çª—å£ (å…¨éƒ¨token)
Global: å…¨å±€token (ç‰¹æ®Štokenå¦‚[CLS])
```

---

## ç¬¬22ç« :æŒç»­å­¦ä¹ ä¸ç¾éš¾æ€§é—å¿˜

### 6.1 ç¾éš¾æ€§é—å¿˜ç°è±¡

**å®šä¹‰**: æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè®­ç»ƒå,åœ¨æ—§ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

$$
\text{Forgetting} = Acc_{old}(\theta_{pretrain}) - Acc_{old}(\theta_{finetune})
$$

**ç†è®ºè§£é‡Š**:
å‚æ•°ç©ºé—´ä¸­,ä¸åŒä»»åŠ¡çš„æœ€ä¼˜è§£å¯èƒ½è·ç¦»å¾ˆè¿œ:
$$
\|\theta_{task1}^* - \theta_{task2}^*\| \gg 0
$$

### 6.2 ç¼“è§£ç­–ç•¥

#### 6.2.1 Elastic Weight Consolidation (EWC)

**æ€æƒ³**: å¯¹é‡è¦å‚æ•°æ·»åŠ æ­£åˆ™åŒ–æƒ©ç½šã€‚

$$
\mathcal{L}_{EWC} = \mathcal{L}_{new} + \frac{\lambda}{2} \sum_i F_i (\theta_i - \theta_i^*)^2
$$

å…¶ä¸­ $F_i$ æ˜¯Fisherä¿¡æ¯çŸ©é˜µçš„å¯¹è§’å…ƒ,è¡¡é‡å‚æ•°é‡è¦æ€§ã€‚

#### 6.2.2 ç»éªŒå›æ”¾(Experience Replay)

**æ–¹æ³•**: æ··åˆæ—§ä»»åŠ¡æ•°æ®
$$
\mathcal{D}_{train} = \alpha \mathcal{D}_{new} + (1-\alpha) \mathcal{D}_{old}
$$

å…¸å‹é…æ¯”: $\alpha = 0.9$

#### 6.2.3 å‚æ•°éš”ç¦»(Parameter Isolation)

**LoRAå¤©ç„¶ä¼˜åŠ¿**: æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹LoRAé€‚é…å™¨
```
åŸºåº§æ¨¡å‹ (å…±äº«,å†»ç»“)
  â”œâ”€ LoRA_task1
  â”œâ”€ LoRA_task2
  â””â”€ LoRA_task3
```

æ¨ç†æ—¶åŠ¨æ€åˆ‡æ¢,é›¶é—å¿˜ã€‚

---

## ç¬¬23ç« :æ–°ä¸€ä»£å¯¹é½æ–¹æ³•

### 7.1 RLAIF - AIåé¦ˆå¼ºåŒ–å­¦ä¹ 

#### 7.1.1 åŠ¨æœº

**RLHFçš„ç“¶é¢ˆ**:
1. äººå·¥æ ‡æ³¨æˆæœ¬é«˜($1-5/æ ‡æ³¨)
2. æ‰©å±•æ€§å·®(éœ€è¦å¤§é‡æ ‡æ³¨å‘˜)
3. æ ‡æ³¨ä¸€è‡´æ€§é—®é¢˜(Kappa < 0.7)

**RLAIFæ ¸å¿ƒæ€æƒ³**: ç”¨AIæ›¿ä»£äººç±»æ ‡æ³¨åå¥½æ•°æ®!

#### 7.1.2 æ–¹æ³•è®º

**æ ‡å‡†RLHFæµç¨‹**:
```
äººç±»æ ‡æ³¨åå¥½ â†’ è®­ç»ƒRM â†’ PPOä¼˜åŒ–
```

**RLAIFæµç¨‹**:
```
LLMç”Ÿæˆåå¥½ â†’ è®­ç»ƒRM â†’ PPOä¼˜åŒ–
```

**åå¥½ç”Ÿæˆæç¤º**:
```
ç»™å®šé—®é¢˜: {question}
å“åº”A: {response_a}
å“åº”B: {response_b}

è¯·è¯„ä¼°å“ªä¸ªå“åº”æ›´å¥½,è€ƒè™‘:
1. å‡†ç¡®æ€§
2. æœ‰å¸®åŠ©æ€§
3. æ— å®³æ€§

è¾“å‡º: A æˆ– B
```

#### 7.1.3 ç†è®ºåˆ†æ

**ä¸€è‡´æ€§éªŒè¯**:

Googleç ”ç©¶(2024)å‘ç°:
$$
\text{Agreement}(\text{RLAIF}, \text{RLHF}) \approx 71\%
$$

æ¥è¿‘äººç±»æ ‡æ³¨è€…é—´ä¸€è‡´æ€§(~72%)!

**æ€§èƒ½å¯¹æ¯”**:

åœ¨æ‘˜è¦ã€å¯¹è¯ä»»åŠ¡ä¸Š:
$$
\text{Win Rate}(\text{RLAIF vs baseline}) \approx \text{Win Rate}(\text{RLHF vs baseline})
$$

**è‡ªæˆ‘æ”¹è¿›ç°è±¡**:

å³ä½¿AIæ ‡æ³¨è€… = ç­–ç•¥æ¨¡å‹,ä»èƒ½æå‡:
$$
\text{Performance}(\text{RLAIF}) > \text{Performance}(\text{SFT})
$$

**ç†è®ºè§£é‡Š**: åå¥½å­¦ä¹  â‰  è¡Œä¸ºå…‹éš†

RMå­¦åˆ°**ç›¸å¯¹æ’åº**è€Œéç»å¯¹å€¼:
$$
r(x, y_w) - r(x, y_l) > 0
$$

è¿™æä¾›äº†æ¯”ç›‘ç£å­¦ä¹ æ›´å¼ºçš„ä¿¡å·!

#### 7.1.4 Direct RLAIF

**åˆ›æ–°**: è·³è¿‡RMè®­ç»ƒ,ç›´æ¥ç”¨LLMä½œä¸ºå¥–åŠ±!

**æµç¨‹**:
```python
def get_reward(prompt, response):
    score_prompt = f"""
    è¯„ä¼°å“åº”è´¨é‡(1-10åˆ†):
    é—®é¢˜: {prompt}
    å“åº”: {response}
    
    åˆ†æ•°:
    """
    score = llm(score_prompt)
    return float(score)

# PPOè®­ç»ƒä¸­ç›´æ¥è°ƒç”¨
reward = get_reward(prompt, generated_response)
```

**ä¼˜åŠ¿**:
- èŠ‚çœRMè®­ç»ƒæ—¶é—´
- æ›´çµæ´»(å¯åŠ¨æ€è°ƒæ•´è¯„åˆ†æ ‡å‡†)

**æŒ‘æˆ˜**:
- LLMæ¨ç†å¼€é”€å¤§
- åˆ†æ•°æ ¡å‡†å›°éš¾

### 7.2 GRPO - ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–

#### 7.2.1 æ ¸å¿ƒåˆ›æ–°

**PPOçš„é—®é¢˜**:
1. éœ€è¦Value Network(é¢å¤–æ˜¾å­˜)
2. ä¼˜åŠ¿ä¼°è®¡ä¾èµ–GAE(å¤æ‚)
3. è®­ç»ƒä¸ç¨³å®š

**GRPOçš„è§£å†³æ–¹æ¡ˆ**:

**æ ¸å¿ƒæ€æƒ³**: ç”¨**ç»„å†…å½’ä¸€åŒ–**æ›¿ä»£Value Network!

å¯¹åŒä¸€é—®é¢˜é‡‡æ · $G$ ä¸ªå“åº”,è®¡ç®—ç»„å†…ä¼˜åŠ¿:
$$
\hat{A}_i = \frac{r_i - \mu_{\text{group}}}{\sigma_{\text{group}}}
$$

å…¶ä¸­:
$$
\mu_{\text{group}} = \frac{1}{G}\sum_{j=1}^G r_j, \quad \sigma_{\text{group}} = \sqrt{\frac{1}{G}\sum_{j=1}^G (r_j - \mu)^2}
$$

**ç›´è§‰**: å¥½åæ˜¯ç›¸å¯¹çš„,åªéœ€çŸ¥é“è¯¥å“åº”åœ¨ç»„å†…æ’å!

#### 7.2.2 å®Œæ•´æ•°å­¦æ¨å¯¼

**é€tokenå¥–åŠ±**:
$$
r_t = r_\phi(q, a_{\leq t}) - \beta \log \frac{\pi_\theta(a_t | q, a_{<t})}{\pi_{\text{ref}}(a_t | q, a_{<t})}
$$

**åºåˆ—å¥–åŠ±**:
$$
R_i = \sum_{t=1}^{|a_i|} r_t
$$

**å½’ä¸€åŒ–ä¼˜åŠ¿**:
$$
\hat{A}_i = \frac{R_i - \mu}{\sigma + \epsilon}
$$

**GRPOç›®æ ‡å‡½æ•°**:
$$
J_{\text{GRPO}}(\theta) = \frac{1}{G} \sum_{i=1}^G \frac{1}{|a_i|} \sum_{t=1}^{|a_i|} \min\left(
\frac{\pi_\theta(a_{i,t}|s, a_{i,<t})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s, a_{i,<t})} \hat{A}_i,\text{clip}\left(\frac{\pi_\theta}{\pi_{\theta_{\text{old}}}}, 1-\epsilon, 1+\epsilon\right) \hat{A}_i
\right)
$$

#### 7.2.3 ä¸PPOå¯¹æ¯”

| ç»´åº¦ | PPO | GRPO |
|------|-----|------|
| **Value Network** | éœ€è¦ | ä¸éœ€è¦ |
| **ä¼˜åŠ¿ä¼°è®¡** | GAE(å¤æ‚) | ç»„å†…å½’ä¸€åŒ–(ç®€å•) |
| **æ˜¾å­˜å ç”¨** | 2xæ¨¡å‹ | 1xæ¨¡å‹ |
| **è®­ç»ƒç¨³å®šæ€§** | ä¸­ç­‰ | é«˜ |
| **å®ç°å¤æ‚åº¦** | é«˜ | ä½ |

**æ˜¾å­˜èŠ‚çœè®¡ç®—**:

PPO:
$$
\text{Memory} = \text{Policy} + \text{Value} + \text{RM} + \text{Ref} = 4M
$$

GRPO:
$$
\text{Memory} = \text{Policy} + \text{RM} + \text{Ref} = 3M
$$

èŠ‚çœ25%!

#### 7.2.4 å®éªŒç»“æœ

DeepSeek-R1ä½¿ç”¨GRPOè®­ç»ƒ,æ€§èƒ½è¶…è¶ŠGPT-4!

**å…³é”®é…ç½®**:
- ç»„å¤§å° $G = 16$
- Clip $\epsilon = 0.2$
- KLç³»æ•° $\beta = 0.04$

### 7.3 å…¶ä»–æ–°æ–¹æ³•

#### 7.3.1 RRHF - Rank Responses to align Human Feedback

**æ ¸å¿ƒæ€æƒ³**: ç›´æ¥ç”¨æ’åºæŸå¤±,è·³è¿‡RM!

**æŸå¤±å‡½æ•°**:
$$
\mathcal{L}_{\text{RRHF}} = -\log \frac{\exp(\log p(y_i | x) / \alpha)}{\sum_{j=1}^K \exp(\log p(y_j | x) / \alpha)}
$$

å…¶ä¸­ $y_1 \succ y_2 \succ \cdots \succ y_K$ ä¸ºæ’åºã€‚

**ä¼˜åŠ¿**:
- æ— éœ€RMå’ŒRL
- ç«¯åˆ°ç«¯è®­ç»ƒ
- ç¨³å®šæ€§é«˜

#### 7.3.2 Self-Rewarding Language Models

**æ ¸å¿ƒæ€æƒ³**: æ¨¡å‹è‡ªå·±ç”Ÿæˆå¥–åŠ±!

**è¿­ä»£æµç¨‹**:
```
ç¬¬1è½®: SFTè®­ç»ƒåŸºç¡€æ¨¡å‹
ç¬¬2è½®: æ¨¡å‹ç”Ÿæˆå“åº” + è‡ªæˆ‘è¯„åˆ† â†’ æ„å»ºåå¥½æ•°æ® â†’ DPOè®­ç»ƒ
ç¬¬3è½®: æ›´å¼ºæ¨¡å‹é‡å¤ç¬¬2è½®
...
```

**æ•°å­¦å½¢å¼**:

æ¨¡å‹åŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡:
1. **ç”Ÿæˆèƒ½åŠ›**:
$$
\max_\theta \mathbb{E}_{x,y} [\log p_\theta(y | x)]
$$

2. **è¯„ä¼°èƒ½åŠ›**:
$$
\max_\theta \mathbb{E}_{x,y_1,y_2} [\log p_\theta(\text{prefer}(y_1, y_2) | x, y_1, y_2)]
$$

**è‡ªæˆ‘æ”¹è¿›æœºåˆ¶**: ç¬¬ $t+1$ è½®çš„å¥–åŠ±å‡½æ•°æ¯”ç¬¬ $t$ è½®æ›´å‡†ç¡®!

#### 7.3.3 Trust Region Alignment (TR-DPO)

**åŠ¨æœº**: DPOçš„KLæƒ©ç½šæ˜¯éšå¼çš„,éš¾ä»¥æ§åˆ¶ã€‚

**æ”¹è¿›**: æ˜¾å¼ä¿¡èµ–åŸŸçº¦æŸ

**ä¼˜åŒ–é—®é¢˜**:
$$
\max_\theta \mathbb{E} \left[\log \sigma(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)})\right]
$$
$$
\text{s.t.} \quad D_{KL}(\pi_\theta \| \pi_{\text{ref}}) \leq \delta
$$

**æ‹‰æ ¼æœ—æ—¥å½¢å¼**:
$$
\mathcal{L}_{\text{TR-DPO}} = \mathcal{L}_{\text{DPO}} + \lambda \max(0, D_{KL} - \delta)
$$

**ä¼˜åŠ¿**:
- ç²¾ç¡®æ§åˆ¶æ¨¡å‹åç§»
- é¿å…æ¨¡å¼å´©æºƒ
- æ›´å¥½çš„æ³›åŒ–

#### 7.3.4 SimPO - Simple Preference Optimization

**æ ¸å¿ƒåˆ›æ–°**: ç”¨**é•¿åº¦å½’ä¸€åŒ–**è§£å†³DPOçš„é•¿åº¦åå¥½é—®é¢˜!

**DPOçš„é—®é¢˜**:

æ¨¡å‹å€¾å‘ç”Ÿæˆæ›´é•¿çš„å“åº”æ¥è·å¾—æ›´é«˜æ¦‚ç‡:
$$
p_\theta(y|x) = \prod_{t=1}^{|y|} p_\theta(y_t | x, y_{<t})
$$

é•¿åº¦è¶Šé•¿, $p_\theta$ è¶Šå° â†’ å¥–åŠ±åå‘çŸ­å“åº”!

**SimPOè§£å†³æ–¹æ¡ˆ**:

å¼•å…¥**å¹³å‡å¯¹æ•°æ¦‚ç‡**:
$$
r_\theta(x, y) = \frac{1}{|y|} \sum_{t=1}^{|y|} \log p_\theta(y_t | x, y_{<t}) - \gamma
$$

å…¶ä¸­ $\gamma$ æ˜¯ç›®æ ‡å¥–åŠ±è¾¹ç•Œ(target reward margin)ã€‚

**SimPOæŸå¤±å‡½æ•°**:
$$
\mathcal{L}_{\text{SimPO}}(\theta) = -\mathbb{E}_{(x,y_w,y_l)} \left[\log \sigma\left(\frac{\beta}{|y_w|}\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \frac{\beta}{|y_l|}\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]
$$

**å…³é”®ç‰¹æ€§**:
1. **æ— éœ€å‚è€ƒæ¨¡å‹**:  $\gamma$ æ›¿ä»£äº†å‚è€ƒç­–ç•¥çš„ä½œç”¨
2. **é•¿åº¦å…¬å¹³**: å½’ä¸€åŒ–æ¶ˆé™¤é•¿åº¦åå¥½
3. **æ›´ç®€å•**: æ¯”DPOå°‘ä¸€ä¸ªè¶…å‚æ•°

**å®éªŒç»“æœ**:

åœ¨AlpacaEval 2.0ä¸Š:
- DPO: 12.5% win rate
- SimPO: **18.7% win rate** (æå‡50%!)

**å‚è€ƒ**: [SimPO: Simple Preference Optimization (arXiv 2024)](https://arxiv.org/abs/2405.14734)

#### 7.3.5 ORPO - Odds Ratio Preference Optimization

**æ ¸å¿ƒåˆ›æ–°**: **å•é˜¶æ®µå¯¹é½** - åŒæ—¶è®­ç»ƒSFTå’Œåå¥½å­¦ä¹ !

**ä¼ ç»Ÿæµç¨‹çš„é—®é¢˜**:
```
Stage 1: SFTè®­ç»ƒ â†’ åŸºç¡€æ¨¡å‹
Stage 2: DPOå¯¹é½ â†’ å¯¹é½æ¨¡å‹
```

åˆ†ä¸¤é˜¶æ®µ = è®­ç»ƒæ—¶é—´ç¿»å€!

**ORPOåˆ›æ–°**:

ç»“åˆç›‘ç£å­¦ä¹ å’Œåå¥½å­¦ä¹ äºä¸€ä¸ªæŸå¤±å‡½æ•°:
$$
\mathcal{L}_{\text{ORPO}} = \mathcal{L}_{\text{SFT}} + \lambda \cdot \mathcal{L}_{\text{OR}}
$$

**SFTéƒ¨åˆ†**:
$$
\mathcal{L}_{\text{SFT}} = -\mathbb{E}_{(x,y_w)} [\log p_\theta(y_w | x)]
$$

**Odds Ratio (OR) éƒ¨åˆ†**:
$$
\mathcal{L}_{\text{OR}} = -\mathbb{E}_{(x,y_w,y_l)} \left[\log \sigma\left(\log \frac{odds_\theta(y_w|x)}{odds_\theta(y_l|x)}\right)\right]
$$

å…¶ä¸­:
$$
odds_\theta(y|x) = \frac{p_\theta(y|x)}{1 - p_\theta(y|x)}
$$

**ç›´è§‰ç†è§£**:

ORæ¯”æ¦‚ç‡æ¯”æ›´æ•æ„Ÿ - å½“ $p$ æ¥è¿‘1æ—¶:
$$
\frac{p_w}{p_l} \approx 1, \quad \text{ä½†} \quad \frac{odds_w}{odds_l} \gg 1
$$

**ä¼˜åŠ¿**:
- **è®­ç»ƒæ•ˆç‡**: ä¸€æ¬¡è®­ç»ƒå®Œæˆå¯¹é½
- **æ˜¾å­˜èŠ‚çœ**: æ— éœ€åŠ è½½å‚è€ƒæ¨¡å‹
- **æ€§èƒ½æå‡**: åœ¨Mistral-7Bä¸Šè¶…è¶ŠDPO 2-3ä¸ªç‚¹

**å‚è€ƒ**: [ORPO: Monolithic Preference Optimization (arXiv 2024)](https://arxiv.org/abs/2403.07691)

#### 7.3.6 GSPO - Group-wise Self-Play Optimization

**æ ¸å¿ƒåˆ›æ–°**: **è‡ªæˆ‘åšå¼ˆ** + **ç¾¤ç»„å¯¹æ¯”**!

**åŠ¨æœº**: ä¸ºä»€ä¹ˆéœ€è¦äººå·¥æ ‡æ³¨åå¥½æ•°æ®? èƒ½å¦è®©æ¨¡å‹è‡ªå·±å¯¹å¼ˆ?

**GSPOæµç¨‹**:

1. **è‡ªæˆ‘é‡‡æ ·**: å¯¹åŒä¸€æç¤ºé‡‡æ · $K$ ä¸ªå“åº”
$$
\{y_1, y_2, \ldots, y_K\} \sim \pi_\theta(\cdot | x)
$$

2. **è‡ªæˆ‘è¯„åˆ†**: ç”¨æ¨¡å‹è‡ªå·±è¯„ä¼°æ¯ä¸ªå“åº”
$$
s_i = \text{Evaluator}(x, y_i)
$$

3. **ç¾¤ç»„æ’åº**: æ„å»ºåå¥½å¯¹
$$
\text{if } s_i > s_j: (x, y_i, y_j) \to \mathcal{D}_{\text{pref}}
$$

4. **DPOè®­ç»ƒ**: ç”¨æ„å»ºçš„åå¥½æ•°æ®è®­ç»ƒ
$$
\mathcal{L}_{\text{GSPO}} = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}_{\text{pref}}} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)\pi_{\text{ref}}(y_l|x)}{\pi_\theta(y_l|x)\pi_{\text{ref}}(y_w|x)}\right)\right]
$$

5. **è¿­ä»£**: æ›´æ–°æ¨¡å‹, é‡å¤1-4

**ä¸Self-Rewardingçš„åŒºåˆ«**:

| ç»´åº¦ | Self-Rewarding | GSPO |
|------|---------------|------|
| **é‡‡æ ·ç­–ç•¥** | å•ä¸ªå“åº” | ç¾¤ç»„é‡‡æ · $(K=4\text{-}16)$ |
| **åå¥½æ„å»º** | LLMç”Ÿæˆè¯„åˆ† | ç¾¤ç»„å†…æ’åº |
| **è®­ç»ƒç¨³å®šæ€§** | ä¸­ç­‰ | æ›´é«˜(ç¾¤ç»„å½’ä¸€åŒ–) |

**å…³é”®æŠ€å·§**:

**æ¸©åº¦é‡‡æ ·**: å¢åŠ å¤šæ ·æ€§
$$
y_k \sim \pi_\theta(\cdot | x, T=1.2)
$$

**ç¾¤ç»„å½’ä¸€åŒ–**: é¿å…è¯„åˆ†é£˜ç§»
$$
\hat{s}_i = \frac{s_i - \mu_{\text{group}}}{\sigma_{\text{group}}}
$$

**å®éªŒç»“æœ** (åŸºäºDeepSeekæŠ¥å‘Š):
- åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Š, GSPOè¿­ä»£3è½®åæ€§èƒ½æå‡15%
- æ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨!

**å‚è€ƒ**: [DeepSeek-R1 Technical Report (2025)](https://github.com/deepseek-ai/DeepSeek-R1)

### 7.4 å¯¹é½æ–¹æ³•å¯¹æ¯”æ€»ç»“

| æ–¹æ³• | å¤æ‚åº¦ | æ•°æ®éœ€æ±‚ | ç¨³å®šæ€§ | æ€§èƒ½ | æˆæœ¬ | ç‰¹ç‚¹ |
|------|--------|---------|--------|------|------|------|
| **RLHF** | é«˜ | æˆå¯¹åå¥½ | ä½ | æœ€é«˜ | å¾ˆé«˜ | é‡‘æ ‡å‡†,4ä¸ªæ¨¡å‹ |
| **RLAIF** | é«˜ | æˆå¯¹åå¥½(AIç”Ÿæˆ) | ä½ | é«˜ | ä¸­ | AIæ›¿ä»£äººå·¥æ ‡æ³¨ |
| **DPO** | ä¸­ | æˆå¯¹åå¥½ | é«˜ | é«˜ | ä½ | ä¸»æµ,ç¨³å®š |
| **SimPO** | ä¸­ | æˆå¯¹åå¥½ | é«˜ | é«˜+ | ä½ | DPO+é•¿åº¦å½’ä¸€åŒ– |
| **ORPO** | ä½ | æˆå¯¹åå¥½ | é«˜ | é«˜ | å¾ˆä½ | å•é˜¶æ®µå¯¹é½ |
| **GRPO** | ä¸­ | ä»…éœ€å¥–åŠ±æ¨¡å‹ | é«˜ | é«˜ | ä¸­ | æ— Value Network |
| **GSPO** | ä¸­ | æ— éœ€æ ‡æ³¨(è‡ªæˆ‘åšå¼ˆ) | é«˜ | é«˜ | ä¸­ | è‡ªæˆ‘æ”¹è¿› |
| **KTO** | ä½ | äºŒå…ƒåé¦ˆ | é«˜ | ä¸­é«˜ | å¾ˆä½ | æ— éœ€æˆå¯¹æ•°æ® |
| **RRHF** | ä½ | æ’åºæ•°æ® | å¾ˆé«˜ | ä¸­ | å¾ˆä½ | ç«¯åˆ°ç«¯æ’åº |

**é€‰æ‹©å»ºè®®** (2025æœ€æ–°):

```
ğŸ† è¿½æ±‚æè‡´æ€§èƒ½: RLHF (ä½†æˆæœ¬é«˜)
ğŸš€ ä¸»æµç¨³å®šé€‰æ‹©: DPO / SimPO (SimPOæ›´ä¼˜)
âš¡ å¿«é€Ÿè®­ç»ƒ: ORPO (å•é˜¶æ®µ,èŠ‚çœ50%æ—¶é—´)
ğŸ’° æˆæœ¬æ•æ„Ÿ: KTO / RRHF (æ•°æ®éœ€æ±‚å°‘)
ğŸ¤– æ— äººå·¥æ ‡æ³¨: RLAIF / GSPO (è‡ªæˆ‘åšå¼ˆ)
ğŸ”§ æ˜¾å­˜å—é™: GRPO (èŠ‚çœ25%æ˜¾å­˜)
ğŸ“Š æ•°æ®æœ‰é™: KTO (äºŒå…ƒåé¦ˆ) / RRHF (æ’åº)
```

**2025è¶‹åŠ¿**:
1. **SimPO** æ­£åœ¨å–ä»£DPOæˆä¸ºæ–°ä¸»æµ
2. **ORPO** åœ¨å¿«é€ŸåŸå‹å¼€å‘ä¸­æµè¡Œ
3. **GSPO** ç”¨äºè‡ªæˆ‘æ”¹è¿›(DeepSeek-R1åŒæ¬¾)

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å¯¹é½æ˜¯å¿…é¡»çš„**: é¢„è®­ç»ƒæ¨¡å‹éœ€è¦å¯¹é½åˆ°äººç±»ä»·å€¼è§‚
2. **RLHFæ˜¯é‡‘æ ‡å‡†**: ä½†å¤æ‚åº¦é«˜,éœ€è¦4ä¸ªæ¨¡å‹
3. **DPOæ˜¯å®ç”¨é€‰æ‹©**: ç®€å•ã€ç¨³å®šã€é«˜æ•ˆ,å·²æˆä¸»æµ
4. **GRPOé™ä½é—¨æ§›**: æ— éœ€Value Network,æ˜¾å­˜èŠ‚çœ25%
5. **RLAIFè§£å†³æ ‡æ³¨**: AIåé¦ˆæ›¿ä»£äººå·¥,æ€§èƒ½ç›¸å½“
6. **é•¿æ–‡æœ¬æ˜¯è¶‹åŠ¿**: RoPE + Flash Attentionæˆä¸ºæ ‡é…
7. **æŒç»­å­¦ä¹ æ˜¯æŒ‘æˆ˜**: LoRAæä¾›ä¼˜é›…è§£å†³æ–¹æ¡ˆ

### å¯¹é½æ–¹æ³•å…¨æ™¯å›¾

```
ä¼ ç»Ÿæ–¹æ³•:
  RLHF (2022) â†’ é‡‘æ ‡å‡†,å¤æ‚åº¦é«˜
  â”œâ”€ SFT â†’ RM â†’ PPO
  â””â”€ éœ€è¦: äººå·¥æ ‡æ³¨ + 4ä¸ªæ¨¡å‹

ç®€åŒ–æ–¹æ³•:
  DPO (2023) â†’ ä¸»æµ,ç¨³å®šé«˜æ•ˆ
  â”œâ”€ ç›´æ¥ä¼˜åŒ–åå¥½
  â””â”€ éœ€è¦: æˆå¯¹æ•°æ® + 2ä¸ªæ¨¡å‹
  
  GRPO (2024) â†’ æœ€æ–°,æ˜¾å­˜å‹å¥½
  â”œâ”€ ç»„å†…å½’ä¸€åŒ–ä¼˜åŠ¿
  â””â”€ éœ€è¦: RM + 3ä¸ªæ¨¡å‹

è‡ªåŠ¨åŒ–æ–¹æ³•:
  RLAIF (2024) â†’ AIåé¦ˆ
  â”œâ”€ æ— éœ€äººå·¥æ ‡æ³¨
  â””â”€ æ€§èƒ½æ¥è¿‘RLHF
  
  Self-Rewarding (2024) â†’ è‡ªæˆ‘æ”¹è¿›
  â””â”€ æ¨¡å‹è‡ªå·±ç”Ÿæˆå¥–åŠ±

å…¶ä»–æ–¹æ³•:
  KTO â†’ äºŒå…ƒåé¦ˆ(é™ä½æ•°æ®æˆæœ¬)
  RRHF â†’ æ’åºæŸå¤±(ç«¯åˆ°ç«¯)
  TR-DPO â†’ ä¿¡èµ–åŸŸçº¦æŸ(æ›´ç¨³å®š)
```

### å®è·µå»ºè®®

**å¯¹é½æ–¹æ³•é€‰æ‹©**:
```
èµ„æºå……è¶³ + éœ€è¦ç²¾ç»†æ§åˆ¶: RLHF
å¿«é€Ÿè¿­ä»£ + èµ„æºå—é™: DPO
æ˜¾å­˜å—é™: GRPO
æ— äººç±»æ ‡æ³¨: RLAIF
æ•°æ®æœ‰é™(ä»…äºŒå…ƒåé¦ˆ): KTO
è¿½æ±‚ç¨³å®šæ€§: RRHF/TR-DPO
```

**é•¿æ–‡æœ¬å¤„ç†**:
```
ä½ç½®ç¼–ç : RoPE (ä¸»æµ) æˆ– ALiBi (ç®€å•)
æ³¨æ„åŠ›ä¼˜åŒ–: Flash Attention (å¿…å¤‡)
è¶…é•¿æ‰©å±•: ä½ç½®æ’å€¼ + ç¨€ç–æ³¨æ„åŠ›
```

### ç†è®ºæ·±åº¦æ€»ç»“

æœ¬ç¯‡æ¶µç›–çš„å®Œæ•´æ•°å­¦æ¨å¯¼:
- **RLHF**: Bradley-Terryæ¨¡å‹ + PPOå®Œæ•´æ¨å¯¼
- **DPO**: ä»RLåˆ°ç›‘ç£å­¦ä¹ çš„ç†è®ºè½¬åŒ–
- **GRPO**: ç»„å†…å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡
- **RoPE**: æ—‹è½¬ä½ç½®ç¼–ç çš„ç›¸å¯¹æ€§è¯æ˜
- **Flash Attention**: IOå¤æ‚åº¦åˆ†æ + åœ¨çº¿Softmax

### æœªæ¥è¶‹åŠ¿

1. **å¯¹é½è‡ªåŠ¨åŒ–**: ä»äººå·¥æ ‡æ³¨ â†’ AIåé¦ˆ â†’ è‡ªæˆ‘æ”¹è¿›
2. **ä¸Šä¸‹æ–‡æ— é™**: ä»128K â†’ 1M tokens â†’ æ— é™ä¸Šä¸‹æ–‡
3. **å¤šç›®æ ‡å¯¹é½**: å¹³è¡¡æœ‰å¸®åŠ©æ€§/æ— å®³æ€§/è¯šå®æ€§
4. **ä¸ªæ€§åŒ–å¯¹é½**: ä¸åŒç”¨æˆ·ä¸åŒåå¥½
5. **åœ¨çº¿å­¦ä¹ **: æŒç»­ä»ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ 

### æœ€æ–°è¿›å±•(2024-2025)

- **DeepSeek-R1**: GRPOè®­ç»ƒ,æ€§èƒ½è¶…GPT-4
- **Gemini 2.0**: åŸç”Ÿå¤šæ¨¡æ€å¯¹é½
- **Claude 3.5**: Constitutional AI + 200Kä¸Šä¸‹æ–‡
- **Llama 3**: GQA + RoPEæ‰©å±•åˆ°128K

---

**æ¨èé˜…è¯»**:
- [InstructGPTè®ºæ–‡](https://arxiv.org/abs/2203.02155) - RLHFèŒƒå¼
- [DPOè®ºæ–‡](https://arxiv.org/abs/2305.18290) - ç›´æ¥åå¥½ä¼˜åŒ–
- [RLAIFè®ºæ–‡](https://arxiv.org/abs/2309.00267) - AIåé¦ˆ
- [GRPOè®ºæ–‡](https://arxiv.org/abs/2402.03300) - ç»„ç›¸å¯¹ä¼˜åŒ–
- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Anthropicå¯¹é½
- [RoPEè®ºæ–‡](https://arxiv.org/abs/2104.09864) - æ—‹è½¬ä½ç½®ç¼–ç 
- [Flash Attention](https://arxiv.org/abs/2205.14135) - IOä¼˜åŒ–æ³¨æ„åŠ›
