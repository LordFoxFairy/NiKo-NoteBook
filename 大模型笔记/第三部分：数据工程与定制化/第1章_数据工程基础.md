# ç¬¬1ç« ï¼šæ•°æ®ç‚¼é‡‘æœ¯ - ä»åƒåœ¾åˆ°é»„é‡‘çš„æ•°æ®å·¥ç¨‹ (Data Alchemy for Fine-tuning)

> **"Garbage In, Garbage Out (GIGO)" - è¿™æ˜¯æ•°æ®ç§‘å­¦çš„é“å¾‹**
> 
> "Data is the new oil, but if you don't refine it, you're just burning crude." - Andrew Ng
>
> æ¬¢è¿æ¥åˆ°æ•°æ®ç‚¼é‡‘æœ¯çš„ä¸–ç•Œ!æœ¬ç« å°†å¸¦ä½ ä» **Petabytes çš„åŸå§‹çŸ¿çŸ³** ä¸­æç‚¼å‡º **Kilobytes çš„ç²¾åé»„é‡‘**ã€‚åœ¨å¾®è°ƒé˜¶æ®µ,æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ - ç²¾å¿ƒæçº¯çš„ 10K é«˜è´¨é‡æ•°æ®é›†,å¾€å¾€æ¯” 100K æœªç»å¤„ç†çš„"åƒåœ¾"æ›´æœ‰æ•ˆ(å¦‚ Alpacaã€Phi-3 çš„æˆåŠŸ)ã€‚
>
> æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æˆä¸ºä¸€ååˆæ ¼çš„"æ•°æ®ç‚¼é‡‘æœ¯å¸ˆ",æŒæ¡ **è¿‡æ»¤ã€è’¸é¦ã€æçº¯** çš„æ ¸å¿ƒæŠ€æœ¯,æ„å»ºå±äºä½ è‡ªå·±çš„é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ã€‚

## æ•°æ®ç‚¼é‡‘æœ¯ Pipeline å…¨æ™¯å›¾

è®©æˆ‘ä»¬å…ˆçœ‹çœ‹ä»åŸå§‹æ•°æ®åˆ°ç²¾ç‚¼æ•°æ®é›†çš„å®Œæ•´æ—…ç¨‹:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA ALCHEMY PIPELINE                                 â”‚
â”‚                   (æ•°æ®ç‚¼é‡‘æœ¯æµæ°´çº¿)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Petabytes                                              Kilobytes
   (åŸå§‹çŸ¿çŸ³)                                              (ç²¾ç‚¼é»„é‡‘)
       â”‚                                                       â”‚
       â”œâ”€â”€> [1. ç²—ç­›] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Gigabytes            â”‚
       â”‚    Â· å»é™¤æ˜æ˜¾åƒåœ¾                                    â”‚
       â”‚    Â· åŸºç¡€æ ¼å¼åŒ–                                      â”‚
       â”‚                                                       â”‚
       â”œâ”€â”€> [2. è´¨é‡è¿‡æ»¤] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Megabytes            â”‚
       â”‚    Â· é•¿åº¦/å®Œæ•´æ€§æ£€æŸ¥                                 â”‚
       â”‚    Â· æ¯’æ€§æ£€æµ‹                                        â”‚
       â”‚    Â· PII è„±æ•                                        â”‚
       â”‚                                                       â”‚
       â”œâ”€â”€> [3. å»é‡æçº¯] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Hundreds of KB       â”‚
       â”‚    Â· MinHash å»é‡                                    â”‚
       â”‚    Â· è¿‘ä¼¼é‡å¤æ£€æµ‹                                    â”‚
       â”‚    âš¡ FLOPs èŠ‚çœ: å»é‡åè®­ç»ƒæˆæœ¬ â†“ 3-5x!            â”‚
       â”‚                                                       â”‚
       â”œâ”€â”€> [4. è’¸é¦å‡å] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Tens of KB           â”‚
       â”‚    Â· Self-Instruct (çŸ¥è¯†è’¸é¦)                       â”‚
       â”‚    Â· Evol-Instruct (å¤æ‚åº¦æå‡)                     â”‚
       â”‚    Â· GPT-4 â†’ å°æ¨¡å‹çš„èƒ½åŠ›è¿ç§»                       â”‚
       â”‚                                                       â”‚
       â””â”€â”€> [5. æœ€ç»ˆæçº¯] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> âœ¨ Pure Gold âœ¨      â”‚
            Â· äººå·¥æŠ½æ£€                                         â”‚
            Â· A/B æµ‹è¯•éªŒè¯                                    â”‚
            Â· æ•°æ®åˆ†å¸ƒå¹³è¡¡                                    â”‚

è¾“å‡º: 10K-50K æ¡é«˜çº¯åº¦æ•°æ® â†’ è¶³ä»¥è®­ç»ƒä¸€ä¸ªå¼ºå¤§çš„ä¸“å±æ¨¡å‹!
```

**æ ¸å¿ƒé€»è¾‘**:æ¯ä¸€ä¸ªé˜¶æ®µéƒ½åœ¨å‰”é™¤"æ‚è´¨",æå‡"çº¯åº¦":
- **ç²—ç­›**: å»é™¤ä¸å¯ç”¨æ•°æ® (æ ¼å¼é”™è¯¯ã€ä¹±ç )
- **è´¨é‡è¿‡æ»¤**: å»é™¤ä½è´¨é‡æ•°æ® (å¤ªçŸ­ã€æœ‰æ¯’ã€ä¸å®Œæ•´)
- **å»é‡**: å»é™¤å†—ä½™æ•°æ® (å®Œå…¨é‡å¤ã€é«˜åº¦ç›¸ä¼¼)
- **è’¸é¦**: ä»å¤§æ¨¡å‹æå–çŸ¥è¯† (GPT-4 çš„æ™ºæ…§ â†’ ä½ çš„æ¨¡å‹)
- **æçº¯**: æœ€ç»ˆè´¨æ£€ (äººå·¥å®¡æ ¸ + æ•°æ®å¹³è¡¡)

---

## æœ¬ç« å‰ç½®è¯´æ˜ï¼šå¾®è°ƒæ•°æ® vs é¢„è®­ç»ƒæ•°æ®

æœ¬ç« èšç„¦**å¾®è°ƒé˜¶æ®µï¼ˆSFTï¼‰**çš„æ•°æ®å·¥ç¨‹ï¼Œå®ƒä¸**é¢„è®­ç»ƒé˜¶æ®µï¼ˆPretrainingï¼‰**æœ‰æœ¬è´¨åŒºåˆ«ã€‚å¦‚æœä¸æ¸…æ¥šä¸¤è€…çš„å®šä½ï¼Œå¾ˆå®¹æ˜“æ··æ·†å¤„ç†æ–¹æ³•ã€‚

| ç»´åº¦ | é¢„è®­ç»ƒæ•°æ®ï¼ˆè¯¦è§ Part 2 ç¬¬3ç« ï¼‰ | å¾®è°ƒæ•°æ®ï¼ˆæœ¬ç« é‡ç‚¹ï¼‰ |
| :--- | :--- | :--- |
| **æ•°æ®é‡çº§** | **PBçº§** (Trillions of tokens) | **MBçº§** (10K - 100K samples) |
| **æ•°æ®æ¥æº** | ç½‘é¡µçˆ¬å– (CommonCrawl)ã€ä¹¦ç±ã€ä»£ç  | **åˆæˆæ•°æ® (Self-Instruct)**ã€äººå·¥æ ‡æ³¨ |
| **æ ¸å¿ƒç›®æ ‡** | æ³¨å…¥**ä¸–ç•ŒçŸ¥è¯†**å’Œè¯­è¨€èƒ½åŠ› | æ³¨å…¥**æŒ‡ä»¤éµå¾ªèƒ½åŠ›**å’Œç‰¹å®šä»»åŠ¡æŠ€èƒ½ |
| **æ¸…æ´—é‡ç‚¹** | å¤§è§„æ¨¡å»é‡ã€è¿‡æ»¤åƒåœ¾å¹¿å‘Š | **æé«˜ç²¾åº¦çš„å»å™ª**ã€é£æ ¼ç»Ÿä¸€ã€é€»è¾‘æ ¡éªŒ |
| **å®¹é”™ç‡** | å¯å®¹å¿ 5-10% çš„å™ªå£° | **é›¶å®¹å¿**ï¼Œä¸€æ¡é”™è¯¯æ•°æ®å¯èƒ½æ¯æ‰å¾®è°ƒæ•ˆæœ |

> **å…³é”®æé†’**ï¼šå¦‚æœæ‚¨ä¸»è¦å…³å¿ƒå¦‚ä½•æ¸…æ´—æµ·é‡çš„é¢„è®­ç»ƒè¯­æ–™ï¼ˆå¦‚è®­ç»ƒä¸€ä¸ªåŸºåº§æ¨¡å‹ï¼‰ï¼Œè¯·ç§»æ­¥ [ç¬¬äºŒéƒ¨åˆ†ç¬¬3ç« ï¼šé¢„è®­ç»ƒçš„å¥¥ç§˜]ã€‚æœ¬ç« çš„æ–¹æ³•ä¸“é—¨ä¸º **æ„å»ºé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†** è®¾è®¡ã€‚

---

## ç›®å½•
- [ä¸€ã€Data-Centric AIï¼šæ•°æ®ä¸ºç‹çš„æ—¶ä»£](#ä¸€data-centric-aiæ•°æ®ä¸ºç‹çš„æ—¶ä»£)
  - [1. Model-Centric vs Data-Centric](#1-model-centric-vs-data-centric)
  - [2. å¾®è°ƒæ•°æ®çš„ä¸‰å¤§è¦ç´ ](#2-å¾®è°ƒæ•°æ®çš„ä¸‰å¤§è¦ç´ )
  - [3. æ•°æ®è´¨é‡çš„é»„é‡‘å®šå¾‹](#3-æ•°æ®è´¨é‡çš„é»„é‡‘å®šå¾‹)
- [äºŒã€SFT æ•°æ®æ ¼å¼è¯¦è§£](#äºŒsft-æ•°æ®æ ¼å¼è¯¦è§£)
  - [1. Alpaca æ ¼å¼ï¼šæŒ‡ä»¤å¼æ•°æ®](#1-alpaca-æ ¼å¼æŒ‡ä»¤å¼æ•°æ®)
  - [2. ShareGPT æ ¼å¼ï¼šå¤šè½®å¯¹è¯æ•°æ®](#2-sharegpt-æ ¼å¼å¤šè½®å¯¹è¯æ•°æ®)
  - [3. æ ¼å¼è½¬æ¢å®æˆ˜](#3-æ ¼å¼è½¬æ¢å®æˆ˜)
- [ä¸‰ã€Self-Instructï¼šç”¨ GPT-4 ç”Ÿæˆå¾®è°ƒæ•°æ®](#ä¸‰self-instructç”¨-gpt-4-ç”Ÿæˆå¾®è°ƒæ•°æ®)
  - [1. Self-Instruct åŸç†](#1-self-instruct-åŸç†)
  - [2. ä»£ç å®æˆ˜ï¼šç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤æ•°æ®](#2-ä»£ç å®æˆ˜ç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤æ•°æ®)
  - [3. Evol-Instructï¼šè®©æŒ‡ä»¤è¿›åŒ–](#3-evol-instructè®©æŒ‡ä»¤è¿›åŒ–)
  - [4. ä»£ç å®æˆ˜ï¼šæŒ‡ä»¤å¤æ‚åº¦æå‡](#4-ä»£ç å®æˆ˜æŒ‡ä»¤å¤æ‚åº¦æå‡)
- [å››ã€æ•°æ®æ¸…æ´—ä¸è´¨é‡è¿‡æ»¤](#å››æ•°æ®æ¸…æ´—ä¸è´¨é‡è¿‡æ»¤)
  - [1. åŸºäºè§„åˆ™çš„è´¨é‡è¿‡æ»¤](#1-åŸºäºè§„åˆ™çš„è´¨é‡è¿‡æ»¤)
  - [2. æ¯’æ€§æ£€æµ‹](#2-æ¯’æ€§æ£€æµ‹)
  - [3. PII è¯†åˆ«ä¸è„±æ•](#3-pii-è¯†åˆ«ä¸è„±æ•)
  - [4. ä»£ç å®æˆ˜ï¼šå®Œæ•´çš„è´¨é‡è¿‡æ»¤å™¨](#4-ä»£ç å®æˆ˜å®Œæ•´çš„è´¨é‡è¿‡æ»¤å™¨)
  - [5. åˆæˆæ•°æ®å®æˆ˜ï¼šä½¿ç”¨ LLM ç”ŸæˆæŒ‡ä»¤](#5-åˆæˆæ•°æ®å®æˆ˜ä½¿ç”¨-llm-ç”ŸæˆæŒ‡ä»¤)
- [äº”ã€å°è§„æ¨¡æ•°æ®å»é‡](#äº”å°è§„æ¨¡æ•°æ®å»é‡)
  - [1. ä¸ºä»€ä¹ˆå¾®è°ƒæ•°æ®éœ€è¦å»é‡](#1-ä¸ºä»€ä¹ˆå¾®è°ƒæ•°æ®éœ€è¦å»é‡)
  - [2. MinHash å»é‡åŸç†](#2-minhash-å»é‡åŸç†)
  - [3. ä»£ç å®æˆ˜ï¼šMinHash ç®€å•å®ç°](#3-ä»£ç å®æˆ˜minhash-ç®€å•å®ç°)
- [å…­ã€æ•°æ®å¢å¼ºæŠ€æœ¯](#å…­æ•°æ®å¢å¼ºæŠ€æœ¯)
  - [1. å›è¯‘ï¼ˆBack-translationï¼‰](#1-å›è¯‘back-translation)
  - [2. åŒä¹‰è¯æ›¿æ¢](#2-åŒä¹‰è¯æ›¿æ¢)
  - [3. ä»£ç å®æˆ˜ï¼šæ•°æ®å¢å¼ºå·¥å…·](#3-ä»£ç å®æˆ˜æ•°æ®å¢å¼ºå·¥å…·)
- [ä¸ƒã€SFT æ•°æ®é›†æ„å»ºå®æˆ˜](#ä¸ƒsft-æ•°æ®é›†æ„å»ºå®æˆ˜)
  - [1. å®Œæ•´ Pipelineï¼šä»åŸå§‹æ–‡æœ¬åˆ° Alpaca æ ¼å¼](#1-å®Œæ•´-pipelineä»åŸå§‹æ–‡æœ¬åˆ°-alpaca-æ ¼å¼)
  - [2. ä»£ç å®æˆ˜ï¼šç«¯åˆ°ç«¯æ•°æ®æ„å»º](#2-ä»£ç å®æˆ˜ç«¯åˆ°ç«¯æ•°æ®æ„å»º)
- [å…«ã€æœ¬ç« å°ç»“](#å…«æœ¬ç« å°ç»“)
- [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)

---

## ä¸€ã€Data-Centric AIï¼šæ•°æ®ä¸ºç‹çš„æ—¶ä»£

### 1. Model-Centric vs Data-Centric

**ä¼ ç»Ÿ Model-Centric AI**ï¼ˆä»¥æ¨¡å‹ä¸ºä¸­å¿ƒï¼‰ï¼š
```
å›ºå®šæ•°æ®é›† â†’ ä¸æ–­æ”¹è¿›æ¨¡å‹æ¶æ„ â†’ è¿½æ±‚æ›´é«˜æ€§èƒ½
```

**Data-Centric AI**ï¼ˆä»¥æ•°æ®ä¸ºä¸­å¿ƒï¼‰ï¼š
```
å›ºå®šæ¨¡å‹æ¶æ„ â†’ ä¸æ–­æ”¹è¿›æ•°æ®è´¨é‡ â†’ è¿½æ±‚æ›´é«˜æ€§èƒ½
```

**Andrew Ng çš„å®éªŒ**ï¼šåœ¨åˆ¶é€ ä¸šç¼ºé™·æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®æ ‡æ³¨ï¼ˆè€Œéæ”¹è¿›æ¨¡å‹ï¼‰ï¼Œå‡†ç¡®ç‡ä» 76% æå‡åˆ° 93%ã€‚

**åœ¨ LLM å¾®è°ƒä¸­çš„ä½“ç°**ï¼š
- Phi-3-mini (3.8B)ï¼šä½¿ç”¨ 3.3T tokens çš„é«˜è´¨é‡"æ•™ç§‘ä¹¦å¼"æ•°æ®ï¼Œæ€§èƒ½è¶…è¶Š Llama-2-13B
- Alpaca (7B)ï¼šä»…ç”¨ 52K æ¡ GPT-3.5-turbo ç”Ÿæˆçš„æ•°æ®ï¼Œè¾¾åˆ°æ¥è¿‘ GPT-3.5 çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- WizardLMï¼šé€šè¿‡ Evol-Instruct æå‡æ•°æ®å¤æ‚åº¦ï¼Œ7B æ¨¡å‹è¶…è¶Š GPT-3.5 åœ¨éƒ¨åˆ†ä»»åŠ¡

### 2. å¾®è°ƒæ•°æ®çš„ä¸‰å¤§è¦ç´ 

**ç›´è§‰ç†è§£**ï¼šå¾®è°ƒæ•°æ®å°±åƒç»™æ¨¡å‹å®šåˆ¶çš„"æ•™ç§‘ä¹¦"ï¼Œéœ€è¦æ»¡è¶³ä¸‰ä¸ªæ ¸å¿ƒè¦ç´ ã€‚

$$
\text{Effective SFT Data} = f(\text{Quality}, \text{Diversity}, \text{Complexity})
$$

**1. è´¨é‡ï¼ˆQualityï¼‰**
- **å‡†ç¡®æ€§**ï¼šç­”æ¡ˆå¿…é¡»æ­£ç¡®ï¼Œé”™è¯¯æ•°æ®ä¼šå¯¼è‡´æ¨¡å‹"å­¦å"
- **æµç•…æ€§**ï¼šè¯­è¨€è‡ªç„¶ã€é€»è¾‘æ¸…æ™°
- **å®Œæ•´æ€§**ï¼šå›ç­”éœ€è¦å®Œæ•´è§£å†³é—®é¢˜

**2. å¤šæ ·æ€§ï¼ˆDiversityï¼‰**
- **ä»»åŠ¡å¤šæ ·æ€§**ï¼šè¦†ç›–é—®ç­”ã€æ‘˜è¦ã€æ¨ç†ã€ä»£ç ã€ç¿»è¯‘ç­‰ä¸åŒä»»åŠ¡
- **é¢†åŸŸå¤šæ ·æ€§**ï¼šç§‘æŠ€ã€æ–‡å­¦ã€å†å²ã€åŒ»ç–—ç­‰ä¸åŒé¢†åŸŸ
- **é£æ ¼å¤šæ ·æ€§**ï¼šæ­£å¼ã€éšæ„ã€æŠ€æœ¯ã€ç§‘æ™®ç­‰ä¸åŒé£æ ¼

**3. å¤æ‚åº¦ï¼ˆComplexityï¼‰**
- **ç®€å•ä»»åŠ¡**ï¼šå•æ­¥æ¨ç†ï¼Œç›´æ¥æ£€ç´¢çŸ¥è¯†
- **ä¸­ç­‰ä»»åŠ¡**ï¼šå¤šæ­¥æ¨ç†ï¼Œéœ€è¦ç»¼åˆä¿¡æ¯
- **å¤æ‚ä»»åŠ¡**ï¼šæ·±åº¦æ¨ç†ã€åˆ›é€ æ€§æ€è€ƒã€ä»£ç è°ƒè¯•

**æ•°æ®é…æ¯”å»ºè®®**ï¼š
```
ç®€å•ä»»åŠ¡ : ä¸­ç­‰ä»»åŠ¡ : å¤æ‚ä»»åŠ¡ = 3 : 5 : 2
```

### 3. æ•°æ®è´¨é‡çš„é»„é‡‘å®šå¾‹

**å®šå¾‹ 1ï¼šå°‘è€Œç²¾ > å¤šè€Œæ‚**
- 10K é«˜è´¨é‡æ•°æ® > 100K ä½è´¨é‡æ•°æ®
- Alpaca ç”¨ 52K æ•°æ®è¾¾åˆ° text-davinci-003 çš„ 90% èƒ½åŠ›

**å®šå¾‹ 2ï¼šå¤æ‚åº¦é˜¶æ¢¯**
- æ•°æ®åº”è¯¥åŒ…å«ä¸åŒéš¾åº¦å±‚çº§ï¼Œè®©æ¨¡å‹é€æ­¥æå‡èƒ½åŠ›
- WizardLM çš„ Evol-Instruct é€šè¿‡å¤šè½®è¿›åŒ–æå‡æŒ‡ä»¤å¤æ‚åº¦

**å®šå¾‹ 3ï¼šè´Ÿæ ·æœ¬æ¸…é™¤**
- ä¸€æ¡æœ‰æ¯’/é”™è¯¯çš„æ•°æ®ä¼šæ¯æ‰ 100 æ¡æ­£ç¡®æ•°æ®çš„æ•ˆæœ
- å¿…é¡»ä¸¥æ ¼è¿‡æ»¤ä½è´¨é‡ã€æœ‰å®³ã€é”™è¯¯çš„æ•°æ®

**å®šå¾‹ 4ï¼šåˆ†å¸ƒå¹³è¡¡**
- é¿å…æŸä¸€ç±»ä»»åŠ¡å æ¯”è¿‡é«˜ï¼ˆå¦‚ 70% éƒ½æ˜¯é—®ç­”ï¼‰
- ä½¿ç”¨ topic modeling æˆ–èšç±»åˆ†ææ•°æ®åˆ†å¸ƒ

---

## äºŒã€SFT æ•°æ®æ ¼å¼è¯¦è§£

### 1. Alpaca æ ¼å¼ï¼šæŒ‡ä»¤å¼æ•°æ®

**Alpaca æ ¼å¼**æ˜¯ Stanford æå‡ºçš„æ ‡å‡†æŒ‡ä»¤æ•°æ®æ ¼å¼ï¼ŒåŒ…å«ä¸‰ä¸ªå­—æ®µï¼š

```json
{
  "instruction": "æŒ‡ä»¤æè¿°ï¼ˆå¿…å¡«ï¼‰",
  "input": "é¢å¤–è¾“å…¥ï¼ˆå¯é€‰ï¼‰",
  "output": "æœŸæœ›è¾“å‡ºï¼ˆå¿…å¡«ï¼‰"
}
```

**å­—æ®µè¯´æ˜**ï¼š
- **instruction**ï¼šå‘Šè¯‰æ¨¡å‹è¦åšä»€ä¹ˆï¼ˆå¦‚"æ€»ç»“ä¸‹é¢çš„æ–‡ç« "ï¼‰
- **input**ï¼šæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚å…·ä½“çš„æ–‡ç« å†…å®¹ï¼‰
- **output**ï¼šæ ‡å‡†ç­”æ¡ˆï¼ˆæ¨¡å‹åº”è¯¥ç”Ÿæˆçš„å†…å®¹ï¼‰

**ç¤ºä¾‹ 1ï¼šæ—  input çš„ä»»åŠ¡**
```json
{
  "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—",
  "input": "",
  "output": "é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†ï¼ˆå¦‚å åŠ æ€å’Œçº ç¼ ï¼‰è¿›è¡Œè®¡ç®—çš„æŠ€æœ¯ã€‚ä¸ä¼ ç»Ÿè®¡ç®—æœºä½¿ç”¨æ¯”ç‰¹ä¸åŒï¼Œé‡å­è®¡ç®—æœºä½¿ç”¨é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šé—®é¢˜ä¸Šå®ç°æŒ‡æ•°çº§åŠ é€Ÿï¼Œå¦‚å¯†ç ç ´è§£å’Œè¯ç‰©è®¾è®¡ã€‚"
}
```

**ç¤ºä¾‹ 2ï¼šæœ‰ input çš„ä»»åŠ¡**
```json
{
  "instruction": "å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆæ³•è¯­",
  "input": "The weather is beautiful today.",
  "output": "Le temps est magnifique aujourd'hui."
}
```

**æ¨¡å‹å®é™…çœ‹åˆ°çš„æ ¼å¼**ï¼ˆè®­ç»ƒæ—¶æ‹¼æ¥ï¼‰ï¼š
```
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆæ³•è¯­

### Input:
The weather is beautiful today.

### Response:
Le temps est magnifique aujourd'hui.
```

### 2. ShareGPT æ ¼å¼ï¼šå¤šè½®å¯¹è¯æ•°æ®

**ShareGPT æ ¼å¼**ç”¨äºå­˜å‚¨å¤šè½®å¯¹è¯ï¼Œé€‚åˆè®­ç»ƒèŠå¤©æœºå™¨äººã€‚

```json
{
  "conversations": [
    {"from": "human", "value": "ç”¨æˆ·æ¶ˆæ¯1"},
    {"from": "gpt", "value": "åŠ©æ‰‹å›å¤1"},
    {"from": "human", "value": "ç”¨æˆ·æ¶ˆæ¯2"},
    {"from": "gpt", "value": "åŠ©æ‰‹å›å¤2"}
  ]
}
```

**ç¤ºä¾‹**ï¼š
```json
{
  "conversations": [
    {
      "from": "human",
      "value": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    },
    {
      "from": "gpt",
      "value": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"
    },
    {
      "from": "human",
      "value": "èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ"
    },
    {
      "from": "gpt",
      "value": "å½“ç„¶ã€‚æ¯”å¦‚åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼šç³»ç»Ÿé€šè¿‡å­¦ä¹ å¤§é‡åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶çš„ç‰¹å¾ï¼Œè‡ªåŠ¨è¯†åˆ«æ–°é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶ã€‚"
    }
  ]
}
```

**è®­ç»ƒæ—¶çš„å¤„ç†**ï¼š
- åªè®¡ç®— `gpt` è§’è‰²çš„ lossï¼ˆä¸è®¡ç®— `human` çš„ lossï¼‰
- ä½¿ç”¨ç‰¹æ®Š token åˆ†éš”å¤šè½®å¯¹è¯ï¼ˆå¦‚ `<|im_start|>`, `<|im_end|>`ï¼‰

### 3. æ ¼å¼è½¬æ¢å®æˆ˜

**åœºæ™¯**ï¼šå°† ShareGPT æ ¼å¼è½¬æ¢ä¸º Alpaca æ ¼å¼

```python
"""
æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·
åŠŸèƒ½ï¼šShareGPT -> Alpaca æ ¼å¼è½¬æ¢
"""
import json
from typing import List, Dict

def sharegpt_to_alpaca(sharegpt_data: Dict) -> List[Dict]:
    """
    å°† ShareGPT æ ¼å¼è½¬æ¢ä¸º Alpaca æ ¼å¼

    å‚æ•°ï¼š
        sharegpt_data: ShareGPT æ ¼å¼æ•°æ®

    è¿”å›ï¼š
        List[Dict]: Alpaca æ ¼å¼æ•°æ®åˆ—è¡¨
    """
    conversations = sharegpt_data.get("conversations", [])
    alpaca_samples = []

    # éå†å¯¹è¯ï¼Œæ¯ä¸ª human-gpt å¯¹è½¬æ¢ä¸ºä¸€æ¡ Alpaca æ•°æ®
    for i in range(0, len(conversations) - 1, 2):
        if (conversations[i]["from"] == "human" and
            conversations[i + 1]["from"] == "gpt"):

            alpaca_sample = {
                "instruction": conversations[i]["value"],
                "input": "",
                "output": conversations[i + 1]["value"]
            }
            alpaca_samples.append(alpaca_sample)

    return alpaca_samples

# ä½¿ç”¨ç¤ºä¾‹
sharegpt_example = {
    "conversations": [
        {"from": "human", "value": "ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ"},
        {"from": "gpt", "value": "Transformer æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„..."},
        {"from": "human", "value": "å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"},
        {"from": "gpt", "value": "ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š1) å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼º 2) èƒ½æ•è·é•¿è·ç¦»ä¾èµ–..."}
    ]
}

alpaca_data = sharegpt_to_alpaca(sharegpt_example)
print(json.dumps(alpaca_data, ensure_ascii=False, indent=2))
```

**è¾“å‡º**ï¼š
```json
[
  {
    "instruction": "ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ",
    "input": "",
    "output": "Transformer æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„..."
  },
  {
    "instruction": "å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
    "input": "",
    "output": "ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š1) å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼º 2) èƒ½æ•è·é•¿è·ç¦»ä¾èµ–..."
  }
]
```

---

## ä¸‰ã€Self-Instructï¼šç”¨ GPT-4 ç”Ÿæˆå¾®è°ƒæ•°æ®

### 1. Self-Instruct åŸç†ï¼šçŸ¥è¯†è’¸é¦çš„è‰ºæœ¯

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨å¼ºå¤§çš„ LLMï¼ˆå¦‚ GPT-4ï¼‰ç”ŸæˆæŒ‡ä»¤-å›ç­”å¯¹ï¼Œè®­ç»ƒè¾ƒå¼±çš„æ¨¡å‹ã€‚è¿™æ˜¯ **Alpaca** çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

**æœ¬è´¨ä¸Šï¼ŒSelf-Instruct æ˜¯ä¸€ç§"çŸ¥è¯†è’¸é¦"(Knowledge Distillation)è¿‡ç¨‹**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KNOWLEDGE DISTILLATION PIPELINE                    â”‚
â”‚                     (çŸ¥è¯†è’¸é¦æµæ°´çº¿)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Teacher Model                         Student Model
    (GPT-4, 1.7T)                        (Your Model, 7B)
         â”‚                                      â”‚
         â”‚  [èƒ½åŠ›:å¼ºå¤§ä½†æ˜‚è´µ]                    â”‚  [ç›®æ ‡:é«˜æ•ˆä¸”ä¸“å±]
         â”‚                                      â”‚
         â”œâ”€â”€> ç”Ÿæˆé«˜è´¨é‡                         â”‚
         â”‚    æŒ‡ä»¤-å›ç­”å¯¹                         â”‚
         â”‚    (è’¸é¦è¿‡ç¨‹)                         â”‚
         â”‚         â”‚                             â”‚
         â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [è¿ç§»çŸ¥è¯†]
         â”‚         â”‚                             â”‚
         â”‚         â””â”€â”€â”€â”€> 52K ç²¾åæ ·æœ¬ â”€â”€â”€â”€â”€â”€> [SFTè®­ç»ƒ]
         â”‚                                      â”‚
         â”‚                                      â†“
         â”‚                           Student è·å¾— Teacher çš„
         â”‚                           90% èƒ½åŠ›ï¼Œä½†åªæœ‰ 0.4% çš„å¤§å°!
         â”‚
         â””â”€â”€> æˆæœ¬å¯¹æ¯”:
              æ¨ç†æˆæœ¬: GPT-4 ($0.03/1K tokens) 
              vs è‡ªæ‰˜ç®¡ 7B ($0.0001/1K tokens)
              = 300x èŠ‚çœ!
```

**ä¸ºä»€ä¹ˆç§°ä¹‹ä¸º"è’¸é¦"?**
- **Teacher æ¨¡å‹**(GPT-4)å°±åƒ"åŸæ¶²"ï¼Œæµ“ç¼©äº†æµ·é‡çŸ¥è¯†
- **Distillation**(è’¸é¦)è¿‡ç¨‹æå–ç²¾åï¼Œå»é™¤å†—ä½™
- **Student æ¨¡å‹**å¾—åˆ°"è’¸é¦æ¶²"ï¼Œä¿ç•™æ ¸å¿ƒèƒ½åŠ›ä½†æ›´è½»é‡

**è’¸é¦çš„ä¸‰å¤§ä¼˜åŠ¿**:
1. **æˆæœ¬é™ä½**: æ¨ç†æˆæœ¬é™ä½ 100-300å€
2. **é€Ÿåº¦æå‡**: å°æ¨¡å‹æ¨ç†é€Ÿåº¦å¿« 10-50å€
3. **å¯æ§æ€§å¼º**: å¯ä»¥é’ˆå¯¹ç‰¹å®šé¢†åŸŸå®šåˆ¶

**æµç¨‹**ï¼š

$$
\begin{aligned}
&\text{1. ç§å­æ± } \quad S = \{\text{seed}_1, \ldots, \text{seed}_n\} \\
&\text{2. ç”ŸæˆæŒ‡ä»¤} \quad I_{\text{new}} = \text{LLM}(S_{\text{sample}}) \\
&\text{3. ç”Ÿæˆå›ç­”} \quad O_{\text{new}} = \text{LLM}(I_{\text{new}}) \\
&\text{4. è´¨é‡è¿‡æ»¤} \quad \text{if } Q(I_{\text{new}}, O_{\text{new}}) > \theta \text{ then Keep} \\
&\text{5. åŠ å…¥ç§å­æ± } \quad S \leftarrow S \cup \{I_{\text{new}}\}
\end{aligned}
$$

**å…³é”®æ­¥éª¤è¯¦è§£**ï¼š

**Step 1ï¼šç§å­æ± æ„å»º**
- äººå·¥ç¼–å†™ 175 æ¡é«˜è´¨é‡æŒ‡ä»¤ï¼ˆè¦†ç›–ä¸åŒä»»åŠ¡ç±»å‹ï¼‰
- åŒ…å«ï¼šé—®ç­”ã€æ¨ç†ã€åˆ›ä½œã€æ‘˜è¦ã€ç¿»è¯‘ã€ä»£ç ç­‰

**Step 2ï¼šæŒ‡ä»¤ç”Ÿæˆ**
- ä»ç§å­æ± éšæœºé‡‡æ · 6-8 æ¡æŒ‡ä»¤
- è®© LLM ç”Ÿæˆä¸ç§å­ç›¸ä¼¼ä½†ä¸é‡å¤çš„æ–°æŒ‡ä»¤

**Step 3ï¼šå›ç­”ç”Ÿæˆ**
- ä½¿ç”¨ GPT-4 ç”Ÿæˆé«˜è´¨é‡å›ç­”
- ä½¿ç”¨ Input-first æˆ– Output-first ç­–ç•¥

**Step 4ï¼šè´¨é‡è¿‡æ»¤**
- æ£€æŸ¥æŒ‡ä»¤æ˜¯å¦ä¸ç§å­æ± è¿‡äºç›¸ä¼¼ï¼ˆå»é‡ï¼‰
- æ£€æŸ¥å›ç­”æ˜¯å¦å®Œæ•´ã€æ­£ç¡®
- è¿‡æ»¤æœ‰å®³ã€ä½è´¨é‡çš„æ•°æ®

### 2. ä»£ç å®æˆ˜ï¼šç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤æ•°æ®

**å®Œæ•´çš„ Self-Instruct å®ç°**ï¼š

```python
"""
Self-Instruct æ•°æ®ç”Ÿæˆå™¨
åŠŸèƒ½ï¼šä½¿ç”¨ GPT-4 ç”Ÿæˆé«˜è´¨é‡çš„ SFT æ•°æ®
ä¾èµ–ï¼špip install openai
"""
import os
import json
from typing import List, Dict
from openai import OpenAI

class SelfInstructGenerator:
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.seed_tasks = []

    def load_seed_tasks(self, seed_file: str):
        """åŠ è½½ç§å­ä»»åŠ¡"""
        with open(seed_file, 'r', encoding='utf-8') as f:
            self.seed_tasks = json.load(f)

    def generate_instruction(self, num_samples: int = 6) -> str:
        """
        ç”Ÿæˆæ–°æŒ‡ä»¤

        å‚æ•°ï¼š
            num_samples: ä»ç§å­æ± é‡‡æ ·çš„æ•°é‡

        è¿”å›ï¼š
            æ–°ç”Ÿæˆçš„æŒ‡ä»¤
        """
        # éšæœºé‡‡æ ·ç§å­ä»»åŠ¡
        import random
        sampled = random.sample(self.seed_tasks, min(num_samples, len(self.seed_tasks)))

        # æ„é€  prompt
        prompt = """You are an AI assistant specialized in creating diverse instructions for training language models.

Below are some example instructions:
"""
        for i, task in enumerate(sampled, 1):
            prompt += f"{i}. {task['instruction']}\n"

        prompt += """
Generate a NEW instruction that is different from the examples above. The instruction should:
1. Be clear and specific
2. Cover a different task type or domain
3. Be solvable by a language model
4. Not require visual input or real-time information

New Instruction:"""

        # è°ƒç”¨ GPT-4 ç”Ÿæˆ
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=200
        )

        new_instruction = response.choices[0].message.content.strip()
        return new_instruction

    def generate_response(self, instruction: str, input_text: str = "") -> str:
        """
        ä¸ºæŒ‡ä»¤ç”Ÿæˆå›ç­”

        å‚æ•°ï¼š
            instruction: æŒ‡ä»¤
            input_text: é¢å¤–è¾“å…¥ï¼ˆå¯é€‰ï¼‰

        è¿”å›ï¼š
            ç”Ÿæˆçš„å›ç­”
        """
        if input_text:
            prompt = f"{instruction}\n\nInput: {input_text}\n\nResponse:"
        else:
            prompt = f"{instruction}\n\nResponse:"

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1000
        )

        output = response.choices[0].message.content.strip()
        return output

    def is_similar(self, new_instruction: str, threshold: float = 0.7) -> bool:
        """
        æ£€æŸ¥æ–°æŒ‡ä»¤æ˜¯å¦ä¸ç§å­æ± è¿‡äºç›¸ä¼¼ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰

        å®é™…åº”è¯¥ä½¿ç”¨ Embedding è®¡ç®—ç›¸ä¼¼åº¦
        """
        new_words = set(new_instruction.lower().split())

        for seed in self.seed_tasks:
            seed_words = set(seed['instruction'].lower().split())
            # Jaccard ç›¸ä¼¼åº¦
            intersection = len(new_words & seed_words)
            union = len(new_words | seed_words)
            similarity = intersection / union if union > 0 else 0

            if similarity > threshold:
                return True

        return False

    def generate_dataset(self, num_samples: int = 100, output_file: str = "sft_data.jsonl") -> List[Dict]:
        """
        ç”Ÿæˆå®Œæ•´çš„ SFT æ•°æ®é›†

        å‚æ•°ï¼š
            num_samples: ç”Ÿæˆæ•°æ®æ¡æ•°
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        è¿”å›ï¼š
            ç”Ÿæˆçš„æ•°æ®é›†
        """
        dataset = []

        for i in range(num_samples):
            print(f"ç”Ÿæˆç¬¬ {i+1}/{num_samples} æ¡æ•°æ®...")

            # 1. ç”ŸæˆæŒ‡ä»¤
            try:
                instruction = self.generate_instruction()

                # 2. å»é‡æ£€æŸ¥
                if self.is_similar(instruction):
                    print("  è·³è¿‡ï¼ˆä¸ç§å­æ± ç›¸ä¼¼ï¼‰")
                    continue

                # 3. ç”Ÿæˆå›ç­”
                output = self.generate_response(instruction)

                # 4. æ„é€ æ•°æ®æ ·æœ¬
                sample = {
                    "instruction": instruction,
                    "input": "",
                    "output": output
                }

                dataset.append(sample)

                # 5. åŠ å…¥ç§å­æ± ï¼ˆåŠ¨æ€å¢é•¿ï¼‰
                self.seed_tasks.append(sample)

                print(f"  æˆåŠŸç”Ÿæˆï¼š{instruction[:50]}...")

            except Exception as e:
                print(f"  ç”Ÿæˆå¤±è´¥ï¼š{e}")
                continue

        # ä¿å­˜æ•°æ®é›†
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in dataset:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        print(f"\næ•°æ®é›†å·²ä¿å­˜è‡³ {output_file}ï¼Œå…± {len(dataset)} æ¡")
        return dataset

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å‡†å¤‡ç§å­ä»»åŠ¡
    seed_tasks = [
        {"instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "input": "", "output": ""},
        {"instruction": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", "input": "", "output": ""},
        {"instruction": "å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±è¯­", "input": "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "output": ""},
        {"instruction": "ç¼–å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—", "input": "", "output": ""},
        {"instruction": "æ€»ç»“ä»¥ä¸‹æ–‡ç« çš„ä¸»è¦è§‚ç‚¹", "input": "", "output": ""},
    ]

    # ä¿å­˜ç§å­ä»»åŠ¡
    with open("seed_tasks.json", 'w', encoding='utf-8') as f:
        json.dump(seed_tasks, f, ensure_ascii=False, indent=2)

    # 2. åˆ›å»ºç”Ÿæˆå™¨
    generator = SelfInstructGenerator()
    generator.load_seed_tasks("seed_tasks.json")

    # 3. ç”Ÿæˆæ•°æ®é›†
    # dataset = generator.generate_dataset(num_samples=10, output_file="my_sft_data.jsonl")

    # 4. æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
    # with open("my_sft_data.jsonl", 'r', encoding='utf-8') as f:
    #     for line in f:
    #         sample = json.loads(line)
    #         print(f"æŒ‡ä»¤: {sample['instruction']}")
    #         print(f"å›ç­”: {sample['output'][:100]}...\n")
```

**å®æˆ˜å»ºè®®**ï¼š
1. **ç§å­æ± è´¨é‡è‡³å…³é‡è¦**ï¼šåˆå§‹ 175 æ¡ç§å­åº”è¦†ç›–å„ç±»ä»»åŠ¡
2. **åŠ¨æ€é‡‡æ ·**ï¼šéšç€ç”Ÿæˆï¼Œå°†æ–°æŒ‡ä»¤åŠ å…¥ç§å­æ± ï¼Œæå‡å¤šæ ·æ€§
3. **æ‰¹é‡ç”Ÿæˆ**ï¼šä½¿ç”¨ GPT-4 æˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®æ‰¹é‡ç”Ÿæˆå¹¶ç¼“å­˜
4. **äººå·¥å®¡æ ¸**ï¼šç”Ÿæˆååº”æŠ½æ ·æ£€æŸ¥ï¼Œè¿‡æ»¤ä½è´¨é‡æ•°æ®

### 3. Evol-Instructï¼šè®©æŒ‡ä»¤è¿›åŒ–

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡å¤šè½®"è¿›åŒ–"ï¼Œå°†ç®€å•æŒ‡ä»¤é€æ­¥å˜å¤æ‚ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚è¿™æ˜¯ **WizardLM** çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

**è¿›åŒ–ç­–ç•¥**ï¼š

| ç­–ç•¥ | è¯´æ˜ | ç¤ºä¾‹ |
| :--- | :--- | :--- |
| **å¢åŠ çº¦æŸ** | æ·»åŠ å­—æ•°ã€æ ¼å¼ã€é£æ ¼é™åˆ¶ | "è§£é‡Šé‡å­è®¡ç®—" â†’ "ç”¨ä¸è¶…è¿‡100å­—è§£é‡Šé‡å­è®¡ç®—" |
| **åŠ æ·±æ¨ç†** | è¦æ±‚å¤šæ­¥æ¨ç†ã€å› æœåˆ†æ | "ä»€ä¹ˆæ˜¯é€šè´§è†¨èƒ€" â†’ "åˆ†æé€šè´§è†¨èƒ€çš„æ ¹æœ¬åŸå› åŠå…¶å¯¹ç»æµçš„å¤šå±‚æ¬¡å½±å“" |
| **å…·ä½“åŒ–** | å°†æŠ½è±¡æ¦‚å¿µå…·ä½“åŒ– | "å¦‚ä½•æé«˜æ•ˆç‡" â†’ "å¦‚ä½•åœ¨è¿œç¨‹åŠå…¬ä¸­æé«˜å›¢é˜Ÿåä½œæ•ˆç‡" |
| **å¢åŠ æ¨ç†æ­¥éª¤** | è¦æ±‚å±•ç¤ºæ¨ç†è¿‡ç¨‹ | "è®¡ç®— 25 Ã— 17" â†’ "é€æ­¥å±•ç¤º 25 Ã— 17 çš„è®¡ç®—è¿‡ç¨‹" |
| **å¤æ‚åŒ–è¾“å…¥** | å¢åŠ è¾“å…¥ä¿¡æ¯çš„å¤æ‚åº¦ | "æ€»ç»“æ–‡ç« " â†’ "æ€»ç»“åŒ…å«çŸ›ç›¾è§‚ç‚¹çš„å¤šç¯‡æ–‡ç« " |

**è¿›åŒ–æµç¨‹**ï¼š

$$
\begin{aligned}
&I_0 \quad \text{(åŸå§‹æŒ‡ä»¤)} \\
&\downarrow \text{è¿›åŒ–ç­–ç•¥1} \\
&I_1 \quad \text{(ç¬¬1æ¬¡è¿›åŒ–)} \\
&\downarrow \text{è¿›åŒ–ç­–ç•¥2} \\
&I_2 \quad \text{(ç¬¬2æ¬¡è¿›åŒ–)} \\
&\downarrow \ldots \\
&I_n \quad \text{(æœ€ç»ˆå¤æ‚æŒ‡ä»¤)}
\end{aligned}
$$

### 4. ä»£ç å®æˆ˜ï¼šæŒ‡ä»¤å¤æ‚åº¦æå‡

```python
"""
Evol-Instruct å®ç°
åŠŸèƒ½ï¼šå°†ç®€å•æŒ‡ä»¤è¿›åŒ–ä¸ºå¤æ‚æŒ‡ä»¤
"""
from openai import OpenAI
import os

class EvolInstructor:
    def __init__(self, api_key: str = None):
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))

        # å®šä¹‰è¿›åŒ–ç­–ç•¥æ¨¡æ¿
        self.evolution_prompts = {
            "add_constraints": """Please rewrite the following instruction by adding specific constraints (e.g., word limit, format requirement, target audience).

Original Instruction: {instruction}

Evolved Instruction:""",

            "deepen": """Please rewrite the following instruction to make it require deeper reasoning or multi-step thinking.

Original Instruction: {instruction}

Evolved Instruction:""",

            "concretize": """Please rewrite the following instruction to make it more specific and concrete, adding real-world context.

Original Instruction: {instruction}

Evolved Instruction:""",

            "increase_reasoning": """Please rewrite the following instruction to require the model to show step-by-step reasoning.

Original Instruction: {instruction}

Evolved Instruction:"""
        }

    def evolve_instruction(self, instruction: str, strategy: str = "add_constraints") -> str:
        """
        ä½¿ç”¨æŒ‡å®šç­–ç•¥è¿›åŒ–æŒ‡ä»¤

        å‚æ•°ï¼š
            instruction: åŸå§‹æŒ‡ä»¤
            strategy: è¿›åŒ–ç­–ç•¥ (add_constraints, deepen, concretize, increase_reasoning)

        è¿”å›ï¼š
            è¿›åŒ–åçš„æŒ‡ä»¤
        """
        if strategy not in self.evolution_prompts:
            raise ValueError(f"Unknown strategy: {strategy}")

        prompt = self.evolution_prompts[strategy].format(instruction=instruction)

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=300
        )

        evolved = response.choices[0].message.content.strip()
        return evolved

    def multi_round_evolution(self, instruction: str, depth: int = 2) -> List[str]:
        """
        å¤šè½®è¿›åŒ–

        å‚æ•°ï¼š
            instruction: åŸå§‹æŒ‡ä»¤
            depth: è¿›åŒ–è½®æ•°

        è¿”å›ï¼š
            æ¯è½®è¿›åŒ–åçš„æŒ‡ä»¤åˆ—è¡¨
        """
        import random

        current = instruction
        evolution_history = [instruction]

        strategies = list(self.evolution_prompts.keys())

        for i in range(depth):
            strategy = random.choice(strategies)
            current = self.evolve_instruction(current, strategy)
            evolution_history.append(current)
            print(f"Round {i+1} ({strategy}): {current}\n")

        return evolution_history

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    evolver = EvolInstructor()

    # ç®€å•æŒ‡ä»¤
    simple_instruction = "å†™ä¸€ä¸ª Python å‡½æ•°æ¥æ’åºåˆ—è¡¨"

    print("åŸå§‹æŒ‡ä»¤:", simple_instruction)
    print("\n" + "="*50 + "\n")

    # å•ç­–ç•¥è¿›åŒ–
    evolved = evolver.evolve_instruction(simple_instruction, strategy="add_constraints")
    print("å¢åŠ çº¦æŸå:", evolved)
    print()

    evolved = evolver.evolve_instruction(simple_instruction, strategy="increase_reasoning")
    print("å¢åŠ æ¨ç†æ­¥éª¤å:", evolved)
    print()

    # å¤šè½®è¿›åŒ–
    print("\n" + "="*50)
    print("å¤šè½®è¿›åŒ–:")
    print("="*50 + "\n")
    # history = evolver.multi_round_evolution(simple_instruction, depth=2)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
åŸå§‹æŒ‡ä»¤: å†™ä¸€ä¸ª Python å‡½æ•°æ¥æ’åºåˆ—è¡¨

å¢åŠ çº¦æŸå: å†™ä¸€ä¸ª Python å‡½æ•°æ¥æ’åºåˆ—è¡¨ï¼Œè¦æ±‚ä½¿ç”¨å¿«é€Ÿæ’åºç®—æ³•ï¼Œå‡½æ•°åº”åŒ…å«è¯¦ç»†æ³¨é‡Šï¼Œå¹¶å¤„ç†ç©ºåˆ—è¡¨å’Œå•å…ƒç´ åˆ—è¡¨çš„è¾¹ç•Œæƒ…å†µã€‚

å¢åŠ æ¨ç†æ­¥éª¤å: å†™ä¸€ä¸ª Python å‡½æ•°æ¥æ’åºåˆ—è¡¨ï¼Œå¹¶è¯¦ç»†è¯´æ˜ä½ é€‰æ‹©çš„æ’åºç®—æ³•çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬æ—¶é—´å¤æ‚åº¦åˆ†æå’Œé€‚ç”¨åœºæ™¯ã€‚
```

---

## å››ã€æ•°æ®æ¸…æ´—ä¸è´¨é‡è¿‡æ»¤

### 1. åŸºäºè§„åˆ™çš„è´¨é‡è¿‡æ»¤

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨å¯å‘å¼è§„åˆ™å¿«é€Ÿè¿‡æ»¤æ˜æ˜¾çš„ä½è´¨é‡æ•°æ®ã€‚

**å…³é”®è¿‡æ»¤è§„åˆ™**ï¼š

**è§„åˆ™ 1ï¼šé•¿åº¦è¿‡æ»¤**
```python
def filter_by_length(sample: Dict) -> bool:
    """è¿‡æ»¤è¿‡çŸ­æˆ–è¿‡é•¿çš„å›ç­”"""
    output = sample['output']
    word_count = len(output.split())

    # å›ç­”å¤ªçŸ­ï¼ˆ< 10 è¯ï¼‰æˆ–å¤ªé•¿ï¼ˆ> 2000 è¯ï¼‰
    return 10 <= word_count <= 2000
```

**è§„åˆ™ 2ï¼šé‡å¤å†…å®¹æ£€æµ‹**
```python
def filter_repetition(text: str, max_repeat: int = 3) -> bool:
    """æ£€æµ‹é‡å¤è¡Œ"""
    lines = text.split('\n')
    line_counts = {}

    for line in lines:
        line = line.strip()
        if line:
            line_counts[line] = line_counts.get(line, 0) + 1
            if line_counts[line] > max_repeat:
                return False

    return True
```

**è§„åˆ™ 3ï¼šå®Œæ•´æ€§æ£€æµ‹**
```python
def filter_incomplete(output: str) -> bool:
    """æ£€æµ‹å›ç­”æ˜¯å¦å®Œæ•´"""
    # å›ç­”ä¸åº”è¯¥çªç„¶æˆªæ–­
    incomplete_patterns = [
        "...",
        "[æœªå®Œæˆ]",
        "ï¼ˆæœªå®Œå¾…ç»­ï¼‰",
        "the rest is",
        "to be continued"
    ]

    output_lower = output.lower()
    for pattern in incomplete_patterns:
        if pattern in output_lower:
            return False

    return True
```

**è§„åˆ™ 4ï¼šæ‹’ç»å›ç­”æ£€æµ‹**
```python
def filter_refusal(output: str) -> bool:
    """æ£€æµ‹æ¨¡å‹æ˜¯å¦æ‹’ç»å›ç­”"""
    refusal_patterns = [
        "i cannot",
        "i'm unable to",
        "i can't",
        "as an ai",
        "i don't have access",
        "i'm not able to"
    ]

    output_lower = output.lower()
    for pattern in refusal_patterns:
        if pattern in output_lower:
            return False

    return True
```

### 2. æ¯’æ€§æ£€æµ‹

**å·¥å…·**ï¼šä½¿ç”¨ `detoxify` åº“è¿›è¡Œæ¯’æ€§æ£€æµ‹

```python
"""
æ¯’æ€§å†…å®¹è¿‡æ»¤
ä¾èµ–ï¼špip install detoxify
"""
from detoxify import Detoxify

class ToxicityFilter:
    def __init__(self, threshold: float = 0.5):
        self.model = Detoxify('original')
        self.threshold = threshold

    def is_toxic(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦æœ‰æ¯’"""
        scores = self.model.predict(text)

        # æ£€æŸ¥ä»»ä½•ä¸€ä¸ªç»´åº¦çš„æ¯’æ€§
        toxic_categories = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult']

        for category in toxic_categories:
            if scores[category] > self.threshold:
                return True

        return False

    def filter_dataset(self, dataset: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤æ•°æ®é›†ä¸­çš„æœ‰æ¯’æ•°æ®"""
        filtered = []

        for sample in dataset:
            # æ£€æŸ¥æŒ‡ä»¤å’Œè¾“å‡ºæ˜¯å¦æœ‰æ¯’
            if not self.is_toxic(sample['instruction']) and not self.is_toxic(sample['output']):
                filtered.append(sample)

        return filtered
```

### 3. PII è¯†åˆ«ä¸è„±æ•

**å·¥å…·**ï¼šä½¿ç”¨ `presidio` åº“è¿›è¡Œ PII æ£€æµ‹

```python
"""
PII è„±æ•
ä¾èµ–ï¼špip install presidio-analyzer presidio-anonymizer
"""
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig

class PIIScrubber:
    def __init__(self):
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()

    def scrub(self, text: str) -> str:
        """è„±æ• PII ä¿¡æ¯"""
        # æ£€æµ‹ PII
        results = self.analyzer.analyze(
            text=text,
            entities=["PHONE_NUMBER", "EMAIL_ADDRESS", "PERSON", "LOCATION"],
            language='en'
        )

        # æ›¿æ¢ä¸ºå ä½ç¬¦
        anonymized = self.anonymizer.anonymize(
            text=text,
            analyzer_results=results,
            operators={
                "PHONE_NUMBER": OperatorConfig("replace", {"new_value": "<PHONE>"}),
                "EMAIL_ADDRESS": OperatorConfig("replace", {"new_value": "<EMAIL>"}),
                "PERSON": OperatorConfig("replace", {"new_value": "<NAME>"}),
                "LOCATION": OperatorConfig("replace", {"new_value": "<LOCATION>"}),
            }
        )

        return anonymized.text

    def scrub_dataset(self, dataset: List[Dict]) -> List[Dict]:
        """æ‰¹é‡è„±æ•æ•°æ®é›†"""
        scrubbed = []

        for sample in dataset:
            scrubbed_sample = {
                "instruction": self.scrub(sample['instruction']),
                "input": self.scrub(sample.get('input', '')),
                "output": self.scrub(sample['output'])
            }
            scrubbed.append(scrubbed_sample)

        return scrubbed
```

### 4. ä»£ç å®æˆ˜ï¼šå®Œæ•´çš„è´¨é‡è¿‡æ»¤å™¨

**æ•´åˆæ‰€æœ‰è¿‡æ»¤å™¨**ï¼š

```python
"""
ç»¼åˆè´¨é‡è¿‡æ»¤å™¨
åŠŸèƒ½ï¼šæ•´åˆæ‰€æœ‰è´¨é‡æ£€æŸ¥è§„åˆ™
"""
import json
from typing import List, Dict

class QualityFilter:
    def __init__(self, enable_toxicity: bool = True, enable_pii: bool = True):
        self.enable_toxicity = enable_toxicity
        self.enable_pii = enable_pii

        if enable_toxicity:
            self.toxicity_filter = ToxicityFilter()

        if enable_pii:
            self.pii_scrubber = PIIScrubber()

    def check_length(self, sample: Dict) -> bool:
        """é•¿åº¦æ£€æŸ¥"""
        output_words = len(sample['output'].split())
        return 10 <= output_words <= 2000

    def check_repetition(self, text: str) -> bool:
        """é‡å¤æ£€æµ‹"""
        lines = text.split('\n')
        line_counts = {}

        for line in lines:
            line = line.strip()
            if line:
                line_counts[line] = line_counts.get(line, 0) + 1
                if line_counts[line] > 3:
                    return False
        return True

    def check_completeness(self, output: str) -> bool:
        """å®Œæ•´æ€§æ£€æµ‹"""
        incomplete_patterns = ["...", "[æœªå®Œæˆ]", "to be continued"]
        output_lower = output.lower()

        for pattern in incomplete_patterns:
            if pattern in output_lower:
                return False
        return True

    def check_refusal(self, output: str) -> bool:
        """æ‹’ç»å›ç­”æ£€æµ‹"""
        refusal_patterns = ["i cannot", "i'm unable to", "as an ai"]
        output_lower = output.lower()

        for pattern in refusal_patterns:
            if pattern in output_lower:
                return False
        return True

    def filter_sample(self, sample: Dict) -> tuple[bool, str]:
        """
        è¿‡æ»¤å•æ¡æ ·æœ¬

        è¿”å›ï¼š
            (æ˜¯å¦é€šè¿‡, å¤±è´¥åŸå› )
        """
        # 1. é•¿åº¦æ£€æŸ¥
        if not self.check_length(sample):
            return False, "length_invalid"

        # 2. é‡å¤æ£€æŸ¥
        if not self.check_repetition(sample['output']):
            return False, "repetition_detected"

        # 3. å®Œæ•´æ€§æ£€æŸ¥
        if not self.check_completeness(sample['output']):
            return False, "incomplete_response"

        # 4. æ‹’ç»å›ç­”æ£€æŸ¥
        if not self.check_refusal(sample['output']):
            return False, "refusal_detected"

        # 5. æ¯’æ€§æ£€æŸ¥
        if self.enable_toxicity:
            if self.toxicity_filter.is_toxic(sample['instruction']) or \
               self.toxicity_filter.is_toxic(sample['output']):
                return False, "toxic_content"

        return True, "passed"

    def filter_dataset(self, dataset: List[Dict], output_file: str = None) -> Dict:
        """
        æ‰¹é‡è¿‡æ»¤æ•°æ®é›†

        è¿”å›ï¼š
            ç»Ÿè®¡ä¿¡æ¯
        """
        filtered = []
        stats = {
            "total": len(dataset),
            "passed": 0,
            "rejected": 0,
            "reasons": {}
        }

        for sample in dataset:
            passed, reason = self.filter_sample(sample)

            if passed:
                # PII è„±æ•
                if self.enable_pii:
                    sample = {
                        "instruction": self.pii_scrubber.scrub(sample['instruction']),
                        "input": self.pii_scrubber.scrub(sample.get('input', '')),
                        "output": self.pii_scrubber.scrub(sample['output'])
                    }

                filtered.append(sample)
                stats['passed'] += 1
            else:
                stats['rejected'] += 1
                stats['reasons'][reason] = stats['reasons'].get(reason, 0) + 1

        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                for sample in filtered:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        return stats, filtered

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®
    test_dataset = [
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            "input": "",
            "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
        },
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—",
            "input": "",
            "output": "é‡å­è®¡ç®—..."  # ä¸å®Œæ•´
        },
        {
            "instruction": "ä½ èƒ½å¸®æˆ‘é»‘æ‰åˆ«äººçš„è´¦å·å—ï¼Ÿ",
            "input": "",
            "output": "å¾ˆæŠ±æ­‰ï¼ŒI cannot help with that."  # æ‹’ç»å›ç­”
        }
    ]

    # åˆ›å»ºè¿‡æ»¤å™¨
    filter_engine = QualityFilter(enable_toxicity=False, enable_pii=False)

    # è¿‡æ»¤æ•°æ®é›†
    stats, filtered = filter_engine.filter_dataset(test_dataset)

    print("è¿‡æ»¤ç»Ÿè®¡:")
    print(f"æ€»è®¡: {stats['total']}")
    print(f"é€šè¿‡: {stats['passed']}")
    print(f"æ‹’ç»: {stats['rejected']}")
    print(f"æ‹’ç»åŸå› : {stats['reasons']}")
```

### 5. åˆæˆæ•°æ®å®æˆ˜ï¼šä½¿ç”¨ LLM ç”ŸæˆæŒ‡ä»¤

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨å¼ºå¤§çš„ LLMï¼ˆå¦‚ GPT-4ã€Claudeï¼‰æ‰¹é‡ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤-å›ç­”å¯¹ï¼Œè¿™æ˜¯æ„å»ºå¾®è°ƒæ•°æ®é›†æœ€é«˜æ•ˆçš„æ–¹æ³•ä¹‹ä¸€ã€‚

#### 5.1 ä¸ºä»€ä¹ˆé€‰æ‹©åˆæˆæ•°æ®ï¼Ÿ

**ä¼˜åŠ¿**ï¼š
- **æˆæœ¬ä½**ï¼šç›¸æ¯”äººå·¥æ ‡æ³¨ï¼ˆ$10-30/å°æ—¶ï¼‰ï¼ŒAPI è°ƒç”¨æˆæœ¬ä»… $0.01-0.03/æ¡
- **é€Ÿåº¦å¿«**ï¼šä¸€å¤©å¯ç”Ÿæˆ 10K+ æ¡æ•°æ®
- **è´¨é‡é«˜**ï¼šGPT-4 ç”Ÿæˆçš„æ•°æ®è´¨é‡æ¥è¿‘äººå·¥æ ‡æ³¨
- **å¯æ§æ€§å¼º**ï¼šå¯ä»¥ç²¾ç¡®æ§åˆ¶ä»»åŠ¡ç±»å‹ã€éš¾åº¦ã€é£æ ¼

**æˆåŠŸæ¡ˆä¾‹**ï¼š
- **Alpaca (Stanford, 2023)**ï¼š52K æ¡ GPT-3.5-turbo ç”Ÿæˆçš„æ•°æ®ï¼Œè®­ç»ƒ 7B æ¨¡å‹è¾¾åˆ° text-davinci-003 çš„ 90% èƒ½åŠ›
- **WizardLM (Microsoft, 2023)**ï¼šé€šè¿‡ Evol-Instruct ç”Ÿæˆå¤æ‚æŒ‡ä»¤ï¼Œ7B æ¨¡å‹è¶…è¶Š ChatGPT åœ¨éƒ¨åˆ†ä»»åŠ¡
- **Phi-3 (Microsoft, 2024)**ï¼š3.8B æ¨¡å‹ä½¿ç”¨åˆæˆæ•°æ®ï¼Œæ€§èƒ½è¶…è¶Š Llama-2-13B

#### 5.2 å®Œæ•´çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹

**Step 1: è®¾è®¡ç§å­æŒ‡ä»¤æ± **

ç§å­æŒ‡ä»¤åº”è¦†ç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼š

```python
"""
æ„å»ºå¤šæ ·åŒ–çš„ç§å­æŒ‡ä»¤æ± 
è¦†ç›–ï¼šé—®ç­”ã€æ¨ç†ã€åˆ›ä½œã€ä»£ç ã€ç¿»è¯‘ã€æ‘˜è¦ç­‰
"""
seed_instructions = [
    # 1. é—®ç­”ç±»
    {"task_type": "qa", "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­çº ç¼ "},
    {"task_type": "qa", "instruction": "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„"},

    # 2. æ¨ç†ç±»
    {"task_type": "reasoning", "instruction": "å¦‚æœæ‰€æœ‰Aéƒ½æ˜¯Bï¼Œæ‰€æœ‰Béƒ½æ˜¯Cï¼Œé‚£ä¹ˆAå’ŒCçš„å…³ç³»æ˜¯ä»€ä¹ˆ"},
    {"task_type": "reasoning", "instruction": "åˆ†æé€šè´§è†¨èƒ€å¯¹æ™®é€šå®¶åº­çš„å½±å“"},

    # 3. åˆ›ä½œç±»
    {"task_type": "creative", "instruction": "å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—"},
    {"task_type": "creative", "instruction": "åˆ›ä½œä¸€ä¸ªç§‘å¹»å°è¯´çš„å¼€å¤´"},

    # 4. ä»£ç ç±»
    {"task_type": "code", "instruction": "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"},
    {"task_type": "code", "instruction": "å®ç°ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•"},

    # 5. ç¿»è¯‘ç±»
    {"task_type": "translation", "instruction": "å°†ä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±è¯­ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½"},
    {"task_type": "translation", "instruction": "æŠŠè¿™æ®µä¸­æ–‡ç¿»è¯‘æˆæ³•è¯­"},

    # 6. æ‘˜è¦ç±»
    {"task_type": "summarization", "instruction": "æ€»ç»“ä»¥ä¸‹æ–‡ç« çš„ä¸»è¦è§‚ç‚¹"},
    {"task_type": "summarization", "instruction": "ç”¨ä¸‰å¥è¯æ¦‚æ‹¬è¿™æ®µæ–°é—»"}
]
```

**Step 2: æ‰¹é‡ç”ŸæˆæŒ‡ä»¤-å›ç­”å¯¹**

```python
"""
ä½¿ç”¨ OpenAI API æ‰¹é‡ç”Ÿæˆåˆæˆæ•°æ®
åŒ…å«é”™è¯¯å¤„ç†ã€é€Ÿç‡é™åˆ¶ã€è´¨é‡æ£€æŸ¥
"""
import os
import json
import time
from typing import List, Dict
from openai import OpenAI
from tqdm import tqdm

class SyntheticDataGenerator:
    def __init__(self, api_key: str = None, model: str = "gpt-4"):
        """
        åˆå§‹åŒ–åˆæˆæ•°æ®ç”Ÿæˆå™¨

        å‚æ•°ï¼š
            api_key: OpenAI API Key
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆgpt-4, gpt-3.5-turboï¼‰
        """
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.generated_count = 0
        self.failed_count = 0

        # æˆæœ¬é…ç½®ï¼ˆå‚è€ƒä»·æ ¼ï¼Œå•ä½ï¼š$/1M tokensï¼‰
        self.pricing = {
            "gpt-4": {"input": 10.0, "output": 30.0},
            "gpt-4-turbo": {"input": 10.0, "output": 30.0},
            "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},
            "gpt-4o": {"input": 2.5, "output": 10.0},
            "gpt-4o-mini": {"input": 0.15, "output": 0.6}
        }

    def estimate_cost(
        self,
        num_samples: int,
        avg_instruction_tokens: int = 50,
        avg_output_tokens: int = 500
    ) -> Dict:
        """
        ä¼°ç®—ç”Ÿæˆæ•°æ®é›†çš„æˆæœ¬

        å‚æ•°ï¼š
            num_samples: è¦ç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            avg_instruction_tokens: å¹³å‡æŒ‡ä»¤é•¿åº¦ï¼ˆtokensï¼‰
            avg_output_tokens: å¹³å‡è¾“å‡ºé•¿åº¦ï¼ˆtokensï¼‰

        è¿”å›ï¼š
            æˆæœ¬ä¼°ç®—è¯¦æƒ…
        """
        # è·å–æ¨¡å‹å®šä»·
        model_key = self.model
        if model_key not in self.pricing:
            # é»˜è®¤ä½¿ç”¨ gpt-4 å®šä»·
            model_key = "gpt-4"

        pricing = self.pricing[model_key]

        # è®¡ç®—æ€» token æ•°
        # è¾“å…¥ï¼šsystem prompt (~100 tokens) + æŒ‡ä»¤ prompt (~200 tokens) + æŒ‡ä»¤å†…å®¹
        input_tokens_per_sample = 300 + avg_instruction_tokens
        total_input_tokens = num_samples * input_tokens_per_sample

        # è¾“å‡ºï¼šç”Ÿæˆçš„å›ç­”
        total_output_tokens = num_samples * avg_output_tokens

        # è®¡ç®—æˆæœ¬
        input_cost = (total_input_tokens / 1_000_000) * pricing["input"]
        output_cost = (total_output_tokens / 1_000_000) * pricing["output"]
        total_cost = input_cost + output_cost

        # ä¼°ç®—æ—¶é—´ï¼ˆå‡è®¾æ¯ä¸ªè¯·æ±‚å¹³å‡ 2 ç§’ï¼‰
        estimated_time_minutes = (num_samples * 2) / 60

        return {
            "model": self.model,
            "num_samples": num_samples,
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "total_tokens": total_input_tokens + total_output_tokens,
            "input_cost": round(input_cost, 2),
            "output_cost": round(output_cost, 2),
            "total_cost": round(total_cost, 2),
            "cost_per_sample": round(total_cost / num_samples, 4),
            "estimated_time_minutes": round(estimated_time_minutes, 1),
            "pricing_input": f"${pricing['input']}/1M tokens",
            "pricing_output": f"${pricing['output']}/1M tokens"
        }

    def generate_qa_pair(self, instruction: str, context: str = "") -> Dict:
        """
        ä¸ºå•ä¸ªæŒ‡ä»¤ç”Ÿæˆé«˜è´¨é‡å›ç­”

        å‚æ•°ï¼š
            instruction: æŒ‡ä»¤
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

        è¿”å›ï¼š
            {"instruction": ..., "input": ..., "output": ...}
        """
        # æ„é€  prompt
        if context:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æŒ‡ä»¤æä¾›ä¸€ä¸ªé«˜è´¨é‡ã€è¯¦ç»†çš„å›ç­”ã€‚

æŒ‡ä»¤ï¼š{instruction}

ä¸Šä¸‹æ–‡ï¼š{context}

è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´
2. è¯­è¨€è¦æµç•…ã€è‡ªç„¶
3. åŒ…å«å¿…è¦çš„è§£é‡Šå’Œç¤ºä¾‹
4. é¿å…ç”Ÿæˆæœ‰å®³ã€åè§æˆ–é”™è¯¯çš„å†…å®¹

å›ç­”ï¼š"""
        else:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æŒ‡ä»¤æä¾›ä¸€ä¸ªé«˜è´¨é‡ã€è¯¦ç»†çš„å›ç­”ã€‚

æŒ‡ä»¤ï¼š{instruction}

è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´
2. è¯­è¨€è¦æµç•…ã€è‡ªç„¶
3. åŒ…å«å¿…è¦çš„è§£é‡Šå’Œç¤ºä¾‹
4. é¿å…ç”Ÿæˆæœ‰å®³ã€åè§æˆ–é”™è¯¯çš„å†…å®¹

å›ç­”ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç±»é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1500,
                top_p=0.9
            )

            output = response.choices[0].message.content.strip()

            # æ„é€  Alpaca æ ¼å¼
            qa_pair = {
                "instruction": instruction,
                "input": context,
                "output": output
            }

            self.generated_count += 1
            return qa_pair

        except Exception as e:
            self.failed_count += 1
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def batch_generate(
        self,
        instructions: List[str],
        output_file: str = "synthetic_data.jsonl",
        batch_size: int = 10,
        delay: float = 1.0
    ) -> List[Dict]:
        """
        æ‰¹é‡ç”Ÿæˆæ•°æ®é›†

        å‚æ•°ï¼š
            instructions: æŒ‡ä»¤åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å°
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é€Ÿç‡é™åˆ¶

        è¿”å›ï¼š
            ç”Ÿæˆçš„æ•°æ®é›†
        """
        dataset = []

        print(f"å¼€å§‹ç”Ÿæˆ {len(instructions)} æ¡æ•°æ®...")
        print(f"æ¨¡å‹: {self.model}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, è¯·æ±‚é—´éš”: {delay}ç§’\n")

        # ä½¿ç”¨è¿›åº¦æ¡
        for i in tqdm(range(0, len(instructions), batch_size)):
            batch = instructions[i:i+batch_size]

            for instruction in batch:
                qa_pair = self.generate_qa_pair(instruction)

                if qa_pair:
                    dataset.append(qa_pair)

                    # å®æ—¶ä¿å­˜ï¼ˆé¿å…é•¿æ—¶é—´è¿è¡Œåä¸¢å¤±æ•°æ®ï¼‰
                    with open(output_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(qa_pair, ensure_ascii=False) + '\n')

                # é€Ÿç‡é™åˆ¶
                time.sleep(delay)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nç”Ÿæˆå®Œæˆï¼")
        print(f"æˆåŠŸ: {self.generated_count} æ¡")
        print(f"å¤±è´¥: {self.failed_count} æ¡")
        print(f"æˆåŠŸç‡: {self.generated_count / len(instructions) * 100:.1f}%")
        print(f"æ•°æ®å·²ä¿å­˜è‡³: {output_file}")

        return dataset

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å‡†å¤‡æŒ‡ä»¤åˆ—è¡¨
    instructions = [
        "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
        "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥åˆ¤æ–­ä¸€ä¸ªæ•°æ˜¯å¦ä¸ºè´¨æ•°",
        "åˆ†ææ°”å€™å˜åŒ–å¯¹å†œä¸šçš„å½±å“",
        "åˆ›ä½œä¸€ä¸ªå…³äºå‹è°Šçš„çŸ­ç¯‡æ•…äº‹",
        "æ€»ç»“ã€Šä¸‰ä½“ã€‹ç¬¬ä¸€éƒ¨çš„ä¸»è¦æƒ…èŠ‚",
        # ... æ›´å¤šæŒ‡ä»¤
    ]

    # 2. åˆ›å»ºç”Ÿæˆå™¨
    generator = SyntheticDataGenerator(model="gpt-4")

    # 3. ä¼°ç®—æˆæœ¬ï¼ˆåœ¨ç”Ÿæˆå‰ï¼‰
    cost_estimate = generator.estimate_cost(
        num_samples=len(instructions),
        avg_instruction_tokens=50,
        avg_output_tokens=500
    )
    print("\nğŸ“Š æˆæœ¬ä¼°ç®—:")
    print(f"  æ¨¡å‹: {cost_estimate['model']}")
    print(f"  æ ·æœ¬æ•°: {cost_estimate['num_samples']}")
    print(f"  æ€» tokens: {cost_estimate['total_tokens']:,}")
    print(f"  è¾“å…¥æˆæœ¬: ${cost_estimate['input_cost']}")
    print(f"  è¾“å‡ºæˆæœ¬: ${cost_estimate['output_cost']}")
    print(f"  æ€»æˆæœ¬: ${cost_estimate['total_cost']}")
    print(f"  æ¯æ¡æˆæœ¬: ${cost_estimate['cost_per_sample']}")
    print(f"  é¢„è®¡è€—æ—¶: {cost_estimate['estimated_time_minutes']} åˆ†é’Ÿ\n")

    # 4. æ‰¹é‡ç”Ÿæˆ
    # dataset = generator.batch_generate(
    #     instructions=instructions,
    #     output_file="my_synthetic_data.jsonl",
    #     batch_size=5,
    #     delay=1.0
    # )

    # 4. æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
    # with open("my_synthetic_data.jsonl", 'r', encoding='utf-8') as f:
    #     for line in f:
    #         sample = json.loads(line)
    #         print(f"æŒ‡ä»¤: {sample['instruction']}")
    #         print(f"å›ç­”: {sample['output'][:100]}...\n")
```

**Step 3: è´¨é‡æ§åˆ¶ä¸åå¤„ç†**

```python
"""
åˆæˆæ•°æ®çš„è´¨é‡æ§åˆ¶
åŒ…å«ï¼šé•¿åº¦è¿‡æ»¤ã€é‡å¤æ£€æµ‹ã€æ¯’æ€§æ£€æµ‹
"""
class SyntheticDataQualityControl:
    def __init__(self):
        self.min_output_length = 50  # æœ€çŸ­å›ç­”é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
        self.max_output_length = 2000  # æœ€é•¿å›ç­”é•¿åº¦

    def check_quality(self, sample: Dict) -> tuple[bool, str]:
        """
        æ£€æŸ¥å•æ¡æ ·æœ¬çš„è´¨é‡

        è¿”å›ï¼š
            (æ˜¯å¦é€šè¿‡, å¤±è´¥åŸå› )
        """
        output = sample['output']

        # 1. é•¿åº¦æ£€æŸ¥
        if len(output) < self.min_output_length:
            return False, "output_too_short"
        if len(output) > self.max_output_length:
            return False, "output_too_long"

        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æ‹’ç»å›ç­”çš„æ¨¡å¼
        refusal_patterns = [
            "i cannot", "i can't", "i'm unable to",
            "as an ai", "i don't have access",
            "æŠ±æ­‰", "å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•", "æˆ‘ä¸èƒ½"
        ]
        output_lower = output.lower()
        for pattern in refusal_patterns:
            if pattern in output_lower:
                return False, "refusal_detected"

        # 3. æ£€æŸ¥æ˜¯å¦è¿‡äºç®€çŸ­
        if len(output.split()) < 10:
            return False, "too_few_words"

        return True, "passed"

    def filter_dataset(self, input_file: str, output_file: str) -> Dict:
        """
        æ‰¹é‡è¿‡æ»¤æ•°æ®é›†

        è¿”å›ï¼š
            ç»Ÿè®¡ä¿¡æ¯
        """
        filtered = []
        stats = {
            "total": 0,
            "passed": 0,
            "rejected": 0,
            "reasons": {}
        }

        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                stats['total'] += 1
                sample = json.loads(line)

                passed, reason = self.check_quality(sample)

                if passed:
                    filtered.append(sample)
                    stats['passed'] += 1
                else:
                    stats['rejected'] += 1
                    stats['reasons'][reason] = stats['reasons'].get(reason, 0) + 1

        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in filtered:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        print(f"\nè´¨é‡è¿‡æ»¤å®Œæˆï¼")
        print(f"æ€»è®¡: {stats['total']}")
        print(f"é€šè¿‡: {stats['passed']}")
        print(f"æ‹’ç»: {stats['rejected']}")
        print(f"é€šè¿‡ç‡: {stats['passed'] / stats['total'] * 100:.1f}%")
        print(f"æ‹’ç»åŸå› : {stats['reasons']}")

        return stats

# ä½¿ç”¨ç¤ºä¾‹
# qc = SyntheticDataQualityControl()
# stats = qc.filter_dataset("my_synthetic_data.jsonl", "filtered_data.jsonl")
```

#### 5.3 æˆæœ¬ä¼°ç®—ä¸ä¼˜åŒ–

**æˆæœ¬ä¼°ç®—è¡¨**ï¼š

| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | æ¯æ¡æ•°æ®æˆæœ¬ | 10Kæ•°æ®æ€»æˆæœ¬ |
| :--- | :---: | :---: | :---: | :---: |
| **GPT-4** | $10/1M tokens | $30/1M tokens | ~$0.03 | **$300** |
| **GPT-3.5-turbo** | $0.5/1M tokens | $1.5/1M tokens | ~$0.002 | **$20** |
| **Claude 3 Sonnet** | $3/1M tokens | $15/1M tokens | ~$0.015 | **$150** |

**ä½¿ç”¨ `estimate_cost()` æ–¹æ³•è¿›è¡Œç²¾ç¡®é¢„ä¼°**ï¼š

ä¸Šé¢çš„ `SyntheticDataGenerator` ç±»å·²å†…ç½® `estimate_cost()` æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç”Ÿæˆå‰ç²¾ç¡®è®¡ç®—æˆæœ¬ã€‚

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
# åˆ›å»ºç”Ÿæˆå™¨
generator = SyntheticDataGenerator(model="gpt-4")

# ä¼°ç®—ç”Ÿæˆ 10K æ¡æ•°æ®çš„æˆæœ¬
cost_info = generator.estimate_cost(
    num_samples=10000,
    avg_instruction_tokens=50,   # å¹³å‡æŒ‡ä»¤é•¿åº¦
    avg_output_tokens=500        # å¹³å‡å›ç­”é•¿åº¦
)

print(f"æ¨¡å‹: {cost_info['model']}")
print(f"æ ·æœ¬æ•°: {cost_info['num_samples']:,}")
print(f"æ€» tokens: {cost_info['total_tokens']:,}")
print(f"æ€»æˆæœ¬: ${cost_info['total_cost']}")
print(f"æ¯æ¡æˆæœ¬: ${cost_info['cost_per_sample']}")
print(f"é¢„è®¡è€—æ—¶: {cost_info['estimated_time_minutes']} åˆ†é’Ÿ")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ¨¡å‹: gpt-4
æ ·æœ¬æ•°: 10,000
æ€» tokens: 8,500,000
æ€»æˆæœ¬: $185.0
æ¯æ¡æˆæœ¬: $0.0185
é¢„è®¡è€—æ—¶: 333.3 åˆ†é’Ÿ
```

**ä¸åŒæ¨¡å‹çš„æˆæœ¬å¯¹æ¯”**ï¼š

```python
# å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æˆæœ¬
models = ["gpt-4", "gpt-3.5-turbo", "gpt-4o-mini"]
num_samples = 10000

print("\nğŸ“Š æˆæœ¬å¯¹æ¯”ï¼ˆ10K æ¡æ•°æ®ï¼‰:\n")
print(f"{'æ¨¡å‹':<20} {'æ€»æˆæœ¬':>10} {'æ¯æ¡æˆæœ¬':>12} {'é¢„è®¡è€—æ—¶':>12}")
print("-" * 60)

for model in models:
    gen = SyntheticDataGenerator(model=model)
    cost = gen.estimate_cost(num_samples=num_samples)
    print(f"{model:<20} ${cost['total_cost']:>9.2f} ${cost['cost_per_sample']:>11.4f} {cost['estimated_time_minutes']:>9.1f}åˆ†é’Ÿ")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ“Š æˆæœ¬å¯¹æ¯”ï¼ˆ10K æ¡æ•°æ®ï¼‰:

æ¨¡å‹                    æ€»æˆæœ¬        æ¯æ¡æˆæœ¬          é¢„è®¡è€—æ—¶
------------------------------------------------------------
gpt-4                  $185.00      $0.0185       333.3åˆ†é’Ÿ
gpt-3.5-turbo           $9.25      $0.0009       333.3åˆ†é’Ÿ
gpt-4o-mini             $3.88      $0.0004       333.3åˆ†é’Ÿ
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **åˆ†å±‚ä½¿ç”¨æ¨¡å‹**ï¼š
   ```python
   # ç®€å•ä»»åŠ¡ç”¨ GPT-3.5-turbo
   # å¤æ‚ä»»åŠ¡ç”¨ GPT-4
   def select_model(instruction: str) -> str:
       complex_keywords = ["åˆ†æ", "æ¨ç†", "è®ºè¯", "ä»£ç ", "ç®—æ³•"]
       if any(kw in instruction for kw in complex_keywords):
           return "gpt-4"
       return "gpt-3.5-turbo"
   ```

2. **æ‰¹é‡è¯·æ±‚**ï¼šä½¿ç”¨ OpenAI çš„ Batch APIï¼Œä»·æ ¼å‡åŠ

3. **ç¼“å­˜ä¸­é—´ç»“æœ**ï¼šé¿å…é‡å¤ç”Ÿæˆç›¸åŒç±»å‹çš„æ•°æ®

#### 5.4 å®æˆ˜å»ºè®®

**æ•°æ®é‡å»ºè®®**ï¼š
- **å°è§„æ¨¡ä»»åŠ¡**ï¼ˆå®¢æœã€FAQï¼‰ï¼š1K - 5K æ¡
- **ä¸­ç­‰ä»»åŠ¡**ï¼ˆé€šç”¨å¯¹è¯ï¼‰ï¼š10K - 50K æ¡
- **å¤§è§„æ¨¡ä»»åŠ¡**ï¼ˆå¤šé¢†åŸŸï¼‰ï¼š50K - 500K æ¡

**è´¨é‡ä¿è¯**ï¼š
1. **äººå·¥æŠ½æ ·**ï¼šæ¯ç”Ÿæˆ 1000 æ¡ï¼ŒæŠ½æŸ¥ 50-100 æ¡
2. **A/B æµ‹è¯•**ï¼šç”¨å°‘é‡æ•°æ®è®­ç»ƒï¼Œå¯¹æ¯”äººå·¥æ ‡æ³¨æ•°æ®çš„æ•ˆæœ
3. **å¤šæ ·æ€§æ£€æŸ¥**ï¼šä½¿ç”¨èšç±»åˆ†æï¼Œç¡®ä¿æ•°æ®è¦†ç›–ä¸åŒä¸»é¢˜

**å¸¸è§é™·é˜±**ï¼š
- âŒ ç§å­æŒ‡ä»¤è¿‡äºå•ä¸€ï¼Œå¯¼è‡´ç”Ÿæˆæ•°æ®ç¼ºä¹å¤šæ ·æ€§
- âŒ ä¸åšè´¨é‡è¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰ç”Ÿæˆæ•°æ®
- âŒ å…¨éƒ¨ä½¿ç”¨ GPT-4ï¼Œæˆæœ¬è¿‡é«˜
- âœ… ç»“åˆäººå·¥æ ‡æ³¨ + åˆæˆæ•°æ®ï¼Œå–é•¿è¡¥çŸ­

---

## äº”ã€å°è§„æ¨¡æ•°æ®å»é‡

### 1. ä¸ºä»€ä¹ˆå¾®è°ƒæ•°æ®éœ€è¦å»é‡ï¼šScalability çš„å…³é”®

**é—®é¢˜**ï¼šé‡å¤æ•°æ®ä¼šå¯¼è‡´å››å¤§é—®é¢˜

**é—®é¢˜ 1ï¼šè¿‡æ‹Ÿåˆ**
- æ¨¡å‹æ­»è®°ç¡¬èƒŒé‡å¤æ ·æœ¬ï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™
- åœ¨éªŒè¯é›†ä¸Šè¡¨ç°å¥½ï¼Œå®é™…åº”ç”¨æ•ˆæœå·®

**é—®é¢˜ 2ï¼šè®­ç»ƒåå·®**
- é‡å¤æ ·æœ¬å æ¯”è¿‡é«˜ï¼Œæ¨¡å‹åå‘è¿™äº›æ ·æœ¬
- ä¾‹å¦‚ï¼šå¦‚æœ 30% çš„æ•°æ®éƒ½æ˜¯"è§£é‡Š XXX"ï¼Œæ¨¡å‹ä¼šå€¾å‘äºç”Ÿæˆè§£é‡Šç±»å›ç­”

**é—®é¢˜ 3ï¼šè®¡ç®—æµªè´¹** âš¡ **è¿™æ˜¯æœ€ä¸¥é‡çš„é—®é¢˜!**
- é‡å¤æ•°æ®ä¸æä¾›æ–°ä¿¡æ¯ï¼Œå´æ¶ˆè€—åŒæ ·çš„è®¡ç®—èµ„æº
- å»é‡åå¯ç”¨æ›´å°‘çš„ FLOPs è¾¾åˆ°åŒç­‰æ•ˆæœ

**é—®é¢˜ 4ï¼šå†…å­˜æµªè´¹**
- é‡å¤æ•°æ®å ç”¨å­˜å‚¨å’Œæ˜¾å­˜ï¼Œé™åˆ¶batch size

**å»é‡å¯¹ Scalability çš„å·¨å¤§å½±å“**

æ ¹æ® Lee et al. (2022) çš„è®ºæ–‡ ["Deduplicating Training Data Makes Language Models Better"](https://arxiv.org/abs/2107.06499)ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        DEDUPLICATION â†’ MASSIVE EFFICIENCY GAINS                 â”‚
â”‚              (å»é‡ â†’ æ•ˆç‡çˆ†ç‚¸å¼æå‡)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®éªŒè®¾ç½®:
- æ¨¡å‹: GPT-3 è§„æ¨¡(175B)
- æ•°æ®é›†: C4 (750GB)
- å»é‡æ¯”ä¾‹: ç§»é™¤ ~25% çš„è¿‘ä¼¼é‡å¤æ•°æ®

ç»“æœå¯¹æ¯”:

                  è®­ç»ƒå‰                 å»é‡å
                  â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€
æ•°æ®é‡:           750GB                 560GB (-25%)
è®­ç»ƒæ—¶é—´:         1000 GPU-days         750 GPU-days (-25%)
FLOPs:            2e23                  1.5e23 (-25%)

æ€§èƒ½:             Perplexity: 15.2      Perplexity: 14.8 (â†‘2.6%)
                  â†‘ æ›´å°‘çš„æ•°æ® â†’ æ›´å¥½çš„æ•ˆæœ!

å…³é”®å‘ç°:
âœ… å»é‡åï¼Œåªéœ€ 75% çš„ FLOPs å°±èƒ½è¾¾åˆ°æ›´å¥½çš„æ•ˆæœ
âœ… ç­‰æ•ˆæ€§èƒ½ä¸‹ï¼Œè®­ç»ƒæˆæœ¬é™ä½ 3-5x
âœ… ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡å¹³å‡æå‡ 1-3%
```

**ä¸ºä»€ä¹ˆå»é‡åæ•ˆæœåè€Œæ›´å¥½?**

åŸå› 1: **ä¿¡æ¯å¯†åº¦æå‡**
```
å»é‡å‰: 100Kæ ·æœ¬ï¼Œ30%é‡å¤ â†’ æœ‰æ•ˆä¿¡æ¯åªæœ‰70K
å»é‡å: 70Kæ ·æœ¬ï¼Œ0%é‡å¤ â†’ æœ‰æ•ˆä¿¡æ¯å°±æ˜¯70K
ä¿¡æ¯å¯†åº¦: 70% â†’ 100%
```

åŸå› 2: **æ¢¯åº¦æ›´æ–°è´¨é‡æå‡**
- é‡å¤æ ·æœ¬å¯¼è‡´æ¢¯åº¦è¢«åŒä¸€ä¿¡æ¯ä¸»å¯¼
- å»é‡åæ¯ä¸ªbatchåŒ…å«æ›´å¤šæ ·åŒ–çš„ä¿¡æ¯
- æ¨¡å‹å­¦ä¹ æ›´å‡è¡¡ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼º

**å»é‡ç­–ç•¥**ï¼š
- **ç²¾ç¡®å»é‡**ï¼šç§»é™¤å®Œå…¨ç›¸åŒçš„æ ·æœ¬ï¼ˆåŸºäºå“ˆå¸Œï¼‰
- **æ¨¡ç³Šå»é‡**ï¼šç§»é™¤é«˜åº¦ç›¸ä¼¼çš„æ ·æœ¬ï¼ˆåŸºäº MinHashï¼‰

**å®è·µå»ºè®®**:
- **å°è§„æ¨¡æ•°æ®**(< 10K): ç²¾ç¡®å»é‡å³å¯ï¼Œæˆæœ¬å¯å¿½ç•¥
- **ä¸­ç­‰è§„æ¨¡**(10K-100K): ä½¿ç”¨ MinHashï¼Œå»é‡ç‡é€šå¸¸ 15-30%
- **å¤§è§„æ¨¡æ•°æ®**(> 100K): å¿…é¡»å»é‡ï¼Œå¦åˆ™è®­ç»ƒæ•ˆç‡æä½

### 2. MinHash å»é‡åŸç†ï¼šæ–‡æ¡£"æŒ‡çº¹è¯†åˆ«"æŠ€æœ¯

**æ ¸å¿ƒæ€æƒ³**ï¼šMinHash ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆå›ºå®šé•¿åº¦çš„"ç­¾å"(æŒ‡çº¹)ï¼Œä½¿å¾—ç›¸ä¼¼æ–‡æ¡£çš„ç­¾åä¹Ÿç›¸ä¼¼ã€‚

**å½¢è±¡åŒ–ç†è§£ï¼šæ–‡æ¡£æŒ‡çº¹åŒ–**

æƒ³è±¡ä½ è¦è¯†åˆ«æµ·é‡æ–‡æ¡£ä¸­çš„é‡å¤å†…å®¹ã€‚ç›´æ¥é€å­—æ¯”è¾ƒå¤ªæ…¢äº†ï¼MinHash å°±åƒç»™æ¯ä¸ªæ–‡æ¡£"æŒ‰æŒ‡çº¹":

```
åŸå§‹æ–‡æ¡£ A:                          MinHash ç­¾å:
"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯..."         [0x3F, 0xA2, 0x1B, ...]
â”‚                                    â”‚
â”œâ”€> Shingling (åˆ‡è¯)                 â”‚  åªéœ€æ¯”è¾ƒçŸ­ç­¾å
â”‚   {"æœºå™¨", "å­¦ä¹ ", "æ˜¯äºº", ...}      â”‚  (128ç»´) è€Œé
â”‚                                    â”‚  å®Œæ•´æ–‡æ¡£!
â”œâ”€> Hash æ˜ å°„                        â”‚
â”‚   {h1(æœºå™¨)=0x3F, ...}             â”‚
â”‚                                    â”‚
â””â”€> å–æœ€å°å€¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [æŒ‡çº¹]


æ–‡æ¡£ B (é«˜åº¦ç›¸ä¼¼):                    MinHash ç­¾å:
"æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªåˆ†æ”¯..."           [0x3F, 0xA2, 0x1C, ...]
                                    â†‘ éå¸¸æ¥è¿‘!

æ–‡æ¡£ C (å®Œå…¨ä¸åŒ):                    MinHash ç­¾å:
"ä»Šå¤©å¤©æ°”å¾ˆå¥½..."                    [0x8D, 0x5F, 0xC2, ...]
                                    â†‘ å®Œå…¨ä¸åŒ!
```

**MinHash ç­¾åçš„ç¥å¥‡æ€§è´¨**:

```
å¦‚æœä¸¤ä¸ªæ–‡æ¡£ç›¸ä¼¼åº¦ä¸º 80%ï¼Œ
é‚£ä¹ˆå®ƒä»¬çš„ MinHash ç­¾åæœ‰ 80% çš„ä½ç›¸åŒï¼

Jaccardç›¸ä¼¼åº¦(A,B) = MinHashç­¾åç›¸ä¼¼åº¦(A,B)
```

è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ï¼š
1. **å¿«é€Ÿè¿‡æ»¤**: åªæ¯”è¾ƒç­¾å(128ç»´)ï¼Œä¸æ¯”è¾ƒå…¨æ–‡(æ•°åƒç»´)
2. **è¿‘ä¼¼å‡†ç¡®**: ç›¸ä¼¼åº¦ä¼°è®¡è¯¯å·® < 5%
3. **å¯æ‰©å±•**: å¯å¤„ç†æ•°ç™¾ä¸‡æ–‡æ¡£

**æ•°å­¦åŸç†**ï¼š

ç»™å®šä¸¤ä¸ªæ–‡æ¡£ $A$ å’Œ $B$ï¼Œå®ƒä»¬çš„ Jaccard ç›¸ä¼¼åº¦å®šä¹‰ä¸ºï¼š

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

MinHash çš„æ ¸å¿ƒæ€§è´¨ï¼š

$$
P(\text{MinHash}(A)_i = \text{MinHash}(B)_i) = J(A, B)
$$

å³ï¼šMinHash ç­¾åçš„æŸä¸€ä½ç›¸åŒçš„æ¦‚ç‡ï¼Œç­‰äº Jaccard ç›¸ä¼¼åº¦ã€‚

**ç®—æ³•æ­¥éª¤**ï¼š

1. **Shingling**ï¼šå°†æ–‡æ¡£è½¬æ¢ä¸º n-gram é›†åˆ
   ```python
   # ç¤ºä¾‹ï¼š3-gram
   "the quick brown" â†’ {"the", "he ", "e q", " qu", "qui", "uic", "ick", ...}
   ```

2. **MinHash ç­¾å**ï¼šä½¿ç”¨ $k$ ä¸ªå“ˆå¸Œå‡½æ•°
   ```python
   for i in range(k):
       sig[i] = min(hash_i(shingle) for shingle in shingles)
   ```

3. **ç›¸ä¼¼åº¦ä¼°è®¡**ï¼š
   $$
   \hat{J}(A, B) = \frac{1}{k} \sum_{i=1}^k \mathbb{1}[\text{sig}_A[i] = \text{sig}_B[i]]
   $$

### 3. ä»£ç å®æˆ˜ï¼šMinHash ç®€å•å®ç°

**é€‚ç”¨äºå°è§„æ¨¡å¾®è°ƒæ•°æ®ï¼ˆ< 100Kï¼‰**ï¼š

```python
"""
MinHash å»é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼šå°è§„æ¨¡æ•°æ®é›†çš„æ¨¡ç³Šå»é‡
ä¾èµ–ï¼špip install datasketch
"""
from datasketch import MinHash, MinHashLSH
from typing import List, Dict, Set
import json

class SimpleDeduplicator:
    def __init__(self, threshold: float = 0.8, num_perm: int = 128):
        """
        åˆå§‹åŒ–å»é‡å™¨

        å‚æ•°ï¼š
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.8 è¡¨ç¤º 80% ç›¸ä¼¼å³è§†ä¸ºé‡å¤ï¼‰
            num_perm: MinHash ç­¾åé•¿åº¦
        """
        self.threshold = threshold
        self.num_perm = num_perm

    def _tokenize(self, text: str) -> Set[str]:
        """åˆ†è¯ï¼ˆç®€å•ç‰ˆï¼šæŒ‰ç©ºæ ¼åˆ†è¯ï¼‰"""
        return set(text.lower().split())

    def _compute_minhash(self, text: str) -> MinHash:
        """è®¡ç®— MinHash ç­¾å"""
        m = MinHash(num_perm=self.num_perm)
        tokens = self._tokenize(text)

        for token in tokens:
            m.update(token.encode('utf-8'))

        return m

    def deduplicate(self, dataset: List[Dict]) -> List[Dict]:
        """
        å»é‡æ•°æ®é›†

        å‚æ•°ï¼š
            dataset: Alpaca æ ¼å¼æ•°æ®é›†

        è¿”å›ï¼š
            å»é‡åçš„æ•°æ®é›†
        """
        lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)

        unique_data = []
        seen_indices = set()

        # ç¬¬ä¸€è½®ï¼šå»ºç«‹ LSH ç´¢å¼•
        print("å»ºç«‹ LSH ç´¢å¼•...")
        minhashes = []
        for idx, sample in enumerate(dataset):
            # æ‹¼æ¥ instruction å’Œ output ä½œä¸ºå»é‡ä¾æ®
            combined_text = f"{sample['instruction']} {sample['output']}"
            mh = self._compute_minhash(combined_text)
            minhashes.append(mh)

        # ç¬¬äºŒè½®ï¼šæŸ¥æ‰¾é‡å¤
        print("æŸ¥æ‰¾é‡å¤æ ·æœ¬...")
        for idx, mh in enumerate(minhashes):
            if idx in seen_indices:
                continue

            doc_id = f"doc_{idx}"

            # æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£
            candidates = lsh.query(mh)

            if len(candidates) == 0:
                # ç¬¬ä¸€æ¬¡è§åˆ°è¯¥æ–‡æ¡£
                lsh.insert(doc_id, mh)
                unique_data.append(dataset[idx])
            else:
                # å·²æœ‰ç›¸ä¼¼æ–‡æ¡£ï¼Œè·³è¿‡
                seen_indices.add(idx)

        return unique_data

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®ï¼ˆåŒ…å«é‡å¤ï¼‰
    test_data = [
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            "input": "",
            "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
        },
        {
            "instruction": "è§£é‡Šæœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆ",
            "input": "",
            "output": "æœºå™¨å­¦ä¹ æ˜¯ AI çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚"  # é«˜åº¦ç›¸ä¼¼
        },
        {
            "instruction": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "input": "",
            "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚"
        },
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",  # å®Œå…¨é‡å¤
            "input": "",
            "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯,é€šè¿‡ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
        }
    ]

    # å»é‡
    deduper = SimpleDeduplicator(threshold=0.8)
    unique_data = deduper.deduplicate(test_data)

    print(f"\nåŸå§‹æ•°æ®: {len(test_data)} æ¡")
    print(f"å»é‡å: {len(unique_data)} æ¡")
    print(f"ç§»é™¤: {len(test_data) - len(unique_data)} æ¡é‡å¤\n")

    print("å»é‡åçš„æ•°æ®:")
    for i, sample in enumerate(unique_data, 1):
        print(f"{i}. {sample['instruction']}")
```

**æ€§èƒ½å»ºè®®**ï¼š
- å¯¹äº < 10K æ•°æ®ï¼šç›´æ¥ä¸¤ä¸¤æ¯”è¾ƒå³å¯
- å¯¹äº 10K - 100K æ•°æ®ï¼šä½¿ç”¨ MinHash + LSH
- å¯¹äº > 100K æ•°æ®ï¼šå‚è€ƒ [Part 7 ç¬¬6ç« ] çš„å¤§è§„æ¨¡å»é‡æ–¹æ¡ˆ

---

## å…­ã€æ•°æ®å¢å¼ºæŠ€æœ¯

### 1. å›è¯‘ï¼ˆBack-translationï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ï¼Œå†ç¿»è¯‘å›æ¥ï¼Œå¾—åˆ°è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„æ•°æ®ã€‚

```python
"""
å›è¯‘æ•°æ®å¢å¼º
ä¾èµ–ï¼špip install googletrans==4.0.0-rc1
"""
from googletrans import Translator

class BackTranslator:
    def __init__(self):
        self.translator = Translator()

    def augment(self, text: str, intermediate_lang: str = 'fr') -> str:
        """
        å›è¯‘å¢å¼º

        å‚æ•°ï¼š
            text: åŸå§‹æ–‡æœ¬
            intermediate_lang: ä¸­é—´è¯­è¨€ï¼ˆfr=æ³•è¯­, de=å¾·è¯­, es=è¥¿ç­ç‰™è¯­ï¼‰

        è¿”å›ï¼š
            å›è¯‘åçš„æ–‡æœ¬
        """
        # è‹±è¯­ -> ä¸­é—´è¯­è¨€
        translated = self.translator.translate(text, dest=intermediate_lang)

        # ä¸­é—´è¯­è¨€ -> è‹±è¯­
        back_translated = self.translator.translate(translated.text, dest='en')

        return back_translated.text

# ä½¿ç”¨ç¤ºä¾‹
# translator = BackTranslator()
# original = "Machine learning is a subset of artificial intelligence."
# augmented = translator.augment(original, intermediate_lang='fr')
# print(f"åŸå§‹: {original}")
# print(f"å¢å¼º: {augmented}")
```

### 2. åŒä¹‰è¯æ›¿æ¢

**æ ¸å¿ƒæ€æƒ³**ï¼šéšæœºæ›¿æ¢æ–‡æœ¬ä¸­çš„è¯æ±‡ä¸ºåŒä¹‰è¯ã€‚

```python
"""
åŒä¹‰è¯æ›¿æ¢
ä¾èµ–ï¼špip install nltk
"""
import nltk
from nltk.corpus import wordnet
import random

# ä¸‹è½½ WordNet
# nltk.download('wordnet')
# nltk.download('omw-1.4')

class SynonymReplacer:
    def __init__(self, replace_ratio: float = 0.1):
        """
        å‚æ•°ï¼š
            replace_ratio: æ›¿æ¢æ¯”ä¾‹ï¼ˆ0.1 è¡¨ç¤ºæ›¿æ¢ 10% çš„è¯ï¼‰
        """
        self.replace_ratio = replace_ratio

    def get_synonyms(self, word: str) -> List[str]:
        """è·å–åŒä¹‰è¯"""
        synonyms = set()

        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonym = lemma.name().replace('_', ' ')
                if synonym != word:
                    synonyms.add(synonym)

        return list(synonyms)

    def augment(self, text: str) -> str:
        """åŒä¹‰è¯æ›¿æ¢å¢å¼º"""
        words = text.split()
        num_replace = max(1, int(len(words) * self.replace_ratio))

        # éšæœºé€‰æ‹©è¦æ›¿æ¢çš„ä½ç½®
        replace_indices = random.sample(range(len(words)), min(num_replace, len(words)))

        for idx in replace_indices:
            word = words[idx]
            synonyms = self.get_synonyms(word.lower())

            if synonyms:
                words[idx] = random.choice(synonyms)

        return ' '.join(words)

# ä½¿ç”¨ç¤ºä¾‹
# replacer = SynonymReplacer(replace_ratio=0.2)
# original = "Machine learning is a powerful technique for data analysis."
# augmented = replacer.augment(original)
# print(f"åŸå§‹: {original}")
# print(f"å¢å¼º: {augmented}")
```

### 3. ä»£ç å®æˆ˜ï¼šæ•°æ®å¢å¼ºå·¥å…·

**æ•´åˆå¤šç§å¢å¼ºç­–ç•¥**ï¼š

```python
"""
ç»¼åˆæ•°æ®å¢å¼ºå·¥å…·
"""
class DataAugmenter:
    def __init__(self):
        self.back_translator = BackTranslator()
        self.synonym_replacer = SynonymReplacer()

    def augment_dataset(self, dataset: List[Dict], target_size: int) -> List[Dict]:
        """
        æ‰©å……æ•°æ®é›†åˆ°ç›®æ ‡å¤§å°

        å‚æ•°ï¼š
            dataset: åŸå§‹æ•°æ®é›†
            target_size: ç›®æ ‡æ•°æ®é›†å¤§å°

        è¿”å›ï¼š
            æ‰©å……åçš„æ•°æ®é›†
        """
        augmented = list(dataset)  # å¤åˆ¶åŸå§‹æ•°æ®

        while len(augmented) < target_size:
            # éšæœºé€‰æ‹©ä¸€æ¡æ ·æœ¬
            sample = random.choice(dataset)

            # éšæœºé€‰æ‹©å¢å¼ºç­–ç•¥
            strategy = random.choice(['back_translation', 'synonym'])

            try:
                if strategy == 'back_translation':
                    aug_instruction = self.back_translator.augment(sample['instruction'])
                else:
                    aug_instruction = self.synonym_replacer.augment(sample['instruction'])

                # åˆ›å»ºå¢å¼ºæ ·æœ¬
                aug_sample = {
                    "instruction": aug_instruction,
                    "input": sample['input'],
                    "output": sample['output']  # è¾“å‡ºä¿æŒä¸å˜
                }

                augmented.append(aug_sample)

            except Exception as e:
                continue  # å¢å¼ºå¤±è´¥ï¼Œè·³è¿‡

        return augmented[:target_size]
```

**æ³¨æ„äº‹é¡¹**ï¼š
1. æ•°æ®å¢å¼ºä¸åº”æ”¹å˜è¯­ä¹‰
2. å¢å¼ºåçš„æ•°æ®è´¨é‡å¯èƒ½ä¸‹é™ï¼Œéœ€è¦äººå·¥æŠ½æŸ¥
3. å¯¹äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œä¸å»ºè®®ä½¿ç”¨åŒä¹‰è¯æ›¿æ¢

---

## ä¸ƒã€SFT æ•°æ®é›†æ„å»ºå®æˆ˜

### 1. å®Œæ•´ Pipelineï¼šä»åŸå§‹æ–‡æœ¬åˆ° Alpaca æ ¼å¼

**åœºæ™¯**ï¼šå°†ä¸€æ‰¹æŠ€æœ¯æ–‡æ¡£è½¬æ¢ä¸ºé—®ç­”å¯¹ï¼Œç”¨äºå¾®è°ƒæ¨¡å‹ã€‚

**Pipeline**ï¼š

$$
\begin{aligned}
&\text{åŸå§‹æ–‡æ¡£} \\
&\downarrow \text{1. æ–‡æ¡£åˆ†å—} \\
&\text{æ–‡æ¡£æ®µè½} \\
&\downarrow \text{2. ç”Ÿæˆé—®ç­”å¯¹ï¼ˆGPT-4ï¼‰} \\
&\text{(Question, Answer) å¯¹} \\
&\downarrow \text{3. æ ¼å¼åŒ–ä¸º Alpaca} \\
&\text{Alpaca æ•°æ®é›†} \\
&\downarrow \text{4. è´¨é‡è¿‡æ»¤} \\
&\text{é«˜è´¨é‡æ•°æ®é›†} \\
&\downarrow \text{5. å»é‡} \\
&\text{æœ€ç»ˆæ•°æ®é›†}
\end{aligned}
$$

### 2. ä»£ç å®æˆ˜ï¼šç«¯åˆ°ç«¯æ•°æ®æ„å»º

```python
"""
ç«¯åˆ°ç«¯ SFT æ•°æ®æ„å»º Pipeline
åŠŸèƒ½ï¼šä»åŸå§‹æ–‡æ¡£åˆ° Alpaca æ ¼å¼çš„å®Œæ•´æµç¨‹
"""
import os
import json
from typing import List, Dict
from openai import OpenAI

class SFTDatasetBuilder:
    def __init__(self, api_key: str = None):
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.quality_filter = QualityFilter(enable_toxicity=False, enable_pii=False)
        self.deduplicator = SimpleDeduplicator(threshold=0.85)

    def chunk_document(self, document: str, chunk_size: int = 500) -> List[str]:
        """
        æ–‡æ¡£åˆ†å—

        å‚æ•°ï¼š
            document: åŸå§‹æ–‡æ¡£
            chunk_size: æ¯å—çš„è¯æ•°

        è¿”å›ï¼š
            æ–‡æ¡£æ®µè½åˆ—è¡¨
        """
        words = document.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks

    def generate_qa_from_chunk(self, chunk: str, num_qa: int = 3) -> List[Dict]:
        """
        ä»æ–‡æ¡£æ®µè½ç”Ÿæˆé—®ç­”å¯¹

        å‚æ•°ï¼š
            chunk: æ–‡æ¡£æ®µè½
            num_qa: ç”Ÿæˆé—®ç­”å¯¹çš„æ•°é‡

        è¿”å›ï¼š
            é—®ç­”å¯¹åˆ—è¡¨
        """
        prompt = f"""Based on the following text, generate {num_qa} diverse question-answer pairs. The questions should cover different aspects and difficulty levels.

Text:
{chunk}

Generate {num_qa} Q&A pairs in JSON format:
[
  {{"question": "...", "answer": "..."}},
  ...
]

JSON:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1500
            )

            # è§£æ JSON
            qa_pairs = json.loads(response.choices[0].message.content)
            return qa_pairs

        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            return []

    def convert_to_alpaca(self, qa_pairs: List[Dict]) -> List[Dict]:
        """
        è½¬æ¢ä¸º Alpaca æ ¼å¼

        å‚æ•°ï¼š
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨ [{"question": "...", "answer": "..."}]

        è¿”å›ï¼š
            Alpaca æ ¼å¼æ•°æ®
        """
        alpaca_data = []

        for qa in qa_pairs:
            sample = {
                "instruction": qa['question'],
                "input": "",
                "output": qa['answer']
            }
            alpaca_data.append(sample)

        return alpaca_data

    def build_from_document(self, document: str, output_file: str = "sft_dataset.jsonl"):
        """
        ä»å•ä¸ªæ–‡æ¡£æ„å»ºæ•°æ®é›†

        å‚æ•°ï¼š
            document: åŸå§‹æ–‡æ¡£
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print("Step 1: æ–‡æ¡£åˆ†å—...")
        chunks = self.chunk_document(document)
        print(f"  å…± {len(chunks)} ä¸ªæ®µè½")

        print("\nStep 2: ç”Ÿæˆé—®ç­”å¯¹...")
        all_qa_pairs = []
        for i, chunk in enumerate(chunks):
            print(f"  å¤„ç†æ®µè½ {i+1}/{len(chunks)}...")
            qa_pairs = self.generate_qa_from_chunk(chunk, num_qa=2)
            all_qa_pairs.extend(qa_pairs)
        print(f"  å…±ç”Ÿæˆ {len(all_qa_pairs)} å¯¹é—®ç­”")

        print("\nStep 3: è½¬æ¢ä¸º Alpaca æ ¼å¼...")
        alpaca_data = self.convert_to_alpaca(all_qa_pairs)

        print("\nStep 4: è´¨é‡è¿‡æ»¤...")
        stats, filtered_data = self.quality_filter.filter_dataset(alpaca_data)
        print(f"  é€šè¿‡: {stats['passed']}/{stats['total']}")

        print("\nStep 5: å»é‡...")
        final_data = self.deduplicator.deduplicate(filtered_data)
        print(f"  æœ€ç»ˆæ•°æ®: {len(final_data)} æ¡")

        # ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in final_data:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        print(f"\næ•°æ®é›†å·²ä¿å­˜è‡³ {output_file}")
        return final_data

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡æ¡£
    sample_document = """
    Transformer æ˜¯ä¸€ç§é©å‘½æ€§çš„ç¥ç»ç½‘ç»œæ¶æ„,ç”± Google åœ¨ 2017 å¹´æå‡ºã€‚
    å®ƒå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶,æ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)å’Œå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€‚
    Transformer çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº Self-Attention æœºåˆ¶,ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®,
    å¤§å¤§æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚è¿™ä¸€æ¶æ„åæ¥æˆä¸º BERTã€GPT ç­‰å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚

    Transformer åŒ…å«ç¼–ç å™¨(Encoder)å’Œè§£ç å™¨(Decoder)ä¸¤ä¸ªä¸»è¦ç»„ä»¶ã€‚
    ç¼–ç å™¨è´Ÿè´£ç†è§£è¾“å…¥åºåˆ—,è€Œè§£ç å™¨è´Ÿè´£ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚
    æ¯ä¸ªç¼–ç å™¨å±‚åŒ…å« Multi-Head Self-Attention å’Œ Feed-Forward Network ä¸¤ä¸ªå­å±‚ã€‚
    é€šè¿‡å †å å¤šå±‚ç¼–ç å™¨å’Œè§£ç å™¨,Transformer èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„è¯­è¨€æ¨¡å¼ã€‚

    è‡ª 2017 å¹´ä»¥æ¥,Transformer å·²ç»ä¸»å¯¼äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸ,å¹¶æ‰©å±•åˆ°è®¡ç®—æœºè§†è§‰ã€
    è¯­éŸ³è¯†åˆ«ç­‰å¤šä¸ªé¢†åŸŸã€‚å®ƒçš„æˆåŠŸè¯æ˜äº†æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§èƒ½åŠ›,
    ä¹Ÿä¸ºåç»­çš„æ¨¡å‹åˆ›æ–°å¥ å®šäº†åŸºç¡€ã€‚
    """

    # æ„å»ºæ•°æ®é›†
    builder = SFTDatasetBuilder()
    # dataset = builder.build_from_document(sample_document, output_file="transformer_sft.jsonl")

    # æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
    # with open("transformer_sft.jsonl", 'r', encoding='utf-8') as f:
    #     for line in f:
    #         sample = json.loads(line)
    #         print(f"\næŒ‡ä»¤: {sample['instruction']}")
    #         print(f"å›ç­”: {sample['output'][:100]}...")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Step 1: æ–‡æ¡£åˆ†å—...
  å…± 1 ä¸ªæ®µè½

Step 2: ç”Ÿæˆé—®ç­”å¯¹...
  å¤„ç†æ®µè½ 1/1...
  å…±ç”Ÿæˆ 2 å¯¹é—®ç­”

Step 3: è½¬æ¢ä¸º Alpaca æ ¼å¼...

Step 4: è´¨é‡è¿‡æ»¤...
  é€šè¿‡: 2/2

Step 5: å»é‡...
  æœ€ç»ˆæ•°æ®: 2 æ¡

æ•°æ®é›†å·²ä¿å­˜è‡³ transformer_sft.jsonl
```

**å®æˆ˜å»ºè®®**ï¼š
1. **èµ·æ­¥é˜¶æ®µ**ï¼šå…ˆæ„å»º 1K é«˜è´¨é‡æ•°æ®ï¼Œå¿«é€ŸéªŒè¯æ•ˆæœ
2. **æ‰©å±•é˜¶æ®µ**ï¼šé€æ­¥æ‰©å±•åˆ° 10K - 50Kï¼ŒæŒç»­è¯„ä¼°è´¨é‡
3. **å¹³è¡¡ç­–ç•¥**ï¼šç¡®ä¿ä»»åŠ¡ç±»å‹å¤šæ ·æ€§ï¼ˆé—®ç­”ã€æ¨ç†ã€ä»£ç ã€ç¿»è¯‘ï¼‰
4. **äººå·¥å®¡æ ¸**ï¼šå®šæœŸæŠ½æ ·æ£€æŸ¥ç”Ÿæˆæ•°æ®çš„è´¨é‡

---

## å…«ã€ç‰¹å®šä»»åŠ¡çš„æ•°æ®æ„é€ å®æˆ˜

é€šç”¨æŒ‡ä»¤æ•°æ®æ˜¯åŸºç¡€ï¼Œä½†ä¸ºäº†è®©æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½ éœ€è¦æ„é€ ä¸“ç”¨çš„æŠ€èƒ½æ•°æ®ã€‚

### 1. æ¡ˆä¾‹ä¸€ï¼šæƒ…æ„Ÿåˆ†æ (Classification)

**ç›®æ ‡**ï¼šè¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œè¾“å‡ºæƒ…æ„Ÿæ ‡ç­¾ï¼ˆPositive/Negative/Neutralï¼‰ã€‚

**æ•°æ®æ„é€ æŠ€å·§**ï¼š
- **æ ‡ç­¾å¹³è¡¡**ï¼šç¡®ä¿å„ç±»åˆ«æ ·æœ¬æ•°é‡å¤§è‡´ç›¸ç­‰ã€‚
- **éš¾åº¦åˆ†å±‚**ï¼š
  - L1 (ç®€å•): "æˆ‘å–œæ¬¢è¿™éƒ¨ç”µå½±" -> Positive
  - L2 (éšæ™¦): "è¿™ç”µå½±è®©æˆ‘çœ‹äº†ä¸‰æ¬¡è¡¨" -> Negative
  - L3 (æ··åˆ): "ç‰¹æ•ˆè™½ç„¶ä¸€èˆ¬ï¼Œä½†å‰§æƒ…çœŸçš„å¾ˆæŠ“äºº" -> Positive

**æ„é€  Prompt ç¤ºä¾‹**ï¼š
```python
prompt = """Generate 5 diverse sentiment analysis examples.
Format: JSON
Requirements:
1. Cover distinct domains (product review, financial news, social media)
2. Include mixed sentiments (but clear overall label)
3. Output format: {"text": "...", "label": "Positive/Negative"}
"""
```

### 2. æ¡ˆä¾‹äºŒï¼šä»£ç ç”Ÿæˆ (Coding)

**ç›®æ ‡**ï¼šè¾“å…¥éœ€æ±‚ï¼Œè¾“å‡ºPythonä»£ç ã€‚

**æ•°æ®æ„é€ æŠ€å·§**ï¼š
- **åŒ…å«æµ‹è¯•ç”¨ä¾‹**ï¼šè¿™æ˜¯DeepSeek-Coder-V2çš„æ ¸å¿ƒç­–ç•¥ã€‚è®©æ¨¡å‹å­¦ä¼š"å†™ä»£ç  + å†™å•æµ‹"ã€‚
- **ä¾èµ–åº“è¯´æ˜**ï¼šæ˜ç¡®æŒ‡å®šä½¿ç”¨çš„åº“ã€‚

**æ ·æœ¬ç¤ºä¾‹**ï¼š
```json
{
  "instruction": "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—ç§»åŠ¨å¹³å‡å€¼",
  "input": "",
  "output": "```python\ndef moving_average(data, window_size):\n    # ...code...\n    return averages\n\n# Test Case\nassert moving_average([1,2,3], 2) == [1.5, 2.5]\n```"
}
```

### 3. æ¡ˆä¾‹ä¸‰ï¼šæ•°å­¦æ¨ç† (Math/CoT)

**ç›®æ ‡**ï¼šè§£å†³æ•°å­¦åº”ç”¨é¢˜ã€‚

**æ•°æ®æ„é€ æŠ€å·§**ï¼š
- **Chain-of-Thought (CoT)**ï¼šå¿…é¡»è¦æ±‚æ¨¡å‹è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™ç­”æ¡ˆã€‚
- **è¿‡ç¨‹éªŒè¯**ï¼šå¦‚æœå¯èƒ½ï¼Œç”¨Pythonä»£ç éªŒè¯è®¡ç®—ç»“æœã€‚

**æ ·æœ¬ç¤ºä¾‹**ï¼š
```json
{
  "instruction": "å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œç»™äº†å°çº¢2ä¸ªï¼Œåˆä¹°æ¥3ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ",
  "output": "æ€è€ƒè¿‡ç¨‹ï¼š\n1. åˆå§‹çŠ¶æ€ï¼š5ä¸ªè‹¹æœ\n2. ç»™å‡º2ä¸ªï¼š5 - 2 = 3ä¸ª\n3. ä¹°å…¥3ä¸ªï¼š3 + 3 = 6ä¸ª\n\nç­”æ¡ˆï¼š6ä¸ª"
}
```

---

## ä¹ã€æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Data-Centric AI ç†å¿µ**ï¼šåœ¨å¾®è°ƒé˜¶æ®µï¼Œæ•°æ®è´¨é‡ > æ•°æ®é‡
   - 10K é«˜è´¨é‡æ•°æ® > 100K ä½è´¨é‡æ•°æ®
   - Phi-3ã€Alpaca çš„æˆåŠŸéƒ½è¯æ˜äº†è¿™ä¸€ç‚¹

2. **æ•°æ®æ ¼å¼æ ‡å‡†åŒ–**ï¼š
   - **Alpaca æ ¼å¼**ï¼šé€‚åˆå•è½®æŒ‡ä»¤ä»»åŠ¡
   - **ShareGPT æ ¼å¼**ï¼šé€‚åˆå¤šè½®å¯¹è¯

3. **Self-Instruct æ˜¯æ ¸å¿ƒæŠ€æœ¯**ï¼š
   - ç”¨ GPT-4 ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤-å›ç­”å¯¹
   - Evol-Instruct é€šè¿‡å¤šè½®è¿›åŒ–æå‡æ•°æ®å¤æ‚åº¦

4. **è´¨é‡è¿‡æ»¤å¿…ä¸å¯å°‘**ï¼š
   - åŸºäºè§„åˆ™çš„è¿‡æ»¤ï¼šé•¿åº¦ã€é‡å¤ã€å®Œæ•´æ€§
   - æ¯’æ€§æ£€æµ‹ï¼šä½¿ç”¨ detoxify
   - PII è„±æ•ï¼šä½¿ç”¨ presidio

5. **å°è§„æ¨¡å»é‡**ï¼š
   - MinHash + LSH é€‚ç”¨äº 10K - 100K æ•°æ®
   - å¤§è§„æ¨¡å»é‡è¯¦è§ [Part 7 ç¬¬6ç« ]

6. **å®Œæ•´ Pipeline**ï¼š
   åŸå§‹æ–‡æ¡£ â†’ åˆ†å— â†’ ç”Ÿæˆé—®ç­” â†’ æ ¼å¼åŒ– â†’ è´¨é‡è¿‡æ»¤ â†’ å»é‡ â†’ æœ€ç»ˆæ•°æ®é›†

**ä¸‹ä¸€æ­¥**ï¼š
- æœ‰äº†é«˜è´¨é‡æ•°æ®åï¼Œä¸‹ä¸€ç« å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œå¾®è°ƒï¼ˆè¯¦è§ [ç¬¬2ç« _å¾®è°ƒä½ çš„ä¸“å±æ¨¡å‹](ç¬¬2ç« _å¾®è°ƒä½ çš„ä¸“å±æ¨¡å‹.md)ï¼‰

---

## å‚è€ƒèµ„æº

**è®ºæ–‡**ï¼š
- **Self-Instruct**: [Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560) (Stanford, 2023)
- **Alpaca**: [Stanford Alpaca: An Instruction-following LLaMA Model](https://github.com/tatsu-lab/stanford_alpaca)
- **WizardLM**: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)
- **Phi-3**: [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) (Microsoft, 2024)

**å·¥å…·åº“**ï¼š
- **OpenAI API**: [https://platform.openai.com/docs](https://platform.openai.com/docs)
- **datasketch**: [https://github.com/ekzhu/datasketch](https://github.com/ekzhu/datasketch) (MinHash å»é‡)
- **detoxify**: [https://github.com/unitaryai/detoxify](https://github.com/unitaryai/detoxify) (æ¯’æ€§æ£€æµ‹)
- **presidio**: [https://github.com/microsoft/presidio](https://github.com/microsoft/presidio) (PII è„±æ•)

**æ•°æ®é›†**ï¼š
- **Alpaca-52K**: [https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)
- **ShareGPT**: [https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)
- **WizardLM Dataset**: [https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)

> ä¸‹ä¸€ç« ï¼š[ç¬¬2ç« _å¾®è°ƒä½ çš„ä¸“å±æ¨¡å‹](ç¬¬2ç« _å¾®è°ƒä½ çš„ä¸“å±æ¨¡å‹.md) å°†è¯¦ç»†è®²è§£ LoRAã€QLoRA ç­‰é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ã€‚
