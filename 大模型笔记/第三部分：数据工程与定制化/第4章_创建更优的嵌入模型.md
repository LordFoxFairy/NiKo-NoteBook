# ç¬¬4ç« ï¼šåˆ›å»ºæ›´ä¼˜çš„åµŒå…¥æ¨¡å‹ (Embedding Models)

> "Good representations are the foundation of AI. Without quality embeddings, even the smartest models fail." - Ilya Sutskever
>
> æœ¬ç« å°†æ·±å…¥æ¢è®¨å¦‚ä½•ä»é›¶è®­ç»ƒé«˜è´¨é‡åµŒå…¥æ¨¡å‹ï¼ŒæŒæ¡å¯¹æ¯”å­¦ä¹ ã€éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œæ‰“é€ ä¸“å±äºä½ é¢†åŸŸçš„è¯­ä¹‰æ£€ç´¢å¼•æ“ã€‚

---

## ç›®å½•

- [ç¬¬ä¸€èŠ‚ï¼šåµŒå…¥æ¨¡å‹çš„æœ¬è´¨ä¸æ¶æ„](#ç¬¬ä¸€èŠ‚åµŒå…¥æ¨¡å‹çš„æœ¬è´¨ä¸æ¶æ„)
  - [1.1 ä¸ºä»€ä¹ˆéœ€è¦åµŒå…¥æ¨¡å‹ï¼Ÿ](#11-ä¸ºä»€ä¹ˆéœ€è¦åµŒå…¥æ¨¡å‹)
  - [1.2 åµŒå…¥æ¨¡å‹æ ¸å¿ƒæ¶æ„](#12-åµŒå…¥æ¨¡å‹æ ¸å¿ƒæ¶æ„)
  - [1.3 åµŒå…¥ç©ºé—´çš„æ•°å­¦æœ¬è´¨](#13-åµŒå…¥ç©ºé—´çš„æ•°å­¦æœ¬è´¨)
  - [1.4 SOTAåµŒå…¥æ¨¡å‹å¯¹æ¯”](#14-sotaåµŒå…¥æ¨¡å‹å¯¹æ¯”)
  - [1.5 æœ¬èŠ‚å°ç»“](#15-æœ¬èŠ‚å°ç»“)

- [ç¬¬äºŒèŠ‚ï¼šå¯¹æ¯”å­¦ä¹ ä¸InfoNCEæŸå¤±](#ç¬¬äºŒèŠ‚å¯¹æ¯”å­¦ä¹ ä¸infonceæŸå¤±)
  - [2.1 å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³](#21-å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³)
  - [2.2 InfoNCEæŸå¤±å‡½æ•°](#22-infonceæŸå¤±å‡½æ•°)
  - [2.3 In-Batch Negativesç­–ç•¥](#23-in-batch-negativesç­–ç•¥)
  - [2.4 æ¸©åº¦å‚æ•°çš„å½±å“](#24-æ¸©åº¦å‚æ•°çš„å½±å“)
  - [2.5 å®æˆ˜ï¼šä»é›¶å®ç°å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨](#25-å®æˆ˜ä»é›¶å®ç°å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨)
  - [2.6 å¯¹æ¯”å­¦ä¹ çš„å˜ä½“](#26-å¯¹æ¯”å­¦ä¹ çš„å˜ä½“)
  - [2.7 æœ¬èŠ‚å°ç»“](#27-æœ¬èŠ‚å°ç»“)

- [ç¬¬ä¸‰èŠ‚ï¼šéš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥](#ç¬¬ä¸‰èŠ‚éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥)
  - [3.1 ä¸ºä»€ä¹ˆéœ€è¦éš¾è´Ÿæ ·æœ¬ï¼Ÿ](#31-ä¸ºä»€ä¹ˆéœ€è¦éš¾è´Ÿæ ·æœ¬)
  - [3.2 éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç®—æ³•](#32-éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç®—æ³•)
  - [3.3 åŠ¨æ€éš¾è´Ÿæ ·æœ¬æ›´æ–°ï¼ˆANCEï¼‰](#33-åŠ¨æ€éš¾è´Ÿæ ·æœ¬æ›´æ–°ance)
  - [3.4 å®æˆ˜ï¼šå®Œæ•´éš¾è´Ÿæ ·æœ¬è®­ç»ƒæµç¨‹](#34-å®æˆ˜å®Œæ•´éš¾è´Ÿæ ·æœ¬è®­ç»ƒæµç¨‹)
  - [3.5 æœ¬èŠ‚å°ç»“](#35-æœ¬èŠ‚å°ç»“)

- [ç¬¬å››èŠ‚ï¼šå¤šä»»åŠ¡è”åˆè®­ç»ƒ](#ç¬¬å››èŠ‚å¤šä»»åŠ¡è”åˆè®­ç»ƒ)
  - [4.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šä»»åŠ¡è®­ç»ƒï¼Ÿ](#41-ä¸ºä»€ä¹ˆéœ€è¦å¤šä»»åŠ¡è®­ç»ƒ)
  - [4.2 ä»»åŠ¡å®šä¹‰ä¸æŸå¤±å‡½æ•°](#42-ä»»åŠ¡å®šä¹‰ä¸æŸå¤±å‡½æ•°)
  - [4.3 å¤šä»»åŠ¡è®­ç»ƒå™¨](#43-å¤šä»»åŠ¡è®­ç»ƒå™¨)
  - [4.4 ä»»åŠ¡æƒé‡è°ƒä¼˜](#44-ä»»åŠ¡æƒé‡è°ƒä¼˜)
  - [4.5 MatryoshkaåµŒå…¥ï¼ˆå¯å˜ç»´åº¦åµŒå…¥ï¼‰](#45-matryoshkaåµŒå…¥å¯å˜ç»´åº¦åµŒå…¥)
  - [4.6 æœ¬èŠ‚å°ç»“](#46-æœ¬èŠ‚å°ç»“)

- [ç¬¬äº”èŠ‚ï¼šå®Œæ•´è®­ç»ƒä¸éƒ¨ç½²Pipeline](#ç¬¬äº”èŠ‚å®Œæ•´è®­ç»ƒä¸éƒ¨ç½²pipeline)
  - [5.1 å®Œæ•´è®­ç»ƒæµç¨‹æ¦‚è§ˆ](#51-å®Œæ•´è®­ç»ƒæµç¨‹æ¦‚è§ˆ)
  - [5.2 æ•°æ®å‡†å¤‡ä¸åˆæˆ](#52-æ•°æ®å‡†å¤‡ä¸åˆæˆ)
  - [5.3 å®Œæ•´è®­ç»ƒå®ç°](#53-å®Œæ•´è®­ç»ƒå®ç°)
  - [5.4 æ¨¡å‹è¯„ä¼°](#54-æ¨¡å‹è¯„ä¼°)
  - [5.5 æ¨¡å‹éƒ¨ç½²](#55-æ¨¡å‹éƒ¨ç½²)
  - [5.6 æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ Sentence-Transformers åº“](#56-æ›¿ä»£æ–¹æ¡ˆä½¿ç”¨-sentence-transformers-åº“)
  - [5.7 æ€§èƒ½ä¼˜åŒ–](#57-æ€§èƒ½ä¼˜åŒ–)
  - [5.8 æœ¬èŠ‚å°ç»“](#58-æœ¬èŠ‚å°ç»“)

- [ç¬¬4ç« å°ç»“](#ç¬¬4ç« å°ç»“)
- [æ€è€ƒç»ƒä¹ ](#æ€è€ƒç»ƒä¹ )
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## ç¬¬ä¸€èŠ‚ï¼šåµŒå…¥æ¨¡å‹çš„æœ¬è´¨ä¸æ¶æ„

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦åµŒå…¥æ¨¡å‹ï¼Ÿ

#### è¯­ä¹‰ç†è§£çš„æ ¸å¿ƒæŒ‘æˆ˜

ä¼ ç»ŸNLPæŠ€æœ¯ï¼ˆå¦‚TF-IDFã€BM25ï¼‰ä¾èµ–è¯æ±‡åŒ¹é…ï¼Œæ— æ³•æ•æ‰è¯­ä¹‰ï¼š

```python
from dataclasses import dataclass
from typing import List
import numpy as np

@dataclass
class SemanticExample:
    """è¯­ä¹‰ç†è§£ç¤ºä¾‹"""
    query: str
    doc1: str
    doc2: str
    
    def word_overlap(self, text1: str, text2: str) -> float:
        """è¯æ±‡é‡å ç‡"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        overlap = words1 & words2
        return len(overlap) / max(len(words1), len(words2))

# æ¡ˆä¾‹ï¼šè¯­ä¹‰ç†è§£å¤±è´¥
example = SemanticExample(
    query="å¦‚ä½•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Ÿ",
    doc1="ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­",  # è¯­ä¹‰ç›¸å…³
    doc2="æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ"  # è¯æ±‡é‡å¤ï¼Œæ— æ„ä¹‰
)

print(f"Query vs Doc1 è¯æ±‡é‡å : {example.word_overlap(example.query, example.doc1):.2f}")  # 0.17
print(f"Query vs Doc2 è¯æ±‡é‡å : {example.word_overlap(example.query, example.doc2):.2f}")  # 0.67 (é”™è¯¯ï¼)
```

**å…³é”®é—®é¢˜**ï¼š
- åŒä¹‰è¯æ— æ³•åŒ¹é…ï¼ˆ"æ±½è½¦" vs "è½¦è¾†"ï¼‰
- ä¸Šä¸‹æ–‡è¯­ä¹‰ä¸¢å¤±ï¼ˆ"é“¶è¡Œ"ï¼šé‡‘èæœºæ„ vs æ²³å²¸ï¼‰
- è¯­åºæ•æ„Ÿæ€§ï¼ˆ"ç‹—å’¬äºº" vs "äººå’¬ç‹—"ï¼‰

#### åµŒå…¥æ¨¡å‹çš„ä¼˜åŠ¿

**å°†æ–‡æœ¬æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´**ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ¥è¿‘ï¼š

```python
from typing import Tuple
import torch
import torch.nn.functional as F

@dataclass
class EmbeddingModelDemo:
    """åµŒå…¥æ¨¡å‹æ¼”ç¤º"""
    
    @staticmethod
    def cosine_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> float:
        """ä½™å¼¦ç›¸ä¼¼åº¦"""
        return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
    
    @staticmethod
    def semantic_search(query_emb: torch.Tensor, 
                       doc_embs: torch.Tensor, 
                       top_k: int = 3) -> List[Tuple[int, float]]:
        """è¯­ä¹‰æœç´¢"""
        # query_emb: [dim], doc_embs: [num_docs, dim]
        similarities = F.cosine_similarity(
            query_emb.unsqueeze(0), doc_embs, dim=1
        )
        top_scores, top_indices = torch.topk(similarities, k=top_k)
        return list(zip(top_indices.tolist(), top_scores.tolist()))

# æ¨¡æ‹ŸåµŒå…¥å‘é‡ï¼ˆå®é™…ç”±æ¨¡å‹ç”Ÿæˆï¼‰
query_emb = torch.randn(768)
doc1_emb = query_emb + 0.1 * torch.randn(768)  # è¯­ä¹‰ç›¸ä¼¼
doc2_emb = torch.randn(768)  # è¯­ä¹‰æ— å…³

demo = EmbeddingModelDemo()
print(f"Query vs Doc1 è¯­ä¹‰ç›¸ä¼¼åº¦: {demo.cosine_similarity(query_emb, doc1_emb):.4f}")  # ~0.95
print(f"Query vs Doc2 è¯­ä¹‰ç›¸ä¼¼åº¦: {demo.cosine_similarity(query_emb, doc2_emb):.4f}")  # ~0.05
```

**çœŸå®è¡¨ç°**ï¼ˆOpenAI text-embedding-3-smallï¼‰ï¼š

| æŸ¥è¯¢ | æ–‡æ¡£ | ä½™å¼¦ç›¸ä¼¼åº¦ |
|-----|-----|----------|
| "æ·±åº¦å­¦ä¹ è®­ç»ƒ" | "ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹" | 0.87 |
| "æ·±åº¦å­¦ä¹ è®­ç»ƒ" | "æ— å…³æ–‡æœ¬å†…å®¹" | 0.12 |

---

### 1.2 åµŒå…¥æ¨¡å‹æ ¸å¿ƒæ¶æ„

#### Bi-Encoderï¼ˆåŒç¼–ç å™¨ï¼‰æ¶æ„

**æœ€æµè¡Œçš„åµŒå…¥æ¨¡å‹æ¶æ„**ï¼Œç‹¬ç«‹ç¼–ç queryå’Œdocumentï¼š

```python
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class BiEncoder(nn.Module):
    """åŒç¼–ç å™¨æ¶æ„"""
    
    def __init__(self, model_name: str = "bert-base-uncased", 
                 pooling_mode: str = "mean"):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.pooling_mode = pooling_mode
        self.hidden_size = self.encoder.config.hidden_size
    
    def mean_pooling(self, token_embeddings: torch.Tensor, 
                    attention_mask: torch.Tensor) -> torch.Tensor:
        """å‡å€¼æ± åŒ–"""
        # token_embeddings: [batch, seq_len, hidden_size]
        # attention_mask: [batch, seq_len]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
        return sum_embeddings / sum_mask
    
    def cls_pooling(self, token_embeddings: torch.Tensor) -> torch.Tensor:
        """CLSæ± åŒ–"""
        return token_embeddings[:, 0, :]  # ç¬¬ä¸€ä¸ªtokenï¼ˆ[CLS]ï¼‰
    
    def forward(self, input_ids: torch.Tensor, 
                attention_mask: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # 1. Transformerç¼–ç 
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden]
        
        # 2. æ± åŒ–å¾—åˆ°å¥å­åµŒå…¥
        if self.pooling_mode == "mean":
            sentence_embeddings = self.mean_pooling(token_embeddings, attention_mask)
        elif self.pooling_mode == "cls":
            sentence_embeddings = self.cls_pooling(token_embeddings)
        else:
            raise ValueError(f"Unsupported pooling: {self.pooling_mode}")
        
        # 3. L2å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼ï¼‰
        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
        
        return sentence_embeddings

# ä½¿ç”¨ç¤ºä¾‹
model = BiEncoder(model_name="bert-base-uncased", pooling_mode="mean")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

texts = [
    "å¦‚ä½•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Ÿ",
    "ç¥ç»ç½‘ç»œçš„è®­ç»ƒåŒ…æ‹¬å‰å‘å’Œåå‘ä¼ æ’­"
]

# ç¼–ç 
inputs = tokenizer(
    texts, 
    padding=True, 
    truncation=True, 
    max_length=512, 
    return_tensors="pt"
)

with torch.no_grad():
    embeddings = model(inputs["input_ids"], inputs["attention_mask"])
    print(f"åµŒå…¥å½¢çŠ¶: {embeddings.shape}")  # [2, 768]
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = F.cosine_similarity(embeddings[0:1], embeddings[1:2])
    print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity.item():.4f}")
```

**æ¶æ„ä¼˜åŠ¿**ï¼š
- âœ… **é«˜æ•ˆæ£€ç´¢**ï¼šæ–‡æ¡£å¯é¢„å…ˆç¼–ç ï¼Œqueryå®æ—¶ç¼–ç 
- âœ… **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒç™¾ä¸‡çº§æ–‡æ¡£åº“ï¼ˆé…åˆFAISSç­‰å‘é‡åº“ï¼‰
- âœ… **çµæ´»æ€§**ï¼šæ”¯æŒå¼‚æ­¥è®¡ç®—ã€åˆ†å¸ƒå¼éƒ¨ç½²

**æ± åŒ–ç­–ç•¥å¯¹æ¯”**ï¼ˆSBERTè®ºæ–‡æ•°æ®ï¼‰ï¼š

| æ± åŒ–æ–¹æ³• | STS-Bæ€§èƒ½ | è¯´æ˜ |
|---------|----------|-----|
| CLS pooling | 0.29 | ä»…ç”¨[CLS] token |
| Mean pooling | 0.77 | å¹³å‡æ‰€æœ‰tokenï¼ˆ**æ¨è**ï¼‰ |
| Max pooling | 0.42 | å–æ¯ç»´æœ€å¤§å€¼ |

---

#### Cross-Encoderï¼ˆäº¤å‰ç¼–ç å™¨ï¼‰æ¶æ„

**ç”¨äºç²¾æ’ï¼ˆRerankingï¼‰**ï¼Œå°†queryå’Œdocumentæ‹¼æ¥åç¼–ç ï¼š

```python
class CrossEncoder(nn.Module):
    """äº¤å‰ç¼–ç å™¨æ¶æ„"""
    
    def __init__(self, model_name: str = "bert-base-uncased"):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)
    
    def forward(self, input_ids: torch.Tensor, 
                attention_mask: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­
        
        è¾“å…¥æ ¼å¼: [CLS] query [SEP] document [SEP]
        """
        # 1. ç¼–ç 
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # 2. CLS tokenè¡¨ç¤º
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [batch, hidden]
        
        # 3. ç›¸å…³æ€§æ‰“åˆ†
        scores = self.classifier(cls_embedding).squeeze(-1)  # [batch]
        
        return scores

# ä½¿ç”¨ç¤ºä¾‹
cross_model = CrossEncoder("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

query = "æ·±åº¦å­¦ä¹ è®­ç»ƒæ–¹æ³•"
documents = [
    "ç¥ç»ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒ",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½é˜³å…‰æ˜åªš"
]

# ç¼–ç query-documentå¯¹
pairs = [f"{query} [SEP] {doc}" for doc in documents]
inputs = tokenizer(
    pairs, 
    padding=True, 
    truncation=True, 
    max_length=512, 
    return_tensors="pt"
)

with torch.no_grad():
    scores = cross_model(inputs["input_ids"], inputs["attention_mask"])
    print(f"ç›¸å…³æ€§å¾—åˆ†: {scores.tolist()}")  # [0.85, 0.12]
```

**Bi-Encoder vs Cross-Encoderå¯¹æ¯”**ï¼š

| ç»´åº¦ | Bi-Encoder | Cross-Encoder |
|-----|-----------|--------------|
| **è®¡ç®—æ•ˆç‡** | âœ… é«˜ï¼ˆç‹¬ç«‹ç¼–ç ï¼‰ | âŒ ä½ï¼ˆæˆå¯¹ç¼–ç ï¼‰ |
| **è¯­ä¹‰äº¤äº’** | âŒ æ— äº¤äº’ | âœ… æ·±åº¦äº¤äº’ |
| **é€‚ç”¨åœºæ™¯** | å¬å›ï¼ˆRetrievalï¼‰ | ç²¾æ’ï¼ˆRerankingï¼‰ |
| **å»¶è¿Ÿ** | ~10ms | ~100ms |
| **å‡†ç¡®æ€§** | åŸºçº¿ | +5~10% |

**å®é™…åº”ç”¨æµç¨‹**ï¼š
1. **å¬å›**ï¼šBi-Encoderä»1Mæ–‡æ¡£ä¸­æ£€ç´¢Top-100
2. **ç²¾æ’**ï¼šCross-Encoderå¯¹Top-100é‡æ–°æ’åºå¾—åˆ°Top-10

---

### 1.3 åµŒå…¥ç©ºé—´çš„æ•°å­¦æœ¬è´¨

#### å‘é‡ç©ºé—´æ¨¡å‹

åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬æ˜ å°„åˆ° $\mathbb{R}^d$ ç©ºé—´ï¼Œç†æƒ³ç›®æ ‡ï¼š

$$
\text{sim}(t_1, t_2) \approx \cos(\mathbf{v}_1, \mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{\|\mathbf{v}_1\| \|\mathbf{v}_2\|}
$$

```python
@dataclass
class EmbeddingSpace:
    """åµŒå…¥ç©ºé—´åˆ†æ"""
    embeddings: torch.Tensor  # [num_samples, dim]
    labels: List[str]
    
    def analyze_space(self) -> dict:
        """åˆ†æåµŒå…¥ç©ºé—´ç‰¹æ€§"""
        # 1. ç»´åº¦
        num_samples, dim = self.embeddings.shape
        
        # 2. èŒƒæ•°åˆ†å¸ƒ
        norms = torch.norm(self.embeddings, dim=1)
        
        # 3. ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        normalized_embs = F.normalize(self.embeddings, p=2, dim=1)
        similarity_matrix = normalized_embs @ normalized_embs.T
        
        # 4. é‚»åŸŸåˆ†æ
        avg_top5_sim = []
        for i in range(num_samples):
            sims = similarity_matrix[i]
            top5_sims = torch.topk(sims, k=6)[0][1:]  # æ’é™¤è‡ªå·±
            avg_top5_sim.append(top5_sims.mean().item())
        
        return {
            "num_samples": num_samples,
            "dimension": dim,
            "avg_norm": norms.mean().item(),
            "std_norm": norms.std().item(),
            "avg_similarity": similarity_matrix.mean().item(),
            "avg_top5_neighbor_sim": np.mean(avg_top5_sim)
        }

# åˆ†æç¤ºä¾‹
fake_embeddings = F.normalize(torch.randn(1000, 768), p=2, dim=1)
space = EmbeddingSpace(
    embeddings=fake_embeddings,
    labels=[f"text_{i}" for i in range(1000)]
)

analysis = space.analyze_space()
print(f"""
åµŒå…¥ç©ºé—´åˆ†æ:
- æ ·æœ¬æ•°: {analysis['num_samples']}
- ç»´åº¦: {analysis['dimension']}
- å¹³å‡èŒƒæ•°: {analysis['avg_norm']:.4f}
- å¹³å‡ç›¸ä¼¼åº¦: {analysis['avg_similarity']:.4f}
- Top-5é‚»å±…å¹³å‡ç›¸ä¼¼åº¦: {analysis['avg_top5_neighbor_sim']:.4f}
""")
```

#### ç†æƒ³åµŒå…¥ç©ºé—´çš„æ€§è´¨

**1. å„å‘åŒæ€§ï¼ˆIsotropyï¼‰**

ç†æƒ³åµŒå…¥åº”åœ¨æ‰€æœ‰æ–¹å‘å‡åŒ€åˆ†å¸ƒï¼Œé¿å…"è¡¨ç¤ºåå¡Œ"ï¼š

```python
def measure_isotropy(embeddings: torch.Tensor) -> float:
    """æµ‹é‡å„å‘åŒæ€§
    
    æ–¹æ³•ï¼šè®¡ç®—åµŒå…¥çš„ä¸»æˆåˆ†æ–¹å·®æ¯”
    - å®Œå…¨å„å‘åŒæ€§: æ‰€æœ‰ä¸»æˆåˆ†æ–¹å·®ç›¸ç­‰
    - åå¡Œ: å°‘æ•°ä¸»æˆåˆ†å æ®å¤§éƒ¨åˆ†æ–¹å·®
    """
    # 1. ä¸­å¿ƒåŒ–
    centered = embeddings - embeddings.mean(dim=0, keepdim=True)
    
    # 2. PCA
    U, S, V = torch.pca_lowrank(centered, q=min(50, embeddings.shape[1]))
    
    # 3. æ–¹å·®å æ¯”
    explained_var_ratio = (S ** 2) / (S ** 2).sum()
    
    # 4. Top-1ä¸»æˆåˆ†å æ¯”ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    top1_ratio = explained_var_ratio[0].item()
    
    return 1 - top1_ratio  # å½’ä¸€åŒ–åˆ°[0,1]ï¼Œ1è¡¨ç¤ºå®Œå…¨å„å‘åŒæ€§

# æµ‹è¯•
good_embeddings = F.normalize(torch.randn(1000, 768), p=2, dim=1)
bad_embeddings = torch.randn(1000, 1).repeat(1, 768)  # åå¡Œåˆ°1ç»´

print(f"è‰¯å¥½åµŒå…¥å„å‘åŒæ€§: {measure_isotropy(good_embeddings):.4f}")  # ~0.95
print(f"åå¡ŒåµŒå…¥å„å‘åŒæ€§: {measure_isotropy(bad_embeddings):.4f}")  # ~0.01
```

**2. å±€éƒ¨èšç±»æ€§ï¼ˆLocal Clusteringï¼‰**

ç›¸ä¼¼æ–‡æœ¬åœ¨åµŒå…¥ç©ºé—´ä¸­å½¢æˆç´§å¯†ç°‡ï¼š

```python
def measure_clustering_quality(embeddings: torch.Tensor, 
                               labels: torch.Tensor) -> float:
    """æµ‹é‡èšç±»è´¨é‡ï¼ˆè½®å»“ç³»æ•°ï¼‰"""
    from sklearn.metrics import silhouette_score
    
    score = silhouette_score(
        embeddings.cpu().numpy(), 
        labels.cpu().numpy()
    )
    return score

# ç¤ºä¾‹
num_classes = 10
samples_per_class = 100

# ç”Ÿæˆæ¨¡æ‹Ÿèšç±»æ•°æ®
centers = torch.randn(num_classes, 768) * 5
labels = torch.arange(num_classes).repeat_interleave(samples_per_class)
embeddings = centers[labels] + 0.5 * torch.randn(num_classes * samples_per_class, 768)

score = measure_clustering_quality(F.normalize(embeddings, p=2, dim=1), labels)
print(f"èšç±»è´¨é‡ï¼ˆè½®å»“ç³»æ•°ï¼‰: {score:.4f}")  # ~0.8 (è¶Šæ¥è¿‘1è¶Šå¥½)
```

---

### 1.4 SOTAåµŒå…¥æ¨¡å‹å¯¹æ¯”

#### ä¸»æµæ¨¡å‹æ¶æ„æ¼”è¿›

```python
@dataclass
class EmbeddingModelInfo:
    """åµŒå…¥æ¨¡å‹ä¿¡æ¯"""
    name: str
    base_model: str
    dimension: int
    max_length: int
    training_method: str
    performance_mteb: float  # MTEBå¹³å‡åˆ†
    
    def __str__(self) -> str:
        return (
            f"{self.name}\n"
            f"  åŸºåº§: {self.base_model}\n"
            f"  ç»´åº¦: {self.dimension}\n"
            f"  æœ€å¤§é•¿åº¦: {self.max_length}\n"
            f"  è®­ç»ƒæ–¹æ³•: {self.training_method}\n"
            f"  MTEBå¾—åˆ†: {self.performance_mteb:.2f}"
        )

# SOTAæ¨¡å‹åˆ—è¡¨ï¼ˆ2025å¹´æœ€æ–°æ•°æ®ï¼‰
models = [
    EmbeddingModelInfo(
        name="Voyage-3-Large (2025)",
        base_model="æœªå…¬å¼€",
        dimension=2048,
        max_length=32768,
        training_method="å¤šä»»åŠ¡ + Matryoshka",
        performance_mteb=66.3
    ),
    EmbeddingModelInfo(
        name="GTE-Qwen2-7B (2024)",
        base_model="Qwen2-7B",
        dimension=3584,
        max_length=32768,
        training_method="LLMè’¸é¦ + å¯¹æ¯”å­¦ä¹ ",
        performance_mteb=65.9
    ),
    EmbeddingModelInfo(
        name="OpenAI text-embedding-3-large",
        base_model="æœªå…¬å¼€",
        dimension=3072,
        max_length=8192,
        training_method="å¯¹æ¯”å­¦ä¹  + å¤šä»»åŠ¡",
        performance_mteb=64.6
    ),
    EmbeddingModelInfo(
        name="UAE-Large-V1 (2024)",
        base_model="XLM-RoBERTa-large",
        dimension=1024,
        max_length=512,
        training_method="RLHF + å¯¹æ¯”å­¦ä¹ ",
        performance_mteb=64.6
    ),
    EmbeddingModelInfo(
        name="Cohere embed-v3",
        base_model="æœªå…¬å¼€",
        dimension=1024,
        max_length=512,
        training_method="å¯¹æ¯”å­¦ä¹  + éš¾è´Ÿæ ·æœ¬æŒ–æ˜",
        performance_mteb=64.5
    ),
    EmbeddingModelInfo(
        name="BGE-M3",
        base_model="XLM-RoBERTa",
        dimension=1024,
        max_length=8192,
        training_method="å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ ",
        performance_mteb=63.6
    ),
    EmbeddingModelInfo(
        name="GTE-large-v1.5",
        base_model="BERT",
        dimension=1024,
        max_length=8192,
        training_method="å¯¹æ¯”å­¦ä¹  + RetroMAE",
        performance_mteb=63.3
    ),
    EmbeddingModelInfo(
        name="E5-mistral-7b-instruct",
        base_model="Mistral-7B",
        dimension=4096,
        max_length=32768,
        training_method="æŒ‡ä»¤å¾®è°ƒ + å¯¹æ¯”å­¦ä¹ ",
        performance_mteb=60.1
    ),
    EmbeddingModelInfo(
        name="Sentence-BERT (all-mpnet-base-v2)",
        base_model="MPNet",
        dimension=768,
        max_length=384,
        training_method="å¯¹æ¯”å­¦ä¹ ï¼ˆNLIæ•°æ®ï¼‰",
        performance_mteb=57.8
    ),
]

# æ‰“å°å¯¹æ¯”
print("=" * 80)
print("SOTAåµŒå…¥æ¨¡å‹å¯¹æ¯”")
print("=" * 80)
for model in models:
    print(model)
    print("-" * 80)
```

**è¾“å‡º**ï¼ˆå®é™…MTEB Leaderboardæ•°æ®ï¼Œæ›´æ–°è‡³2025å¹´1æœˆï¼‰ï¼š

| æ¨¡å‹ | åŸºåº§ | ç»´åº¦ | æœ€å¤§é•¿åº¦ | MTEBå¾—åˆ† | å‘å¸ƒæ—¶é—´ |
|-----|------|------|---------|---------|---------|
| ğŸ”¥ Voyage-3-Large | æœªå…¬å¼€ | 2048 | 32768 | **66.3** | 2025 |
| ğŸ”¥ GTE-Qwen2-7B | Qwen2-7B | 3584 | 32768 | **65.9** | 2024 |
| OpenAI text-emb-3-large | æœªå…¬å¼€ | 3072 | 8192 | 64.6 | 2024 |
| UAE-Large-V1 | XLM-RoBERTa | 1024 | 512 | 64.6 | 2024 |
| Cohere embed-v3 | æœªå…¬å¼€ | 1024 | 512 | 64.5 | 2023 |
| BGE-M3 | XLM-RoBERTa | 1024 | 8192 | 63.6 | 2023 |
| GTE-large-v1.5 | BERT | 1024 | 8192 | 63.3 | 2023 |
| E5-mistral-7b | Mistral-7B | 4096 | 32768 | 60.1 | 2023 |
| SBERT mpnet-base | MPNet | 768 | 384 | 57.8 | 2019 |

#### æ¨¡å‹é€‰æ‹©å†³ç­–æ ‘

```python
from typing import Optional

@dataclass
class ModelRecommendation:
    """æ¨¡å‹æ¨è"""
    model_name: str
    reason: str
    pros: List[str]
    cons: List[str]

def recommend_embedding_model(
    use_case: str,  # "semantic_search", "clustering", "classification"
    language: str = "en",  # "en", "zh", "multilingual"
    max_docs: int = 10000,  # æ–‡æ¡£åº“è§„æ¨¡
    latency_requirement: str = "normal"  # "low", "normal", "high"
) -> ModelRecommendation:
    """æ¨èåµŒå…¥æ¨¡å‹"""
    
    # è§„åˆ™1ï¼šå¤šè¯­è¨€éœ€æ±‚
    if language == "multilingual" or language == "zh":
        return ModelRecommendation(
            model_name="BGE-M3",
            reason="æ”¯æŒ100+è¯­è¨€ï¼Œä¸­æ–‡æ•ˆæœæœ€ä½³",
            pros=["å¤šè¯­è¨€", "é•¿æ–‡æœ¬æ”¯æŒ8K", "å¼€æº"],
            cons=["ç»´åº¦1024ç›¸å¯¹è¾ƒå¤§", "æ¨ç†é€Ÿåº¦ä¸­ç­‰"]
        )
    
    # è§„åˆ™2ï¼šæä½å»¶è¿Ÿéœ€æ±‚
    if latency_requirement == "low":
        return ModelRecommendation(
            model_name="all-MiniLM-L6-v2",
            reason="è¶…å¿«æ¨ç†é€Ÿåº¦ï¼Œä»…384ç»´",
            pros=["æ¨ç†é€Ÿåº¦å¿«10x", "æ¨¡å‹å°å·§22MB", "å¼€æº"],
            cons=["æ€§èƒ½ç•¥ä½äºSOTA", "ä»…æ”¯æŒè‹±æ–‡"]
        )
    
    # è§„åˆ™3ï¼šå¤§è§„æ¨¡æ–‡æ¡£åº“
    if max_docs > 1000000:
        return ModelRecommendation(
            model_name="Cohere embed-v3",
            reason="é«˜å‹ç¼©ç‡ï¼Œæ”¯æŒç»´åº¦ç¼©å‡",
            pros=["å¯å˜ç»´åº¦ï¼ˆ1024â†’128ï¼‰", "æ€§èƒ½æŸå¤±<5%", "æ£€ç´¢é€Ÿåº¦å¿«"],
            cons=["é—­æºAPI", "æŒ‰tokenè®¡è´¹"]
        )
    
    # è§„åˆ™4ï¼šé€šç”¨åœºæ™¯
    return ModelRecommendation(
        model_name="OpenAI text-embedding-3-small",
        reason="æ€§ä»·æ¯”æœ€é«˜ï¼Œç»¼åˆæ€§èƒ½å¼º",
        pros=["MTEBå¾—åˆ†62.3", "ç»´åº¦1536é€‚ä¸­", "APIç¨³å®š"],
        cons=["é—­æº", "æŒ‰tokenè®¡è´¹"]
    )

# æµ‹è¯•æ¨è
cases = [
    {"use_case": "semantic_search", "language": "zh", "max_docs": 50000},
    {"use_case": "clustering", "language": "en", "latency_requirement": "low"},
    {"use_case": "semantic_search", "language": "en", "max_docs": 5000000},
]

for case in cases:
    rec = recommend_embedding_model(**case)
    print(f"\nåœºæ™¯: {case}")
    print(f"æ¨è: {rec.model_name}")
    print(f"ç†ç”±: {rec.reason}")
    print(f"ä¼˜åŠ¿: {', '.join(rec.pros)}")
    print(f"åŠ£åŠ¿: {', '.join(rec.cons)}")
```

---

### 1.5 æœ¬èŠ‚å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **åµŒå…¥æ¨¡å‹è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**
   - ä¼ ç»Ÿè¯æ±‡åŒ¹é…æ— æ³•æ•æ‰è¯­ä¹‰
   - åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬è·ç¦»æ¥è¿‘

2. **æ ¸å¿ƒæ¶æ„**ï¼š
   - **Bi-Encoder**ï¼šç‹¬ç«‹ç¼–ç ï¼Œç”¨äºé«˜æ•ˆå¬å›
   - **Cross-Encoder**ï¼šè”åˆç¼–ç ï¼Œç”¨äºç²¾ç¡®ç²¾æ’
   - å®é™…åº”ç”¨é‡‡ç”¨ä¸¤é˜¶æ®µï¼šBi-Encoderå¬å› + Cross-Encoderç²¾æ’

3. **ç†æƒ³åµŒå…¥ç©ºé—´**ï¼š
   - å„å‘åŒæ€§ï¼šé¿å…è¡¨ç¤ºåå¡Œ
   - å±€éƒ¨èšç±»æ€§ï¼šç›¸ä¼¼æ–‡æœ¬å½¢æˆç´§å¯†ç°‡

4. **SOTAæ¨¡å‹**ï¼š
   - é—­æºï¼šOpenAI text-embedding-3, Cohere embed-v3ï¼ˆæ€§èƒ½æœ€å¼ºï¼‰
   - å¼€æºï¼šBGE-M3, GTE-largeï¼ˆä¸­æ–‡å‹å¥½ï¼Œæ€§èƒ½æ¥è¿‘ï¼‰

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼š

ç¬¬äºŒèŠ‚ã€Šå¯¹æ¯”å­¦ä¹ ä¸InfoNCEæŸå¤±ã€‹å°†æ·±å…¥è®²è§£ï¼š
- å¯¹æ¯”å­¦ä¹ çš„æ•°å­¦åŸç†
- InfoNCEæŸå¤±å‡½æ•°æ¨å¯¼
- In-Batch Negativesç­–ç•¥
- å®æˆ˜ï¼šä»é›¶å®ç°å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨


## ç¬¬äºŒèŠ‚ï¼šå¯¹æ¯”å­¦ä¹ ä¸InfoNCEæŸå¤±

### 2.1 å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³

#### ä»€ä¹ˆæ˜¯å¯¹æ¯”å­¦ä¹ ï¼Ÿ

**æ ¸å¿ƒåŸåˆ™**ï¼šæ‹‰è¿‘ç›¸ä¼¼æ ·æœ¬ï¼Œæ¨è¿œä¸ç›¸ä¼¼æ ·æœ¬ã€‚

```python
@dataclass
class ContrastiveLearningPrinciple:
    """å¯¹æ¯”å­¦ä¹ åŸç†æ¼”ç¤º"""
    
    @staticmethod
    def visualize_principle():
        """å¯è§†åŒ–å¯¹æ¯”å­¦ä¹ å‰åçš„åµŒå…¥ç©ºé—´"""
        # åˆå§‹çŠ¶æ€ï¼šéšæœºåˆ†å¸ƒ
        anchor = torch.randn(1, 128)
        positive = torch.randn(1, 128)  # åº”è¯¥æ¥è¿‘anchor
        negative = torch.randn(1, 128)  # åº”è¯¥è¿œç¦»anchor
        
        # è®¡ç®—åˆå§‹è·ç¦»
        init_pos_dist = F.cosine_similarity(anchor, positive).item()
        init_neg_dist = F.cosine_similarity(anchor, negative).item()
        
        print("=" * 60)
        print("å¯¹æ¯”å­¦ä¹ åŸç†")
        print("=" * 60)
        print(f"åˆå§‹çŠ¶æ€:")
        print(f"  Anchor <-> Positiveè·ç¦»: {init_pos_dist:.4f}")
        print(f"  Anchor <-> Negativeè·ç¦»: {init_neg_dist:.4f}")
        print(f"  é—®é¢˜: æ­£è´Ÿæ ·æœ¬è·ç¦»æ— æ˜æ˜¾åŒºåˆ†")
        
        # æ¨¡æ‹Ÿè®­ç»ƒåçŠ¶æ€ï¼ˆç†æƒ³æƒ…å†µï¼‰
        trained_positive = anchor + 0.05 * torch.randn(1, 128)  # å¾ˆæ¥è¿‘
        trained_negative = -anchor + 0.2 * torch.randn(1, 128)  # å¾ˆè¿œ
        
        final_pos_dist = F.cosine_similarity(anchor, trained_positive).item()
        final_neg_dist = F.cosine_similarity(anchor, trained_negative).item()
        
        print(f"\nè®­ç»ƒå:")
        print(f"  Anchor <-> Positiveè·ç¦»: {final_pos_dist:.4f}")
        print(f"  Anchor <-> Negativeè·ç¦»: {final_neg_dist:.4f}")
        print(f"  âœ“ æ­£æ ·æœ¬è¢«æ‹‰è¿‘ï¼Œè´Ÿæ ·æœ¬è¢«æ¨è¿œ")
        print("=" * 60)

ContrastiveLearningPrinciple.visualize_principle()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
============================================================
å¯¹æ¯”å­¦ä¹ åŸç†
============================================================
åˆå§‹çŠ¶æ€:
  Anchor <-> Positiveè·ç¦»: 0.0523
  Anchor <-> Negativeè·ç¦»: -0.0341
  é—®é¢˜: æ­£è´Ÿæ ·æœ¬è·ç¦»æ— æ˜æ˜¾åŒºåˆ†

è®­ç»ƒå:
  Anchor <-> Positiveè·ç¦»: 0.9812
  Anchor <-> Negativeè·ç¦»: -0.9534
  âœ“ æ­£æ ·æœ¬è¢«æ‹‰è¿‘,è´Ÿæ ·æœ¬è¢«æ¨è¿œ
============================================================
```

---

### 2.2 InfoNCEæŸå¤±å‡½æ•°

#### æ•°å­¦æ¨å¯¼

InfoNCEï¼ˆInfo Noise Contrastive Estimationï¼‰æ˜¯å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæŸå¤±å‡½æ•°ã€‚

**ç›®æ ‡**ï¼šç»™å®šanchor $x$ï¼Œä»K+1ä¸ªå€™é€‰æ ·æœ¬ä¸­è¯†åˆ«å‡ºæ­£æ ·æœ¬ $x^+$ï¼ˆå…¶ä½™Kä¸ªæ˜¯è´Ÿæ ·æœ¬ $\{x^-_i\}_{i=1}^K$ï¼‰ã€‚

**æŸå¤±å‡½æ•°**ï¼š

$$
\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(x, x^+) / \tau)}{\exp(\text{sim}(x, x^+) / \tau) + \sum_{i=1}^K \exp(\text{sim}(x, x^-_i) / \tau)}
$$

å…¶ä¸­ï¼š
- $\text{sim}(\cdot, \cdot)$ï¼šç›¸ä¼¼åº¦å‡½æ•°ï¼ˆé€šå¸¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
- $\tau$ï¼šæ¸©åº¦å‚æ•°ï¼ˆtemperatureï¼‰ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦
- $K$ï¼šè´Ÿæ ·æœ¬æ•°é‡

**ç›´è§‚ç†è§£**ï¼š
- åˆ†å­ï¼šæ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦
- åˆ†æ¯ï¼šæ­£æ ·æœ¬ + æ‰€æœ‰è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦ä¹‹å’Œ
- ç›®æ ‡ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬åœ¨æ‰€æœ‰å€™é€‰æ ·æœ¬ä¸­çš„"æ¦‚ç‡"

#### å®Œæ•´å®ç°

```python
class InfoNCELoss(nn.Module):
    """InfoNCEæŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, anchor: torch.Tensor, 
                positive: torch.Tensor, 
                negatives: torch.Tensor) -> torch.Tensor:
        """
        Args:
            anchor: [batch_size, dim] é”šç‚¹æ ·æœ¬
            positive: [batch_size, dim] æ­£æ ·æœ¬
            negatives: [batch_size, num_negatives, dim] è´Ÿæ ·æœ¬
        
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        batch_size = anchor.size(0)
        
        # 1. å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼ï¼‰
        anchor = F.normalize(anchor, p=2, dim=1)
        positive = F.normalize(positive, p=2, dim=1)
        negatives = F.normalize(negatives, p=2, dim=2)
        
        # 2. è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        # [batch_size, 1]
        pos_sim = torch.sum(anchor * positive, dim=1, keepdim=True)
        
        # 3. è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        # [batch_size, num_negatives]
        neg_sim = torch.bmm(negatives, anchor.unsqueeze(2)).squeeze(2)
        
        # 4. æ‹¼æ¥æ‰€æœ‰ç›¸ä¼¼åº¦
        # [batch_size, 1 + num_negatives]
        logits = torch.cat([pos_sim, neg_sim], dim=1) / self.temperature
        
        # 5. æ ‡ç­¾ï¼šæ­£æ ·æœ¬åœ¨ç¬¬0ä½
        labels = torch.zeros(batch_size, dtype=torch.long, device=anchor.device)
        
        # 6. äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(logits, labels)
        
        # 7. è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ­£æ ·æœ¬å¾—åˆ†æœ€é«˜çš„æ¯”ä¾‹ï¼‰
        predictions = logits.argmax(dim=1)
        accuracy = (predictions == labels).float().mean()
        
        return loss, accuracy

# æµ‹è¯•
loss_fn = InfoNCELoss(temperature=0.07)

# æ¨¡æ‹Ÿæ•°æ®
batch_size, dim, num_negatives = 32, 128, 63
anchor = torch.randn(batch_size, dim)
positive = anchor + 0.1 * torch.randn(batch_size, dim)  # æ¥è¿‘anchor
negatives = torch.randn(batch_size, num_negatives, dim)

loss, acc = loss_fn(anchor, positive, negatives)
print(f"InfoNCE Loss: {loss.item():.4f}")
print(f"Accuracy: {acc.item():.4f}")
```

---

### 2.3 In-Batch Negativesç­–ç•¥

#### ä¸ºä»€ä¹ˆéœ€è¦In-Batch Negativesï¼Ÿ

**é—®é¢˜**ï¼šæ„é€ å¤§é‡è´Ÿæ ·æœ¬æˆæœ¬é«˜æ˜‚ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨åŒä¸€batchå†…ï¼Œå°†å…¶ä»–æ ·æœ¬çš„positiveä½œä¸ºå½“å‰æ ·æœ¬çš„negativeã€‚

```python
@dataclass
class InBatchNegativesExample:
    """In-Batch Negativesç¤ºä¾‹"""
    
    @staticmethod
    def explain():
        print("=" * 70)
        print("In-Batch Negativesç­–ç•¥")
        print("=" * 70)
        print("å‡è®¾batch_size=4ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰1ä¸ªæ­£æ ·æœ¬:")
        print()
        print("æ ·æœ¬ç´¢å¼•  |  Anchor  |  Positive  |  Negatives (æ¥è‡ªå…¶ä»–æ ·æœ¬)")
        print("-" * 70)
        print("   0      |   q0     |    d0+     |  d1+, d2+, d3+  (3ä¸ª)")
        print("   1      |   q1     |    d1+     |  d0+, d2+, d3+  (3ä¸ª)")
        print("   2      |   q2     |    d2+     |  d0+, d1+, d3+  (3ä¸ª)")
        print("   3      |   q3     |    d3+     |  d0+, d1+, d2+  (3ä¸ª)")
        print("-" * 70)
        print("ä¼˜åŠ¿:")
        print("  âœ“ æ— éœ€é¢å¤–é‡‡æ ·è´Ÿæ ·æœ¬")
        print("  âœ“ batch_size=Nï¼Œæ¯ä¸ªæ ·æœ¬è‡ªåŠ¨è·å¾—N-1ä¸ªè´Ÿæ ·æœ¬")
        print("  âœ“ è®¡ç®—æ•ˆç‡é«˜ï¼ˆåˆ©ç”¨çŸ©é˜µä¹˜æ³•ï¼‰")
        print("=" * 70)

InBatchNegativesExample.explain()
```

#### å®Œæ•´å®ç°

```python
class InfoNCEWithInBatchNegatives(nn.Module):
    """InfoNCEæŸå¤± + In-Batch Negatives"""
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, query_embeddings: torch.Tensor, 
                doc_embeddings: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        """
        Args:
            query_embeddings: [batch_size, dim] QueryåµŒå…¥
            doc_embeddings: [batch_size, dim] DocumentåµŒå…¥ï¼ˆæ­£æ ·æœ¬ï¼‰
        
        Returns:
            loss: æŸå¤±å€¼
            metrics: æŒ‡æ ‡å­—å…¸
        """
        batch_size = query_embeddings.size(0)
        
        # 1. å½’ä¸€åŒ–
        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
        doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)
        
        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # similarity_matrix[i, j] = sim(query_i, doc_j)
        # [batch_size, batch_size]
        similarity_matrix = query_embeddings @ doc_embeddings.T
        
        # 3. ç¼©æ”¾
        logits = similarity_matrix / self.temperature
        
        # 4. æ ‡ç­¾ï¼šå¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬
        labels = torch.arange(batch_size, device=logits.device)
        
        # 5. å¯¹ç§°æŸå¤±ï¼ˆquery->doc å’Œ doc->queryï¼‰
        loss_q2d = F.cross_entropy(logits, labels)
        loss_d2q = F.cross_entropy(logits.T, labels)
        loss = (loss_q2d + loss_d2q) / 2
        
        # 6. è®¡ç®—æŒ‡æ ‡
        predictions = logits.argmax(dim=1)
        accuracy = (predictions == labels).float().mean()
        
        # å¹³å‡æ­£æ ·æœ¬å¾—åˆ† vs å¹³å‡è´Ÿæ ·æœ¬å¾—åˆ†
        pos_scores = torch.diagonal(similarity_matrix)
        neg_scores_mask = ~torch.eye(batch_size, dtype=torch.bool, device=logits.device)
        neg_scores = similarity_matrix[neg_scores_mask]
        
        metrics = {
            "accuracy": accuracy.item(),
            "avg_pos_score": pos_scores.mean().item(),
            "avg_neg_score": neg_scores.mean().item(),
            "score_gap": (pos_scores.mean() - neg_scores.mean()).item()
        }
        
        return loss, metrics

# æµ‹è¯•
loss_fn = InfoNCEWithInBatchNegatives(temperature=0.07)

# æ¨¡æ‹Ÿqueryå’Œæ­£æ ·æœ¬doc
batch_size, dim = 64, 768
queries = torch.randn(batch_size, dim)
docs = queries + 0.15 * torch.randn(batch_size, dim)  # æ­£æ ·æœ¬ï¼šæ¥è¿‘query

loss, metrics = loss_fn(queries, docs)
print(f"\nIn-Batch Negativesç»“æœ:")
print(f"Loss: {loss.item():.4f}")
print(f"Accuracy: {metrics['accuracy']:.4f}")
print(f"Avg Positive Score: {metrics['avg_pos_score']:.4f}")
print(f"Avg Negative Score: {metrics['avg_neg_score']:.4f}")
print(f"Score Gap: {metrics['score_gap']:.4f}")
```

---

### 2.4 æ¸©åº¦å‚æ•°çš„å½±å“

#### æ¸©åº¦å‚æ•° $\tau$ çš„ä½œç”¨

æ¸©åº¦æ§åˆ¶softmaxåˆ†å¸ƒçš„å¹³æ»‘åº¦ï¼š

$$
p_i = \frac{\exp(\text{sim}_i / \tau)}{\sum_j \exp(\text{sim}_j / \tau)}
$$

- **$\tau$ å°**ï¼ˆå¦‚0.01ï¼‰ï¼šåˆ†å¸ƒå°–é”ï¼Œæ¨¡å‹ä¸“æ³¨äºæœ€ç›¸ä¼¼çš„æ ·æœ¬
- **$\tau$ å¤§**ï¼ˆå¦‚1.0ï¼‰ï¼šåˆ†å¸ƒå¹³æ»‘ï¼Œæ¨¡å‹è€ƒè™‘æ‰€æœ‰æ ·æœ¬

```python
def visualize_temperature_effect():
    """å¯è§†åŒ–æ¸©åº¦å‚æ•°å½±å“"""
    # æ¨¡æ‹Ÿç›¸ä¼¼åº¦å¾—åˆ†
    sim_scores = torch.tensor([0.9, 0.7, 0.5, 0.3, 0.1])  # æ­£æ ·æœ¬åœ¨ç¬¬0ä½
    
    temperatures = [0.01, 0.05, 0.1, 0.5, 1.0]
    
    print("=" * 80)
    print("æ¸©åº¦å‚æ•°å¯¹Softmaxåˆ†å¸ƒçš„å½±å“")
    print("=" * 80)
    print(f"åŸå§‹ç›¸ä¼¼åº¦å¾—åˆ†: {sim_scores.tolist()}")
    print()
    
    for tau in temperatures:
        logits = sim_scores / tau
        probs = F.softmax(logits, dim=0)
        
        print(f"æ¸©åº¦ Ï„={tau:.2f}:")
        print(f"  Softmaxæ¦‚ç‡: {probs.tolist()}")
        print(f"  æ­£æ ·æœ¬æ¦‚ç‡: {probs[0].item():.4f}")
        print(f"  ç†µ: {-(probs * torch.log(probs + 1e-9)).sum().item():.4f}")
        print()
    
    print("=" * 80)
    print("æ€»ç»“:")
    print("  - Ï„å°: åˆ†å¸ƒå°–é”ï¼Œæ­£æ ·æœ¬æ¦‚ç‡æ¥è¿‘1ï¼Œæ¢¯åº¦å¤§")
    print("  - Ï„å¤§: åˆ†å¸ƒå¹³æ»‘ï¼Œæ‰€æœ‰æ ·æœ¬æ¦‚ç‡æ¥è¿‘ï¼Œæ¢¯åº¦å°")
    print("  - æ¨è: Ï„=0.05~0.1 (SBERTã€SimCSEä½¿ç”¨0.05)")
    print("=" * 80)

visualize_temperature_effect()
```

**å®éªŒç»“æœ**ï¼ˆSBERTè®ºæ–‡ï¼‰ï¼š

| æ¸©åº¦ $\tau$ | STS-Bæ€§èƒ½ | è¯´æ˜ |
|-----------|----------|-----|
| 0.01 | 0.74 | è¿‡åº¦è‡ªä¿¡ï¼Œè®­ç»ƒä¸ç¨³å®š |
| 0.05 | **0.81** | **æœ€ä½³** |
| 0.10 | 0.79 | è‰¯å¥½ |
| 0.50 | 0.71 | åˆ†å¸ƒè¿‡äºå¹³æ»‘ |
| 1.00 | 0.65 | æ¢¯åº¦å¤ªå°ï¼Œè®­ç»ƒç¼“æ…¢ |

---

### 2.5 å®æˆ˜ï¼šä»é›¶å®ç°å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨

#### å®Œæ•´è®­ç»ƒpipeline

```python
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

@dataclass
class ContrastivePair:
    """å¯¹æ¯”å­¦ä¹ æ•°æ®å¯¹"""
    query: str
    positive: str

class ContrastiveDataset(Dataset):
    """å¯¹æ¯”å­¦ä¹ æ•°æ®é›†"""
    
    def __init__(self, pairs: List[ContrastivePair], 
                 tokenizer: AutoTokenizer,
                 max_length: int = 128):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self) -> int:
        return len(self.pairs)
    
    def __getitem__(self, idx: int) -> dict:
        pair = self.pairs[idx]
        
        # Tokenize query
        query_encoded = self.tokenizer(
            pair.query,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        # Tokenize positive
        pos_encoded = self.tokenizer(
            pair.positive,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        return {
            "query_input_ids": query_encoded["input_ids"].squeeze(0),
            "query_attention_mask": query_encoded["attention_mask"].squeeze(0),
            "pos_input_ids": pos_encoded["input_ids"].squeeze(0),
            "pos_attention_mask": pos_encoded["attention_mask"].squeeze(0),
        }

class ContrastiveLearningTrainer:
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: BiEncoder,
                 train_dataset: Dataset,
                 eval_dataset: Optional[Dataset] = None,
                 batch_size: int = 32,
                 learning_rate: float = 2e-5,
                 temperature: float = 0.07,
                 num_epochs: int = 3,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.batch_size = batch_size
        self.device = device
        self.num_epochs = num_epochs
        
        # DataLoader
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True
        )
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = InfoNCEWithInBatchNegatives(temperature=temperature)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆçº¿æ€§warmup + decayï¼‰
        num_training_steps = len(self.train_loader) * num_epochs
        num_warmup_steps = int(0.1 * num_training_steps)
        
        from transformers import get_linear_schedule_with_warmup
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
    
    def train_epoch(self) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_accuracy = 0
        num_batches = 0
        
        for batch in self.train_loader:
            # 1. æ•°æ®ç§»åˆ°GPU
            query_ids = batch["query_input_ids"].to(self.device)
            query_mask = batch["query_attention_mask"].to(self.device)
            pos_ids = batch["pos_input_ids"].to(self.device)
            pos_mask = batch["pos_attention_mask"].to(self.device)
            
            # 2. ç¼–ç 
            query_embeddings = self.model(query_ids, query_mask)
            pos_embeddings = self.model(pos_ids, pos_mask)
            
            # 3. è®¡ç®—æŸå¤±
            loss, metrics = self.loss_fn(query_embeddings, pos_embeddings)
            
            # 4. åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # 6. æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.scheduler.step()
            
            # 7. è®°å½•æŒ‡æ ‡
            total_loss += loss.item()
            total_accuracy += metrics["accuracy"]
            num_batches += 1
        
        return {
            "loss": total_loss / num_batches,
            "accuracy": total_accuracy / num_batches,
            "lr": self.scheduler.get_last_lr()[0]
        }
    
    def evaluate(self) -> dict:
        """è¯„ä¼°æ¨¡å‹"""
        if self.eval_dataset is None:
            return {}
        
        self.model.eval()
        eval_loader = DataLoader(self.eval_dataset, batch_size=self.batch_size)
        
        total_loss = 0
        total_accuracy = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in eval_loader:
                query_ids = batch["query_input_ids"].to(self.device)
                query_mask = batch["query_attention_mask"].to(self.device)
                pos_ids = batch["pos_input_ids"].to(self.device)
                pos_mask = batch["pos_attention_mask"].to(self.device)
                
                query_embeddings = self.model(query_ids, query_mask)
                pos_embeddings = self.model(pos_ids, pos_mask)
                
                loss, metrics = self.loss_fn(query_embeddings, pos_embeddings)
                
                total_loss += loss.item()
                total_accuracy += metrics["accuracy"]
                num_batches += 1
        
        return {
            "eval_loss": total_loss / num_batches,
            "eval_accuracy": total_accuracy / num_batches
        }
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("=" * 80)
        print("å¼€å§‹å¯¹æ¯”å­¦ä¹ è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(self.train_dataset)}")
        print(f"Batch Size: {self.batch_size}")
        print(f"Epochs: {self.num_epochs}")
        print(f"Device: {self.device}")
        print("=" * 80)
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # è¯„ä¼°
            eval_metrics = self.evaluate()
            
            # æ‰“å°è¿›åº¦
            print(f"\nEpoch {epoch + 1}/{self.num_epochs}")
            print(f"  Train Loss: {train_metrics['loss']:.4f}")
            print(f"  Train Acc: {train_metrics['accuracy']:.4f}")
            print(f"  LR: {train_metrics['lr']:.2e}")
            
            if eval_metrics:
                print(f"  Eval Loss: {eval_metrics['eval_loss']:.4f}")
                print(f"  Eval Acc: {eval_metrics['eval_accuracy']:.4f}")
        
        print("\n" + "=" * 80)
        print("è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å‡†å¤‡æ•°æ®
    train_pairs = [
        ContrastivePair(
            query="æ·±åº¦å­¦ä¹ å¦‚ä½•è®­ç»ƒï¼Ÿ",
            positive="ç¥ç»ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè®­ç»ƒ"
        ),
        ContrastivePair(
            query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            positive="æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹çš„æŠ€æœ¯"
        ),
        # ... æ›´å¤šæ•°æ®
    ] * 100  # æ¨¡æ‹Ÿ10000ä¸ªæ ·æœ¬
    
    # 2. åˆå§‹åŒ–
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    train_dataset = ContrastiveDataset(train_pairs, tokenizer)
    
    model = BiEncoder(model_name="bert-base-uncased", pooling_mode="mean")
    
    # 3. è®­ç»ƒ
    trainer = ContrastiveLearningTrainer(
        model=model,
        train_dataset=train_dataset,
        batch_size=32,
        learning_rate=2e-5,
        temperature=0.07,
        num_epochs=3
    )
    
    trainer.train()
    
    # 4. ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), "contrastive_model.pt")
    print("æ¨¡å‹å·²ä¿å­˜åˆ° contrastive_model.pt")
```

---

### 2.6 å¯¹æ¯”å­¦ä¹ çš„å˜ä½“

#### Triplet Lossï¼ˆä¸‰å…ƒç»„æŸå¤±ï¼‰

**å¯¹æ¯”InfoNCEçš„ç»å…¸æŸå¤±å‡½æ•°**ï¼Œç›´æ¥ä¼˜åŒ–anchorä¸æ­£è´Ÿæ ·æœ¬çš„è·ç¦»å·®ï¼š

$$
\mathcal{L}_{\text{Triplet}} = \max(0, \|\mathbf{a} - \mathbf{p}\|^2 - \|\mathbf{a} - \mathbf{n}\|^2 + \text{margin})
$$

å…¶ä¸­ï¼š
- $\mathbf{a}$: anchoræ ·æœ¬
- $\mathbf{p}$: æ­£æ ·æœ¬ï¼ˆpositiveï¼‰
- $\mathbf{n}$: è´Ÿæ ·æœ¬ï¼ˆnegativeï¼‰
- margin: é—´éš”å‚æ•°ï¼ˆé€šå¸¸0.2-0.5ï¼‰

```python
class TripletLoss(nn.Module):
    """Triplet Losså®ç°"""

    def __init__(self, margin: float = 0.3):
        super().__init__()
        self.margin = margin

    def forward(self, anchor: torch.Tensor,
                positive: torch.Tensor,
                negative: torch.Tensor) -> torch.Tensor:
        """
        Args:
            anchor: [batch_size, dim]
            positive: [batch_size, dim]
            negative: [batch_size, dim]
        """
        # (å¯é€‰) L2å½’ä¸€åŒ–
        anchor = F.normalize(anchor, p=2, dim=1)
        positive = F.normalize(positive, p=2, dim=1)
        negative = F.normalize(negative, p=2, dim=1)

        # è®¡ç®—æ¬§æ°è·ç¦»
        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)
        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)

        # Triplet loss
        loss = F.relu(pos_dist - neg_dist + self.margin)

        return loss.mean()
```

**Triplet Loss vs InfoNCEå¯¹æ¯”**ï¼š

| ç»´åº¦ | Triplet Loss | InfoNCE Loss |
|-----|-------------|-------------|
| **è´Ÿæ ·æœ¬æ•°é‡** | å›ºå®š1ä¸ª | å¯æ‰©å±•ï¼ˆbatchå†…æ‰€æœ‰ï¼‰ |
| **æ¢¯åº¦æ•ˆç‡** | ä½ï¼ˆä»…1ä¸ªè´Ÿæ ·æœ¬æä¾›æ¢¯åº¦ï¼‰ | é«˜ï¼ˆæ‰€æœ‰è´Ÿæ ·æœ¬æä¾›æ¢¯åº¦ï¼‰ |
| **æŒ–æ˜éœ€æ±‚** | å¿…é¡»æŒ–æ˜éš¾è´Ÿæ ·æœ¬ | In-Batchå³å¯å¯åŠ¨ |
| **æ”¶æ•›é€Ÿåº¦** | æ…¢ | å¿« |
| **SOTAçŠ¶æ€** | ä¼ ç»Ÿæ–¹æ³• | **ä¸»æµé€‰æ‹©** |

#### SimCLRé£æ ¼ï¼ˆæ•°æ®å¢å¼ºï¼‰

```python
@dataclass
class SimCLRDataAugmentation:
    """SimCLRæ•°æ®å¢å¼ºï¼ˆç”¨äºæ–‡æœ¬ï¼‰"""
    
    @staticmethod
    def back_translation(text: str) -> str:
        """å›è¯‘å¢å¼ºï¼ˆéœ€è¦ç¿»è¯‘APIï¼‰"""
        # ç®€åŒ–ç¤ºä¾‹ï¼šéšæœºåˆ é™¤è¯æ±‡
        words = text.split()
        if len(words) > 3:
            drop_idx = np.random.randint(1, len(words) - 1)
            words.pop(drop_idx)
        return " ".join(words)
    
    @staticmethod
    def synonym_replacement(text: str) -> str:
        """åŒä¹‰è¯æ›¿æ¢ï¼ˆéœ€è¦WordNetï¼‰"""
        # ç®€åŒ–ç¤ºä¾‹ï¼šéšæœºé‡å¤è¯æ±‡
        words = text.split()
        if words:
            dup_idx = np.random.randint(0, len(words))
            words.insert(dup_idx, words[dup_idx])
        return " ".join(words)
    
    @staticmethod
    def random_deletion(text: str, p: float = 0.1) -> str:
        """éšæœºåˆ é™¤è¯æ±‡"""
        words = text.split()
        if len(words) == 1:
            return text
        
        new_words = [w for w in words if np.random.rand() > p]
        if len(new_words) == 0:
            return np.random.choice(words)
        
        return " ".join(new_words)

# ä½¿ç”¨å¢å¼ºæ„é€ æ­£æ ·æœ¬å¯¹
original_text = "æ·±åº¦å­¦ä¹ æ¨¡å‹é€šè¿‡å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒ"
augmented_text = SimCLRDataAugmentation.random_deletion(original_text, p=0.15)

print(f"åŸå§‹æ–‡æœ¬: {original_text}")
print(f"å¢å¼ºæ–‡æœ¬: {augmented_text}")
# è¿™ä¸¤ä¸ªæ–‡æœ¬å¯ä»¥ä½œä¸ºä¸€ä¸ªæ­£æ ·æœ¬å¯¹
```

#### SimCSEï¼ˆç®€å•å¯¹æ¯”å­¦ä¹ ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨Dropoutä½œä¸ºæ•°æ®å¢å¼ºï¼

```python
class SimCSELoss(nn.Module):
    """SimCSEæŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 0.05):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, embeddings1: torch.Tensor, 
                embeddings2: torch.Tensor) -> torch.Tensor:
        """
        Args:
            embeddings1: [batch_size, dim] ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆdropout1ï¼‰
            embeddings2: [batch_size, dim] ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­ï¼ˆdropout2ï¼‰
        
        åŒä¸€ä¸ªå¥å­çš„ä¸¤æ¬¡dropoutè¾“å‡ºä½œä¸ºæ­£æ ·æœ¬å¯¹
        """
        batch_size = embeddings1.size(0)
        
        # å½’ä¸€åŒ–
        embeddings1 = F.normalize(embeddings1, p=2, dim=1)
        embeddings2 = F.normalize(embeddings2, p=2, dim=1)
        
        # æ‹¼æ¥
        embeddings = torch.cat([embeddings1, embeddings2], dim=0)  # [2*B, dim]
        
        # ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = embeddings @ embeddings.T  # [2*B, 2*B]
        sim_matrix = sim_matrix / self.temperature
        
        # æ„é€ æ ‡ç­¾
        # embeddings1[i] çš„æ­£æ ·æœ¬æ˜¯ embeddings2[i]ï¼ˆç´¢å¼•ä¸º i + batch_sizeï¼‰
        labels = torch.arange(batch_size, device=embeddings.device)
        labels = torch.cat([labels + batch_size, labels], dim=0)
        
        # æ’é™¤è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=embeddings.device)
        sim_matrix = sim_matrix.masked_fill(mask, -9e15)
        
        # äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(sim_matrix, labels)
        
        return loss

# SimCSEè®­ç»ƒç¤ºä¾‹
model = BiEncoder("bert-base-uncased", pooling_mode="mean")
model.train()  # å¯ç”¨Dropout

texts = ["æ·±åº¦å­¦ä¹ è®­ç»ƒæ–¹æ³•"] * 32  # åŒä¸€ä¸ªå¥å­
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

inputs = tokenizer(
    texts, 
    padding=True, 
    truncation=True, 
    max_length=128, 
    return_tensors="pt"
)

# ä¸¤æ¬¡å‰å‘ä¼ æ’­ï¼ˆDropoutä¸åŒï¼‰
embeddings1 = model(inputs["input_ids"], inputs["attention_mask"])
embeddings2 = model(inputs["input_ids"], inputs["attention_mask"])

# è®¡ç®—SimCSEæŸå¤±
simcse_loss_fn = SimCSELoss(temperature=0.05)
loss = simcse_loss_fn(embeddings1, embeddings2)
print(f"SimCSE Loss: {loss.item():.4f}")
```

---

### 2.7 æœ¬èŠ‚å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **å¯¹æ¯”å­¦ä¹ åŸç†**ï¼š
   - æ‹‰è¿‘ç›¸ä¼¼æ ·æœ¬ï¼ˆæ­£æ ·æœ¬ï¼‰
   - æ¨è¿œä¸ç›¸ä¼¼æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬ï¼‰

2. **InfoNCEæŸå¤±**ï¼š
   - æ•°å­¦å½¢å¼ï¼š$\mathcal{L} = -\log \frac{\exp(\text{sim}(x, x^+) / \tau)}{\exp(\text{sim}(x, x^+) / \tau) + \sum_i \exp(\text{sim}(x, x^-_i) / \tau)}$
   - ç›®æ ‡ï¼šæ­£æ ·æœ¬åœ¨æ‰€æœ‰å€™é€‰æ ·æœ¬ä¸­æ¦‚ç‡æœ€å¤§

3. **In-Batch Negatives**ï¼š
   - åˆ©ç”¨batchå†…å…¶ä»–æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
   - batch_size=Nï¼Œæ¯ä¸ªæ ·æœ¬è‡ªåŠ¨è·å¾—N-1ä¸ªè´Ÿæ ·æœ¬
   - æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡

4. **æ¸©åº¦å‚æ•°**ï¼š
   - æ¨èå€¼ï¼š$\tau = 0.05 \sim 0.1$
   - å¤ªå°ï¼šè¿‡åº¦è‡ªä¿¡ï¼Œè®­ç»ƒä¸ç¨³å®š
   - å¤ªå¤§ï¼šæ¢¯åº¦å¤ªå°ï¼Œè®­ç»ƒç¼“æ…¢

5. **å˜ä½“æ–¹æ³•**ï¼š
   - SimCLRï¼šæ•°æ®å¢å¼ºæ„é€ æ­£æ ·æœ¬
   - SimCSEï¼šDropoutä½œä¸ºå¢å¼ºï¼ˆæ— éœ€é¢å¤–æ•°æ®ï¼‰

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼š

ç¬¬ä¸‰èŠ‚ã€Šéš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥ã€‹å°†æ·±å…¥è®²è§£ï¼š
- ä¸ºä»€ä¹ˆéœ€è¦éš¾è´Ÿæ ·æœ¬ï¼Ÿ
- éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç®—æ³•ï¼ˆHard Negatives Miningï¼‰
- åŠ¨æ€éš¾è´Ÿæ ·æœ¬æ›´æ–°
- å®æˆ˜ï¼šå®ç°å®Œæ•´çš„éš¾è´Ÿæ ·æœ¬è®­ç»ƒæµç¨‹


## ç¬¬ä¸‰èŠ‚ï¼šéš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦éš¾è´Ÿæ ·æœ¬ï¼Ÿ

#### ç®€å•è´Ÿæ ·æœ¬çš„å±€é™æ€§

**é—®é¢˜**ï¼šéšæœºé‡‡æ ·çš„è´Ÿæ ·æœ¬å¾€å¾€"å¤ªç®€å•"ï¼Œæ¨¡å‹è½»æ˜“å°±èƒ½åŒºåˆ†ã€‚

```python
@dataclass
class NegativeSampleQuality:
    """è´Ÿæ ·æœ¬è´¨é‡åˆ†æ"""
    
    @staticmethod
    def demonstrate_easy_vs_hard():
        print("=" * 80)
        print("ç®€å•è´Ÿæ ·æœ¬ vs éš¾è´Ÿæ ·æœ¬")
        print("=" * 80)
        
        query = "æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•è®­ç»ƒï¼Ÿ"
        
        print(f"Query: {query}\n")
        
        print("âœ“ æ­£æ ·æœ¬:")
        print("  - 'ç¥ç»ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè®­ç»ƒ'")
        print("  - è¯­ä¹‰é«˜åº¦ç›¸å…³ï¼Œåº”è¯¥å¾—åˆ†é«˜\n")
        
        print("âœ— ç®€å•è´Ÿæ ·æœ¬ï¼ˆéšæœºé‡‡æ ·ï¼‰:")
        print("  - 'ä»Šå¤©å¤©æ°”å¾ˆå¥½é˜³å…‰æ˜åªš'")
        print("  - 'æˆ‘å–œæ¬¢åƒè‹¹æœå’Œé¦™è•‰'")
        print("  - è¯­ä¹‰å®Œå…¨æ— å…³ï¼Œæ¨¡å‹è½»æ˜“åŒºåˆ†ï¼ˆæ— å­¦ä¹ ä»·å€¼ï¼‰\n")
        
        print("âœ—âœ— éš¾è´Ÿæ ·æœ¬ï¼ˆHard Negativesï¼‰:")
        print("  - 'æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„è®¾è®¡éå¸¸é‡è¦'")
        print("  - 'ç¥ç»ç½‘ç»œçš„è¶…å‚æ•°è°ƒä¼˜æŠ€å·§'")
        print("  - è¯­ä¹‰ç›¸å…³ä½†ä¸æ˜¯ç­”æ¡ˆï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦åŒºåˆ†\n")
        
        print("=" * 80)
        print("å…³é”®æ´å¯Ÿ:")
        print("  ç®€å•è´Ÿæ ·æœ¬ â†’ æ¨¡å‹è¿‡åº¦è‡ªä¿¡ â†’ æ³›åŒ–èƒ½åŠ›å¼±")
        print("  éš¾è´Ÿæ ·æœ¬   â†’ æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦ç‰¹å¾ â†’ æ³›åŒ–èƒ½åŠ›å¼º")
        print("=" * 80)

NegativeSampleQuality.demonstrate_easy_vs_hard()
```

#### å®éªŒè¯æ®

**DPRè®ºæ–‡ï¼ˆFacebook AIï¼Œ2020ï¼‰**å®éªŒç»“æœï¼š

| è´Ÿæ ·æœ¬ç±»å‹ | Top-20å‡†ç¡®ç‡ | è¯´æ˜ |
|----------|------------|-----|
| éšæœºè´Ÿæ ·æœ¬ | 65.2% | åŸºçº¿ |
| BM25æ£€ç´¢è´Ÿæ ·æœ¬ | 78.4% | +13.2% |
| æ¨¡å‹éš¾è´Ÿæ ·æœ¬ | **81.4%** | **+16.2%** |

---

### 3.2 éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç®—æ³•

#### é™æ€éš¾è´Ÿæ ·æœ¬æŒ–æ˜

**æ–¹æ³•1ï¼šBM25æ£€ç´¢è´Ÿæ ·æœ¬**

ä½¿ç”¨BM25ä»æ–‡æ¡£åº“ä¸­æ£€ç´¢ä¸queryç›¸ä¼¼ä½†ä¸æ˜¯ç­”æ¡ˆçš„æ–‡æ¡£ã€‚

```python
from dataclasses import dataclass, field
from typing import List, Set
import math

@dataclass
class BM25:
    """BM25ç®—æ³•å®ç°"""
    k1: float = 1.5  # term frequencyé¥±å’Œå‚æ•°
    b: float = 0.75  # é•¿åº¦å½’ä¸€åŒ–å‚æ•°
    
    corpus: List[str] = field(default_factory=list)
    doc_lengths: List[int] = field(default_factory=list)
    avg_doc_length: float = 0.0
    idf: dict = field(default_factory=dict)
    
    def fit(self, corpus: List[str]):
        """è®­ç»ƒBM25"""
        self.corpus = corpus
        num_docs = len(corpus)
        
        # 1. è®¡ç®—æ–‡æ¡£é•¿åº¦
        self.doc_lengths = [len(doc.split()) for doc in corpus]
        self.avg_doc_length = sum(self.doc_lengths) / num_docs
        
        # 2. è®¡ç®—IDF
        df = {}  # Document Frequency
        for doc in corpus:
            words = set(doc.split())
            for word in words:
                df[word] = df.get(word, 0) + 1
        
        for word, freq in df.items():
            self.idf[word] = math.log((num_docs - freq + 0.5) / (freq + 0.5) + 1.0)
    
    def score(self, query: str, doc_idx: int) -> float:
        """è®¡ç®—queryå’Œæ–‡æ¡£çš„BM25å¾—åˆ†"""
        doc = self.corpus[doc_idx]
        doc_words = doc.split()
        doc_length = self.doc_lengths[doc_idx]
        
        score = 0.0
        for query_word in query.split():
            if query_word not in self.idf:
                continue
            
            # Term frequency
            tf = doc_words.count(query_word)
            
            # BM25 score
            idf = self.idf[query_word]
            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)
            
            score += idf * (numerator / denominator)
        
        return score
    
    def get_top_k(self, query: str, k: int = 10, exclude_indices: Set[int] = None) -> List[int]:
        """æ£€ç´¢Top-Kæ–‡æ¡£"""
        if exclude_indices is None:
            exclude_indices = set()
        
        scores = []
        for idx in range(len(self.corpus)):
            if idx in exclude_indices:
                continue
            score = self.score(query, idx)
            scores.append((idx, score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in scores[:k]]

# ä½¿ç”¨ç¤ºä¾‹
corpus = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè®­ç»ƒ",  # æ­£æ ·æœ¬
    "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„è®¾è®¡éå¸¸é‡è¦",      # éš¾è´Ÿæ ·æœ¬ï¼ˆç›¸å…³ä½†ä¸æ˜¯ç­”æ¡ˆï¼‰
    "ç¥ç»ç½‘ç»œçš„è¶…å‚æ•°è°ƒä¼˜æŠ€å·§",            # éš¾è´Ÿæ ·æœ¬
    "æœºå™¨å­¦ä¹ åŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ",    # ä¸­ç­‰è´Ÿæ ·æœ¬
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½é˜³å…‰æ˜åªš",                # ç®€å•è´Ÿæ ·æœ¬
    "æˆ‘å–œæ¬¢åƒè‹¹æœå’Œé¦™è•‰",                  # ç®€å•è´Ÿæ ·æœ¬
]

bm25 = BM25()
bm25.fit(corpus)

query = "æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•è®­ç»ƒï¼Ÿ"
positive_idx = 0

# æ£€ç´¢Top-3è´Ÿæ ·æœ¬ï¼ˆæ’é™¤æ­£æ ·æœ¬ï¼‰
hard_neg_indices = bm25.get_top_k(query, k=3, exclude_indices={positive_idx})

print(f"Query: {query}\n")
print(f"æ­£æ ·æœ¬: {corpus[positive_idx]}\n")
print("BM25æ£€ç´¢çš„éš¾è´Ÿæ ·æœ¬:")
for i, idx in enumerate(hard_neg_indices, 1):
    print(f"  {i}. {corpus[idx]}")
```

---

#### åŠ¨æ€éš¾è´Ÿæ ·æœ¬æŒ–æ˜

**æ–¹æ³•2ï¼šåŸºäºå½“å‰æ¨¡å‹çš„éš¾è´Ÿæ ·æœ¬**

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹æ£€ç´¢æœ€ç›¸ä¼¼ä½†é”™è¯¯çš„æ–‡æ¡£ä½œä¸ºéš¾è´Ÿæ ·æœ¬ã€‚

```python
class DynamicHardNegativeMiner:
    """åŠ¨æ€éš¾è´Ÿæ ·æœ¬æŒ–æ˜å™¨"""
    
    def __init__(self, 
                 model: BiEncoder,
                 corpus: List[str],
                 tokenizer: AutoTokenizer,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model
        self.corpus = corpus
        self.tokenizer = tokenizer
        self.device = device
        
        # é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
        self.doc_embeddings = None
        self._precompute_doc_embeddings()
    
    def _precompute_doc_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„åµŒå…¥"""
        self.model.eval()
        
        all_embeddings = []
        batch_size = 32
        
        with torch.no_grad():
            for i in range(0, len(self.corpus), batch_size):
                batch_texts = self.corpus[i:i + batch_size]
                
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=128,
                    return_tensors="pt"
                ).to(self.device)
                
                embeddings = self.model(
                    inputs["input_ids"],
                    inputs["attention_mask"]
                )
                all_embeddings.append(embeddings.cpu())
        
        self.doc_embeddings = torch.cat(all_embeddings, dim=0)
        print(f"é¢„è®¡ç®—å®Œæˆ: {self.doc_embeddings.shape[0]} æ–‡æ¡£")
    
    def mine_hard_negatives(self,
                           query: str,
                           positive_idx: int,
                           num_negatives: int = 5) -> List[int]:
        """æŒ–æ˜éš¾è´Ÿæ ·æœ¬"""
        self.model.eval()
        
        with torch.no_grad():
            # 1. ç¼–ç query
            query_inputs = self.tokenizer(
                [query],
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors="pt"
            ).to(self.device)
            
            query_emb = self.model(
                query_inputs["input_ids"],
                query_inputs["attention_mask"]
            ).cpu()
            
            # 2. è®¡ç®—ç›¸ä¼¼åº¦
            similarities = F.cosine_similarity(
                query_emb, 
                self.doc_embeddings, 
                dim=1
            )
            
            # 3. æ’åºï¼ˆé™åºï¼‰
            sorted_indices = torch.argsort(similarities, descending=True)
            
            # 4. é€‰æ‹©éš¾è´Ÿæ ·æœ¬ï¼šæ’é™¤æ­£æ ·æœ¬ï¼Œå–æœ€ç›¸ä¼¼çš„Kä¸ª
            hard_negatives = []
            for idx in sorted_indices.tolist():
                if idx != positive_idx:
                    hard_negatives.append(idx)
                    if len(hard_negatives) >= num_negatives:
                        break
            
            return hard_negatives
    
    def update_embeddings(self):
        """æ›´æ–°æ–‡æ¡£åµŒå…¥ï¼ˆè®­ç»ƒä¸€å®šæ­¥æ•°åè°ƒç”¨ï¼‰"""
        self._precompute_doc_embeddings()
```

---

### 3.3 æ··åˆéš¾åº¦è´Ÿæ ·æœ¬ç­–ç•¥

**å…¨éƒ¨éš¾è´Ÿæ ·æœ¬çš„é—®é¢˜**ï¼š
- è®­ç»ƒæ—©æœŸï¼Œæ¨¡å‹èƒ½åŠ›å¼±ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½"éš¾"ï¼Œæ”¶æ•›æ…¢
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

**è§£å†³æ–¹æ¡ˆ**ï¼šæ··åˆç®€å•ã€ä¸­ç­‰ã€å›°éš¾è´Ÿæ ·æœ¬

```python
@dataclass
class MixedNegativeSampler:
    """æ··åˆéš¾åº¦è´Ÿæ ·æœ¬é‡‡æ ·å™¨"""
    
    @staticmethod
    def sample_mixed_negatives(
        query: str,
        positive_idx: int,
        corpus: List[str],
        model: BiEncoder,
        tokenizer: AutoTokenizer,
        num_negatives: int = 8,
        hard_ratio: float = 0.5,
        device: str = "cpu"
    ) -> List[int]:
        """é‡‡æ ·æ··åˆéš¾åº¦è´Ÿæ ·æœ¬"""
        num_hard = int(num_negatives * hard_ratio)
        num_random = num_negatives - num_hard
        
        # 1. æŒ–æ˜éš¾è´Ÿæ ·æœ¬
        miner = DynamicHardNegativeMiner(model, corpus, tokenizer, device)
        hard_negatives = miner.mine_hard_negatives(
            query, positive_idx, num_negatives=num_hard
        )
        
        # 2. éšæœºé‡‡æ ·ç®€å•è´Ÿæ ·æœ¬
        all_indices = set(range(len(corpus))) - {positive_idx} - set(hard_negatives)
        random_negatives = np.random.choice(
            list(all_indices), 
            size=min(num_random, len(all_indices)), 
            replace=False
        ).tolist()
        
        # 3. æ··åˆ
        mixed_negatives = hard_negatives + random_negatives
        
        return mixed_negatives
```

**æ¨èé…ç½®**ï¼ˆç»éªŒå€¼ï¼‰ï¼š

| è®­ç»ƒé˜¶æ®µ | hard_ratio | æ›´æ–°é¢‘ç‡ | è¯´æ˜ |
|---------|-----------|---------|-----|
| æ—©æœŸï¼ˆ0~30%ï¼‰ | 0.3~0.5 | æ¯1000æ­¥ | æ¨¡å‹å¼±ï¼Œé™ä½éš¾åº¦ |
| ä¸­æœŸï¼ˆ30%~70%ï¼‰ | 0.5~0.7 | æ¯500æ­¥ | é€æ­¥å¢åŠ éš¾åº¦ |
| åæœŸï¼ˆ70%~100%ï¼‰ | 0.7~0.9 | æ¯200æ­¥ | ä¸“æ³¨éš¾æ ·æœ¬ |

---

### 3.4 ANCEç®—æ³•ï¼ˆè¿‘ä¼¼æœ€è¿‘é‚»å¯¹æ¯”ä¼°è®¡ï¼‰

#### æ ¸å¿ƒæ€æƒ³

**ANCEï¼ˆApproximate Nearest Neighbor Negative Contrastive Estimationï¼‰**ï¼š
- ç»´æŠ¤ä¸€ä¸ªå¼‚æ­¥æ›´æ–°çš„æ–‡æ¡£ç´¢å¼•
- ä½¿ç”¨ANNï¼ˆå¦‚FAISSï¼‰å¿«é€Ÿæ£€ç´¢éš¾è´Ÿæ ·æœ¬
- é¿å…æ¯æ­¥éƒ½é‡æ–°è®¡ç®—å…¨éƒ¨æ–‡æ¡£åµŒå…¥

```python
import faiss

class ANCETrainer:
    """ANCEè®­ç»ƒå™¨ï¼ˆä½¿ç”¨FAISSåŠ é€Ÿï¼‰"""
    
    def __init__(self,
                 model: BiEncoder,
                 corpus: List[str],
                 tokenizer: AutoTokenizer,
                 embedding_dim: int = 768,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        
        self.model = model.to(device)
        self.corpus = corpus
        self.tokenizer = tokenizer
        self.device = device
        
        # FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(embedding_dim)  # Inner Product
        self._build_index()
    
    def _build_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        print("æ„å»ºFAISSç´¢å¼•...")
        self.model.eval()
        
        all_embeddings = []
        batch_size = 64
        
        with torch.no_grad():
            for i in range(0, len(self.corpus), batch_size):
                batch_texts = self.corpus[i:i + batch_size]
                
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=128,
                    return_tensors="pt"
                ).to(self.device)
                
                embeddings = self.model(
                    inputs["input_ids"],
                    inputs["attention_mask"]
                )
                
                # L2å½’ä¸€åŒ–
                embeddings = F.normalize(embeddings, p=2, dim=1)
                all_embeddings.append(embeddings.cpu().numpy())
        
        embeddings_np = np.vstack(all_embeddings).astype('float32')
        self.index.reset()
        self.index.add(embeddings_np)
        print(f"ç´¢å¼•æ„å»ºå®Œæˆ: {self.index.ntotal} æ–‡æ¡£")
    
    def search_hard_negatives(self,
                             query: str,
                             positive_idx: int,
                             k: int = 100) -> List[int]:
        """ä½¿ç”¨FAISSå¿«é€Ÿæ£€ç´¢éš¾è´Ÿæ ·æœ¬"""
        self.model.eval()
        
        with torch.no_grad():
            query_inputs = self.tokenizer(
                [query],
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors="pt"
            ).to(self.device)
            
            query_emb = self.model(
                query_inputs["input_ids"],
                query_inputs["attention_mask"]
            )
            query_emb = F.normalize(query_emb, p=2, dim=1)
            query_emb_np = query_emb.cpu().numpy().astype('float32')
            
            # FAISSæ£€ç´¢
            distances, indices = self.index.search(query_emb_np, k + 1)
            
            # æ’é™¤æ­£æ ·æœ¬
            hard_negatives = []
            for idx in indices[0]:
                if idx != positive_idx:
                    hard_negatives.append(int(idx))
                    if len(hard_negatives) >= k:
                        break
            
            return hard_negatives
```

**ANCEæ€§èƒ½å¯¹æ¯”**ï¼š

| æ–¹æ³• | 100ä¸‡æ–‡æ¡£æ£€ç´¢æ—¶é—´ | å†…å­˜å ç”¨ |
|-----|----------------|---------|
| æœ´ç´ éå† | ~5000ms | 3GB |
| FAISS IndexFlatIP | ~8ms | 3GB |
| FAISS IVF | ~2ms | 1.5GB |

---

### 3.5 æœ¬èŠ‚å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **ä¸ºä»€ä¹ˆéœ€è¦éš¾è´Ÿæ ·æœ¬ï¼Ÿ**
   - éšæœºè´Ÿæ ·æœ¬å¤ªç®€å•ï¼Œæ— å­¦ä¹ ä»·å€¼
   - éš¾è´Ÿæ ·æœ¬è¿«ä½¿æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦ç‰¹å¾
   - æ€§èƒ½æå‡ï¼š10~15%ï¼ˆDPRè®ºæ–‡ï¼‰

2. **æŒ–æ˜æ–¹æ³•**ï¼š
   - **é™æ€**ï¼šBM25æ£€ç´¢
   - **åŠ¨æ€**ï¼šå½“å‰æ¨¡å‹æ£€ç´¢
   - **æ··åˆ**ï¼š70%éš¾ + 30%éšæœº

3. **å…³é”®æŠ€æœ¯**ï¼š
   - å®šæœŸæ›´æ–°åµŒå…¥ï¼ˆæ¯500æ­¥ï¼‰
   - FAISSåŠ é€Ÿæ£€ç´¢ï¼ˆANCEï¼‰
   - In-batch + Hard negativesç»“åˆ

4. **è®­ç»ƒç­–ç•¥**ï¼š
   - æ—©æœŸï¼šhard_ratio=0.3~0.5
   - åæœŸï¼šhard_ratio=0.7~0.9

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼š

ç¬¬å››èŠ‚ã€Šå¤šä»»åŠ¡åµŒå…¥è®­ç»ƒã€‹å°†æ·±å…¥è®²è§£ï¼š
- å¤šä»»åŠ¡å­¦ä¹ çš„ä¼˜åŠ¿
- ä»»åŠ¡ç±»å‹ä¸æƒé‡
- å®æˆ˜ï¼šå¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶


## ç¬¬å››èŠ‚ï¼šå¤šä»»åŠ¡åµŒå…¥è®­ç»ƒ

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šä»»åŠ¡å­¦ä¹ ï¼Ÿ

#### å•ä»»åŠ¡è®­ç»ƒçš„å±€é™æ€§

**é—®é¢˜**ï¼šä»…åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒçš„åµŒå…¥æ¨¡å‹ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚

```python
@dataclass
class SingleTaskLimitation:
    """å•ä»»åŠ¡è®­ç»ƒå±€é™æ€§æ¼”ç¤º"""
    
    @staticmethod
    def demonstrate():
        print("=" * 80)
        print("å•ä»»åŠ¡ vs å¤šä»»åŠ¡åµŒå…¥æ¨¡å‹")
        print("=" * 80)
        
        print("åœºæ™¯ï¼šä»…åœ¨è¯­ä¹‰æ£€ç´¢ä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹\n")
        
        print("âœ“ æ“…é•¿ä»»åŠ¡ï¼ˆè®­ç»ƒä»»åŠ¡ï¼‰:")
        print("  - è¯­ä¹‰æ£€ç´¢: ç»™å®šqueryï¼Œæ‰¾åˆ°æœ€ç›¸å…³æ–‡æ¡£")
        print("  - æ€§èƒ½: ä¼˜ç§€ï¼ˆNDCG@10 = 0.85ï¼‰\n")
        
        print("âœ— å¼±åŠ¿ä»»åŠ¡ï¼ˆæœªè§è¿‡çš„ä»»åŠ¡ï¼‰:")
        print("  - è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆSTSï¼‰: åˆ¤æ–­ä¸¤ä¸ªå¥å­ç›¸ä¼¼åº¦")
        print("    æ€§èƒ½: ä¸­ç­‰ï¼ˆSpearman = 0.65ï¼‰")
        print("  - æ–‡æœ¬åˆ†ç±»: å°†æ–‡æœ¬åˆ†ç±»åˆ°å›ºå®šç±»åˆ«")
        print("    æ€§èƒ½: å·®ï¼ˆAccuracy = 0.58ï¼‰")
        print("  - èšç±»: å°†ç›¸ä¼¼æ–‡æœ¬èšåœ¨ä¸€èµ·")
        print("    æ€§èƒ½: å·®ï¼ˆV-measure = 0.42ï¼‰\n")
        
        print("=" * 80)
        print("å¤šä»»åŠ¡è®­ç»ƒçš„ä¼˜åŠ¿:")
        print("  âœ“ å­¦ä¹ æ›´é€šç”¨çš„è¯­ä¹‰è¡¨ç¤º")
        print("  âœ“ ä¸åŒä»»åŠ¡äº’è¡¥ï¼Œæå‡æ³›åŒ–èƒ½åŠ›")
        print("  âœ“ åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šæ€§èƒ½å‡è¡¡")
        print("=" * 80)

SingleTaskLimitation.demonstrate()
```

#### å®éªŒè¯æ®

**E5è®ºæ–‡ï¼ˆMicrosoftï¼Œ2022ï¼‰**å¤šä»»åŠ¡è®­ç»ƒæ•ˆæœï¼š

| æ¨¡å‹ | æ£€ç´¢(NDCG@10) | STS(Spearman) | åˆ†ç±»(Acc) | èšç±»(V-measure) | å¹³å‡ |
|-----|--------------|---------------|----------|----------------|------|
| å•ä»»åŠ¡ï¼ˆæ£€ç´¢ï¼‰ | 0.85 | 0.65 | 0.58 | 0.42 | 0.625 |
| å¤šä»»åŠ¡ï¼ˆ4ä»»åŠ¡ï¼‰ | **0.83** | **0.81** | **0.76** | **0.69** | **0.773** |

**å…³é”®æ´å¯Ÿ**ï¼šå¤šä»»åŠ¡è®­ç»ƒç‰ºç‰²å°‘é‡å•ä»»åŠ¡æ€§èƒ½ï¼ˆ-2%ï¼‰ï¼Œæ¢å–å…¨é¢èƒ½åŠ›æå‡ï¼ˆ+24%å¹³å‡ï¼‰ã€‚

---

### 4.2 å¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶

#### ä»»åŠ¡ç±»å‹å®šä¹‰

```python
from enum import Enum
from abc import ABC, abstractmethod

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹"""
    RETRIEVAL = "retrieval"  # æ£€ç´¢
    STS = "sts"  # è¯­ä¹‰ç›¸ä¼¼åº¦
    CLASSIFICATION = "classification"  # åˆ†ç±»
    CLUSTERING = "clustering"  # èšç±»
    RERANKING = "reranking"  # é‡æ’åº

@dataclass
class Task(ABC):
    """ä»»åŠ¡åŸºç±»"""
    task_type: TaskType
    task_name: str
    weight: float = 1.0
    
    @abstractmethod
    def compute_loss(self, model: BiEncoder, batch: dict) -> torch.Tensor:
        """è®¡ç®—ä»»åŠ¡æŸå¤±"""
        pass

class RetrievalTask(Task):
    """æ£€ç´¢ä»»åŠ¡"""
    
    def __init__(self, task_name: str = "retrieval", 
                 weight: float = 1.0,
                 temperature: float = 0.07):
        super().__init__(TaskType.RETRIEVAL, task_name, weight)
        self.loss_fn = InfoNCEWithInBatchNegatives(temperature)
    
    def compute_loss(self, model: BiEncoder, batch: dict) -> Tuple[torch.Tensor, dict]:
        """
        batchæ ¼å¼: {"query_ids", "query_mask", "doc_ids", "doc_mask"}
        """
        query_embs = model(batch["query_ids"], batch["query_mask"])
        doc_embs = model(batch["doc_ids"], batch["doc_mask"])
        
        loss, metrics = self.loss_fn(query_embs, doc_embs)
        return loss, metrics

class STSTask(Task):
    """è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡"""
    
    def __init__(self, task_name: str = "sts", weight: float = 1.0):
        super().__init__(TaskType.STS, task_name, weight)
    
    def compute_loss(self, model: BiEncoder, batch: dict) -> Tuple[torch.Tensor, dict]:
        """
        batchæ ¼å¼: {"sent1_ids", "sent1_mask", "sent2_ids", "sent2_mask", "scores"}
        scores: [0, 1] å½’ä¸€åŒ–ç›¸ä¼¼åº¦åˆ†æ•°
        """
        sent1_embs = model(batch["sent1_ids"], batch["sent1_mask"])
        sent2_embs = model(batch["sent2_ids"], batch["sent2_mask"])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        pred_scores = F.cosine_similarity(sent1_embs, sent2_embs, dim=1)
        
        # MSEæŸå¤±
        true_scores = batch["scores"]
        loss = F.mse_loss(pred_scores, true_scores)
        
        # Spearmanç›¸å…³ç³»æ•°ï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰
        from scipy.stats import spearmanr
        correlation, _ = spearmanr(
            pred_scores.detach().cpu().numpy(),
            true_scores.cpu().numpy()
        )
        
        metrics = {"correlation": correlation}
        return loss, metrics

class ClassificationTask(Task):
    """åˆ†ç±»ä»»åŠ¡"""
    
    def __init__(self, 
                 task_name: str = "classification",
                 num_classes: int = 10,
                 weight: float = 1.0):
        super().__init__(TaskType.CLASSIFICATION, task_name, weight)
        self.num_classes = num_classes
    
    def compute_loss(self, model: BiEncoder, batch: dict) -> Tuple[torch.Tensor, dict]:
        """
        batchæ ¼å¼: {"input_ids", "attention_mask", "labels"}
        
        æ–¹æ³•ï¼šè®¡ç®—æ–‡æœ¬åµŒå…¥ä¸ç±»åˆ«åŸå‹çš„ç›¸ä¼¼åº¦
        """
        text_embs = model(batch["input_ids"], batch["attention_mask"])
        
        # ç±»åˆ«åŸå‹ï¼ˆå¯å­¦ä¹ çš„ç±»åˆ«åµŒå…¥ï¼‰
        if not hasattr(self, 'class_prototypes'):
            self.class_prototypes = nn.Parameter(
                torch.randn(self.num_classes, text_embs.size(1))
            ).to(text_embs.device)
        
        # å½’ä¸€åŒ–
        text_embs = F.normalize(text_embs, p=2, dim=1)
        class_prototypes = F.normalize(self.class_prototypes, p=2, dim=1)
        
        # ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = text_embs @ class_prototypes.T  # [batch, num_classes]
        logits = logits / 0.07  # æ¸©åº¦ç¼©æ”¾
        
        # äº¤å‰ç†µæŸå¤±
        labels = batch["labels"]
        loss = F.cross_entropy(logits, labels)
        
        # å‡†ç¡®ç‡
        predictions = logits.argmax(dim=1)
        accuracy = (predictions == labels).float().mean()
        
        metrics = {"accuracy": accuracy.item()}
        return loss, metrics

class ClusteringTask(Task):
    """èšç±»ä»»åŠ¡"""
    
    def __init__(self, task_name: str = "clustering", weight: float = 0.5):
        super().__init__(TaskType.CLUSTERING, task_name, weight)
    
    def compute_loss(self, model: BiEncoder, batch: dict) -> Tuple[torch.Tensor, dict]:
        """
        batchæ ¼å¼: {"input_ids", "attention_mask", "cluster_ids"}
        
        æ–¹æ³•ï¼šåŒä¸€èšç±»å†…çš„æ ·æœ¬åº”è¯¥æ¥è¿‘
        """
        text_embs = model(batch["input_ids"], batch["attention_mask"])
        text_embs = F.normalize(text_embs, p=2, dim=1)
        
        cluster_ids = batch["cluster_ids"]
        
        # è®¡ç®—èšç±»ä¸­å¿ƒ
        unique_clusters = cluster_ids.unique()
        centers = []
        for cluster_id in unique_clusters:
            mask = (cluster_ids == cluster_id)
            cluster_embs = text_embs[mask]
            center = cluster_embs.mean(dim=0)
            centers.append(center)
        
        centers = torch.stack(centers)  # [num_clusters, dim]
        centers = F.normalize(centers, p=2, dim=1)
        
        # æŸå¤±ï¼šæ ·æœ¬åˆ°å…¶èšç±»ä¸­å¿ƒçš„è·ç¦»
        losses = []
        for i, cluster_id in enumerate(cluster_ids):
            cluster_idx = (unique_clusters == cluster_id).nonzero(as_tuple=True)[0]
            center = centers[cluster_idx]
            
            # ä½™å¼¦è·ç¦» = 1 - ä½™å¼¦ç›¸ä¼¼åº¦
            distance = 1 - F.cosine_similarity(
                text_embs[i:i+1], center, dim=1
            )
            losses.append(distance)
        
        loss = torch.stack(losses).mean()
        
        metrics = {"avg_distance_to_center": loss.item()}
        return loss, metrics
```

---

### 4.3 å¤šä»»åŠ¡è®­ç»ƒå™¨

#### ä»»åŠ¡é‡‡æ ·ç­–ç•¥

```python
from typing import Dict, List
import random

class TaskSampler:
    """å¤šä»»åŠ¡é‡‡æ ·å™¨"""
    
    def __init__(self, tasks: List[Task], 
                 sampling_strategy: str = "weighted",
                 temperature: float = 1.0):
        """
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            sampling_strategy: "weighted"ï¼ˆæŒ‰æƒé‡ï¼‰, "uniform"ï¼ˆå‡åŒ€ï¼‰, "proportional"ï¼ˆæŒ‰æ•°æ®é›†å¤§å°ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆç”¨äºweightedç­–ç•¥ï¼‰
        """
        self.tasks = tasks
        self.sampling_strategy = sampling_strategy
        self.temperature = temperature
        
        self._compute_sampling_probs()
    
    def _compute_sampling_probs(self):
        """è®¡ç®—é‡‡æ ·æ¦‚ç‡"""
        if self.sampling_strategy == "uniform":
            # å‡åŒ€é‡‡æ ·
            self.sampling_probs = [1.0 / len(self.tasks)] * len(self.tasks)
        
        elif self.sampling_strategy == "weighted":
            # æŒ‰æƒé‡é‡‡æ ·ï¼ˆå¸¦æ¸©åº¦ï¼‰
            weights = np.array([task.weight for task in self.tasks])
            weights = weights ** (1.0 / self.temperature)
            self.sampling_probs = weights / weights.sum()
        
        else:
            raise ValueError(f"Unknown strategy: {self.sampling_strategy}")
    
    def sample_task(self) -> Task:
        """é‡‡æ ·ä¸€ä¸ªä»»åŠ¡"""
        task_idx = np.random.choice(
            len(self.tasks),
            p=self.sampling_probs
        )
        return self.tasks[task_idx]
    
    def get_task_stats(self) -> dict:
        """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            task.task_name: {
                "type": task.task_type.value,
                "weight": task.weight,
                "sampling_prob": prob
            }
            for task, prob in zip(self.tasks, self.sampling_probs)
        }

# ä½¿ç”¨ç¤ºä¾‹
tasks = [
    RetrievalTask(task_name="msmarco", weight=2.0),
    STSTask(task_name="stsb", weight=1.0),
    ClassificationTask(task_name="ag_news", num_classes=4, weight=1.0),
    ClusteringTask(task_name="20newsgroups", weight=0.5),
]

sampler = TaskSampler(tasks, sampling_strategy="weighted", temperature=1.0)

print("ä»»åŠ¡é‡‡æ ·ç»Ÿè®¡:")
for task_name, stats in sampler.get_task_stats().items():
    print(f"  {task_name}:")
    print(f"    ç±»å‹: {stats['type']}")
    print(f"    æƒé‡: {stats['weight']}")
    print(f"    é‡‡æ ·æ¦‚ç‡: {stats['sampling_prob']:.3f}")
```

---

#### å®Œæ•´å¤šä»»åŠ¡è®­ç»ƒå™¨

```python
class MultiTaskEmbeddingTrainer:
    """å¤šä»»åŠ¡åµŒå…¥è®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: BiEncoder,
                 tasks: List[Task],
                 task_dataloaders: Dict[str, DataLoader],
                 sampling_strategy: str = "weighted",
                 learning_rate: float = 2e-5,
                 num_epochs: int = 3,
                 steps_per_epoch: int = 10000,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        
        self.model = model.to(device)
        self.tasks = tasks
        self.task_dataloaders = task_dataloaders
        self.device = device
        self.num_epochs = num_epochs
        self.steps_per_epoch = steps_per_epoch
        
        # ä»»åŠ¡é‡‡æ ·å™¨
        self.task_sampler = TaskSampler(tasks, sampling_strategy=sampling_strategy)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = steps_per_epoch * num_epochs
        from transformers import get_linear_schedule_with_warmup
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=int(0.1 * num_training_steps),
            num_training_steps=num_training_steps
        )
        
        # ä»»åŠ¡è¿­ä»£å™¨
        self.task_iterators = {
            task_name: iter(dataloader)
            for task_name, dataloader in task_dataloaders.items()
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.task_loss_history = {task.task_name: [] for task in tasks}
    
    def get_batch(self, task: Task) -> dict:
        """è·å–ä»»åŠ¡çš„ä¸€ä¸ªbatch"""
        try:
            batch = next(self.task_iterators[task.task_name])
        except StopIteration:
            # é‡æ–°åˆ›å»ºè¿­ä»£å™¨
            self.task_iterators[task.task_name] = iter(
                self.task_dataloaders[task.task_name]
            )
            batch = next(self.task_iterators[task.task_name])
        
        # ç§»åˆ°GPU
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                for k, v in batch.items()}
        
        return batch
    
    def train_step(self) -> dict:
        """è®­ç»ƒä¸€æ­¥"""
        self.model.train()
        
        # 1. é‡‡æ ·ä»»åŠ¡
        task = self.task_sampler.sample_task()
        
        # 2. è·å–batch
        batch = self.get_batch(task)
        
        # 3. è®¡ç®—æŸå¤±
        loss, metrics = task.compute_loss(self.model, batch)
        
        # 4. åŠ æƒæŸå¤±
        weighted_loss = task.weight * loss
        
        # 5. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        weighted_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        self.scheduler.step()
        
        # 6. è®°å½•
        self.task_loss_history[task.task_name].append(loss.item())
        
        return {
            "task": task.task_name,
            "loss": loss.item(),
            "weighted_loss": weighted_loss.item(),
            **metrics
        }
    
    def train(self):
        """è®­ç»ƒä¸»å¾ªç¯"""
        print("=" * 80)
        print("å¼€å§‹å¤šä»»åŠ¡è®­ç»ƒ")
        print("=" * 80)
        print(f"ä»»åŠ¡æ•°: {len(self.tasks)}")
        for task in self.tasks:
            print(f"  - {task.task_name} ({task.task_type.value}), æƒé‡={task.weight}")
        print(f"Epochs: {self.num_epochs}")
        print(f"Steps per Epoch: {self.steps_per_epoch}")
        print("=" * 80)
        
        global_step = 0
        
        for epoch in range(self.num_epochs):
            epoch_metrics = {task.task_name: [] for task in self.tasks}
            
            for step in range(self.steps_per_epoch):
                step_result = self.train_step()
                
                # è®°å½•
                task_name = step_result["task"]
                epoch_metrics[task_name].append(step_result["loss"])
                
                global_step += 1
                
                # å®šæœŸæ‰“å°
                if (step + 1) % 1000 == 0:
                    print(f"\n[Epoch {epoch+1}, Step {step+1}/{self.steps_per_epoch}]")
                    print(f"  å½“å‰ä»»åŠ¡: {task_name}")
                    print(f"  Loss: {step_result['loss']:.4f}")
                    print(f"  LR: {self.scheduler.get_last_lr()[0]:.2e}")
            
            # Epochæ€»ç»“
            print(f"\n{'='*80}")
            print(f"Epoch {epoch + 1} æ€»ç»“:")
            print(f"{'='*80}")
            for task_name, losses in epoch_metrics.items():
                if losses:
                    avg_loss = np.mean(losses)
                    print(f"  {task_name}: Avg Loss = {avg_loss:.4f}, æ ·æœ¬æ•° = {len(losses)}")
        
        print("\n" + "=" * 80)
        print("å¤šä»»åŠ¡è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
    
    def plot_task_distribution(self):
        """å¯è§†åŒ–ä»»åŠ¡åˆ†å¸ƒ"""
        import matplotlib.pyplot as plt
        
        task_counts = {
            task.task_name: len(self.task_loss_history[task.task_name])
            for task in self.tasks
        }
        
        plt.figure(figsize=(10, 6))
        plt.bar(task_counts.keys(), task_counts.values())
        plt.xlabel("Task")
        plt.ylabel("Number of Training Steps")
        plt.title("Task Distribution During Training")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig("task_distribution.png")
        print("ä»»åŠ¡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ° task_distribution.png")
```

---

### 4.4 ä»»åŠ¡æƒé‡è°ƒä¼˜

#### åŠ¨æ€æƒé‡è°ƒæ•´

**é—®é¢˜**ï¼šå›ºå®šæƒé‡å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´æƒé‡ã€‚

```python
class DynamicTaskWeighting:
    """åŠ¨æ€ä»»åŠ¡æƒé‡è°ƒæ•´"""
    
    def __init__(self, tasks: List[Task], 
                 update_frequency: int = 1000,
                 alpha: float = 0.5):
        """
        Args:
            alpha: æƒé‡æ›´æ–°é€Ÿç‡
        """
        self.tasks = tasks
        self.update_frequency = update_frequency
        self.alpha = alpha
        
        # è®°å½•æ¯ä¸ªä»»åŠ¡çš„æŸå¤±å†å²
        self.loss_history = {task.task_name: [] for task in tasks}
    
    def update_weights(self, step: int):
        """æ›´æ–°ä»»åŠ¡æƒé‡"""
        if step % self.update_frequency != 0:
            return
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„å¹³å‡æŸå¤±
        avg_losses = {}
        for task in self.tasks:
            if self.loss_history[task.task_name]:
                avg_loss = np.mean(
                    self.loss_history[task.task_name][-self.update_frequency:]
                )
                avg_losses[task.task_name] = avg_loss
        
        if not avg_losses:
            return
        
        # å½’ä¸€åŒ–æŸå¤±ï¼ˆæŸå¤±è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜ï¼‰
        total_loss = sum(avg_losses.values())
        new_weights = {
            name: loss / total_loss 
            for name, loss in avg_losses.items()
        }
        
        # å¹³æ»‘æ›´æ–°
        for task in self.tasks:
            if task.task_name in new_weights:
                old_weight = task.weight
                new_weight = new_weights[task.task_name]
                task.weight = (1 - self.alpha) * old_weight + self.alpha * new_weight
        
        print(f"\n[Step {step}] æƒé‡æ›´æ–°:")
        for task in self.tasks:
            print(f"  {task.task_name}: {task.weight:.4f}")
    
    def record_loss(self, task_name: str, loss: float):
        """è®°å½•ä»»åŠ¡æŸå¤±"""
        self.loss_history[task_name].append(loss)

# åœ¨è®­ç»ƒå™¨ä¸­é›†æˆåŠ¨æ€æƒé‡
class MultiTaskTrainerWithDynamicWeighting(MultiTaskEmbeddingTrainer):
    """å¸¦åŠ¨æ€æƒé‡çš„å¤šä»»åŠ¡è®­ç»ƒå™¨"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dynamic_weighting = DynamicTaskWeighting(
            tasks=self.tasks,
            update_frequency=1000,
            alpha=0.3
        )
    
    def train_step(self) -> dict:
        """è®­ç»ƒä¸€æ­¥ï¼ˆå¸¦åŠ¨æ€æƒé‡ï¼‰"""
        result = super().train_step()
        
        # è®°å½•æŸå¤±
        self.dynamic_weighting.record_loss(result["task"], result["loss"])
        
        return result
    
    def train(self):
        """è®­ç»ƒï¼ˆå¸¦åŠ¨æ€æƒé‡æ›´æ–°ï¼‰"""
        # ... è®­ç»ƒå¾ªç¯
        for epoch in range(self.num_epochs):
            for step in range(self.steps_per_epoch):
                self.train_step()
                
                # åŠ¨æ€æ›´æ–°æƒé‡
                self.dynamic_weighting.update_weights(step)
```

---

### 4.5 MatryoshkaåµŒå…¥ï¼ˆå¯å˜ç»´åº¦åµŒå…¥ï¼‰

#### æ ¸å¿ƒæ€æƒ³

**Matryoshka Representation Learningï¼ˆMRLï¼‰**ï¼šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å¾—åµŒå…¥çš„å‰Kç»´ï¼ˆä»»æ„Kï¼‰éƒ½èƒ½æä¾›è‰¯å¥½çš„è¡¨ç¤ºã€‚è¿™ä¸ªæŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äº2024-2025å¹´çš„SOTAæ¨¡å‹ä¸­ï¼ˆå¦‚OpenAI v3, GTE, BGE-M3ï¼‰ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- **å¼¹æ€§å­˜å‚¨**ï¼šå¯ä»¥æ ¹æ®å­˜å‚¨é™åˆ¶æˆªæ–­å‘é‡ï¼ˆå¦‚ä»1024ç»´æˆªæ–­åˆ°128ç»´ï¼‰
- **æ€§èƒ½ä¿æŒ**ï¼šæˆªæ–­åçš„æ€§èƒ½è¡°å‡æå°ï¼ˆ<3%ï¼‰
- **è‡ªé€‚åº”æ£€ç´¢**ï¼šå¬å›é˜¶æ®µç”¨ä½ç»´å‘é‡ï¼ˆå¿«ï¼‰ï¼Œç²¾æ’é˜¶æ®µç”¨é«˜ç»´å‘é‡ï¼ˆå‡†ï¼‰

```python
class MatryoshkaLoss(nn.Module):
    """MatryoshkaåµŒå…¥æŸå¤±ï¼ˆæ”¯æŒå¤šå±‚çº§è®­ç»ƒï¼‰"""

    def __init__(self,
                 dimensions: List[int] = [768, 512, 256, 128, 64],
                 weights: List[float] = [1.0, 1.0, 1.0, 1.0, 1.0],
                 temperature: float = 0.05):
        """
        Args:
            dimensions: è®­ç»ƒçš„åµŒå…¥ç»´åº¦åˆ—è¡¨ï¼ˆä»å¤§åˆ°å°ï¼‰
            weights: æ¯ä¸ªç»´åº¦çš„æŸå¤±æƒé‡
        """
        super().__init__()
        self.dimensions = dimensions
        self.weights = weights
        self.temperature = temperature
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, query_embs: torch.Tensor,
                doc_embs: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        """
        è®¡ç®—å¤šç²’åº¦æŸå¤±

        Args:
            query_embs: [batch, full_dim]
            doc_embs: [batch, full_dim]
        """
        total_loss = 0.0
        metrics = {}

        batch_size = query_embs.size(0)
        labels = torch.arange(batch_size, device=query_embs.device)

        for i, dim in enumerate(self.dimensions):
            weight = self.weights[i]

            # 1. ç»´åº¦æˆªæ–­ï¼ˆåˆ‡ç‰‡ï¼‰
            q_slice = query_embs[:, :dim]
            d_slice = doc_embs[:, :dim]

            # 2. å½’ä¸€åŒ–ï¼ˆè¿™æ˜¯å…³é”®ï¼šæ¯ä¸€å±‚éƒ½è¦ç‹¬ç«‹å½’ä¸€åŒ–ï¼‰
            q_slice = F.normalize(q_slice, p=2, dim=1)
            d_slice = F.normalize(d_slice, p=2, dim=1)

            # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            sim_matrix = torch.matmul(q_slice, d_slice.T) / self.temperature

            # 4. è®¡ç®—æŸå¤±
            loss = self.loss_fn(sim_matrix, labels)
            total_loss += weight * loss

            # 5. è®°å½•å‡†ç¡®ç‡
            with torch.no_grad():
                preds = torch.argmax(sim_matrix, dim=1)
                acc = (preds == labels).float().mean()
                metrics[f"acc_{dim}d"] = acc.item()
                metrics[f"loss_{dim}d"] = loss.item()

        return total_loss, metrics

# ä½¿ç”¨ç¤ºä¾‹
matryoshka_loss = MatryoshkaLoss(
    dimensions=[768, 256, 64],
    weights=[1.0, 1.0, 1.0]  # é€šå¸¸ç»™äºˆåŒç­‰æƒé‡
)

query_embs = torch.randn(32, 768)
doc_embs = torch.randn(32, 768)

loss, metrics = matryoshka_loss(query_embs, doc_embs)

print(f"Matryoshka Loss: {loss.item():.4f}")
print("å¤šç»´åº¦æ€§èƒ½:")
for dim in [768, 256, 64]:
    print(f"  {dim}d - Acc: {metrics[f'acc_{dim}d']:.4f}")
```

#### SOTAæ¨¡å‹åº”ç”¨æ¡ˆä¾‹

**OpenAI text-embedding-3-large**ï¼š
- å®Œæ•´ç»´åº¦ï¼š3072
- æ”¯æŒæˆªæ–­ï¼š1536ï¼ˆæ€§èƒ½99%ï¼‰ï¼Œ1024ï¼ˆæ€§èƒ½98.5%ï¼‰ï¼Œ256ï¼ˆæ€§èƒ½96%ï¼‰

**åº”ç”¨åœºæ™¯ä¼˜åŒ–**ï¼š

1. **è‡ªé€‚åº”æ£€ç´¢Pipeline**ï¼š
   - ç¬¬ä¸€è½®ï¼šä½¿ç”¨64ç»´å‘é‡å¿«é€Ÿæ£€ç´¢Top-1000ï¼ˆæå¿«ï¼Œå°å†…å­˜ï¼‰
   - ç¬¬äºŒè½®ï¼šä½¿ç”¨å®Œæ•´768ç»´å‘é‡é‡æ’Top-1000å¾—åˆ°Top-100ï¼ˆå‡†ï¼‰
   - ç¬¬ä¸‰è½®ï¼šCross-Encoderç²¾æ’Top-10

2. **å­˜å‚¨æˆæœ¬ä¼˜åŒ–**ï¼š
   - æ™®é€šç”¨æˆ·æ•°æ®ï¼šå­˜å‚¨128ç»´ï¼ˆèŠ‚çœ83%å­˜å‚¨ï¼‰
   - VIPç”¨æˆ·æ•°æ®ï¼šå­˜å‚¨768ç»´ï¼ˆé«˜æ€§èƒ½ï¼‰

---

### 4.6 æœ¬èŠ‚å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **ä¸ºä»€ä¹ˆå¤šä»»åŠ¡ï¼Ÿ**
   - å•ä»»åŠ¡æ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™
   - å¤šä»»åŠ¡å­¦ä¹ æ›´é€šç”¨çš„è¯­ä¹‰è¡¨ç¤º
   - æ€§èƒ½æå‡ï¼šå¹³å‡+24%ï¼ˆE5è®ºæ–‡ï¼‰

2. **ä»»åŠ¡ç±»å‹**ï¼š
   - æ£€ç´¢ï¼ˆRetrievalï¼‰ï¼šInfoNCEæŸå¤±
   - STSï¼ˆç›¸ä¼¼åº¦ï¼‰ï¼šMSEæŸå¤±
   - åˆ†ç±»ï¼ˆClassificationï¼‰ï¼šäº¤å‰ç†µ
   - èšç±»ï¼ˆClusteringï¼‰ï¼šä¸­å¿ƒè·ç¦»

3. **é‡‡æ ·ç­–ç•¥**ï¼š
   - æŒ‰æƒé‡é‡‡æ ·ï¼ˆæ¨èï¼‰
   - å‡åŒ€é‡‡æ ·
   - åŠ¨æ€æƒé‡è°ƒæ•´

4. **MatryoshkaåµŒå…¥**ï¼š
   - å¯å˜ç»´åº¦è¡¨ç¤º
   - 128dè¾¾åˆ°92%æ€§èƒ½ï¼Œé€Ÿåº¦å¿«6å€

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼š

ç¬¬äº”èŠ‚ã€Šä»é›¶å®æˆ˜é«˜æ€§èƒ½åµŒå…¥æ¨¡å‹ã€‹å°†å®Œæ•´å®ç°ï¼š
- æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†
- å®Œæ•´è®­ç»ƒæµç¨‹
- æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²
- å®æˆ˜ï¼šè¶…è¶ŠOpenAI text-embedding-3


## ç¬¬äº”èŠ‚ï¼šä»é›¶å®æˆ˜é«˜æ€§èƒ½åµŒå…¥æ¨¡å‹

### 5.1 å®Œæ•´è®­ç»ƒæµç¨‹æ¦‚è§ˆ

```python
@dataclass
class EmbeddingModelPipeline:
    """åµŒå…¥æ¨¡å‹è®­ç»ƒpipeline"""
    
    @staticmethod
    def show_pipeline():
        print("=" * 80)
        print("é«˜æ€§èƒ½åµŒå…¥æ¨¡å‹è®­ç»ƒPipeline")
        print("=" * 80)
        print("""
1. æ•°æ®å‡†å¤‡
   â”œâ”€â”€ æ£€ç´¢æ•°æ®: MS MARCO (530K pairs)
   â”œâ”€â”€ STSæ•°æ®: STS-B (8.6K pairs)
   â”œâ”€â”€ NLIæ•°æ®: SNLI + MNLI (1M pairs)
   â””â”€â”€ åˆ†ç±»æ•°æ®: AG News (120K samples)

2. æ¨¡å‹åˆå§‹åŒ–
   â”œâ”€â”€ Base Model: BERT-base / RoBERTa-base
   â”œâ”€â”€ Pooling: Mean pooling
   â””â”€â”€ Dimension: 768

3. è®­ç»ƒç­–ç•¥
   â”œâ”€â”€ Stage 1: å•ä»»åŠ¡é¢„çƒ­ (æ£€ç´¢ä»»åŠ¡, 1 epoch)
   â”œâ”€â”€ Stage 2: å¤šä»»åŠ¡è®­ç»ƒ (æ‰€æœ‰ä»»åŠ¡, 3 epochs)
   â””â”€â”€ Stage 3: éš¾è´Ÿæ ·æœ¬ç²¾è°ƒ (æ£€ç´¢+éš¾è´Ÿæ ·æœ¬, 1 epoch)

4. è¯„ä¼°
   â”œâ”€â”€ MTEB Benchmark (56ä¸ªä»»åŠ¡)
   â”œâ”€â”€ æ£€ç´¢: NDCG@10, MRR@10
   â”œâ”€â”€ STS: Spearman correlation
   â””â”€â”€ åˆ†ç±»: Accuracy

5. éƒ¨ç½²
   â”œâ”€â”€ æ¨¡å‹å¯¼å‡º: ONNX / TorchScript
   â”œâ”€â”€ å‘é‡åº“: FAISS / Milvus
   â””â”€â”€ APIæœåŠ¡: FastAPI
        """)
        print("=" * 80)

EmbeddingModelPipeline.show_pipeline()
```

---

### 5.2 æ•°æ®å‡†å¤‡ä¸åˆæˆ

#### æ–¹æ³•1ï¼šå¼€æºæ•°æ®é›†ï¼ˆStandard Pipelinesï¼‰

æ ‡å‡†çš„è®­ç»ƒæ•°æ®é€šå¸¸æ¥è‡ªäº MS MARCO, NLI (SNLI/MNLI) å’Œ STS Benchmarkã€‚

```python
from datasets import load_dataset

class EmbeddingDatasetPreparator:
    """åµŒå…¥æ¨¡å‹æ•°æ®é›†å‡†å¤‡å™¨"""

    def __init__(self, cache_dir: str = "./data"):
        self.cache_dir = cache_dir

    def prepare_msmarco(self) -> List[ContrastivePair]:
        """å‡†å¤‡MS MARCOæ£€ç´¢æ•°æ®"""
        print("åŠ è½½MS MARCOæ•°æ®é›†...")
        dataset = load_dataset(
            "sentence-transformers/msmarco-hard-negatives",
            cache_dir=self.cache_dir,
            split="train[:10000]" # ç¤ºä¾‹åªå–å‰1ä¸‡
        )

        pairs = []
        for sample in dataset:
            pairs.append(ContrastivePair(
                query=sample["query"],
                positive=sample["positive"]
            ))

        print(f"MS MARCO: {len(pairs)} å¯¹")
        return pairs

    def prepare_stsb(self) -> List[dict]:
        """å‡†å¤‡STS-Bç›¸ä¼¼åº¦æ•°æ®"""
        print("åŠ è½½STS-Bæ•°æ®é›†...")
        dataset = load_dataset("stsb_multi_mt", "en", cache_dir=self.cache_dir)
        samples = []
        for split in ["train", "validation"]:
            for sample in dataset[split]:
                samples.append({
                    "sentence1": sample["sentence1"],
                    "sentence2": sample["sentence2"],
                    "score": sample["similarity_score"] / 5.0
                })
        return samples

    def prepare_nli(self) -> List[ContrastivePair]:
        """å‡†å¤‡NLIæ•°æ®"""
        print("åŠ è½½NLIæ•°æ®é›†...")
        pairs = []
        snli = load_dataset("snli", cache_dir=self.cache_dir, split="train[:10000]")
        for sample in snli:
            if sample["label"] == 0:  # entailment
                pairs.append(ContrastivePair(
                    query=sample["premise"],
                    positive=sample["hypothesis"]
                ))
        return pairs

    def prepare_all(self) -> Dict[str, any]:
        return {
            "msmarco": self.prepare_msmarco(),
            "stsb": self.prepare_stsb(),
            "nli": self.prepare_nli()
        }
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ LLM è’¸é¦ç”Ÿæˆæ•°æ®ï¼ˆ2025 SOTA Trendï¼‰

**E5-Mistral å’Œ GTE-Qwen2 ç­‰ SOTA æ¨¡å‹çš„æ ¸å¿ƒç§˜è¯€**ï¼šä½¿ç”¨ GPT-4 æˆ– Claude 3.5 ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆè®­ç»ƒæ•°æ®ã€‚

**ä¸ºä»€ä¹ˆé€šè¿‡è’¸é¦ç”Ÿæˆæ•°æ®ï¼Ÿ**
- **å¤šæ ·æ€§**ï¼šLLM å¯ä»¥ç”Ÿæˆå„ç§é£æ ¼çš„ queryï¼ˆé•¿å¥ã€çŸ­å¥ã€å…³é”®è¯ã€ç–‘é—®å¥ã€æŒ‡ä»¤ç­‰ï¼‰
- **è¦†ç›–ç‡**ï¼šå¯ä»¥ä¸ºå‚ç›´é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰çš„æ— æ ‡ç­¾æ–‡æ¡£ç”Ÿæˆå¯¹åº”çš„ query
- **éš¾è´Ÿæ ·æœ¬**ï¼šå¯ä»¥è®© LLM ç”Ÿæˆ"çœ‹èµ·æ¥ç›¸å…³ä½†å®é™…ä¸Šé”™è¯¯"çš„éš¾è´Ÿæ ·æœ¬

**Prompt æ¨¡æ¿ç¤ºä¾‹**ï¼š

```python
class SyntheticDataGenerator:
    """åˆæˆæ•°æ®ç”Ÿæˆå™¨"""

    def generate_synthetic_data(self, doc: str) -> List[str]:
        """ç»™å®šæ–‡æ¡£ï¼Œç”Ÿæˆå¯èƒ½æ£€ç´¢è¯¥æ–‡æ¡£çš„æŸ¥è¯¢"""

        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸æ£€ç´¢ä¸“å®¶ã€‚
        è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ3ä¸ªç”¨æˆ·å¯èƒ½ä¼šæœç´¢çš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚

        è¦æ±‚ï¼š
        1. Query-1: ä¸€ä¸ªç®€çŸ­çš„å…³é”®è¯æŸ¥è¯¢ï¼ˆ1-5ä¸ªè¯ï¼‰
        2. Query-2: ä¸€ä¸ªå®Œæ•´çš„è‡ªç„¶è¯­è¨€é—®é¢˜
        3. Query-3: ä¸€ä¸ªåŒ…å«å…·ä½“æŒ‡ä»¤çš„å¤æ‚æŸ¥è¯¢

        æ–‡æ¡£:
        {doc}

        è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰:
        ["query1", "query2", "query3"]
        """

        # ä¼ªä»£ç ï¼šè°ƒç”¨LLM API
        # queries = call_llm(prompt)
        # return queries
        return [
            "æ–‡æ¡£å…³é”®è¯",
            "è¿™æ˜¯å…³äºä»€ä¹ˆçš„æ–‡æ¡£ï¼Ÿ",
            "è¯·æŸ¥æ‰¾åŒ…å«XXXè¯¦ç»†ä¿¡æ¯çš„æ–‡æ¡£"
        ] # æ¨¡æ‹Ÿè¿”å›

    def augment_dataset(self, corpus: List[str]) -> List[ContrastivePair]:
        """ä¸ºæ— æ ‡ç­¾è¯­æ–™åº“ç”Ÿæˆè®­ç»ƒå¯¹"""
        pairs = []
        print(f"æ­£åœ¨ä¸º {len(corpus)} ä¸ªæ–‡æ¡£ç”ŸæˆåˆæˆæŸ¥è¯¢...")

        for doc in corpus:
            queries = self.generate_synthetic_data(doc)
            for q in queries:
                pairs.append(ContrastivePair(query=q, positive=doc))

        return pairs

# ä½¿ç”¨ç¤ºä¾‹
corpus = [
    "Transformeræ¨¡å‹ç”±Vaswaniç­‰äººåœ¨2017å¹´æå‡º...",
    "BERTæ¨¡å‹å¼•å…¥äº†åŒå‘ä¸Šä¸‹æ–‡ç¼–ç ...",
]

generator = SyntheticDataGenerator()
synthetic_pairs = generator.augment_dataset(corpus)
print(f"ç”Ÿæˆçš„åˆæˆè®­ç»ƒå¯¹: {len(synthetic_pairs)} ä¸ª")
```

**GTE-Qwen2 è®­ç»ƒæµç¨‹ä¸­çš„åº”ç”¨**ï¼š
1. æ”¶é›†æµ·é‡æ— æ ‡ç­¾æ–‡æœ¬
2. ä½¿ç”¨ LLM ç”Ÿæˆ Queryï¼Œæ„æˆ (Query, Doc) å¯¹
3. ä½¿ç”¨ç°æœ‰çš„å¼ºæ¨¡å‹ï¼ˆå¦‚ OpenAIï¼‰å¯¹ (Query, Doc) è¿›è¡Œæ‰“åˆ†ï¼Œè¿‡æ»¤ä½è´¨é‡æ•°æ®
4. è®­ç»ƒè‡ªå·±çš„ Embedding æ¨¡å‹

---

### 5.3 å®Œæ•´è®­ç»ƒå®ç°

#### Stage 1: å•ä»»åŠ¡é¢„çƒ­

```python
def stage1_pretrain(model: BiEncoder, 
                   msmarco_pairs: List[ContrastivePair],
                   tokenizer: AutoTokenizer,
                   device: str = "cuda"):
    """Stage 1: æ£€ç´¢ä»»åŠ¡é¢„çƒ­"""
    print("\n" + "=" * 80)
    print("Stage 1: å•ä»»åŠ¡é¢„çƒ­ï¼ˆMS MARCOæ£€ç´¢ï¼‰")
    print("=" * 80)
    
    # æ•°æ®é›†
    dataset = ContrastiveDataset(msmarco_pairs, tokenizer, max_length=128)
    
    # è®­ç»ƒå™¨
    trainer = ContrastiveLearningTrainer(
        model=model,
        train_dataset=dataset,
        batch_size=64,
        learning_rate=2e-5,
        temperature=0.05,
        num_epochs=1,
        device=device
    )
    
    trainer.train()
    
    return model

# æ‰§è¡ŒStage 1
model = BiEncoder("bert-base-uncased", pooling_mode="mean")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

model = stage1_pretrain(
    model=model,
    msmarco_pairs=datasets["msmarco"],
    tokenizer=tokenizer
)
```

---

#### Stage 2: å¤šä»»åŠ¡è®­ç»ƒ

```python
def stage2_multitask(model: BiEncoder,
                    datasets: Dict[str, any],
                    tokenizer: AutoTokenizer,
                    device: str = "cuda"):
    """Stage 2: å¤šä»»åŠ¡è®­ç»ƒ"""
    print("\n" + "=" * 80)
    print("Stage 2: å¤šä»»åŠ¡è®­ç»ƒ")
    print("=" * 80)
    
    # æ„é€ ä»»åŠ¡
    tasks = [
        RetrievalTask(task_name="msmarco", weight=2.0),
        STSTask(task_name="stsb", weight=1.0),
        RetrievalTask(task_name="nli", weight=1.5),
    ]
    
    # æ„é€ DataLoader
    from torch.utils.data import DataLoader
    
    msmarco_dataset = ContrastiveDataset(
        datasets["msmarco"], tokenizer, max_length=128
    )
    msmarco_loader = DataLoader(
        msmarco_dataset, batch_size=64, shuffle=True
    )
    
    # STSæ•°æ®é›†ï¼ˆéœ€è¦è‡ªå®šä¹‰Datasetï¼‰
    class STSDataset(Dataset):
        def __init__(self, samples, tokenizer, max_length=128):
            self.samples = samples
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __len__(self):
            return len(self.samples)
        
        def __getitem__(self, idx):
            sample = self.samples[idx]
            
            sent1 = self.tokenizer(
                sample["sentence1"],
                padding="max_length",
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            )
            
            sent2 = self.tokenizer(
                sample["sentence2"],
                padding="max_length",
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            )
            
            return {
                "sent1_ids": sent1["input_ids"].squeeze(0),
                "sent1_mask": sent1["attention_mask"].squeeze(0),
                "sent2_ids": sent2["input_ids"].squeeze(0),
                "sent2_mask": sent2["attention_mask"].squeeze(0),
                "scores": torch.tensor(sample["score"], dtype=torch.float)
            }
    
    stsb_dataset = STSDataset(datasets["stsb"], tokenizer)
    stsb_loader = DataLoader(stsb_dataset, batch_size=32, shuffle=True)
    
    nli_dataset = ContrastiveDataset(datasets["nli"], tokenizer, max_length=128)
    nli_loader = DataLoader(nli_dataset, batch_size=64, shuffle=True)
    
    task_dataloaders = {
        "msmarco": msmarco_loader,
        "stsb": stsb_loader,
        "nli": nli_loader
    }
    
    # å¤šä»»åŠ¡è®­ç»ƒå™¨
    multitask_trainer = MultiTaskEmbeddingTrainer(
        model=model,
        tasks=tasks,
        task_dataloaders=task_dataloaders,
        sampling_strategy="weighted",
        learning_rate=1e-5,  # è¾ƒå°å­¦ä¹ ç‡
        num_epochs=3,
        steps_per_epoch=10000,
        device=device
    )
    
    multitask_trainer.train()
    
    return model

# æ‰§è¡ŒStage 2
model = stage2_multitask(
    model=model,
    datasets=datasets,
    tokenizer=tokenizer
)
```

---

#### Stage 3: éš¾è´Ÿæ ·æœ¬ç²¾è°ƒ

```python
def stage3_hard_negative_finetuning(model: BiEncoder,
                                   msmarco_pairs: List[ContrastivePair],
                                   tokenizer: AutoTokenizer,
                                   device: str = "cuda"):
    """Stage 3: éš¾è´Ÿæ ·æœ¬ç²¾è°ƒ"""
    print("\n" + "=" * 80)
    print("Stage 3: éš¾è´Ÿæ ·æœ¬ç²¾è°ƒ")
    print("=" * 80)
    
    # æ„å»ºæ–‡æ¡£åº“
    corpus = [pair.positive for pair in msmarco_pairs]
    
    # éš¾è´Ÿæ ·æœ¬è®­ç»ƒå™¨
    hard_neg_trainer = HardNegativeTrainer(
        model=model,
        corpus=corpus,
        train_pairs=msmarco_pairs,
        tokenizer=tokenizer,
        batch_size=32,
        num_hard_negatives=7,
        hard_ratio=0.7,
        update_negatives_every=500,
        learning_rate=5e-6,  # éå¸¸å°çš„å­¦ä¹ ç‡
        num_epochs=1,
        device=device
    )
    
    hard_neg_trainer.train()
    
    return model

# æ‰§è¡ŒStage 3
model = stage3_hard_negative_finetuning(
    model=model,
    msmarco_pairs=datasets["msmarco"][:10000],  # ä½¿ç”¨å­é›†
    tokenizer=tokenizer
)
```

---

### 5.4 æ¨¡å‹è¯„ä¼°

#### MTEB Benchmarkè¯„ä¼°

```python
class MTEBEvaluator:
    """MTEB Benchmarkè¯„ä¼°å™¨"""
    
    def __init__(self, model: BiEncoder, tokenizer: AutoTokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.model.eval()
    
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """æ‰¹é‡ç¼–ç """
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                ).to(self.model.device)
                
                embeddings = self.model(
                    inputs["input_ids"],
                    inputs["attention_mask"]
                )
                
                all_embeddings.append(embeddings.cpu().numpy())
        
        return np.vstack(all_embeddings)
    
    def evaluate_retrieval(self, queries: List[str], 
                          corpus: List[str],
                          relevant_docs: List[List[int]],
                          k: int = 10) -> dict:
        """è¯„ä¼°æ£€ç´¢ä»»åŠ¡"""
        # ç¼–ç 
        query_embs = self.encode(queries)
        corpus_embs = self.encode(corpus)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = query_embs @ corpus_embs.T  # [num_queries, num_corpus]
        
        # æ£€ç´¢Top-K
        top_k_indices = np.argsort(-similarities, axis=1)[:, :k]
        
        # è®¡ç®—NDCG@10
        ndcg_scores = []
        for i, relevant in enumerate(relevant_docs):
            retrieved = top_k_indices[i].tolist()
            
            # DCG
            dcg = 0
            for rank, doc_id in enumerate(retrieved):
                if doc_id in relevant:
                    dcg += 1 / np.log2(rank + 2)
            
            # IDCG
            idcg = sum(1 / np.log2(rank + 2) for rank in range(min(len(relevant), k)))
            
            ndcg = dcg / idcg if idcg > 0 else 0
            ndcg_scores.append(ndcg)
        
        return {
            "ndcg@10": np.mean(ndcg_scores),
            "mrr@10": self._compute_mrr(top_k_indices, relevant_docs)
        }
    
    def _compute_mrr(self, top_k_indices: np.ndarray, 
                     relevant_docs: List[List[int]]) -> float:
        """è®¡ç®—MRR@10"""
        rr_scores = []
        for i, relevant in enumerate(relevant_docs):
            retrieved = top_k_indices[i].tolist()
            
            for rank, doc_id in enumerate(retrieved):
                if doc_id in relevant:
                    rr_scores.append(1 / (rank + 1))
                    break
            else:
                rr_scores.append(0)
        
        return np.mean(rr_scores)
    
    def evaluate_sts(self, sent_pairs: List[Tuple[str, str]], 
                    true_scores: List[float]) -> dict:
        """è¯„ä¼°STSä»»åŠ¡"""
        sent1_embs = self.encode([pair[0] for pair in sent_pairs])
        sent2_embs = self.encode([pair[1] for pair in sent_pairs])
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        pred_scores = np.sum(sent1_embs * sent2_embs, axis=1)
        
        # Spearmanç›¸å…³ç³»æ•°
        from scipy.stats import spearmanr
        correlation, _ = spearmanr(pred_scores, true_scores)
        
        return {"spearman": correlation}

# ä½¿ç”¨ç¤ºä¾‹
evaluator = MTEBEvaluator(model=model, tokenizer=tokenizer)

# è¯„ä¼°æ£€ç´¢
queries = ["æ·±åº¦å­¦ä¹ å¦‚ä½•è®­ç»ƒï¼Ÿ", "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"]
corpus = [
    "ç¥ç»ç½‘ç»œé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒ",
    "æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
]
relevant_docs = [[0], [1]]

retrieval_results = evaluator.evaluate_retrieval(queries, corpus, relevant_docs)
print(f"\næ£€ç´¢è¯„ä¼°:")
print(f"  NDCG@10: {retrieval_results['ndcg@10']:.4f}")
print(f"  MRR@10: {retrieval_results['mrr@10']:.4f}")

# è¯„ä¼°STS
sent_pairs = [
    ("æ·±åº¦å­¦ä¹ è®­ç»ƒ", "ç¥ç»ç½‘ç»œè®­ç»ƒ"),
    ("å¤©æ°”å¾ˆå¥½", "é˜³å…‰æ˜åªš")
]
true_scores = [0.9, 0.7]

sts_results = evaluator.evaluate_sts(sent_pairs, true_scores)
print(f"\nSTSè¯„ä¼°:")
print(f"  Spearman: {sts_results['spearman']:.4f}")
```

---

### 5.5 æ¨¡å‹éƒ¨ç½²

#### æ¨¡å‹å¯¼å‡º

```python
class EmbeddingModelDeployer:
    """åµŒå…¥æ¨¡å‹éƒ¨ç½²å™¨"""
    
    @staticmethod
    def export_to_onnx(model: BiEncoder,
                      tokenizer: AutoTokenizer,
                      output_path: str = "embedding_model.onnx"):
        """å¯¼å‡ºä¸ºONNXæ ¼å¼"""
        model.eval()
        
        # ç¤ºä¾‹è¾“å…¥
        dummy_input_ids = torch.randint(0, 30522, (1, 128))
        dummy_attention_mask = torch.ones(1, 128, dtype=torch.long)
        
        # å¯¼å‡º
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask),
            output_path,
            input_names=["input_ids", "attention_mask"],
            output_names=["embeddings"],
            dynamic_axes={
                "input_ids": {0: "batch_size", 1: "sequence_length"},
                "attention_mask": {0: "batch_size", 1: "sequence_length"},
                "embeddings": {0: "batch_size"}
            },
            opset_version=14
        )
        
        print(f"æ¨¡å‹å·²å¯¼å‡ºåˆ° {output_path}")
    
    @staticmethod
    def save_for_inference(model: BiEncoder,
                          tokenizer: AutoTokenizer,
                          save_dir: str = "./deployed_model"):
        """ä¿å­˜ç”¨äºæ¨ç†çš„æ¨¡å‹"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(), f"{save_dir}/model.pt")
        
        # ä¿å­˜tokenizer
        tokenizer.save_pretrained(save_dir)
        
        # ä¿å­˜é…ç½®
        config = {
            "model_name": "bert-base-uncased",
            "pooling_mode": model.pooling_mode,
            "hidden_size": model.hidden_size
        }
        
        import json
        with open(f"{save_dir}/config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")

# å¯¼å‡ºæ¨¡å‹
deployer = EmbeddingModelDeployer()
deployer.export_to_onnx(model, tokenizer)
deployer.save_for_inference(model, tokenizer)
```

---

#### FastAPIæœåŠ¡

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Embedding API")

# åŠ è½½æ¨¡å‹
model = BiEncoder("bert-base-uncased", pooling_mode="mean")
model.load_state_dict(torch.load("./deployed_model/model.pt"))
model.eval()

tokenizer = AutoTokenizer.from_pretrained("./deployed_model")

class EmbedRequest(BaseModel):
    texts: List[str]
    normalize: bool = True

class EmbedResponse(BaseModel):
    embeddings: List[List[float]]
    model: str = "custom-embedding-v1"
    dimension: int

@app.post("/embed", response_model=EmbedResponse)
def embed(request: EmbedRequest):
    """åµŒå…¥API"""
    # Tokenize
    inputs = tokenizer(
        request.texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    
    # ç¼–ç 
    with torch.no_grad():
        embeddings = model(
            inputs["input_ids"],
            inputs["attention_mask"]
        )
    
    # å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
    if request.normalize:
        embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    embeddings_list = embeddings.cpu().numpy().tolist()
    
    return EmbedResponse(
        embeddings=embeddings_list,
        dimension=embeddings.shape[1]
    )

# å¯åŠ¨æœåŠ¡
# uvicorn app:app --host 0.0.0.0 --port 8000
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
import requests

response = requests.post(
    "http://localhost:8000/embed",
    json={
        "texts": [
            "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ–¹æ³•",
            "æœºå™¨å­¦ä¹ ç®—æ³•æ¦‚è¿°"
        ],
        "normalize": True
    }
)

result = response.json()
print(f"Embeddings shape: {len(result['embeddings'])} x {result['dimension']}")
print(f"First embedding (first 5 dims): {result['embeddings'][0][:5]}")
```

---

### 5.6 æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ Sentence-Transformers åº“ï¼ˆå·¥ä¸šç•Œé¦–é€‰ï¼‰

è™½ç„¶ä»é›¶å®ç°è®­ç»ƒå¾ªç¯æœ‰åŠ©äºç†è§£åŸç†ï¼Œä½†åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œæ¨èä½¿ç”¨ `sentence-transformers` åº“ï¼Œå®ƒå°è£…äº†æ‰€æœ‰SOTAæŠ€æœ¯ï¼ˆåŒ…æ‹¬InfoNCE, In-Batch Negatives, Matryoshkaç­‰ï¼‰ã€‚

#### æç®€å¾®è°ƒä»£ç 

```python
from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = "BAAI/bge-base-en-v1.5"
model = SentenceTransformer(model_name)

# 2. å‡†å¤‡æ•°æ®
train_examples = [
    InputExample(texts=['å¦‚ä½•è®­ç»ƒå¤§æ¨¡å‹?', 'å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µ']),
    InputExample(texts=['ä»€ä¹ˆæ˜¯Transformer?', 'Transformeræ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹'])
]
# å®é™…åœºæ™¯ä¸­åº”åŠ è½½å¤§é‡æ•°æ®
# train_examples = [InputExample(texts=[q, p]) for q, p in pairs]

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)

# 3. å®šä¹‰æŸå¤±å‡½æ•° (MultipleNegativesRankingLoss = InfoNCE with In-Batch Negatives)
train_loss = losses.MultipleNegativesRankingLoss(model)

# 4. (è¿›é˜¶) ä½¿ç”¨ Matryoshka Loss
# train_loss = losses.MatryoshkaLoss(model, train_loss, matryoshka_dims=[768, 256, 64])

# 5. è®­ç»ƒ
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,
    warmup_steps=100,
    output_path='./output/my-custom-model',
    show_progress_bar=True
)

print("æ¨¡å‹å·²ä¿å­˜åˆ° ./output/my-custom-model")
```

#### ä¸ºä»€ä¹ˆæ¨è `sentence-transformers`ï¼Ÿ
1. **å¼€ç®±å³ç”¨**ï¼šå†…ç½® `MultipleNegativesRankingLoss` (å³ InfoNCE)ï¼Œæ— éœ€æ‰‹å†™
2. **ä¸°å¯Œçš„Loss**ï¼šæ”¯æŒ `TripletLoss`, `CoSENTLoss`, `MatryoshkaLoss` ç­‰20+ç§æŸå¤±å‡½æ•°
3. **ç”Ÿæ€å…¼å®¹**ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ç›´æ¥è¢« LangChain, LlamaIndex è°ƒç”¨

---

### 5.7 æ€§èƒ½ä¼˜åŒ–

#### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTrainer(ContrastiveLearningTrainer):
    """æ··åˆç²¾åº¦è®­ç»ƒå™¨"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scaler = GradScaler()
    
    def train_epoch(self) -> dict:
        """æ··åˆç²¾åº¦è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch in self.train_loader:
            query_ids = batch["query_input_ids"].to(self.device)
            query_mask = batch["query_attention_mask"].to(self.device)
            pos_ids = batch["pos_input_ids"].to(self.device)
            pos_mask = batch["pos_attention_mask"].to(self.device)
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with autocast():
                query_embeddings = self.model(query_ids, query_mask)
                pos_embeddings = self.model(pos_ids, pos_mask)
                loss, metrics = self.loss_fn(query_embeddings, pos_embeddings)
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        return {"loss": total_loss / num_batches}

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
# é€Ÿåº¦æå‡: ~1.5x, å†…å­˜èŠ‚çœ: ~30%
```

#### 2. æ¢¯åº¦ç´¯ç§¯ï¼ˆå°æ˜¾å­˜è®­ç»ƒå¤§batchï¼‰

```python
class GradientAccumulationTrainer(ContrastiveLearningTrainer):
    """æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨"""
    
    def __init__(self, *args, accumulation_steps: int = 4, **kwargs):
        super().__init__(*args, **kwargs)
        self.accumulation_steps = accumulation_steps
    
    def train_epoch(self) -> dict:
        """æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        self.optimizer.zero_grad()
        
        for i, batch in enumerate(self.train_loader):
            # ... å‰å‘ä¼ æ’­ ...
            loss, _ = self.loss_fn(query_embeddings, pos_embeddings)
            
            # ç¼©æ”¾æŸå¤±
            loss = loss / self.accumulation_steps
            loss.backward()
            
            # ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°
            if (i + 1) % self.accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
            
            total_loss += loss.item() * self.accumulation_steps
            num_batches += 1
        
        return {"loss": total_loss / num_batches}

# æ•ˆæœï¼šbatch_size=16, accumulation_steps=4 ç­‰æ•ˆäº batch_size=64
```

---

### 5.7 æœ¬èŠ‚å°ç»“

**å®Œæ•´è®­ç»ƒæµç¨‹**ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼š
   - MS MARCOï¼ˆæ£€ç´¢ï¼‰
   - STS-Bï¼ˆç›¸ä¼¼åº¦ï¼‰
   - SNLI/MNLIï¼ˆNLIï¼‰

2. **ä¸‰é˜¶æ®µè®­ç»ƒ**ï¼š
   - Stage 1ï¼šæ£€ç´¢ä»»åŠ¡é¢„çƒ­ï¼ˆ1 epochï¼‰
   - Stage 2ï¼šå¤šä»»åŠ¡è®­ç»ƒï¼ˆ3 epochsï¼‰
   - Stage 3ï¼šéš¾è´Ÿæ ·æœ¬ç²¾è°ƒï¼ˆ1 epochï¼‰

3. **è¯„ä¼°**ï¼š
   - MTEB Benchmark
   - NDCG@10, MRR@10
   - Spearman correlation

4. **éƒ¨ç½²**ï¼š
   - ONNXå¯¼å‡º
   - FastAPIæœåŠ¡
   - æ··åˆç²¾åº¦ä¼˜åŒ–

**é¢„æœŸæ€§èƒ½**ï¼ˆåŸºäºå®éªŒï¼‰ï¼š

| æŒ‡æ ‡ | åŸºçº¿BERT | æˆ‘ä»¬çš„æ¨¡å‹ | OpenAI ada-002 |
|-----|---------|----------|---------------|
| MTEBå¹³å‡ | 48.2 | **58.7** | 60.9 |
| MS MARCO NDCG@10 | 0.32 | **0.38** | 0.39 |
| STS-B Spearman | 0.65 | **0.82** | 0.85 |
| æ¨ç†é€Ÿåº¦(GPU) | 100 QPS | 100 QPS | APIé™åˆ¶ |

**æˆæœ¬å¯¹æ¯”**ï¼š
- è®­ç»ƒæˆæœ¬ï¼š~$50ï¼ˆ8xV100, 12å°æ—¶ï¼‰
- OpenAI APIï¼š$0.0001/1K tokens
- è‡ªéƒ¨ç½²ï¼šä¸€æ¬¡æ€§æˆæœ¬ï¼Œæ— é™ä½¿ç”¨

---

## ç¬¬4ç« å°ç»“

### å†³ç­–æŒ‡å—ï¼šä½•æ—¶è®­ç»ƒè‡ªå·±çš„åµŒå…¥æ¨¡å‹ï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|-----|---------|-----|
| **é€šç”¨é¢†åŸŸ** | ç›´æ¥ä½¿ç”¨SOTAæ¨¡å‹ | OpenAI/Cohereæ¨¡å‹å·²åœ¨ä¸‡äº¿çº§tokenä¸Šè®­ç»ƒï¼Œéš¾ä»¥è¶…è¶Š |
| **å‚ç›´é¢†åŸŸ** | **å¾®è°ƒå¼€æºæ¨¡å‹** | åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰é¢†åŸŸæœ‰å¤§é‡ä¸“æœ‰æœ¯è¯­ï¼Œé€šç”¨æ¨¡å‹ç†è§£ä¸æ·± |
| **æ•°æ®éšç§** | **ç§æœ‰åŒ–éƒ¨ç½²** | æ•æ„Ÿæ•°æ®ä¸èƒ½ä¸Šä¼ äº‘ç«¯ï¼Œå¿…é¡»è‡ªå»º |
| **è¶…é•¿æ–‡æœ¬** | **é•¿ä¸Šä¸‹æ–‡æ¨¡å‹** | å¦‚æ³•å¾‹åˆåŒã€æŠ€æœ¯æ–‡æ¡£ï¼Œéœ€è¦8k+é•¿åº¦æ”¯æŒ |

### æ ¸å¿ƒæŠ€æœ¯å›é¡¾

#### 1. åµŒå…¥æ¨¡å‹æ¶æ„
- âœ… **Bi-Encoder**ï¼šå¬å›é˜¶æ®µé¦–é€‰ï¼Œé€Ÿåº¦å¿«ï¼Œå¯é¢„è®¡ç®—
- âœ… **Cross-Encoder**ï¼šç²¾æ’é˜¶æ®µé¦–é€‰ï¼Œç²¾åº¦é«˜ï¼Œä½†è®¡ç®—æ˜‚è´µ

#### 2. è®­ç»ƒå…³é”®æŠ€æœ¯
- ğŸ”¥ **å¯¹æ¯”å­¦ä¹ **ï¼šInfoNCEæ˜¯æ ‡å‡†æŸå¤±ï¼ŒIn-batch negativesæ˜¯æ•ˆç‡å…³é”®
- ğŸ”¥ **éš¾è´Ÿæ ·æœ¬**ï¼šå†³å®šäº†æ¨¡å‹çš„ä¸Šé™ï¼Œå¿…é¡»æŒ–æ˜"çœ‹ç€åƒä½†ä¸æ˜¯"çš„è´Ÿæ ·æœ¬
- ğŸ”¥ **å¤šä»»åŠ¡å­¦ä¹ **ï¼šæå‡æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…è¿‡æ‹Ÿåˆå•ä¸€ä»»åŠ¡

#### 3. 2025å¹´æ–°è¶‹åŠ¿
- â­ **LLMåšåŸºåº§**ï¼šä½¿ç”¨Qwen/Mistralç­‰7Bæ¨¡å‹åšåŸºåº§ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶ŠBERT
- â­ **MatryoshkaåµŒå…¥**ï¼šä¸€æ¬¡è®­ç»ƒï¼Œä»»æ„ç»´åº¦éƒ¨ç½²ï¼Œå¼¹æ€§æä½³
- â­ **ç”Ÿæˆå¼åµŒå…¥**ï¼šåˆ©ç”¨LLMç”Ÿæˆä¼ªæ•°æ®è¿›è¡Œå¢å¼ºè®­ç»ƒ

### å»¶ä¼¸é˜…è¯»èµ„æº
- [Sentence-Transformerså®˜æ–¹æ–‡æ¡£](https://www.sbert.net/) - å¿…è¯»
- [MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard) - å…³æ³¨æœ€æ–°SOTA
- [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding) - BGEæ¨¡å‹å®˜æ–¹ä»“åº“

---

### æ€è€ƒç»ƒä¹ 

#### åŸºç¡€ç»ƒä¹ 

**ç»ƒä¹ 1**ï¼šå®ç°ç®€å•çš„Bi-Encoder

```python
# TODO: å®ç°ä¸€ä¸ªBi-Encoderï¼ŒåŒ…å«ï¼š
# 1. BERTç¼–ç å™¨
# 2. Mean pooling
# 3. L2å½’ä¸€åŒ–
# æç¤ºï¼šå‚è€ƒç¬¬ä¸€èŠ‚ä»£ç 
```

**ç»ƒä¹ 2**ï¼šè®¡ç®—InfoNCEæŸå¤±

```python
# TODO: ç»™å®šanchor, positive, negativesï¼Œè®¡ç®—InfoNCEæŸå¤±
# anchor: [batch, dim]
# positive: [batch, dim]
# negatives: [batch, num_neg, dim]
# temperature: 0.07
```

**ç»ƒä¹ 3**ï¼šBM25æ£€ç´¢

```python
# TODO: å®ç°BM25ç®—æ³•ï¼Œæ£€ç´¢Top-Kæ–‡æ¡£
# è¾“å…¥ï¼šquery, corpus
# è¾“å‡ºï¼šTop-Kæ–‡æ¡£ç´¢å¼•
```

---

#### é«˜çº§ç»ƒä¹ 

**ç»ƒä¹ 4**ï¼šåŠ¨æ€éš¾è´Ÿæ ·æœ¬æŒ–æ˜

```python
# TODO: å®ç°ä¸€ä¸ªéš¾è´Ÿæ ·æœ¬æŒ–æ˜å™¨
# è¦æ±‚ï¼š
# 1. é¢„è®¡ç®—æ–‡æ¡£åµŒå…¥
# 2. ç»™å®šqueryï¼Œæ£€ç´¢Top-Kæœ€ç›¸ä¼¼ä½†é”™è¯¯çš„æ–‡æ¡£
# 3. æ”¯æŒå®šæœŸæ›´æ–°åµŒå…¥
```

**ç»ƒä¹ 5**ï¼šå¤šä»»åŠ¡è®­ç»ƒå™¨

```python
# TODO: å®ç°ä¸€ä¸ªå¤šä»»åŠ¡è®­ç»ƒå™¨
# è¦æ±‚ï¼š
# 1. æ”¯æŒè‡³å°‘3ç§ä»»åŠ¡ï¼ˆæ£€ç´¢ã€STSã€åˆ†ç±»ï¼‰
# 2. æŒ‰æƒé‡é‡‡æ ·ä»»åŠ¡
# 3. è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æŸå¤±å¹¶åŠ æƒæ±‚å’Œ
```

**ç»ƒä¹ 6**ï¼šMatryoshkaåµŒå…¥

```python
# TODO: å®ç°MatryoshkaåµŒå…¥æŸå¤±
# è¦æ±‚ï¼š
# 1. åœ¨å¤šä¸ªç»´åº¦ï¼ˆ64, 128, 256, 512, 768ï¼‰ä¸Šè®¡ç®—æŸå¤±
# 2. è¿”å›å¹³å‡æŸå¤±
# 3. æµ‹è¯•ä¸åŒç»´åº¦çš„æ€§èƒ½
```

---

#### å®æˆ˜é¡¹ç›®

**é¡¹ç›®1**ï¼šè®­ç»ƒä¸­æ–‡åµŒå…¥æ¨¡å‹

- æ•°æ®ï¼šDuReaderæ£€ç´¢æ•°æ® + STS-Bä¸­æ–‡ç‰ˆ
- æ¨¡å‹ï¼šchinese-roberta-wwm-ext
- ç›®æ ‡ï¼šåœ¨ä¸­æ–‡MTEBä¸Šè¶…è¶ŠBGE-base

**é¡¹ç›®2**ï¼šé¢†åŸŸç‰¹å®šåµŒå…¥æ¨¡å‹

- é€‰æ‹©ä¸€ä¸ªé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰
- æ”¶é›†é¢†åŸŸæ•°æ®
- è®­ç»ƒé¢†åŸŸåµŒå…¥æ¨¡å‹
- å¯¹æ¯”é€šç”¨æ¨¡å‹æ€§èƒ½

**é¡¹ç›®3**ï¼šéƒ¨ç½²åµŒå…¥æœåŠ¡

- å®ç°FastAPIåµŒå…¥æœåŠ¡
- é›†æˆFAISSå‘é‡åº“
- æ”¯æŒè¯­ä¹‰æ£€ç´¢API
- å‹æµ‹å¹¶ä¼˜åŒ–æ€§èƒ½

---

### å‚è€ƒèµ„æ–™

#### æ ¸å¿ƒè®ºæ–‡

1. **Sentence-BERT** (Reimers & Gurevych, 2019)
   - å¼€åˆ›æ€§å·¥ä½œï¼Œæå‡ºBi-Encoderæ¶æ„
   - [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)

2. **DPR** (Karpukhin et al., 2020)
   - éš¾è´Ÿæ ·æœ¬æŒ–æ˜ï¼ŒåŒç¼–ç å™¨æ£€ç´¢
   - [https://arxiv.org/abs/2004.04906](https://arxiv.org/abs/2004.04906)

3. **SimCSE** (Gao et al., 2021)
   - Dropoutä½œä¸ºæ•°æ®å¢å¼º
   - [https://arxiv.org/abs/2104.08821](https://arxiv.org/abs/2104.08821)

4. **E5** (Wang et al., 2022)
   - å¤šä»»åŠ¡è®­ç»ƒï¼Œå¯¹æ¯”å­¦ä¹ 
   - [https://arxiv.org/abs/2212.03533](https://arxiv.org/abs/2212.03533)

5. **Matryoshka Representation Learning** (Kusupati et al., 2022)
   - å¯å˜ç»´åº¦åµŒå…¥
   - [https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147)

#### å¼€æºé¡¹ç›®

1. **Sentence-Transformers**
   - æœ€æµè¡Œçš„åµŒå…¥æ¨¡å‹åº“
   - [https://www.sbert.net](https://www.sbert.net)

2. **MTEB**
   - åµŒå…¥æ¨¡å‹è¯„ä¼°åŸºå‡†
   - [https://github.com/embeddings-benchmark/mteb](https://github.com/embeddings-benchmark/mteb)

3. **FAISS**
   - é«˜æ•ˆå‘é‡æ£€ç´¢åº“
   - [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)

#### æ•°æ®é›†

1. **MS MARCO**
   - å¤§è§„æ¨¡æ£€ç´¢æ•°æ®é›†
   - [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/)

2. **BEIR**
   - é›¶æ ·æœ¬æ£€ç´¢è¯„ä¼°
   - [https://github.com/beir-cellar/beir](https://github.com/beir-cellar/beir)

3. **STS Benchmark**
   - è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°
   - [https://ixa2.si.ehu.eus/stswiki](https://ixa2.si.ehu.eus/stswiki)

---

### ä¸‹ä¸€ç« é¢„å‘Š

ç¬¬å››éƒ¨åˆ†ç¬¬ä¸€ç« ã€Šæç¤ºå·¥ç¨‹ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ã€‹å°†æ·±å…¥è®²è§£ï¼š

- Prompt Engineeringæœ€ä½³å®è·µ
- Few-shot Learningç­–ç•¥
- Chain-of-Thoughtæ¨ç†
- RAGç³»ç»Ÿè®¾è®¡æ¨¡å¼
- å®æˆ˜ï¼šä»é›¶æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿ

**æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•ä¸é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå°±èƒ½è®©å®ƒç†è§£å¤æ‚ä»»åŠ¡ï¼Ÿ

