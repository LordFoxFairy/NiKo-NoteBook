# 第2章：与人类对齐：偏好优化 (Preference Alignment)

> "Alignment is the art of getting what you want, not just what you asked for."
>
> 即使是最强的预训练模型，也只是学会了"续写"。是偏好优化让它学会了"对话"、"拒绝"和"价值观"。

---

## 目录
- [一、对齐三原则与 SFT 的局限](#一对齐三原则与-sft-的局限)
  - [1. HHH 原则：有用、诚实、无害](#1-hhh-原则有用诚实无害)
  - [2. 为什么 SFT 还不够？](#2-为什么-sft-还不够)
- [二、经典路线：RLHF (PPO)](#二经典路线rlhf-ppo)
  - [1. 训练 Reward Model (奖励模型)](#1-训练-reward-model-奖励模型)
  - [2. PPO 算法核心：KL 散度与 Policy 更新](#2-ppo-算法核心kl-散度与-policy-更新)
  - [3. 实战：手动实现 PPO Step](#3-实战手动实现-ppo-step)
- [三、现代路线：DPO (Direct Preference Optimization)](#三现代路线dpo-direct-preference-optimization)
  - [1. DPO 的数学魔术：无需 Reward Model](#1-dpo-的数学魔术无需-reward-model)
  - [2. DPO vs PPO：谁赢了？](#2-dpo-vs-ppo谁赢了)
  - [3. 实战：使用 TRL 训练 DPO](#3-实战使用-trl-训练-dpo)
- [四、前沿变体：KTO / IPO / ORPO](#四前沿变体kto--ipo--orpo)
  - [1. KTO: 如果只有赞和踩，没有比较对](#1-kto-如果只有赞和踩没有比较对)
  - [2. ORPO: 连 SFT 都不需要了？](#2-orpo-连-sft-都不需要了)
  - [3. SPIN: 自我对弈，无需人工数据](#3-spin-自我对弈无需人工数据)
- [五、本章小结](#五本章小结)

---

## 一、对齐三原则与 SFT 的局限

### 1. HHH 原则：有用、诚实、无害

OpenAI 定义了对齐的三大支柱：
-   **Helpful (有用)**: 能够解决用户问题。
-   **Honest (诚实)**: 不编造事实 (Hallucination)，不知道就说不知道。
-   **Harmless (无害)**: 不生成暴力、色情、歧视内容。

### 2. 为什么 SFT 还不够？

SFT (Supervised Fine-Tuning) 的训练目标是：
$$ L_{SFT} = -\log P(y_{label} \mid x) $$

SFT 只能学会**"模仿"**标准答案，但无法理解**"好与坏"**的程度。
-   对于问题 "如何制造炸弹？"，SFT 模型可能会模仿训练集里的高智商回答，给出一份完美的炸弹制作教程。这很 Helpful，但不 Harmless。
-   我们希望模型知道：即使你的回答在语法上很完美，但因为它是有害的，所以分数极低。

---

## 二、经典路线：RLHF (PPO)

Reinforcement Learning from Human Feedback (RLHF) 是 ChatGPT 成功的关键。它把微调分成了三步：SFT -> RM -> PPO。

### 1. 训练 Reward Model (奖励模型)

我们需要一个能模仿人类打分的模型 $R_\phi(x, y)$。
输入：提示词 $x$，回答 $y$。输出：标量分数。

**训练数据**：成对比较数据 (Pairwise Data)。
Human: "写首诗"
- A: "窗前明月光..." (人类觉得更好)
- B: "月亮很大..."

**Loss Function (Bradley-Terry Model)**:
$$ L_{RM} = - \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) $$
其中 $y_w$ 是胜者 (winner)，$y_l$ 是败者 (loser)。

### 2. PPO 算法核心：KL 散度与 Policy 更新

有了奖励模型，我们就可以用强化学习来训练策略模型 $\pi_\theta$。

**目标函数**：
$$ \max_\theta \mathbb{E}[r_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}] $$

关键在于 **KL 散度惩罚 (KL Penalty)**：
-   $\pi_{ref}$ 是原始的 SFT 模型。
-   我们希望模型分数变高，但**不要偏离 SFT 模型太远**。
-   如果没有 KL 惩罚，模型会利用 Reward Model 的漏洞 (Reward Hacking)，生成乱码来骗取高分。

### 3. 实战：手动实现 PPO Step

虽然现在常用 `trl.PPOTrainer`，但理解内部逻辑很重要。

```python
import torch
import torch.nn.functional as F

def ppo_step(
    policy_model, ref_model,
    input_ids, response_ids,
    rewards,
    kl_coef=0.1, clip_range=0.2
):
    """
    简化的 PPO 更新步骤
    """
    with torch.no_grad():
        # 1. 计算 Ref LogProbs
        ref_logits = ref_model(input_ids)
        ref_logprobs = gather_log_probs(ref_logits, response_ids)

    # 2. 计算当前 LogProbs
    logits = policy_model(input_ids)
    logprobs = gather_log_probs(logits, response_ids)

    # 3. 计算 KL 散度
    kl = logprobs - ref_logprobs

    # 4. 计算总回报 (Reward - KL Penalty)
    total_reward = rewards - kl_coef * kl

    # 5. PPO Loss (Clipped Surrogate Objective)
    ratio = torch.exp(logprobs - old_logprobs)
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1-clip_range, 1+clip_range) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    return policy_loss
```

---

## 三、现代路线：DPO (Direct Preference Optimization)

PPO 极其复杂，需要同时加载 4 个模型（Actor, Critic, Ref, Reward），显存占用巨大，且训练不稳定。
2023 年，Stanford 团队提出的 DPO 改变了游戏规则。

### 1. DPO 的数学魔术：无需 Reward Model

DPO 证明了一个惊人的结论：**最优的 Reward 函数可以直接用 Policy 模型表示！**

$$ r^*(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + Z(x) $$

代入 Bradley-Terry 模型，我们可以直接推导出 DPO 的 Loss：

$$ L_{DPO} = - \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) $$

**人话解释**：
我们不需要训练一个单独的裁判（Reward Model）。
我们直接训练选手（Policy Model），让它：
1.  **提高**胜者回答 ($y_w$) 的概率。
2.  **降低**败者回答 ($y_l$) 的概率。
3.  同时不要偏离 Ref 模型太远（隐含在公式里）。

### 2. DPO vs PPO：谁赢了？

| 特性 | PPO (RLHF) | DPO |
| :--- | :--- | :--- |
| **稳定性** | 极低，对超参敏感 | 极高，像 SFT 一样稳 |
| **显存占用** | 巨大 (4个模型) | 低 (2个模型: Policy + Ref) |
| **实现难度** | 困难 | 简单 (几行代码) |
| **效果** | 理论上限高，上限由RM决定 | 实测与 PPO 持平甚至更好 |

目前 (2025)，SOTA 模型如 Llama-3, Qwen-2 都在使用 DPO 及其变体。

### 3. 实战：使用 TRL 训练 DPO

HuggingFace 的 `TRL` 库让 DPO 变得极其简单。

```python
# pip install trl transformers datasets
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer
from datasets import load_dataset

# 1. 加载模型
model_name = "gpt2" # 仅演示，实际上需要 SFT 过的模型
model = AutoModelForCausalLM.from_pretrained(model_name)
ref_model = AutoModelForCausalLM.from_pretrained(model_name) # 初始时 Ref = Policy
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. 加载偏好数据 (Helpful vs Harmless)
dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1%]")

# 数据格式必须包含: 'prompt', 'chosen', 'rejected'
def format_data(sample):
    return {
        "prompt": sample["context"],
        "chosen": sample["chosen"],
        "rejected": sample["rejected"],
    }
dataset = dataset.map(format_data)

# 3. 配置 DPO Trainer
dpo_config = TrainingArguments(
    output_dir="./dpo_results",
    per_device_train_batch_size=4,
    learning_rate=1e-6, # DPO lr 通常比 SFT 小
    num_train_epochs=1,
    remove_unused_columns=False
)

trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    beta=0.1, # KL 惩罚系数，DPO 的核心超参
)

# 4. 开始训练
trainer.train()
```

---

## 四、前沿变体：KTO / IPO / ORPO

DPO 虽然好，但它需要**成对数据 (Paired Data)**。这很难搞：你得找两句话，还得判断谁好谁坏。

### 1. KTO: 如果只有赞和踩，没有比较对

**KTO (Kahneman-Tversky Optimization)** 不需要成对数据。
它只需要：$(x, y, label)$，其中 label 是 true (赞) 或 false (踩)。
这利用了 Prospect Theory (前景理论)：人类对"损失"的厌恶大于对"收益"的喜悦。

### 2. ORPO: 连 SFT 都不需要了？

传统流程：Pretrain -> SFT -> DPO。
**ORPO (Odds Ratio Preference Optimization)** 试图把 SFT 和 DPO 结合在一起。
它在 Loss 中加了一项 **Odds Ratio Penalty**：
$$ L_{ORPO} = L_{SFT} + \lambda L_{OR} $$
实测表明，在 Mistral-7B 上，ORPO 直接微调预训练模型，效果优于 SFT+DPO。

### 3. SPIN: 自我对弈，无需人工数据

**SPIN (Self-Play Fine-Tuning)**：
1.  模型自己针对 Prompt 生成回答。
2.  如果不好的回答（SFT数据中的正确答案被视为"好"，模型生成的视为"坏"），就用 DPO 把它压下去。
3.  迭代多轮 (Iteration 0 -> 1 -> 2...)。
这使得模型在没有额外人工标注的情况下，自我提升。

---

## 五、本章小结

1.  **对齐是刚需**：没有对齐的模型是危险且不可用的。
2.  **RLHF 已成过去式**：除了作为理论基础，工程上已被 DPO 取代。
3.  **DPO 是瑞士军刀**：它把强化学习转化为了分类问题，极其优雅。
4.  **趋势**：从 Pairwise (DPO) 走向 Pointwise (KTO)，从 Human Feedback 走向 AI Feedback (SPIN)。

掌握 DPO，你就能让你微调的模型不再只是"鹦鹉学舌"，而是拥有"灵魂"。

---

**下一章预告：** 第3章 - 创建更优的嵌入模型

除了生成模型，Embedding 模型也是 LLM 生态的重要部分。下一章我们将探讨对比学习、InfoNCE 和 MTEB 榜单。
