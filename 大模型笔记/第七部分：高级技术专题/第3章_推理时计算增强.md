# 第3章：推理时计算增强 (Test-Time Compute)

> "Intuition is nothing more than the outcome of earlier intellectual experience." - Einstein
>
> System 1 是直觉，System 2 是逻辑。让模型在推理时"多想一会儿" (Thinking Fast and Slow)。

---

## 目录
- [一、从 System 1 到 System 2 的范式转变](#一从-system-1-到-system-2-的范式转变)
  - [1. 什么是 System 1 vs System 2？](#1-什么是-system-1-vs-system-2)
  - [2. 推理时计算 Scaling Law](#2-推理时计算-scaling-law)
- [二、基础推理增强：让思想“飞”一会儿](#二基础推理增强让思想飞一会儿)
  - [1. Chain-of-Thought (CoT) 的数学本质](#1-chain-of-thought-cot-的数学本质)
  - [2. Self-Consistency (CoT-SC)：集体智慧](#2-self-consistency-cot-sc集体智慧)
  - [3. Least-to-Most Prompting：由易到难](#3-least-to-most-prompting由易到难)
- [三、进阶：树与图的搜索 (Tree & Graph Search)](#三进阶树与图的搜索-tree--graph-search)
  - [1. Tree of Thoughts (ToT)：思维之树](#1-tree-of-thoughts-tot思维之树)
  - [2. Graph of Thoughts (GoT)：思维之网](#2-graph-of-thoughts-got思维之网)
  - [3. 实战：手写一个 ToT 24点求解器](#3-实战手写一个-tot-24点求解器)
- [四、核心组件：验证器 (Verifier) 与 PRM](#四核心组件验证器-verifier-与-prm)
  - [1. ORM (结果奖励) vs PRM (过程奖励)](#1-orm-结果奖励-vs-prm-过程奖励)
  - [2. 为什么 PRM 是 System 2 的关键？](#2-为什么-prm-是-system-2-的关键)
  - [3. 实战：训练一个简单的数学题 Verifier](#3-实战训练一个简单的数学题-verifier)
- [五、终极形态：MCTS 与 AlphaGo 范式](#五终极形态mcts-与-alphago-范式)
  - [1. 为什么 LLM 需要 MCTS？](#1-为什么-llm-需要-mcts)
  - [2. PUCT 算法在 LLM 中的映射](#2-puct-算法在-llm-中的映射)
  - [3. DeepSeek-R1 / OpenAI o1 架构猜想](#3-deepseek-r1--openai-o1-架构猜想)
- [六、本章小结](#六本章小结)

---

## 一、从 System 1 到 System 2 的范式转变

在 2023 年之前，大模型主要是 **System 1 (快思考)**：输入 Prompt，模型凭“直觉”立刻输出 Token。
到了 2024/2025 年，随着 OpenAI o1 和 DeepSeek-R1 的出现，**System 2 (慢思考)** 成为主流：模型在输出答案前，会进行漫长的“内心独白”或“潜意识搜索”。

### 1. 什么是 System 1 vs System 2？

| 维度 | System 1 (快思考) | System 2 (慢思考) |
| :--- | :--- | :--- |
| **代表模型** | GPT-3.5, LLaMA-2 | OpenAI o1, DeepSeek-R1 |
| **思维方式** | 直觉、联想、模式匹配 | 逻辑、规划、搜索、反思 |
| **响应速度** | 毫秒级 | 秒级甚至分钟级 |
| **计算消耗** | $O(N)$ (线性) | $O(N \times K)$ (甚至指数级) |
| **适用场景** | 闲聊、写诗、简单翻译 | 数学证明、复杂编程、科学发现 |
| **出错特征** | 容易产生幻觉，一本正经胡说八道 | 容易陷入死循环，过度纠结 |

### 2. 推理时计算 Scaling Law

传统 Scaling Law (Kaplan et al.) 关注的是**训练时计算 (Training Compute)**。
DeepMind 和 OpenAI 的研究表明，**推理时计算 (Test-Time Compute)** 同样遵循 Scaling Law：

> **性能提升 = f(模型参数量, 训练数据量, 推理思考时间)**

**三个重要发现：**
1.  **小模型 + 长思考 ≈ 大模型 + 短思考**：给 LLaMA-7B 足够的思考时间（如采样 1000 次），其数学解题能力可以超越 GPT-4。
2.  **边际递减**：简单地增加采样次数 ($N$)，效果呈对数增长 ($Acc \propto \log N$)。
3.  **最优分配**：对于简单问题，无需多想；对于困难问题，想得越久效果越好。

---

## 二、基础推理增强：让思想“飞”一会儿

### 1. Chain-of-Thought (CoT) 的数学本质

CoT (Wei et al., 2022) 的咒语是 *"Let's think step by step"*。

**直觉解释**：把大问题拆解为小问题。
**概率解释**：
设问题为 $Q$，答案为 $A$，中间推理步骤为 $Z$。
直接生成 $P(A|Q)$ 很难，因为 $Q$ 和 $A$ 的语义距离太远。
CoT 引入隐变量 $Z$，将问题转化为：

$$
P(A|Q) = \sum_{Z} P(A|Z, Q) P(Z|Q)
$$

模型先生成高概率的推理路径 $Z$（这一步比较容易，因为 $Z$ 是 $Q$ 的自然延续），再基于 $Z$ 生成 $A$（这一步也容易，因为 $A$ 就在 $Z$ 的结尾）。

**Zero-Shot CoT**：仅仅添加 *"Let's think step by step"*。
**Few-Shot CoT**：提供带推理过程的示例。

### 2. Self-Consistency (CoT-SC)：集体智慧

如果 CoT 是“一个人的深思熟虑”，那么 Self-Consistency 就是“一群人的民主投票”。

**核心算法**：
1.  对同一个 Prompt，采样 $K$ 个不同的推理路径 (Temperature > 0)。
2.  解析出这 $K$ 个路径得出的最终答案。
3.  **多数投票 (Majority Voting)**：选择出现次数最多的答案。

**代码实现**：

```python
from collections import Counter

def self_consistency(model, prompt, k=10):
    """
    CoT-SC: 采样 k 次取众数
    """
    answers = []

    # 1. 并行采样 k 次
    responses = model.generate(
        prompt,
        n=k,
        temperature=0.7,  # 需要一定的随机性
        stop=["\n\n"]
    )

    # 2. 提取每个回复中的最终答案 (假设有 extract_answer 函数)
    for resp in responses:
        ans = extract_answer(resp)
        if ans is not None:
            answers.append(ans)

    if not answers:
        return None

    # 3. 多数投票
    counter = Counter(answers)
    most_common_ans, count = counter.most_common(1)[0]
    confidence = count / len(answers)

    return most_common_ans, confidence

def extract_answer(text):
    """简单提取器示例：提取 'The answer is' 后面的内容"""
    if "The answer is" in text:
        return text.split("The answer is")[-1].strip().strip(".")
    return None
```

**为什么有效？**
假设模型单次推理的错误率是 $\epsilon$。如果错误是随机分布的（不同的胡说八道），而正确答案是唯一的。多次采样的结果往往会聚敛在正确答案上。
实测中，CoT-SC 是**提分性价比最高**的方法（MATH 数据集上能提 10-20%）。

### 3. Least-to-Most Prompting：由易到难

针对复杂问题（如“在这个数组中，倒数第二个质数与第一个奇数的乘积是多少？”），**Least-to-Most** 策略分两步走：

1.  **Decomposition (分解)**：让模型先把大问题分解为子问题列表。
    > Q: How to solve complex problem?
    > A: 1. Find the first odd number. 2. Find the second to last prime. 3. Multiply them.

2.  **Sequential Solving (顺序求解)**：依次求解子问题，把上一步的答案拼接到 Context 中。

---

## 三、进阶：树与图的搜索 (Tree & Graph Search)

CoT 是线性的 (Chain)。但这不符合人类思维。我们在思考复杂问题时，往往会**试错 (Trial and Error)**、**回溯 (Backtracking)**、**合并 (Merge)**。

这就需要树 (Tree) 甚至图 (Graph) 结构。

### 1. Tree of Thoughts (ToT)：思维之树

ToT (Yao et al., 2023) 将推理建模为状态空间搜索。

**四大组件**：
1.  **Thought Decomposition**: 将推理拆分为步骤 (thought step)。
2.  **Though Generator**: 给定当前状态 $s$，生成 $k$ 个可能的下一步 $z^{(1)}...z^{(k)}$。
3.  **State Evaluator**: 评估当前状态是否有希望 (Value Function)。
4.  **Search Algorithm**: BFS (广度优先) 或 DFS (深度优先)。

### 2. Graph of Thoughts (GoT)：思维之网

GoT (Besta et al., 2023) 进一步放宽限制，允许思维节点之间形成任意有向图。

**核心操作**：
- **Aggregation (聚合)**：将多个思维分支的结果汇总（例如：三个写作方案取长补短，合成一个）。
- **Refinement (精炼)**：对当前思维进行自我修正。
- **Loop (循环)**：允许循环迭代优化。

### 3. 实战：手写一个 ToT 24点求解器

24点游戏是 ToT 的经典 Benchmark。这是简化版实现：

```python
import itertools

class ToTSolver24:
    """Tree-of-Thoughts 24点求解器"""
    def __init__(self, model_client):
        self.client = model_client

    def get_proposals(self, numbers):
        """
        生成器：给定当前数字集合，生成下一步可能的运算
        例如: [4, 9, 10, 13] -> Possible next steps: 4+9=13 (left: 10, 13, 13)
        """
        prompt = f"""
        现有数字: {numbers}
        目标: 24
        请列出所有可能的下一步运算（只做一步，主要考虑加减乘除）。
        输出格式: num1 op num2 = result (剩余数字: ...)
        请给出 5 个最有希望的下一步。
        """
        response = self.client.generate(prompt, n=1)
        # ... 解析 response 提取 proposals ...
        return parsed_proposals

    def evaluate(self, state):
        """
        评估器：判断当前数字集合能否算出 24
        返回: score (0.0 - 1.0)
        """
        if len(state) == 1:
            return 1.0 if abs(state[0] - 24) < 1e-5 else 0.0

        prompt = f"""
        现有数字: {state}
        请评估：这些数字通过四则运算得到 24 的可能性有多大？
        输出一个 0.0 到 1.0 之间的分数，只输出数字。
        """
        score = float(self.client.generate(prompt))
        return score

    def bfs_solve(self, initial_numbers, beam_width=5):
        """Beam Search"""
        current_states = [initial_numbers] # 状态就是数字列表

        for step in range(3): # 4个数字需要3步运算
            next_states = []

            for state in current_states:
                # 1. Generate: 生成后续
                proposals = self.get_proposals(state)

                # 2. Evaluate: 评估并剪枝
                for p in proposals:
                    new_state = p['remaining_numbers']
                    score = self.evaluate(new_state)

                    if score > 0.1: # 简单过滤
                        next_states.append({'state': new_state, 'score': score, 'history': p['history']})

            # 3. Select: 选出 Top-K
            next_states.sort(key=lambda x: x['score'], reverse=True)
            current_states = [x['state'] for x in next_states[:beam_width]]

            # 检查是否完成
            for s in current_states:
                if len(s) == 1 and abs(s[0] - 24) < 1e-5:
                    return "Solved!"

        return "Failed"
```

---

## 四、核心组件：验证器 (Verifier) 与 PRM

ToT 的核心在于“谁来评估这一步好不好”。依赖 LLM 自己评估（Self-Evaluation）往往不准。这就需要训练专门的 **Verifier**。

### 1. ORM (结果奖励) vs PRM (过程奖励)

| 类型 | ORM (Outcome Reward Model) | PRM (Process Reward Model) |
| :--- | :--- | :--- |
| **定义** | 只在推理结束时给分（对/错） | 对推理的**每一步**都打分 |
| **反馈粒度** | 粗粒度 (Sparse) | 细粒度 (Dense) |
| **训练数据** | 容易获取 (只需最终答案) | 极难获取 (需人工标注每步逻辑) |
| **搜索效率** | 只有走到终点才知道对错，搜索效率低 | 走错一步立马知道，及时止损，效率高 |
| **代表作** | GPT-4 早期版本 | OpenAI o1, DeepSeek-Math |

**数学视角**：
ORM 估计的是 $V(s_{end})$。
PRM 估计的是 $V(s_t)$，即价值函数。

### 2. 为什么 PRM 是 System 2 的关键？

在长链推理（如数学证明，步骤 50+）中，如果使用 ORM，只有当模型完整推理正确时才能得到正反馈。这就像走迷宫，只有走到出口才给糖吃。
PRM 则是在每个路口都告诉你“路走对了”或“死胡同”。

**Let's Verify Step by Step (OpenAI, 2023)** 论文证明：
**PRM 显著优于 ORM**，尤其是在困难的数学数据集（MATH）上。

### 3. 实战：训练一个简单的数学题 Verifier

```python
import torch
from transformers import AutoModelForSequenceClassification

class MathVerifier(torch.nn.Module):
    def __init__(self, model_name):
        super().__init__()
        # 使用 BERT 或 RoBERTa 作为分类器
        self.encoder = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=1 # 回归打分
        )

    def forward(self, problem, step_history, current_step):
        """
        输入：
        - problem: 原题
        - step_history: 之前的步骤
        - current_step: 当前要评估的步骤
        """
        # 拼接 context
        text = f"Problem: {problem}\nHistory: {step_history}\nCheck Step: {current_step}"

        # 预测该步骤无论是"正确的"还是"有用的" (score)
        return self.encoder(text).logits
```

**标注数据格式 (PRM800K)**：
```json
{
  "problem": "Find x...",
  "solution": [
    {"step": "Let x = 2y", "label": "correct"},
    {"step": "Then 4y + 2 = 10", "label": "correct"},
    {"step": "So y = 3", "label": "error"}  <-- 此处标注错误
  ]
}
```

---

## 五、终极形态：MCTS 与 AlphaGo 范式

MCTS (蒙特卡洛树搜索) 是 AlphaGo 战胜李世石的核武器。现在的趋势是把这一套用在 LLM 推理上。

### 1. 为什么 LLM 需要 MCTS？

LLM 生成答案其实就是在 Token 空间中**下棋**。
- 棋盘状态 $s$ = 当前生成的 Context
- 落子动作 $a$ = 下一个 Token (或 Step)
- 胜负判定 = 最终答案是否通过测试/验证

AlphaGo 不需要人类告诉它怎么下（Supervised Learning），它通过自我对弈（RL）+ MCTS 搜索学会了超越人类的策略。
DeepSeek-R1 / OpenAI o1 的核心愿景就是：**通过 MCTS + RL，让模型在数学/代码领域实现自我进化**。

### 2. PUCT 算法在 LLM 中的映射

MCTS 的核心选择公式 (PUCT)：

$$
a_t = \arg\max_{a} \left( Q(s, a) + c_{puct} \cdot P(s, a) \cdot \frac{\sqrt{N(s)}}{1 + N(s, a)} \right)
$$

在 LLM 中：
- **$P(s, a)$ (先验概率)**：由 **Policy Model** (就是 LLM 本身) 给出的 Token 概率。
- **$Q(s, a)$ (价值评估)**：由 **Value Model** (PRM) 给出的步骤平均得分。
- **$N(s, a)$ (访问次数)**：这个节点被探索了多少次。

**流程**：
1.  **Selection**: 顺着树往下走，总是选 PUCT 分数最高的节点。
2.  **Expansion**: 走到叶子节点，让 LLM 生成个新的步骤。
3.  **Evaluation**: 用 PRM 给这个步骤打分。
4.  **Backup**: 把分数反向传导回祖先节点，更新 $Q$ 值。

### 3. DeepSeek-R1 / OpenAI o1 架构猜想

虽然官方没有公开，但业界普遍推测其架构如下：

1.  **训练阶段 (System 1 -> System 2)**：
    使用 RL (如 PPO 或 GRPO)，奖励函数不仅仅是“答案对不对”，还包含了“思考过程是否合理”。
    甚至，使用 MCTS 生成高质量的推理路径作为训练数据（SFT）。

2.  **推理阶段 (Test-Time Search)**：
    用户输入 Prompt 后，模型并不直接输出，而是在后台进行 MCTS 搜索。
    - 生成多个隐式思维链 (Hidden Chain of Thought)。
    - 自我评估、回溯、修正。
    - 最终只输出经过验证的最佳路径（或摘要）。

---

## 六、本章小结

推理时计算增强是目前大模型领域**最前沿、最硬核**的方向。它标志着 LLM 从“鹦鹉学舌”走向“逻辑推理”。

1.  **Scaling Law 2.0**：推理时的算力可以换取智能。
2.  **组件三件套**：
    - **Generator (Policy)**: 会发散思维的模型 (LLM)。
    - **Verifier (Value)**: 会找茬纠错的判官 (PRM)。
    - **Searcher**: 协调二者的算法 (MCTS/ToT)。
3.  **应用建议**：
    - 对于简单 RAG 任务：CoT 足够。
    - 对于复杂 Agent 规划：必须上 ReAct 或 ToT。
    - 对于严谨数学/代码任务：需要 CoT-SC 或 Best-of-N。

---

**下一章预告：** 第4章 - 推理模型专题 (DeepSeek-R1 / OpenAI o1)

在下一章中，我们将专门拆解 DeepSeek-R1 的技术细节，探讨 GRPO 算法和长链推理的训练秘籍。
