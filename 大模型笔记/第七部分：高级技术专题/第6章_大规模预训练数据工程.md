# 第6章：大规模预训练数据工程

> "Garbage In, Garbage Out" 在大模型时代依然是真理。不同的是，我们现在面对的是PB级别的"Garbage"，需要TB级别的"Gold"。

**本章定位**：
- 聚焦**大规模（TB/PB级）**预训练数据处理，区别于微调数据（SFT）的小规模（MB/GB级）精细处理
- 核心技术栈：**Spark**、**MinHash LSH**、**CCNet**、**Datatrove**
- 覆盖从CommonCrawl原始网页到高质量Pre-train Corpus的完整流水线
- 解决分布式去重、质量评分、隐私过滤等工程难题

**学习目标**：
- 掌握TB级数据的分布式处理架构（Spark/Ray）
- 深入理解MinHash LSH分桶去重算法与实现
- 理解CCNet pipeline设计思想与关键过滤器
- 能够构建一个工业级的预训练数据处理流水线

---

## 目录
- [一、预训练数据 vs 微调数据](#一预训练数据-vs-微调数据)
- [二、大规模数据采集与清洗流水线](#二大规模数据采集与清洗流水线)
  - [1. CCNet Pipeline架构](#1-ccnet-pipeline架构)
  - [2. CommonCrawl采集实战](#2-commoncrawl采集实战)
  - [3. 质量过滤（Quality Filtering）](#3-质量过滤quality-filtering)
- [三、核心算法：MinHash LSH去重](#三核心算法minhash-lsh去重)
  - [1. 为什么要MinHash？](#1-为什么要minhash)
  - [2. 算法原理：Jaccard相似度与哈希签名](#2-算法原理jaccard相似度与哈希签名)
  - [3. Spark分布式实现（TB级去重）](#3-spark分布式实现tb级去重)
- [四、合成数据生成（Synthetic Data）](#四合成数据生成synthetic-data)
  - [1. Self-Instruct与Evol-Instruct](#1-self-instruct与evol-instruct)
  - [2. 教科书级数据合成（Textbooks Are All You Need）](#2-教科书级数据合成textbooks-are-all-you-need)
- [五、数据配比与课程学习](#五数据配比与课程学习)
  - [1. 数据配比（Data Mixture）](#1-数据配比data-mixture)
  - [2. 课程学习（Curriculum Learning）](#2-课程学习curriculum-learning)
- [六、本章小结](#六本章小结)

---

## 一、预训练数据 vs 微调数据

这不仅仅是数量的区别，更是工程架构和处理逻辑的根本差异。

| 维度 | 预训练数据 (Pre-training) | 微调数据 (SFT/RLHF) |
|-----|-------------------------|-------------------|
| **规模** | TB ~ PB (Trillions of tokens) | MB ~ GB (Thousands of samples) |
| **来源** | 互联网 (CommonCrawl)、书籍、代码 | 人工标注、合成数据、专家修正 |
| **目标** | 学习世界知识、语言模式、推理能力 | 学习指令遵循、对齐人类偏好 |
| **核心挑战** | **去重**、性能优化、存储成本 | **质量**、多样性、Prompt设计 |
| **技术栈** | Spark, Ray, CCNet, MinHash LSH | Pandas, Python脚本, 人工审核 |
| **处理耗时** | 数周 ~ 数月 (集群) | 数小时 ~ 数天 (单机/小集群) |

> **关键认知**：预训练数据工程本质上是一个**大数据（Big Data）**工程问题，而微调数据工程是一个**数据质量（Data Quality）**问题。

---

## 二、大规模数据采集与清洗流水线

业界标杆是Meta的**CCNet**（Used for training Llama models）和Hugging Face的**Datatrove**。

### 1. CCNet Pipeline架构

一个标准的预训练数据处理流水线包含以下步骤：

```mermaid
graph TD
    A[CommonCrawl (WARC)] -->|提取文本| B[Text Extraction]
    B -->|语言识别| C[Language ID]
    C -->|规则过滤| D[Heuristic Filtering]
    D -->|模型过滤| E[Model-based Filtering]
    E -->|文档级去重| F[Exact Dedup]
    F -->|模糊去重| G[Fuzzy Dedup (MinHash)]
    G -->|PII去除| H[PII Removal]
    H -->|Tokenization| I[Training Data]
```

### 2. CommonCrawl采集实战

CommonCrawl是最大的开源网页数据集（PB级）。

**Datatrove实现采集**：

```python
from datatrove.pipeline.readers import WarcReader
from datatrove.pipeline.filters import LanguageFilter
from datatrove.executor import LocalPipelineExecutor

pipeline = [
    # 1. 读取WARC (Web ARChive)
    WarcReader(
        data_folder="s3://commoncrawl/crawl-data/CC-MAIN-2024-10/",
        limit=1000,  # 测试用
    ),

    # 2. 语言过滤 (保留中文)
    LanguageFilter(
        language="zh",
        language_threshold=0.8
    ),
]

# 分布式执行器（支持Slurm, Local）
executor = LocalPipelineExecutor(
    pipeline=pipeline,
    workers=16
)
executor.run()
```

### 3. 质量过滤（Quality Filtering）

**规则过滤（Heuristics）**：
- **Gopher规则**：
  - 单词数 $\in [50, 10000]$
  - 平均单词长度 $\in [3, 10]$
  - 符号与单词比率 $< 0.1$
  - `...` (省略号) 占比过高则丢弃

**模型过滤（Model-based）**：
- **困惑度（Perplexity）过滤**：使用轻量级语言模型（如KenLM或小BERT）计算PPL。
  - PPL过高 → 乱码/噪声
  - PPL过低 → 重复文本/简单模式

- **FastText分类器**：训练一个二分类器（High Quality vs Low Quality），使用Wikipedia作为正样本。

```python
# 伪代码：PPL过滤
def filter_by_ppl(text, model, threshold=1000):
    ppl = calculate_ppl(text, model)
    if ppl > threshold:
        return False # 丢弃（可能是乱码）
    return True
```

---

## 三、核心算法：MinHash LSH去重

在TB级数据中，两两比较计算相似度是$O(N^2)$，是不可能的任务。MinHash LSH将复杂度降为$O(N)$。

### 1. 为什么要MinHash？

- **精确去重（MD5/SHA）**：只能去除完全一样的文档。一旦有一个标点不同，哈希值就完全不同。
- **模糊去重（Fuzzy Dedup）**：我们需要找出"几乎一样"的文档（如虽有广告Banner不同，但正文相同的网页）。

### 2. 算法原理：Jaccard相似度与哈希签名

**关键定理**：
两个集合的Jaccard相似度等于它们的MinHash签名相同的概率。

$$ P(h_{\min}(A) = h_{\min}(B)) = J(A, B) = \frac{|A \cap B|}{|A \cup B|} $$

**MinHash签名生成**：
1. 将文档分词为Shingles（n-grams）。
2. 使用$K$个不同的哈希函数对Shingles进行哈希。
3. 对每个哈希函数，取最小值作为签名的一部分。
4. 得到长度为$K$的签名向量。

**LSH（局部敏感哈希）分桶**：
为了避免两两比较签名，使用LSH进行分桶：
- 将$K$个签名分为$b$个Band，每个Band有$r$行。
- 只有当两个文档在至少一个Band中完全一致时，才计算它们的真实相似度。

### 3. Spark分布式实现（TB级去重）

这是本章的核心工程代码。在单机上跑不动TB级数据，必须用Spark。

```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import MinHashLSH
from pyspark.ml.feature import CountVectorizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import ArrayType, StringType

# 1. 初始化Spark
spark = SparkSession.builder \
    .appName("MinHashDedup") \
    .config("spark.executor.memory", "32g") \
    .getOrCreate()

# 2. 读取数据 (假设是Parquet格式)
df = spark.read.parquet("s3://my-bucket/pretrain-data/")

# 3. 分词 (Shingling)
# 简单实现：按空格分词，实际可用jieba或tiktoken
tokenizer = udf(lambda x: x.split(" "), ArrayType(StringType()))
df = df.withColumn("tokens", tokenizer(col("text")))

# 4. 向量化 (CountVectorizer)
# MinHash输入需要是稀疏向量
cv = CountVectorizer(inputCol="tokens", outputCol="features", vocabSize=1000000, minDF=2)
model = cv.fit(df)
df_vectorized = model.transform(df)

# 5. MinHash LSH
mh = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=5)
lsh_model = mh.fit(df_vectorized)

# 6. 计算相似对 (Self-Join)
# distance < 0.2 意味着 Jaccard similarity > 0.8
similarity_threshold = 0.2
mn_pairs = lsh_model.approxSimilarityJoin(df_vectorized, df_vectorized, similarity_threshold, distCol="distance")

# 7. 过滤重复
# 保留ID较小的文档，丢弃ID较大的
deduped_ids = mn_pairs.filter(col("datasetA.id") < col("datasetB.id")) \
    .select(col("datasetB.id").alias("duplicate_id")) \
    .distinct()

# 反向过滤
final_df = df.join(deduped_ids, df.id == deduped_ids.duplicate_id, "left_anti")

# 8. 保存
final_df.write.parquet("s3://my-bucket/deduped-data/")
```

**工程优化Tips**：
- **Band & Row选择**：这是一门艺术。通常 $K=128$。想要高召回率（Recall），增加Band数量；想要高精确率（Precision），增加Row数量。
- **Connected Components**：如果A≈B，B≈C，那么A,B,C属于同一个连通图。在严格去重中，整个连通图只保留一个。可以使用Spark GraphX实现。

---

## 四、合成数据生成（Synthetic Data）

当互联网数据（Web Text）被用光了怎么办？答案是：让模型自己写书。

### 1. Self-Instruct与Evol-Instruct

这是**微调**阶段常用的技术（如Alpaca, WizardLM），但在**预训练**阶段也开始使用。

- **Self-Instruct**：用少量种子Prompt，让模型生成新的Instruction-Response对。
- **Evol-Instruct**：不断"进化"指令难度（Deepen, Broaden, Complicate）。

### 2. 教科书级数据合成（Textbooks Are All You Need）

Microsoft Phi系列模型的成功秘诀：**高质量合成数据**。

**核心思想**：
CommonCrawl充斥着碎片化、低质量的文本。Phi团队使用GPT-4生成了大量"教科书风格"（Textbook-quality）的数据：
- 逻辑清晰
- 知识密度高
- 结构化强

**生成Prompt示例**：
```markdown
Write a textbook section about Linear Algebra.
Target audience: Undergraduate students.
Style: Rigorous, clear, with examples and proofs.
Format: Markdown.
Topic: Eigenvalues and Eigenvectors.
```

这种合成数据训练出的1.3B模型，性能击败了训练在Web Text上的7B模型。

---

## 五、数据配比与课程学习

### 1. 数据配比（Data Mixture）

你不能只喂代码，也不能只喂小说。一个健康的"饮食金字塔"（Llama 3参考）：

| 数据类型 | 比例 | 作用 |
|---------|-----|------|
| **Web Text** | 50-80% | 通用世界知识、语言流畅度 |
| **Code** | 10-20% | 逻辑推理能力、编程能力 |
| **Math** | 2-5% | 数学推理、精确计算 |
| **Wikipedia/Books** | 5-10% | 高质量知识、长文本建模 |
| **Scientific Papers** | 1-5% | 专业领域知识 |

**动态权重**：在训练过程中，配比通常不是固定的。

### 2. 课程学习（Curriculum Learning）

人类学习是先学加减法，再学微积分。模型也应该如此。

- **阶段一（Early Stage）**：简单、高质量、基础数据（如Wikipedia, Textbooks）。让模型快速学会语法和基础知识。
- **阶段二（Middle Stage）**：大规模Web Text。扩展知识面。
- **阶段三（Late Stage / Annealing）**：高质量指令数据、推理数据。提升特定能力，为SFT做准备。

**关键技巧**：在最后阶段（Cool-down），使用极高质量的数据（如Math, Code）可以显著提升最终Benchmark分数。

---

## 六、本章小结

### 核心要点

1.  **规模差异**：预训练数据工程是大数据工程（Spark/Ray），微调数据工程是数据质量工程。
2.  **流水线**：CCNet是标准参考架构（提取 -> 过滤 -> 去重 -> PII -> Tokenize）。
3.  **MinHash LSH**：解决TB级数据去重的唯一解，$O(N)$复杂度，利用Jaccard相似度原理。
4.  **合成数据**：Phi系列证明了"教科书级"合成数据可以实现"小模型大智慧"。
5.  **数据配比**：Code和Math数据不仅仅是为了写代码和算数，更是为了提升**通用推理能力**。

### 面试必背

**Q: 如何在10TB文本中找出相似的文档？**
A: 不能用$O(N^2)$的两两比较。使用**MinHash LSH**。
1. 将文档转为Shingles集合。
2. 计算MinHash签名（降维）。
3. 使用LSH分桶（Bands & Rows策略），只有桶内碰撞的候选对才计算真实Jaccard与相似度。
4. 使用Spark或Ray进行分布式实现。

**Q: 预训练数据清洗有哪些关键步骤？**
A:
1. **启发式过滤**（长度、符号比、去重）。
2. **模型过滤**（PPL困惑度、分类器）。
3. **去重**（Exact Dedup & Fuzzy Dedup）。
4. **去隐私**（PII Removal）。

---

**下一章预告**：
至此，我们完成了**第七部分：高级技术专题**的所有内容：
1. 长上下文技术（RoPE, Ring Attention）
2. 新型架构（MoE, Mamba）
3. 推理加速（Speculative Decoding, KV Cache）
4. 推理模型（O1, CoT）
5. 安全与可解释性（SAE, Injection）
6. 大规模数据工程（MinHash, Spark）

恭喜！你已经掌握了大模型领域最硬核的底层技术。
