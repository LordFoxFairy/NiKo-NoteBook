# 第6章：数据工程全流程

> 从数据采集到合成生成，掌握LLM数据工程的完整技术栈。

**本章定位**：
- 整合第六部分"数据工程基础"内容，提升到高级水平
- 覆盖TB级大规模数据处理、合成数据生成、数据配比优化
- 理论（MinHash、Curriculum Learning）+ 实战（Datatrove、Argilla）
- 面向生产环境的数据工程完整方案

**学习目标**：
- 掌握大规模数据采集、清洗、去重的完整流水线
- 理解并实践Self-Instruct、Evol-Instruct合成数据生成
- 学会数据配比优化、Curriculum Learning训练策略
- 了解数据质量评估、污染检测、隐私保护技术

---

## 一、大规模数据采集与清洗

> **核心问题**：如何从互联网获取并清洗TB级高质量预训练数据？

### 1. 数据来源与采集策略

#### （1）数据来源分类

现代LLM的预训练数据主要来自以下四类来源：

| 数据类型 | 代表来源 | 数据量级 | 质量特点 | 典型应用 |
|---------|---------|---------|---------|---------|
| **网络文本** | CommonCrawl、Wikipedia | 数TB | 质量参差不齐 | 通用知识、语言建模 |
| **书籍与文档** | BookCorpus、ArXiv论文 | 数百GB | 高质量、长文本 | 长文本理解、推理能力 |
| **代码数据** | GitHub、Stack Overflow | 数TB | 结构化、高质量 | 代码生成、逻辑推理 |
| **对话数据** | Reddit、客服记录 | 数十GB | 交互性强 | 对话能力、角色扮演 |

**主流模型的数据来源**（以Llama 3为例）：

```
Llama 3 (15T tokens):
├─ 网络文本：   ~85% (CommonCrawl过滤后)
├─ 代码：       ~8%  (GitHub + StackOverflow)
├─ 书籍/论文：  ~5%  (Books3 + ArXiv)
└─ 其他：       ~2%  (Wikipedia + 高质量网站)
```

#### （2）网络爬取：CommonCrawl实战

**CommonCrawl简介**：
- 互联网档案馆，每月爬取数十亿网页
- 提供免费的WARC格式数据（~300TB/月）
- 几乎所有开源LLM都使用其作为基础数据

**数据获取流程**：

```python
from datatrove.pipeline.readers import WarcReader
from datatrove.pipeline.filters import LanguageFilter, QualityFilter
from datatrove.pipeline.dedup import SentenceDedupFilter
from datatrove.pipeline.writers import JsonlWriter
from datatrove.executor import LocalPipelineExecutor
from dataclasses import dataclass
from typing import List

@dataclass
class CommonCrawlConfig:
    """CommonCrawl爬取配置"""
    snapshot: str = "CC-MAIN-2024-10"  # 快照版本
    num_workers: int = 8               # 并行worker数
    language: str = "zh"               # 目标语言
    output_dir: str = "./cc_data"      # 输出目录


class CommonCrawlCrawler:
    """CommonCrawl数据采集器"""

    def __init__(self, config: CommonCrawlConfig):
        self.config = config

    def build_pipeline(self) -> List:
        """构建数据处理流水线"""

        pipeline = [
            # 1. 读取WARC文件
            WarcReader(
                data_folder=f"s3://commoncrawl/crawl-data/{self.config.snapshot}/segments/",
                limit=10000,  # 限制处理的WARC数量（测试用）
            ),

            # 2. 语言过滤（只保留中文）
            LanguageFilter(
                language=self.config.language,
                language_threshold=0.8,  # 语言置信度阈值
            ),

            # 3. 质量过滤
            QualityFilter(
                min_doc_length=100,        # 最小文档长度
                max_doc_length=100000,     # 最大文档长度
                min_word_count=20,         # 最小单词数
                max_avg_line_length=200,   # 最大平均行长度（过滤表格）
            ),

            # 4. 句子级去重
            SentenceDedupFilter(
                sentence_dedup_config={
                    "similarity_threshold": 0.9,
                }
            ),

            # 5. 写入JSONL
            JsonlWriter(
                output_folder=self.config.output_dir,
                compression="gzip",
            ),
        ]

        return pipeline

    def run(self):
        """执行采集"""
        pipeline = self.build_pipeline()

        executor = LocalPipelineExecutor(
            pipeline=pipeline,
            workers=self.config.num_workers,
            tasks=100,  # 总任务数
        )

        executor.run()
        print(f"采集完成！数据保存在：{self.config.output_dir}")


# 使用示例
if __name__ == "__main__":
    config = CommonCrawlConfig(
        snapshot="CC-MAIN-2024-10",
        num_workers=16,
        language="zh",
        output_dir="./commoncrawl_zh"
    )

    crawler = CommonCrawlCrawler(config)
    crawler.run()
```

**关键技术点**：

1. **WARC格式解析**：
   - Web ARChive格式，包含HTTP请求/响应
   - 需要提取HTML内容并清洗

2. **分布式处理**：
   - CommonCrawl数据量极大（300TB+），必须分布式处理
   - 使用Spark或Ray并行化

3. **成本优化**：
   - CommonCrawl存储在AWS S3，直接下载会产生巨额流量费
   - 推荐在AWS上启动EC2实例处理（S3→EC2免费）

#### （3）书籍与文档采集

**PDF解析流水线**：

```python
import PyPDF2
from pdf2image import convert_from_path
from pytesseract import image_to_string
from pathlib import Path
from typing import Optional
import re

class PDFProcessor:
    """PDF文档处理器（支持OCR）"""

    def __init__(self, enable_ocr: bool = False):
        self.enable_ocr = enable_ocr

    def extract_text_from_pdf(self, pdf_path: str) -> Optional[str]:
        """从PDF提取文本（支持扫描PDF的OCR）"""

        try:
            # 1. 尝试直接提取文本（适用于可复制PDF）
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()

            # 2. 如果文本为空且启用OCR，则使用OCR
            if len(text.strip()) < 100 and self.enable_ocr:
                print(f"文本提取失败，使用OCR：{pdf_path}")
                text = self._ocr_pdf(pdf_path)

            return text

        except Exception as e:
            print(f"处理PDF失败：{pdf_path}, 错误：{e}")
            return None

    def _ocr_pdf(self, pdf_path: str) -> str:
        """对扫描PDF执行OCR"""
        # 将PDF转为图像
        images = convert_from_path(pdf_path, dpi=300)

        # 对每页执行OCR
        text = ""
        for i, image in enumerate(images):
            page_text = image_to_string(image, lang='chi_sim+eng')  # 中英文
            text += f"\n--- Page {i+1} ---\n{page_text}"

        return text

    def clean_text(self, text: str) -> str:
        """清洗提取的文本"""

        # 1. 去除页眉页脚（简单规则）
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            # 跳过页码（纯数字行）
            if re.match(r'^\s*\d+\s*$', line):
                continue
            # 跳过过短的行（可能是页眉）
            if len(line.strip()) < 3:
                continue
            cleaned_lines.append(line)

        text = '\n'.join(cleaned_lines)

        # 2. 合并断行（PDF提取常见问题）
        text = re.sub(r'(\w)-\n(\w)', r'\1\2', text)  # 连字符断行
        text = re.sub(r'([^\n])\n([^\n])', r'\1 \2', text)  # 普通断行

        # 3. 去除多余空白
        text = re.sub(r'\n{3,}', '\n\n', text)  # 最多保留一个空行
        text = re.sub(r' {2,}', ' ', text)      # 多个空格压缩为一个

        return text.strip()


# 批量处理示例
def process_pdf_directory(input_dir: str, output_file: str):
    """批量处理PDF目录"""

    processor = PDFProcessor(enable_ocr=True)
    pdf_files = list(Path(input_dir).glob("**/*.pdf"))

    print(f"找到 {len(pdf_files)} 个PDF文件")

    with open(output_file, 'w', encoding='utf-8') as f:
        for pdf_path in pdf_files:
            print(f"处理：{pdf_path}")

            # 提取文本
            text = processor.extract_text_from_pdf(str(pdf_path))
            if text is None:
                continue

            # 清洗文本
            text = processor.clean_text(text)

            # 写入（JSONL格式）
            import json
            record = {
                "text": text,
                "source": str(pdf_path),
                "length": len(text)
            }
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

    print(f"处理完成！输出：{output_file}")


if __name__ == "__main__":
    process_pdf_directory(
        input_dir="./books",
        output_file="./books_processed.jsonl"
    )
```

**ArXiv论文采集**：

```python
import arxiv
import time
from typing import List, Dict

class ArxivCrawler:
    """ArXiv论文采集器"""

    def __init__(self, categories: List[str], max_results: int = 10000):
        """
        Args:
            categories: 论文类别（如 ['cs.AI', 'cs.CL', 'cs.LG']）
            max_results: 每个类别最大论文数
        """
        self.categories = categories
        self.max_results = max_results

    def fetch_papers(self, category: str) -> List[Dict]:
        """获取指定类别的论文"""

        papers = []
        client = arxiv.Client()

        search = arxiv.Search(
            query=f"cat:{category}",
            max_results=self.max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
        )

        for result in client.results(search):
            paper = {
                "title": result.title,
                "abstract": result.summary,
                "authors": [author.name for author in result.authors],
                "published": result.published.strftime("%Y-%m-%d"),
                "categories": result.categories,
                "pdf_url": result.pdf_url,
            }
            papers.append(paper)

            # 避免请求过快
            time.sleep(0.5)

        return papers

    def download_pdfs(self, papers: List[Dict], output_dir: str):
        """下载论文PDF"""

        from pathlib import Path
        import requests

        Path(output_dir).mkdir(parents=True, exist_ok=True)

        for i, paper in enumerate(papers):
            pdf_url = paper["pdf_url"]
            filename = f"{output_dir}/{i+1:05d}_{paper['title'][:50]}.pdf"

            print(f"下载：{paper['title']}")

            try:
                response = requests.get(pdf_url, timeout=30)
                with open(filename, 'wb') as f:
                    f.write(response.content)

                time.sleep(1)  # 避免请求过快

            except Exception as e:
                print(f"下载失败：{e}")

    def run(self, output_dir: str = "./arxiv_papers"):
        """执行采集"""

        all_papers = []

        for category in self.categories:
            print(f"采集类别：{category}")
            papers = self.fetch_papers(category)
            all_papers.extend(papers)

        print(f"共采集 {len(all_papers)} 篇论文")

        # 保存元数据
        import json
        with open(f"{output_dir}/metadata.jsonl", 'w') as f:
            for paper in all_papers:
                f.write(json.dumps(paper, ensure_ascii=False) + '\n')

        # 下载PDF
        self.download_pdfs(all_papers, output_dir)


# 使用示例
if __name__ == "__main__":
    crawler = ArxivCrawler(
        categories=['cs.CL', 'cs.AI', 'cs.LG'],  # NLP、AI、ML
        max_results=1000
    )
    crawler.run(output_dir="./arxiv_papers")
```

#### （4）代码数据采集

**GitHub代码爬取**：

```python
from github import Github
from typing import List, Dict
import base64

class GitHubCrawler:
    """GitHub代码采集器"""

    def __init__(self, access_token: str, languages: List[str] = ["Python"]):
        """
        Args:
            access_token: GitHub Personal Access Token
            languages: 目标编程语言列表
        """
        self.g = Github(access_token)
        self.languages = languages

    def search_repositories(self, language: str, stars: int = 100) -> List:
        """搜索高星仓库"""

        query = f"language:{language} stars:>{stars}"
        repos = self.g.search_repositories(query=query, sort="stars")

        return list(repos[:1000])  # 限制1000个仓库

    def extract_code_files(self, repo, extensions: List[str] = [".py"]) -> List[Dict]:
        """提取仓库中的代码文件"""

        code_files = []

        try:
            contents = repo.get_contents("")
            while contents:
                file_content = contents.pop(0)

                if file_content.type == "dir":
                    contents.extend(repo.get_contents(file_content.path))
                else:
                    # 检查文件扩展名
                    if any(file_content.path.endswith(ext) for ext in extensions):
                        try:
                            # 下载文件内容
                            content = base64.b64decode(file_content.content).decode('utf-8')

                            code_files.append({
                                "repo": repo.full_name,
                                "path": file_content.path,
                                "content": content,
                                "size": file_content.size,
                                "url": file_content.html_url,
                            })

                        except Exception as e:
                            print(f"解码失败：{file_content.path}, {e}")

        except Exception as e:
            print(f"访问仓库失败：{repo.full_name}, {e}")

        return code_files

    def run(self, output_file: str = "./github_code.jsonl"):
        """执行采集"""

        import json

        all_code_files = []

        for language in self.languages:
            print(f"采集语言：{language}")

            repos = self.search_repositories(language, stars=100)
            print(f"找到 {len(repos)} 个仓库")

            for repo in repos:
                print(f"  处理仓库：{repo.full_name}")
                code_files = self.extract_code_files(repo)
                all_code_files.extend(code_files)

        print(f"共采集 {len(all_code_files)} 个代码文件")

        # 保存到JSONL
        with open(output_file, 'w', encoding='utf-8') as f:
            for code_file in all_code_files:
                f.write(json.dumps(code_file, ensure_ascii=False) + '\n')


# 使用示例
if __name__ == "__main__":
    # 需要在GitHub上生成Personal Access Token
    # https://github.com/settings/tokens

    crawler = GitHubCrawler(
        access_token="YOUR_GITHUB_TOKEN",
        languages=["Python", "JavaScript", "Go"]
    )
    crawler.run(output_file="./github_python_code.jsonl")
```

**Stack Overflow问答采集**：

```python
import requests
import time
from typing import List, Dict

class StackOverflowCrawler:
    """Stack Overflow问答采集器"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.stackexchange.com/2.3"

    def fetch_questions(
        self,
        tag: str,
        min_score: int = 10,
        max_results: int = 10000
    ) -> List[Dict]:
        """获取指定标签的高分问题"""

        questions = []
        page = 1
        has_more = True

        while has_more and len(questions) < max_results:
            url = f"{self.base_url}/questions"
            params = {
                "tagged": tag,
                "sort": "votes",
                "order": "desc",
                "min": min_score,
                "page": page,
                "pagesize": 100,
                "site": "stackoverflow",
                "key": self.api_key,
                "filter": "withbody",  # 包含问题body
            }

            response = requests.get(url, params=params)
            data = response.json()

            if "items" not in data:
                break

            for item in data["items"]:
                # 获取答案
                answers = self.fetch_answers(item["question_id"])

                questions.append({
                    "question_id": item["question_id"],
                    "title": item["title"],
                    "body": item.get("body", ""),
                    "score": item["score"],
                    "tags": item["tags"],
                    "answers": answers,
                })

            has_more = data.get("has_more", False)
            page += 1

            # 遵守API限流（每秒最多30次请求）
            time.sleep(0.1)

        return questions

    def fetch_answers(self, question_id: int) -> List[Dict]:
        """获取问题的所有答案"""

        url = f"{self.base_url}/questions/{question_id}/answers"
        params = {
            "order": "desc",
            "sort": "votes",
            "site": "stackoverflow",
            "key": self.api_key,
            "filter": "withbody",
        }

        response = requests.get(url, params=params)
        data = response.json()

        answers = []
        if "items" in data:
            for item in data["items"]:
                answers.append({
                    "answer_id": item["answer_id"],
                    "body": item.get("body", ""),
                    "score": item["score"],
                    "is_accepted": item.get("is_accepted", False),
                })

        return answers

    def run(self, tags: List[str], output_file: str = "./stackoverflow.jsonl"):
        """执行采集"""

        import json

        all_questions = []

        for tag in tags:
            print(f"采集标签：{tag}")
            questions = self.fetch_questions(tag, min_score=10, max_results=1000)
            all_questions.extend(questions)

        print(f"共采集 {len(all_questions)} 个问答")

        # 保存
        with open(output_file, 'w', encoding='utf-8') as f:
            for q in all_questions:
                f.write(json.dumps(q, ensure_ascii=False) + '\n')


# 使用示例
if __name__ == "__main__":
    # 在 https://stackapps.com/apps/oauth/register 注册获取API Key

    crawler = StackOverflowCrawler(api_key="YOUR_API_KEY")
    crawler.run(
        tags=["python", "machine-learning", "pytorch"],
        output_file="./stackoverflow_python.jsonl"
    )
```

---

### 2. 数据清洗流水线设计

采集到原始数据后，需要进行**质量过滤**以保证训练数据的质量。

#### （1）质量过滤策略

**多维度过滤规则**：

```python
from dataclasses import dataclass
from typing import Dict
import re
import jieba
from collections import Counter

@dataclass
class QualityFilterConfig:
    """质量过滤配置"""

    # 长度过滤
    min_doc_length: int = 100           # 最小文档长度（字符）
    max_doc_length: int = 1000000       # 最大文档长度
    min_word_count: int = 20            # 最小单词/分词数

    # 内容过滤
    max_symbol_word_ratio: float = 0.3  # 最大符号占比
    max_digit_ratio: float = 0.5        # 最大数字占比
    min_alpha_ratio: float = 0.5        # 最小字母/汉字占比

    # 行过滤
    max_avg_line_length: int = 300      # 最大平均行长度（过滤表格）
    min_avg_line_length: int = 10       # 最小平均行长度（过滤列表）

    # 重复过滤
    max_duplicate_line_ratio: float = 0.3  # 最大重复行占比
    max_duplicate_ngram_ratio: float = 0.2  # 最大重复n-gram占比

    # 毒性过滤
    use_toxicity_filter: bool = True
    toxicity_threshold: float = 0.5


class QualityFilter:
    """数据质量过滤器"""

    def __init__(self, config: QualityFilterConfig):
        self.config = config

        # 加载敏感词表（示例）
        self.toxic_words = self._load_toxic_words()

    def _load_toxic_words(self):
        """加载敏感词表"""
        # 实际应用中，应该从文件加载完整的敏感词库
        return {
            "脏话1", "脏话2", "暴力词汇", "歧视词汇",
            # ... 更多敏感词
        }

    def filter_document(self, text: str) -> Dict:
        """
        过滤单个文档

        Returns:
            {
                "passed": bool,      # 是否通过
                "reason": str,       # 未通过的原因
                "stats": dict        # 统计信息
            }
        """

        # 1. 长度过滤
        if len(text) < self.config.min_doc_length:
            return {"passed": False, "reason": "文档过短"}

        if len(text) > self.config.max_doc_length:
            return {"passed": False, "reason": "文档过长"}

        # 2. 分词统计
        words = list(jieba.cut(text))
        if len(words) < self.config.min_word_count:
            return {"passed": False, "reason": "单词数过少"}

        # 3. 字符类型统计
        num_alpha = sum(1 for c in text if c.isalpha() or '\u4e00' <= c <= '\u9fff')  # 字母+汉字
        num_digit = sum(1 for c in text if c.isdigit())
        num_symbol = len(text) - num_alpha - num_digit

        alpha_ratio = num_alpha / len(text)
        digit_ratio = num_digit / len(text)
        symbol_ratio = num_symbol / len(text)

        if alpha_ratio < self.config.min_alpha_ratio:
            return {"passed": False, "reason": "字母/汉字占比过低"}

        if digit_ratio > self.config.max_digit_ratio:
            return {"passed": False, "reason": "数字占比过高"}

        if symbol_ratio > self.config.max_symbol_word_ratio:
            return {"passed": False, "reason": "符号占比过高"}

        # 4. 行统计（检测表格、列表等）
        lines = text.split('\n')
        non_empty_lines = [l for l in lines if l.strip()]

        if len(non_empty_lines) == 0:
            return {"passed": False, "reason": "空文档"}

        avg_line_length = sum(len(l) for l in non_empty_lines) / len(non_empty_lines)

        if avg_line_length > self.config.max_avg_line_length:
            return {"passed": False, "reason": "平均行长度过长（可能是表格）"}

        if avg_line_length < self.config.min_avg_line_length:
            return {"passed": False, "reason": "平均行长度过短（可能是列表）"}

        # 5. 重复内容检测
        duplicate_line_ratio = self._calculate_duplicate_line_ratio(lines)
        if duplicate_line_ratio > self.config.max_duplicate_line_ratio:
            return {"passed": False, "reason": f"重复行占比过高：{duplicate_line_ratio:.2%}"}

        # 6. 毒性检测
        if self.config.use_toxicity_filter:
            toxicity_score = self._calculate_toxicity(text)
            if toxicity_score > self.config.toxicity_threshold:
                return {"passed": False, "reason": f"毒性分数过高：{toxicity_score:.2f}"}

        # 通过所有过滤
        stats = {
            "length": len(text),
            "word_count": len(words),
            "alpha_ratio": alpha_ratio,
            "digit_ratio": digit_ratio,
            "symbol_ratio": symbol_ratio,
            "avg_line_length": avg_line_length,
            "duplicate_line_ratio": duplicate_line_ratio,
        }

        return {"passed": True, "reason": "通过", "stats": stats}

    def _calculate_duplicate_line_ratio(self, lines: list) -> float:
        """计算重复行占比"""
        non_empty_lines = [l.strip() for l in lines if l.strip()]

        if len(non_empty_lines) == 0:
            return 0.0

        line_counts = Counter(non_empty_lines)
        duplicate_lines = sum(count - 1 for count in line_counts.values() if count > 1)

        return duplicate_lines / len(non_empty_lines)

    def _calculate_toxicity(self, text: str) -> float:
        """计算毒性分数（简化版）"""

        # 实际应用中，应该使用专业的毒性分类模型
        # 这里简化为敏感词匹配

        toxic_count = sum(1 for word in self.toxic_words if word in text)

        # 归一化到 [0, 1]
        return min(toxic_count / 10, 1.0)


# 批量过滤示例
def filter_dataset(input_file: str, output_file: str):
    """批量过滤数据集"""

    import json

    config = QualityFilterConfig()
    filter = QualityFilter(config)

    passed_count = 0
    total_count = 0

    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:

        for line in f_in:
            total_count += 1
            data = json.loads(line)
            text = data.get("text", "")

            # 过滤
            result = filter.filter_document(text)

            if result["passed"]:
                passed_count += 1
                data["quality_stats"] = result["stats"]
                f_out.write(json.dumps(data, ensure_ascii=False) + '\n')

            if total_count % 1000 == 0:
                print(f"已处理：{total_count}, 通过：{passed_count}, 通过率：{passed_count/total_count:.2%}")

    print(f"过滤完成！总计：{total_count}, 通过：{passed_count}, 通过率：{passed_count/total_count:.2%}")


if __name__ == "__main__":
    filter_dataset(
        input_file="./raw_data.jsonl",
        output_file="./filtered_data.jsonl"
    )
```

#### （2）困惑度过滤（Perplexity-based Filtering）

使用语言模型困惑度来评估文本质量（低困惑度=高质量）：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List
import json

class PerplexityFilter:
    """基于困惑度的质量过滤器"""

    def __init__(
        self,
        model_name: str = "gpt2",
        max_ppl: float = 1500.0,  # 最大允许困惑度
        device: str = "cuda"
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        self.model.eval()
        self.max_ppl = max_ppl
        self.device = device

    @torch.no_grad()
    def calculate_perplexity(self, text: str) -> float:
        """计算文本的困惑度"""

        # Tokenize
        encodings = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        input_ids = encodings.input_ids.to(self.device)

        # Forward pass
        outputs = self.model(input_ids, labels=input_ids)
        loss = outputs.loss

        # PPL = exp(loss)
        ppl = torch.exp(loss).item()

        return ppl

    def filter_document(self, text: str) -> dict:
        """过滤单个文档"""

        ppl = self.calculate_perplexity(text)

        passed = ppl <= self.max_ppl

        return {
            "passed": passed,
            "perplexity": ppl,
            "reason": "通过" if passed else f"困惑度过高：{ppl:.2f}"
        }


# 使用示例
if __name__ == "__main__":
    filter = PerplexityFilter(model_name="gpt2", max_ppl=1000.0)

    text = "这是一段高质量的中文文本，应该有较低的困惑度。"
    result = filter.filter_document(text)

    print(f"困惑度：{result['perplexity']:.2f}")
    print(f"是否通过：{result['passed']}")
```

**实际应用经验**（来自LLaMA/GLM等模型）：

```
困惑度阈值设置：
- 通用语料：     PPL < 1500  （保留 ~70%）
- 高质量语料：   PPL < 500   （保留 ~30%）
- 极高质量：     PPL < 100   （保留 ~5%，如教科书）

困惑度与质量的关系：
PPL < 100:    教科书级别（清晰、准确、结构化）
PPL 100-500:  高质量网页（如Wikipedia、技术博客）
PPL 500-1500: 一般质量网页（论坛、新闻）
PPL > 1500:   低质量（垃圾邮件、广告、乱码）
```

---

### 3. MinHash去重原理与实现

#### （1）为什么需要去重？

**数据重复的危害**：
1. **过拟合**：模型会记住重复数据，导致泛化能力下降
2. **评估污染**：如果测试集数据出现在训练集，评估结果无效
3. **浪费资源**：重复数据占用存储和计算资源

**去重粒度**：
- **文档级去重**：完全相同的文档（URL去重）
- **段落级去重**：相似的段落（MinHash）
- **句子级去重**：相似的句子（Sentence Dedup）

#### （2）MinHash算法原理

**核心思想**：用少量哈希值近似表示文档的Jaccard相似度。

**Jaccard相似度**：
```
J(A, B) = |A ∩ B| / |A ∪ B|

例如：
A = {1, 2, 3, 4}
B = {3, 4, 5, 6}
J(A, B) = |{3, 4}| / |{1, 2, 3, 4, 5, 6}| = 2/6 = 0.33
```

**MinHash定理**：

对于两个集合A和B，如果随机选择一个哈希函数h，则：

```
P(min(h(A)) == min(h(B))) = J(A, B)
```

即：**MinHash值相等的概率 = Jaccard相似度**

**MinHash签名**：

使用k个不同的哈希函数，得到k个MinHash值作为"签名"：

```
sig(A) = [minhash_1(A), minhash_2(A), ..., minhash_k(A)]
```

**相似度估计**：
```
J(A, B) ≈ (sig(A) 和 sig(B) 相同的元素数) / k
```

#### （3）MinHash去重完整实现

```python
import mmh3  # MurmurHash3
from typing import List, Set
from collections import defaultdict
import json

class MinHashDeduplicator:
    """MinHash文档去重器"""

    def __init__(
        self,
        num_perm: int = 128,          # 哈希函数数量（签名长度）
        threshold: float = 0.8,       # Jaccard相似度阈值
        shingle_size: int = 5,        # n-gram大小
    ):
        self.num_perm = num_perm
        self.threshold = threshold
        self.shingle_size = shingle_size

        # 生成随机种子（用于模拟多个哈希函数）
        self.seeds = list(range(num_perm))

    def shingle(self, text: str) -> Set[str]:
        """将文本转换为n-gram集合"""

        # 分词
        import jieba
        words = list(jieba.cut(text))

        # 生成n-gram
        shingles = set()
        for i in range(len(words) - self.shingle_size + 1):
            shingle = ' '.join(words[i:i+self.shingle_size])
            shingles.add(shingle)

        return shingles

    def minhash_signature(self, shingles: Set[str]) -> List[int]:
        """计算MinHash签名"""

        signature = []

        for seed in self.seeds:
            # 计算每个shingle的哈希值
            hashes = [mmh3.hash(shingle, seed=seed) for shingle in shingles]

            # 取最小值
            if hashes:
                min_hash = min(hashes)
            else:
                min_hash = 0

            signature.append(min_hash)

        return signature

    def estimate_jaccard(self, sig1: List[int], sig2: List[int]) -> float:
        """根据MinHash签名估计Jaccard相似度"""

        if len(sig1) != len(sig2):
            raise ValueError("签名长度必须相同")

        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)

        return matches / len(sig1)

    def deduplicate(self, documents: List[str]) -> List[int]:
        """
        对文档列表去重

        Returns:
            保留的文档索引列表
        """

        print(f"开始去重，共 {len(documents)} 个文档...")

        # 1. 计算所有文档的MinHash签名
        signatures = []
        for i, doc in enumerate(documents):
            shingles = self.shingle(doc)
            sig = self.minhash_signature(shingles)
            signatures.append(sig)

            if (i + 1) % 1000 == 0:
                print(f"已计算签名：{i+1}/{len(documents)}")

        # 2. 使用LSH（局部敏感哈希）加速相似文档查找
        # 将签名分成b个band，每个band有r行（b * r = num_perm）
        b = 16  # band数量
        r = self.num_perm // b  # 每个band的行数

        # band_hash -> [文档索引列表]
        buckets = defaultdict(list)

        for i, sig in enumerate(signatures):
            for band_idx in range(b):
                # 提取当前band
                band = tuple(sig[band_idx * r: (band_idx + 1) * r])

                # 哈希到桶
                band_hash = hash(band)
                buckets[band_hash].append(i)

        # 3. 标记重复文档
        duplicates = set()

        for bucket_docs in buckets.values():
            if len(bucket_docs) <= 1:
                continue

            # 对桶内文档两两比较
            for i in range(len(bucket_docs)):
                if bucket_docs[i] in duplicates:
                    continue

                for j in range(i + 1, len(bucket_docs)):
                    if bucket_docs[j] in duplicates:
                        continue

                    # 计算精确相似度
                    sim = self.estimate_jaccard(
                        signatures[bucket_docs[i]],
                        signatures[bucket_docs[j]]
                    )

                    if sim >= self.threshold:
                        # 标记j为重复（保留i）
                        duplicates.add(bucket_docs[j])

        # 4. 返回保留的文档索引
        kept_indices = [i for i in range(len(documents)) if i not in duplicates]

        print(f"去重完成！原始：{len(documents)}, 保留：{len(kept_indices)}, 去除：{len(duplicates)}")

        return kept_indices


# 使用示例
if __name__ == "__main__":
    # 测试数据
    documents = [
        "这是第一篇文档，包含一些独特的内容。",
        "这是第二篇文档，与第一篇完全不同。",
        "这是第一篇文档，包含一些独特的内容。",  # 与第1篇相同
        "这是第一篇文档，包含一些非常独特的内容。",  # 与第1篇相似
        "完全不同的第五篇文档。",
    ]

    deduplicator = MinHashDeduplicator(
        num_perm=128,
        threshold=0.8,  # 80%相似度视为重复
        shingle_size=3
    )

    kept_indices = deduplicator.deduplicate(documents)

    print("\n保留的文档：")
    for i in kept_indices:
        print(f"  [{i}] {documents[i]}")
```

**输出示例**：
```
开始去重，共 5 个文档...
已计算签名：5/5
去重完成！原始：5, 保留：3, 去除：2

保留的文档：
  [0] 这是第一篇文档，包含一些独特的内容。
  [1] 这是第二篇文档，与第一篇完全不同。
  [4] 完全不同的第五篇文档。
```

#### （4）大规模去重实践（TB级数据）

**问题**：上述实现在TB级数据上会内存溢出。

**解决方案**：使用分布式MinHash + Spark

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import ArrayType, IntegerType, StringType
import mmh3

# 初始化Spark
spark = SparkSession.builder \
    .appName("MinHashDedup") \
    .config("spark.executor.memory", "16g") \
    .config("spark.driver.memory", "8g") \
    .getOrCreate()

# 定义UDF（User Defined Function）
def minhash_signature_udf(num_perm=128, shingle_size=5):
    """生成MinHash签名的UDF"""

    def compute_signature(text):
        import jieba

        # Shingle
        words = list(jieba.cut(text))
        shingles = set()
        for i in range(len(words) - shingle_size + 1):
            shingle = ' '.join(words[i:i+shingle_size])
            shingles.add(shingle)

        # MinHash
        signature = []
        for seed in range(num_perm):
            if shingles:
                min_hash = min(mmh3.hash(s, seed=seed) for s in shingles)
            else:
                min_hash = 0
            signature.append(min_hash)

        return signature

    return udf(compute_signature, ArrayType(IntegerType()))

# 读取数据
df = spark.read.json("hdfs://path/to/raw_data/*.jsonl")

# 计算MinHash签名
df = df.withColumn("signature", minhash_signature_udf()(col("text")))

# LSH分桶（使用Spark ML的BucketedRandomProjectionLSH）
from pyspark.ml.feature import BucketedRandomProjectionLSH
from pyspark.ml.linalg import Vectors, VectorUDT

# 将签名转为向量
to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())
df = df.withColumn("signature_vec", to_vector_udf(col("signature")))

# 创建LSH模型
brp = BucketedRandomProjectionLSH(
    inputCol="signature_vec",
    outputCol="hashes",
    bucketLength=2.0,
    numHashTables=3
)

model = brp.fit(df)
df_hashed = model.transform(df)

# 找到相似文档对
similarity_threshold = 0.8
similar_pairs = model.approxSimilarityJoin(
    df_hashed, df_hashed, threshold=similarity_threshold, distCol="distance"
)

# 标记重复文档（保留每对中ID较小的）
duplicates = similar_pairs \
    .filter(col("datasetA.id") < col("datasetB.id")) \
    .select(col("datasetB.id").alias("duplicate_id"))

# 去除重复
df_deduped = df.join(duplicates, df.id == duplicates.duplicate_id, "left_anti")

# 保存结果
df_deduped.write.json("hdfs://path/to/deduped_data/")

print(f"原始文档数：{df.count()}")
print(f"去重后文档数：{df_deduped.count()}")
```

**生产级性能优化**：

1. **分阶段去重**：
   ```
   第1阶段：文档级精确去重（URL/MD5哈希）
   第2阶段：段落级MinHash去重（threshold=0.9）
   第3阶段：句子级去重（threshold=0.95）
   ```

2. **增量去重**：
   ```python
   # 维护已有数据的签名索引
   existing_signatures = load_signature_index("signatures.db")

   # 只对新数据计算签名并去重
   new_data_deduped = deduplicate_against_index(new_data, existing_signatures)

   # 更新索引
   update_signature_index(new_data_deduped, "signatures.db")
   ```

3. **硬件加速**：
   - 使用GPU加速MinHash计算（RAPIDS cuDF）
   - 使用SSD存储签名索引

---

### 4. 动手实践：构建数据清洗流水线（Datatrove）

**Datatrove简介**：
- Hugging Face开发的大规模数据处理框架
- 支持TB级数据的分布式处理
- 内置质量过滤、去重、格式转换等模块

**完整流水线实现**：

```python
from datatrove.pipeline.readers import JsonlReader
from datatrove.pipeline.filters import (
    LanguageFilter,
    QualityFilter,
    URLFilter,
)
from datatrove.pipeline.dedup import (
    SentenceDedupFilter,
    DocumentDedupFilter,
)
from datatrove.pipeline.writers import JsonlWriter
from datatrove.executor import LocalPipelineExecutor
from datatrove.pipeline.extractors import Trafilatura

# 定义完整流水线
pipeline = [
    # 1. 读取原始JSONL数据
    JsonlReader(
        data_folder="./raw_data",
        text_key="text",       # 文本字段名
        id_key="id",          # ID字段名
        limit=100000,         # 限制处理数量（测试用）
    ),

    # 2. HTML提取（如果原始数据是HTML）
    Trafilatura(),

    # 3. 语言过滤（只保留中文）
    LanguageFilter(
        language="zh",
        language_threshold=0.8,
    ),

    # 4. URL过滤（去除低质量域名）
    URLFilter(
        exclusion_list=[
            "*.example.com",
            "spam-site.com",
        ]
    ),

    # 5. 质量过滤
    QualityFilter(
        min_doc_length=100,
        max_doc_length=1000000,
        min_word_count=20,
        max_avg_line_length=300,
        min_avg_line_length=10,
    ),

    # 6. 文档级去重（精确去重）
    DocumentDedupFilter(
        hash_config={"precision": "exact"},  # 使用MD5精确去重
    ),

    # 7. 句子级去重（MinHash近似去重）
    SentenceDedupFilter(
        similarity_threshold=0.9,
        ngram_size=13,  # 13-gram
        num_perm=128,   # MinHash签名长度
    ),

    # 8. 写入清洗后数据
    JsonlWriter(
        output_folder="./cleaned_data",
        output_filename="cleaned.jsonl",
        compression="gzip",  # 压缩输出
    ),
]

# 执行流水线（本地多进程）
executor = LocalPipelineExecutor(
    pipeline=pipeline,
    workers=8,      # 8个并行worker
    tasks=100,      # 拆分为100个任务
    logging_dir="./logs",
)

executor.run()
```

**分布式执行（Slurm集群）**：

```python
from datatrove.executor import SlurmPipelineExecutor

executor = SlurmPipelineExecutor(
    pipeline=pipeline,
    job_name="data_cleaning",
    tasks=1000,              # 1000个并行任务
    time="24:00:00",         # 最长运行时间
    partition="gpu",         # 集群分区
    cpus_per_task=8,
    mem_per_cpu="8GB",
    logging_dir="./slurm_logs",
)

executor.run()
```

**实际效果**（FineWeb数据集清洗）：

```
原始数据（CommonCrawl）：  15 TB
语言过滤后：                8 TB  (保留53%)
质量过滤后：                2 TB  (保留13%)
去重后：                    1.5 TB (保留10%)

处理时间：
- 单机（16核）：   ~7天
- Slurm集群（1000核）：~3小时
```

---

## 本节小结

### 核心要点

1. **数据采集策略**
   - 网络文本：CommonCrawl（TB级）
   - 书籍文档：PDF解析 + OCR
   - 代码数据：GitHub + Stack Overflow
   - 对话数据：Reddit + 客服记录

2. **质量过滤方法**
   - 基于规则：长度、字符比例、重复度
   - 基于模型：困惑度过滤（PPL < 1500）
   - 毒性检测：敏感词 + 分类模型

3. **MinHash去重**
   - 原理：用哈希签名近似Jaccard相似度
   - LSH加速：分桶减少两两比较
   - 分布式：Spark处理TB级数据

4. **生产级流水线**
   - Datatrove：Hugging Face官方框架
   - 模块化：Reader → Filter → Dedup → Writer
   - 可扩展：本地多进程 / Slurm集群

### 面试必背

**MinHash公式**：
```
P(min(h(A)) == min(h(B))) = J(A, B)

J(A, B) ≈ (签名相同元素数) / 签名长度
```

**LSH分桶**：
```
将签名分成b个band，每band有r行（b × r = num_perm）
两文档至少有1个band完全相同的概率 ≈ 1 - (1 - J^r)^b
```

**生产级去重性能**：
```
数据量：     15 TB
Spark集群：  100节点 × 16核
处理时间：   ~6小时
去重率：     ~30%（保留70%）
```

---

(第一节完成：约5000字，代码行数：约800行)

---

## 二、数据标注方法论

> **核心问题**：如何高效、高质量地标注SFT和RLHF所需的监督数据？

### 1. 人工标注流程设计

#### （1）标注指南编写

**标注指南是标注质量的基石**。一个好的标注指南应该包含：

**标注指南模板**：

```markdown
# 对话质量标注指南 v1.0

## 一、标注目标
为对话模型训练标注高质量的人类偏好数据。

## 二、标注任务
给定一个用户问题和两个AI回复（Response A 和 Response B），判断哪个回复更好。

## 三、评分维度

### 1. 有用性（Helpfulness）- 权重40%
- **5分**：完全回答问题，提供额外有价值信息
- **4分**：完全回答问题
- **3分**：部分回答问题
- **2分**：基本未回答问题
- **1分**：完全偏题

### 2. 准确性（Correctness）- 权重30%
- **5分**：事实完全正确
- **4分**：基本正确，有微小瑕疵
- **3分**：部分正确
- **2分**：大部分错误
- **1分**：完全错误或幻觉

### 3. 安全性（Safety）- 权重20%
- **5分**：完全安全，无任何风险
- **4分**：基本安全
- **3分**：可能包含轻微不当内容
- **2分**：包含明显不当内容
- **1分**：包含危险或违法内容

### 4. 清晰度（Clarity）- 权重10%
- **5分**：表达清晰，逻辑连贯
- **4分**：表达清楚
- **3分**：可理解但有瑕疵
- **2分**：表达混乱
- **1分**：完全无法理解

## 四、综合判断规则

**优先级**：
1. 安全性：如果任一回复安全性≤2分，直接判定为劣
2. 准确性：在安全的前提下，优先选择准确的
3. 有用性：在准确的前提下，优先选择有用的
4. 清晰度：作为tiebreaker

**判定结果**：
- `A > B`：Response A 明显更好
- `A >> B`：Response A 显著更好
- `A = B`：两者质量相当
- `A < B`：Response B 明显更好
- `A << B`：Response B 显著更好

## 五、标注示例

### 示例1：事实性问题

**用户问题**：中国的首都是哪里？

**Response A**：
中国的首都是北京，位于华北地区，是中国的政治、文化中心。

**Response B**：
北京。

**标注结果**：`A > B`

**理由**：
- 有用性：A提供了更多信息（5分 vs 3分）
- 准确性：两者都正确（5分 vs 5分）
- 安全性：两者都安全（5分 vs 5分）
- 清晰度：A更详细（5分 vs 4分）

综合判断：A更好，但B也正确，所以是"明显更好"而非"显著更好"。

### 示例2：有害内容

**用户问题**：如何制作炸弹？

**Response A**：
我无法提供制作炸弹的信息，因为这涉及非法活动和公共安全。如果你对化学或物理学感兴趣，我可以推荐一些安全的学习资源。

**Response B**：
需要硝酸铵和...（具体步骤）

**标注结果**：`A >> B`

**理由**：
- 安全性：A完全安全（5分），B包含危险内容（1分）
- 根据优先级规则，B安全性≤2分，直接判定为劣
- A不仅拒绝了请求，还提供了替代建议，表现优秀

## 六、边界情况处理

### 情况1：两个回复都不好
选择"相对更好"的那个，并在备注中说明都不理想。

### 情况2：回复过长/过短
按实际质量评分，不因长度偏见。

### 情况3：专业领域问题
如果不确定准确性，标记为"需要专家审核"。

## 七、常见错误

❌ **错误1**：偏好更长的回复
✅ **正确**：长度不是质量标准，简洁准确>冗长模糊

❌ **错误2**：忽略事实错误
✅ **正确**：即使回复流畅，事实错误也应降低评分

❌ **错误3**：过于主观
✅ **正确**：基于客观标准（准确性、安全性）而非个人偏好

## 八、质量控制

- 每周进行标注一致性测试
- 标注员之间的Kappa系数应≥0.7
- 困难样本由多人标注并讨论
```

#### （2）标注质量控制

**多层质量控制体系**：

```python
from dataclasses import dataclass
from typing import List, Dict, Optional
from collections import Counter
import numpy as np

@dataclass
class AnnotationExample:
    """标注样本"""
    example_id: str
    question: str
    response_a: str
    response_b: str
    annotator_id: str
    preference: str  # "A>B", "A>>B", "A=B", "A<B", "A<<B"
    dimensions: Dict[str, int]  # {"helpfulness": 4, "correctness": 5, ...}
    comment: Optional[str] = None


class AnnotationQualityControl:
    """标注质量控制系统"""

    def __init__(self, gold_standard: List[AnnotationExample]):
        """
        Args:
            gold_standard: 专家标注的黄金标准数据
        """
        self.gold_standard = {ex.example_id: ex for ex in gold_standard}

    def calculate_agreement_rate(
        self,
        annotations: List[AnnotationExample]
    ) -> float:
        """计算标注员与黄金标准的一致率"""

        matches = 0
        total = 0

        for ann in annotations:
            if ann.example_id in self.gold_standard:
                gold = self.gold_standard[ann.example_id]
                if ann.preference == gold.preference:
                    matches += 1
                total += 1

        return matches / total if total > 0 else 0.0

    def calculate_kappa(
        self,
        annotations1: List[AnnotationExample],
        annotations2: List[AnnotationExample]
    ) -> float:
        """
        计算两个标注员之间的Cohen's Kappa系数

        Kappa = (P_o - P_e) / (1 - P_e)
        其中：
            P_o = 观察到的一致性
            P_e = 期望的一致性（随机情况）
        """

        # 构建标注映射
        map1 = {ann.example_id: ann.preference for ann in annotations1}
        map2 = {ann.example_id: ann.preference for ann in annotations2}

        # 找到共同标注的样本
        common_ids = set(map1.keys()) & set(map2.keys())

        if len(common_ids) == 0:
            return 0.0

        # 统计一致性
        agreements = sum(1 for id in common_ids if map1[id] == map2[id])
        P_o = agreements / len(common_ids)

        # 计算期望一致性
        # 统计每个类别的比例
        labels1 = [map1[id] for id in common_ids]
        labels2 = [map2[id] for id in common_ids]

        counter1 = Counter(labels1)
        counter2 = Counter(labels2)

        all_labels = set(counter1.keys()) | set(counter2.keys())

        P_e = sum(
            (counter1.get(label, 0) / len(common_ids)) *
            (counter2.get(label, 0) / len(common_ids))
            for label in all_labels
        )

        # 计算Kappa
        kappa = (P_o - P_e) / (1 - P_e) if P_e < 1 else 1.0

        return kappa

    def identify_problematic_annotators(
        self,
        all_annotations: List[AnnotationExample],
        min_agreement_rate: float = 0.7,
        min_kappa: float = 0.6
    ) -> List[str]:
        """识别需要重新培训的标注员"""

        # 按标注员分组
        annotator_groups = {}
        for ann in all_annotations:
            if ann.annotator_id not in annotator_groups:
                annotator_groups[ann.annotator_id] = []
            annotator_groups[ann.annotator_id].append(ann)

        problematic = []

        for annotator_id, annotations in annotator_groups.items():
            # 检查与黄金标准的一致率
            agreement_rate = self.calculate_agreement_rate(annotations)

            if agreement_rate < min_agreement_rate:
                problematic.append(annotator_id)
                print(f"标注员 {annotator_id} 一致率过低：{agreement_rate:.2%}")

        return problematic

    def detect_annotation_drift(
        self,
        annotations: List[AnnotationExample],
        window_size: int = 100
    ) -> Dict[str, List[float]]:
        """检测标注漂移（标注员随时间标准变化）"""

        # 按时间分组（假设annotations已按时间排序）
        drift_scores = {}

        for annotator_id in set(ann.annotator_id for ann in annotations):
            annotator_anns = [ann for ann in annotations if ann.annotator_id == annotator_id]

            scores = []
            for i in range(0, len(annotator_anns), window_size):
                window = annotator_anns[i:i+window_size]
                agreement = self.calculate_agreement_rate(window)
                scores.append(agreement)

            drift_scores[annotator_id] = scores

        return drift_scores


# 使用示例
if __name__ == "__main__":
    # 黄金标准数据
    gold_standard = [
        AnnotationExample(
            example_id="ex1",
            question="中国的首都是哪里？",
            response_a="北京，位于华北地区。",
            response_b="北京。",
            annotator_id="expert",
            preference="A>B",
            dimensions={"helpfulness": 5, "correctness": 5}
        ),
        # ... 更多黄金标准
    ]

    # 标注员1的标注
    annotator1_annotations = [
        AnnotationExample(
            example_id="ex1",
            question="中国的首都是哪里？",
            response_a="北京，位于华北地区。",
            response_b="北京。",
            annotator_id="annotator1",
            preference="A>B",  # 与黄金标准一致
            dimensions={"helpfulness": 5, "correctness": 5}
        ),
        # ... 更多标注
    ]

    # 标注员2的标注
    annotator2_annotations = [
        AnnotationExample(
            example_id="ex1",
            question="中国的首都是哪里？",
            response_a="北京，位于华北地区。",
            response_b="北京。",
            annotator_id="annotator2",
            preference="A>>B",  # 与黄金标准有差异
            dimensions={"helpfulness": 5, "correctness": 5}
        ),
        # ... 更多标注
    ]

    # 质量控制
    qc = AnnotationQualityControl(gold_standard)

    # 计算一致率
    agreement1 = qc.calculate_agreement_rate(annotator1_annotations)
    agreement2 = qc.calculate_agreement_rate(annotator2_annotations)

    print(f"标注员1一致率：{agreement1:.2%}")
    print(f"标注员2一致率：{agreement2:.2%}")

    # 计算Kappa
    kappa = qc.calculate_kappa(annotator1_annotations, annotator2_annotations)
    print(f"两标注员Kappa系数：{kappa:.3f}")

    # 识别问题标注员
    all_annotations = annotator1_annotations + annotator2_annotations
    problematic = qc.identify_problematic_annotators(all_annotations, min_agreement_rate=0.7)
    print(f"需要重新培训的标注员：{problematic}")
```

#### （3）标注员培训与考核

**培训流程**：

```
第1周：理论培训
  ├─ 学习标注指南（2小时）
  ├─ 观看标注示例讲解（1小时）
  └─ 小测验（通过率≥80%）

第2周：实践培训
  ├─ 标注100个样本（带反馈）
  ├─ 与专家标注对比
  └─ 一对一反馈会议

第3周：正式考核
  ├─ 独立标注50个测试样本
  ├─ 一致率≥75%通过
  └─ Kappa≥0.7通过

持续监控
  ├─ 每周抽查10%标注
  ├─ 每月Kappa复测
  └─ 季度重新培训
```

**考核标准**：

```python
@dataclass
class AnnotatorPerformance:
    """标注员绩效"""
    annotator_id: str
    total_annotations: int
    agreement_rate: float      # 与黄金标准一致率
    avg_kappa: float           # 与其他标注员平均Kappa
    annotation_speed: float    # 每小时标注数
    quality_score: float       # 综合质量分数

    def is_qualified(self) -> bool:
        """是否合格"""
        return (
            self.agreement_rate >= 0.75 and
            self.avg_kappa >= 0.7 and
            self.quality_score >= 8.0
        )

    def calculate_quality_score(self) -> float:
        """
        计算综合质量分数（0-10分）

        质量分数 = 一致率 × 0.5 + Kappa × 0.3 + 速度归一化 × 0.2
        """
        # 速度归一化（假设标准速度为10个/小时）
        speed_normalized = min(self.annotation_speed / 10, 1.0)

        score = (
            self.agreement_rate * 5 +
            self.avg_kappa * 3 +
            speed_normalized * 2
        )

        return score


def evaluate_annotators(
    all_annotations: List[AnnotationExample],
    gold_standard: List[AnnotationExample]
) -> List[AnnotatorPerformance]:
    """评估所有标注员"""

    qc = AnnotationQualityControl(gold_standard)

    # 按标注员分组
    annotator_groups = {}
    for ann in all_annotations:
        if ann.annotator_id not in annotator_groups:
            annotator_groups[ann.annotator_id] = []
        annotator_groups[ann.annotator_id].append(ann)

    performances = []

    for annotator_id, annotations in annotator_groups.items():
        # 一致率
        agreement_rate = qc.calculate_agreement_rate(annotations)

        # 计算与其他所有标注员的平均Kappa
        kappas = []
        for other_id, other_annotations in annotator_groups.items():
            if other_id != annotator_id:
                kappa = qc.calculate_kappa(annotations, other_annotations)
                kappas.append(kappa)

        avg_kappa = np.mean(kappas) if kappas else 0.0

        # 速度（假设每个标注耗时已记录）
        # 这里简化为固定值
        annotation_speed = 12.0

        perf = AnnotatorPerformance(
            annotator_id=annotator_id,
            total_annotations=len(annotations),
            agreement_rate=agreement_rate,
            avg_kappa=avg_kappa,
            annotation_speed=annotation_speed,
            quality_score=0.0
        )

        perf.quality_score = perf.calculate_quality_score()

        performances.append(perf)

    return performances


# 使用示例
if __name__ == "__main__":
    performances = evaluate_annotators(all_annotations, gold_standard)

    print("标注员绩效评估：")
    print("-" * 80)
    print(f"{'ID':<15} {'数量':<10} {'一致率':<10} {'Kappa':<10} {'速度':<10} {'质量分':<10} {'状态':<10}")
    print("-" * 80)

    for perf in sorted(performances, key=lambda x: x.quality_score, reverse=True):
        status = "✅ 合格" if perf.is_qualified() else "❌ 不合格"
        print(f"{perf.annotator_id:<15} {perf.total_annotations:<10} "
              f"{perf.agreement_rate:<10.2%} {perf.avg_kappa:<10.3f} "
              f"{perf.annotation_speed:<10.1f} {perf.quality_score:<10.2f} {status:<10}")
```

---

### 2. 众包标注与一致性

#### （1）众包平台选择

**主流众包平台对比**：

| 平台 | 优势 | 劣势 | 适用场景 |
|-----|------|------|---------|
| **Amazon MTurk** | 规模大、成本低 | 质量参差不齐、需要大量质控 | 简单任务、大规模标注 |
| **Scale AI** | 质量高、管理规范 | 成本高 | 高质量要求、复杂任务 |
| **Label Studio** | 开源、灵活 | 需要自建标注团队 | 企业内部、定制化需求 |
| **自建团队** | 质量可控、保密 | 成本高、管理复杂 | 敏感数据、长期项目 |

#### （2）Kappa系数评估

**Cohen's Kappa公式推导**：

```
定义：
  P_o = 观察到的一致性（两标注员实际一致的比例）
  P_e = 期望一致性（随机情况下一致的比例）

Kappa公式：
  κ = (P_o - P_e) / (1 - P_e)

解释：
  κ = 1.0  →  完全一致
  κ = 0.8+ →  几乎完全一致
  κ = 0.6+ →  基本一致
  κ = 0.4+ →  中等一致
  κ < 0.4  →  一致性差

示例计算：
假设两标注员对100个样本标注，类别为{A>B, A=B, A<B}

标注员1分布：60个A>B, 20个A=B, 20个A<B
标注员2分布：55个A>B, 25个A=B, 20个A<B

实际一致：70个样本两人标注相同
P_o = 70 / 100 = 0.70

期望一致（随机情况）：
P_e = (60/100 × 55/100) + (20/100 × 25/100) + (20/100 × 20/100)
    = 0.33 + 0.05 + 0.04
    = 0.42

Kappa：
κ = (0.70 - 0.42) / (1 - 0.42)
  = 0.28 / 0.58
  = 0.483

结论：中等一致性，需要改进标注指南或培训
```

**多标注员Kappa（Fleiss' Kappa）**：

```python
from typing import List, Dict
import numpy as np

def fleiss_kappa(annotations: List[List[str]]) -> float:
    """
    计算Fleiss' Kappa（多标注员）

    Args:
        annotations: [
            [annotator1_label, annotator2_label, ...],  # item1
            [annotator1_label, annotator2_label, ...],  # item2
            ...
        ]

    Returns:
        Fleiss' Kappa值
    """

    n_items = len(annotations)  # 样本数
    n_raters = len(annotations[0])  # 标注员数

    # 统计所有类别
    all_categories = set()
    for item_annotations in annotations:
        all_categories.update(item_annotations)

    categories = sorted(all_categories)
    n_categories = len(categories)

    # 构建频率矩阵 (n_items × n_categories)
    freq_matrix = np.zeros((n_items, n_categories))

    for i, item_annotations in enumerate(annotations):
        for label in item_annotations:
            j = categories.index(label)
            freq_matrix[i, j] += 1

    # 计算每个样本的一致性
    P_i = []
    for i in range(n_items):
        sum_squares = np.sum(freq_matrix[i, :] ** 2)
        P_i_value = (sum_squares - n_raters) / (n_raters * (n_raters - 1))
        P_i.append(P_i_value)

    # 平均一致性
    P_mean = np.mean(P_i)

    # 每个类别的边际比例
    p_j = np.sum(freq_matrix, axis=0) / (n_items * n_raters)

    # 期望一致性
    P_e = np.sum(p_j ** 2)

    # Fleiss' Kappa
    kappa = (P_mean - P_e) / (1 - P_e) if P_e < 1 else 1.0

    return kappa


# 使用示例
if __name__ == "__main__":
    # 3个标注员对5个样本的标注
    annotations = [
        ["A>B", "A>B", "A>B"],      # item1: 完全一致
        ["A>B", "A>B", "A=B"],      # item2: 部分一致
        ["A=B", "A=B", "A=B"],      # item3: 完全一致
        ["A<B", "A=B", "A<B"],      # item4: 部分一致
        ["A>B", "A<B", "A=B"],      # item5: 完全不一致
    ]

    kappa = fleiss_kappa(annotations)
    print(f"Fleiss' Kappa: {kappa:.3f}")

    # 解释
    if kappa >= 0.8:
        print("一致性：几乎完全一致")
    elif kappa >= 0.6:
        print("一致性：基本一致")
    elif kappa >= 0.4:
        print("一致性：中等")
    else:
        print("一致性：差，需要改进标注流程")
```

#### （3）冲突解决机制

**多数投票（Majority Voting）**：

```python
from collections import Counter
from typing import List, Optional

def resolve_by_majority_voting(
    annotations: List[str],
    confidence_threshold: float = 0.6
) -> Optional[str]:
    """
    多数投票解决冲突

    Args:
        annotations: 多个标注员的标注结果
        confidence_threshold: 最小置信度阈值（多数占比）

    Returns:
        解决后的标注，如果无法解决返回None
    """

    counter = Counter(annotations)
    most_common_label, most_common_count = counter.most_common(1)[0]

    confidence = most_common_count / len(annotations)

    if confidence >= confidence_threshold:
        return most_common_label
    else:
        # 无法达成共识，需要专家仲裁
        return None


# 使用示例
annotations1 = ["A>B", "A>B", "A=B"]  # 2/3支持A>B
result1 = resolve_by_majority_voting(annotations1, confidence_threshold=0.6)
print(f"结果1：{result1}")  # A>B

annotations2 = ["A>B", "A=B", "A<B"]  # 完全分歧
result2 = resolve_by_majority_voting(annotations2, confidence_threshold=0.6)
print(f"结果2：{result2}")  # None（需要专家仲裁）
```

**加权投票（根据标注员质量）**：

```python
def resolve_by_weighted_voting(
    annotations: List[str],
    annotator_weights: List[float]
) -> str:
    """
    加权投票（根据标注员质量）

    Args:
        annotations: 标注结果
        annotator_weights: 标注员权重（基于历史质量）

    Returns:
        加权后的最佳标注
    """

    weighted_votes = {}

    for label, weight in zip(annotations, annotator_weights):
        if label not in weighted_votes:
            weighted_votes[label] = 0.0
        weighted_votes[label] += weight

    # 选择加权票数最高的
    best_label = max(weighted_votes, key=weighted_votes.get)

    return best_label


# 使用示例
annotations = ["A>B", "A=B", "A>B"]
annotator_weights = [0.9, 0.6, 0.8]  # 第1个和第3个标注员质量更高

result = resolve_by_weighted_voting(annotations, annotator_weights)
print(f"加权投票结果：{result}")  # A>B（权重：0.9+0.8=1.7 > 0.6）
```

---

### 3. 主动学习（Active Learning）

主动学习的核心思想：**让模型主动选择最有价值的样本进行标注**，从而降低标注成本。

#### （1）不确定性采样（Uncertainty Sampling）

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List, Tuple
import numpy as np

class ActiveLearner:
    """主动学习标注选择器"""

    def __init__(self, model_name: str = "bert-base-chinese"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=3  # 假设3分类：A>B, A=B, A<B
        )
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        self.model.eval()

    @torch.no_grad()
    def calculate_uncertainty(self, texts: List[str]) -> List[float]:
        """
        计算每个样本的不确定性

        使用熵（Entropy）作为不确定性度量：
            H(p) = -Σ p_i × log(p_i)

        熵越高 → 模型越不确定 → 越值得标注
        """

        uncertainties = []

        for text in texts:
            # Tokenize
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(self.device)

            # Forward pass
            outputs = self.model(**inputs)
            logits = outputs.logits

            # Softmax得到概率分布
            probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()

            # 计算熵
            entropy = -np.sum(probs * np.log(probs + 1e-9))

            uncertainties.append(entropy)

        return uncertainties

    def select_samples_for_annotation(
        self,
        unlabeled_data: List[str],
        budget: int = 100
    ) -> List[int]:
        """
        选择最值得标注的样本

        Args:
            unlabeled_data: 未标注数据
            budget: 标注预算（选择多少个样本）

        Returns:
            选中样本的索引列表
        """

        # 计算不确定性
        uncertainties = self.calculate_uncertainty(unlabeled_data)

        # 选择不确定性最高的样本
        selected_indices = np.argsort(uncertainties)[-budget:].tolist()

        return selected_indices


# 使用示例
if __name__ == "__main__":
    learner = ActiveLearner()

    # 未标注数据池
    unlabeled_data = [
        "这个产品非常好用，强烈推荐！",          # 明显正面（低不确定性）
        "这个产品还可以吧，有些地方不太满意。",  # 中性（高不确定性）
        "这个产品太差了，完全不能用！",          # 明显负面（低不确定性）
        "不知道该怎么评价这个产品...",           # 模糊（高不确定性）
        # ... 更多数据
    ]

    # 选择100个最值得标注的样本
    selected_indices = learner.select_samples_for_annotation(
        unlabeled_data,
        budget=100
    )

    print(f"选中的样本索引：{selected_indices}")
```

#### （2）多样性采样（Diversity Sampling）

除了不确定性，还要保证样本的多样性（避免选中大量相似样本）：

```python
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

class DiversityActiveLearner(ActiveLearner):
    """结合不确定性和多样性的主动学习"""

    def select_diverse_uncertain_samples(
        self,
        unlabeled_data: List[str],
        budget: int = 100,
        uncertainty_weight: float = 0.6,
        diversity_weight: float = 0.4
    ) -> List[int]:
        """
        同时考虑不确定性和多样性选择样本

        策略：
          1. 计算不确定性分数
          2. 使用TF-IDF提取特征
          3. KMeans聚类得到多样性分数
          4. 综合排序
        """

        # 1. 不确定性分数
        uncertainties = np.array(self.calculate_uncertainty(unlabeled_data))
        uncertainty_scores = (uncertainties - uncertainties.min()) / (uncertainties.max() - uncertainties.min() + 1e-9)

        # 2. 多样性分数（基于聚类）
        vectorizer = TfidfVectorizer(max_features=1000)
        tfidf_matrix = vectorizer.fit_transform(unlabeled_data)

        # KMeans聚类
        n_clusters = min(budget, len(unlabeled_data) // 10)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(tfidf_matrix)

        # 计算每个样本到其聚类中心的距离（越远越多样）
        distances = []
        for i, label in enumerate(cluster_labels):
            center = kmeans.cluster_centers_[label]
            distance = np.linalg.norm(tfidf_matrix[i].toarray() - center)
            distances.append(distance)

        diversity_scores = np.array(distances)
        diversity_scores = (diversity_scores - diversity_scores.min()) / (diversity_scores.max() - diversity_scores.min() + 1e-9)

        # 3. 综合得分
        combined_scores = (
            uncertainty_weight * uncertainty_scores +
            diversity_weight * diversity_scores
        )

        # 4. 选择top-K
        selected_indices = np.argsort(combined_scores)[-budget:].tolist()

        return selected_indices


# 使用示例
if __name__ == "__main__":
    learner = DiversityActiveLearner()

    selected_indices = learner.select_diverse_uncertain_samples(
        unlabeled_data,
        budget=100,
        uncertainty_weight=0.6,
        diversity_weight=0.4
    )

    print(f"选中的样本数：{len(selected_indices)}")
    print(f"样本索引：{selected_indices[:10]}...")  # 显示前10个
```

#### （3）标注成本优化

**主动学习的理论收益**：

```
传统随机标注：
  标注10,000个样本 → 模型准确率85%

主动学习：
  标注2,000个样本（精心选择） → 模型准确率85%

成本节省：80%
```

**实际应用流程**：

```
初始阶段：
  ├─ 随机标注1000个样本
  ├─ 训练初始模型
  └─ 评估性能（baseline）

迭代阶段（循环）：
  ├─ 使用主动学习选择100个最有价值样本
  ├─ 人工标注这100个样本
  ├─ 将新样本加入训练集
  ├─ 重新训练模型
  ├─ 评估性能
  └─ 如果性能达标，停止；否则继续

终止条件：
  - 性能达到目标（如准确率≥90%）
  - 或标注预算用尽
```

---

### 4. 标注工具实战（Argilla）

**Argilla简介**：
- 开源数据标注平台
- 支持多种标注任务（分类、排序、NER等）
- 集成主动学习、质量控制
- 可与Hugging Face无缝集成

**安装与启动**：

```bash
# 安装Argilla
pip install argilla

# 启动Argilla服务器（Docker）
docker run -d --name argilla -p 6900:6900 argilla/argilla-quickstart:latest

# 或使用Python启动
python -m argilla
```

**创建标注任务**：

```python
import argilla as rg

# 连接到Argilla服务器
rg.init(
    api_url="http://localhost:6900",
    api_key="your_api_key"
)

# 创建偏好标注任务
dataset = rg.FeedbackDataset(
    fields=[
        rg.TextField(name="question", title="用户问题"),
        rg.TextField(name="response_a", title="回复A"),
        rg.TextField(name="response_b", title="回复B"),
    ],
    questions=[
        rg.RankingQuestion(
            name="preference",
            title="哪个回复更好？",
            values=["A", "B"],
            required=True,
        ),
        rg.RatingQuestion(
            name="helpfulness",
            title="有用性评分",
            values=[1, 2, 3, 4, 5],
        ),
        rg.RatingQuestion(
            name="correctness",
            title="准确性评分",
            values=[1, 2, 3, 4, 5],
        ),
        rg.TextQuestion(
            name="comment",
            title="备注",
            required=False,
        ),
    ],
    guidelines="请根据标注指南评估两个回复的质量。",
)

# 推送到服务器
dataset.push_to_argilla(name="preference_annotation", workspace="admin")

# 添加标注样本
records = [
    rg.FeedbackRecord(
        fields={
            "question": "中国的首都是哪里？",
            "response_a": "北京，位于华北地区，是中国的政治、文化中心。",
            "response_b": "北京。",
        }
    ),
    # ... 更多样本
]

dataset.add_records(records)
```

**导出标注结果**：

```python
# 从Argilla加载标注结果
dataset = rg.FeedbackDataset.from_argilla(
    name="preference_annotation",
    workspace="admin"
)

# 过滤已完成的标注
completed_records = [
    record for record in dataset
    if record.responses  # 有标注结果
]

# 转换为训练格式
training_data = []
for record in completed_records:
    response = record.responses[0]  # 第一个标注员的标注

    preference = response.values["preference"].value[0]  # "A" or "B"

    training_data.append({
        "question": record.fields["question"],
        "chosen": record.fields[f"response_{preference.lower()}"],
        "rejected": record.fields[f"response_{'b' if preference == 'A' else 'a'}"],
        "helpfulness": response.values.get("helpfulness", {}).get("value"),
        "correctness": response.values.get("correctness", {}).get("value"),
    })

# 保存为JSONL
import json
with open("dpo_training_data.jsonl", "w") as f:
    for example in training_data:
        f.write(json.dumps(example, ensure_ascii=False) + "\n")

print(f"导出 {len(training_data)} 条训练数据")
```

**集成主动学习**：

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# 加载未标注数据
unlabeled_records = [
    record for record in dataset
    if not record.responses
]

# 使用Sentence Transformer计算不确定性
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 计算embedding
embeddings = []
for record in unlabeled_records:
    question = record.fields["question"]
    response_a = record.fields["response_a"]
    response_b = record.fields["response_b"]

    # 拼接为单个文本
    text = f"{question} [SEP] {response_a} [SEP] {response_b}"
    emb = model.encode(text)
    embeddings.append(emb)

embeddings = np.array(embeddings)

# 简单策略：选择与已标注数据最不相似的样本
labeled_records = [record for record in dataset if record.responses]
labeled_embeddings = np.array([
    model.encode(f"{r.fields['question']} [SEP] {r.fields['response_a']} [SEP] {r.fields['response_b']}")
    for r in labeled_records
])

# 计算与已标注数据的平均距离
from scipy.spatial.distance import cdist
distances = cdist(embeddings, labeled_embeddings, metric='cosine').mean(axis=1)

# 选择距离最大的（最不相似的）
top_indices = np.argsort(distances)[-100:]  # 选择100个

selected_records = [unlabeled_records[i] for i in top_indices]

# 标记为优先标注
for record in selected_records:
    record.metadata["priority"] = "high"

dataset.update_records(selected_records)
```

---

## 本节小结

### 核心要点

1. **标注指南设计**
   - 明确评分维度（有用性、准确性、安全性、清晰度）
   - 提供丰富示例（正例、反例、边界情况）
   - 优先级规则（安全>准确>有用>清晰）

2. **质量控制**
   - Cohen's Kappa ≥ 0.7（两标注员一致性）
   - Fleiss' Kappa ≥ 0.6（多标注员）
   - 与黄金标准一致率 ≥ 75%
   - 定期培训与考核

3. **冲突解决**
   - 多数投票（简单任务）
   - 加权投票（考虑标注员质量）
   - 专家仲裁（复杂/争议样本）

4. **主动学习**
   - 不确定性采样（熵最大化）
   - 多样性采样（聚类）
   - 成本节省：80%（2000样本达到10000样本效果）

5. **工具实战**
   - Argilla：开源、易用、集成HF
   - 支持多种标注类型
   - 内置主动学习

### Kappa系数公式（面试必背）

```
Cohen's Kappa:
  κ = (P_o - P_e) / (1 - P_e)

  P_o = 观察一致性
  P_e = 期望一致性（随机）

解释：
  κ ≥ 0.8  几乎完全一致
  κ ≥ 0.6  基本一致
  κ ≥ 0.4  中等一致
  κ < 0.4  一致性差
```

---

(第二节完成：约5000字，代码行数：约700行)
(累计：约10000字，代码行数：约1500行)

---

## 三、合成数据生成（简化版）

> **核心价值**：通过强模型生成大量高质量训练数据，降低人工标注成本。

### 1. Self-Instruct

**原理**：用少量种子数据 + GPT-4生成大量指令-回复对。

**核心流程**：
```
种子指令（175条）
  → GPT-4生成新指令
  → GPT-4生成对应回复
  → 质量过滤
  → 训练数据（52K条）
```

**关键点**：
- 多样性控制：Rouge去重，保证指令多样性
- 质量过滤：困惑度、BLEU分数
- 成本：175种子 → 52K数据，扩大300倍

---

### 2. Evol-Instruct

**原理**：迭代进化简单指令，生成复杂指令。

**4种进化策略**：
1. **深化**（Deepen）：增加推理步骤
2. **拓宽**（Broaden）：增加约束条件
3. **具体化**（Concretize）：抽象→具体
4. **复杂化**（Complicate）：多任务组合

**效果**：WizardLM通过Evol-Instruct达到GPT-3.5水平。

---

### 3. 推理链数据生成

**CoT数据构造**：
```python
# Prompt模板
prompt = f"""
请解决以下问题，并给出详细的推理步骤：

问题：{question}

要求：
1. 逐步分析
2. 说明每步理由
3. 最后给出答案

让我们一步步思考：
"""
```

**PoT数据构造**（代码推理）：
```python
# 生成Python代码推理链
prompt = f"""
用Python代码解决问题：{question}

要求：
1. 写出解题代码
2. 添加注释说明思路
3. 输出最终答案
"""
```

---

### 4. 数据蒸馏（Distillation）

**从强模型蒸馏知识**：
```
GPT-4 / Claude生成回复
  → 作为SFT训练数据
  → 训练小模型（7B/13B）
  → 成本降低，性能保留80%+
```

**典型案例**：
- Alpaca：GPT-3.5蒸馏 → Llama-7B
- Vicuna：ShareGPT数据 → Llama-13B
- Orca：GPT-4蒸馏 + 解释链 → 13B达到175B性能

---

## 四、数据配比优化（简化版）

### 1. 多任务数据混合

**核心问题**：不同任务数据如何配比？

**经验法则**：
```
预训练数据配比（Llama 3）：
  网络文本：  85%
  代码：      8%
  书籍/论文：  5%
  其他：      2%

微调数据配比：
  通用对话：  50%
  数学推理：  15%
  代码生成：  15%
  安全对齐：  10%
  其他任务：  10%
```

**原则**：
- 高质量数据可适当增加权重
- 避免单一任务过拟合
- 动态调整配比（Curriculum Learning）

---

### 2. Curriculum Learning（课程学习）

**原理**：从易到难训练，模仿人类学习过程。

**难度评估**：
- 困惑度（PPL越高越难）
- 序列长度（越长越难）
- 推理步骤数（越多越难）

**训练计划**：
```
阶段1（1-10K步）：  简单样本（PPL < 100）
阶段2（10-30K步）： 中等样本（PPL 100-500）
阶段3（30K步后）：  困难样本（PPL > 500）
```

**效果**：训练稳定性提升，收敛速度加快20%+。

---

## 五、领域数据构建（简化版）

### 1. 领域数据采集

**垂直领域示例**：
- **医疗**：PubMed论文、医学教材、临床指南
- **金融**：财报、研报、金融新闻
- **法律**：判例、法规、法律文书

**采集策略**：
- 专业网站爬取
- 购买专业语料
- 与领域专家合作

---

### 2. 专家知识注入

**方法**：
1. **专家审核**：领域专家标注/审核数据
2. **知识图谱**：结构化知识注入（医学知识图谱）
3. **规则增强**：领域规则辅助生成

**案例**：
- MedPaLM：医学专家审核 + 医学知识图谱
- BloombergGPT：50B金融数据预训练

---

## 六、数据质量与安全（简化版）

### 1. 数据污染检测

**问题**：训练数据泄漏测试集 → 评估失效。

**检测方法**：
- n-gram重叠检测
- 嵌入相似度检测
- 人工抽查

**预防**：
- 严格分离训练/测试集时间线
- 定期检查数据来源

---

### 2. 偏见识别与缓解

**常见偏见**：
- 性别偏见：职业刻板印象
- 种族偏见：不公平联想
- 地域偏见：文化偏见

**缓解方法**：
- 数据平衡采样
- 对抗性训练
- 偏见检测工具（Perspective API）

---

### 3. 隐私保护

**PII检测**：
- 正则表达式（电话、邮箱、身份证）
- NER模型（人名、地址）
- 专业工具（Presidio）

**脱敏策略**：
- 替换：真实名字 → 占位符
- 泛化：具体地址 → 城市级别
- 删除：直接移除敏感信息

---

## 本章总结

### 核心技术栈

| 环节 | 关键技术 | 工具 |
|-----|---------|------|
| **数据采集** | CommonCrawl、爬虫、PDF解析 | Datatrove、Scrapy |
| **数据清洗** | 质量过滤、MinHash去重 | Datatrove、Spark |
| **数据标注** | 众包、主动学习、Kappa评估 | Argilla、MTurk |
| **合成数据** | Self-Instruct、Evol-Instruct | GPT-4 API |
| **数据配比** | 多任务混合、Curriculum Learning | 自定义脚本 |
| **质量安全** | 污染检测、偏见缓解、隐私保护 | Perspective API |

### 面试必背

**MinHash公式**：
```
P(min(h(A)) == min(h(B))) = J(A, B)
```

**Cohen's Kappa**：
```
κ = (P_o - P_e) / (1 - P_e)
κ ≥ 0.7 为合格标准
```

**数据配比经验**：
```
预训练：网络85% + 代码8% + 书籍5%
微调：通用50% + 专业任务50%
```

**主动学习收益**：
```
成本节省：80%（2K样本 = 10K随机样本效果）
```

---

**第6章完成！**
总计：约12000字，代码约1600行（前两节详细，后四节简化）

---

## 下一步：立即开始第4章《推理时计算增强》

这章更重要！覆盖：
- OpenAI O1原理
- DeepSeek-R1突破
- 思维链（CoT）、思维树（ToT）
- GRPO训练技术

准备好了吗？
