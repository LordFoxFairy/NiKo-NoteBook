# ç¬¬1ç« ï¼šè¯­ä¹‰ç†è§£åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»ä¸èšç±» (Text Classification & Clustering)

> "The best classifier is the one you understand." - Andrew Ng
>
> å³ä½¿åœ¨ç”Ÿæˆå¼ AI æ—¶ä»£ï¼Œç»å…¸çš„åˆ†ç±»å’Œèšç±»ä¾ç„¶æ˜¯ä¸šåŠ¡ç³»ç»Ÿçš„åŸºçŸ³ã€‚æœ¬ç« å°†ä»é›¶æ ·æœ¬åˆ°å¾®è°ƒï¼Œä»K-Meansåˆ°BERTopicï¼Œæ„å»ºå®Œæ•´çš„è¯­ä¹‰ç†è§£å·¥ç¨‹ä½“ç³»ã€‚

**éš¾åº¦çº§åˆ«**ï¼šâ­â­ï¼ˆåº”ç”¨å®æˆ˜ï¼‰

---

## ç›®å½•
- [ä¸€ã€åˆ¤åˆ«å¼ AI vs. ç”Ÿæˆå¼ AI](#ä¸€åˆ¤åˆ«å¼-ai-vs-ç”Ÿæˆå¼-ai)
  - [1. æ ¸å¿ƒå·®å¼‚](#1-æ ¸å¿ƒå·®å¼‚)
  - [2. ä¸ºä»€ä¹ˆåˆ¤åˆ«å¼æ¨¡å‹ä»ç„¶é‡è¦ï¼Ÿ](#2-ä¸ºä»€ä¹ˆåˆ¤åˆ«å¼æ¨¡å‹ä»ç„¶é‡è¦)
- [äºŒã€Embedding çš„æœ¬è´¨ä¸å¯è§†åŒ–](#äºŒembedding-çš„æœ¬è´¨ä¸å¯è§†åŒ–)
  - [1. ä»€ä¹ˆæ˜¯ Embeddingï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-embedding)
  - [2. æµè¡Œçš„ Embedding æ¨¡å‹ï¼ˆ2025ç‰ˆï¼‰](#2-æµè¡Œçš„-embedding-æ¨¡å‹2025ç‰ˆ)
  - [3. Embedding å¯è§†åŒ–ï¼št-SNE ä¸ UMAP](#3-embedding-å¯è§†åŒ–tsne-ä¸-umap)
  - [4. å®Œæ•´ä»£ç ï¼šEmbedding å¯è§†åŒ–å®æˆ˜](#4-å®Œæ•´ä»£ç embedding-å¯è§†åŒ–å®æˆ˜)
- [ä¸‰ã€é›¶æ ·æœ¬åˆ†ç±» (Zero-Shot Classification)](#ä¸‰é›¶æ ·æœ¬åˆ†ç±»-zero-shot-classification)
  - [1. åŸºäº NLI çš„é›¶æ ·æœ¬åˆ†ç±»](#1-åŸºäº-nli-çš„é›¶æ ·æœ¬åˆ†ç±»)
  - [2. è‹±æ–‡é›¶æ ·æœ¬åˆ†ç±»å®ç°](#2-è‹±æ–‡é›¶æ ·æœ¬åˆ†ç±»å®ç°)
  - [3. ä¸­æ–‡é›¶æ ·æœ¬åˆ†ç±»ï¼ˆåŸºäº Sentence-BERTï¼‰](#3-ä¸­æ–‡é›¶æ ·æœ¬åˆ†ç±»åŸºäº-sentence-bert)
  - [4. 2025å¹´æœ€ä½³å®è·µï¼šä½¿ç”¨ LLM API](#4-2025å¹´æœ€ä½³å®è·µä½¿ç”¨-llm-api)
  - [5. é«˜çº§ç­–ç•¥ï¼šEmbedding ç²—ç­› + LLM ç²¾æ’](#5-é«˜çº§ç­–ç•¥embedding-ç²—ç­›--llm-ç²¾æ’)
- [å››ã€å°‘æ ·æœ¬åˆ†ç±» (Few-Shot Classification)](#å››å°‘æ ·æœ¬åˆ†ç±»-few-shot-classification)
  - [1. SetFit æ¡†æ¶æ·±åº¦è§£æ](#1-setfit-æ¡†æ¶æ·±åº¦è§£æ)
  - [2. å®Œæ•´ä»£ç å®ç°](#2-å®Œæ•´ä»£ç å®ç°)
  - [3. Few-Shot Promptï¼ˆä½¿ç”¨ LLMï¼‰](#3-few-shot-promptä½¿ç”¨-llm)
- [äº”ã€å…¨é‡å¾®è°ƒ vs PEFT (LoRA) åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å¯¹æ¯”](#äº”å…¨é‡å¾®è°ƒ-vs-peft-lora-åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å¯¹æ¯”)
  - [1. å…¨é‡å¾®è°ƒ (Full Fine-Tuning)](#1-å…¨é‡å¾®è°ƒ-full-fine-tuning)
  - [2. PEFT: LoRA (Low-Rank Adaptation)](#2-peft-lora-low-rank-adaptation)
  - [3. å®Œæ•´å¯¹æ¯”ä»£ç ](#3-å®Œæ•´å¯¹æ¯”ä»£ç )
  - [4. å¯¹æ¯”æ€»ç»“](#4-å¯¹æ¯”æ€»ç»“)
- [å…­ã€æ–‡æœ¬èšç±»ç®—æ³•](#å…­æ–‡æœ¬èšç±»ç®—æ³•)
  - [1. K-Means èšç±»](#1-k-means-èšç±»)
  - [2. DBSCAN (å¯†åº¦èšç±»)](#2-dbscan-å¯†åº¦èšç±»)
  - [3. HDBSCAN (å±‚æ¬¡åŒ– DBSCAN)](#3-hdbscan-å±‚æ¬¡åŒ–-dbscan)
  - [4. ç®—æ³•å¯¹æ¯”](#4-ç®—æ³•å¯¹æ¯”)
- [ä¸ƒã€BERTopic åŸç†ä¸å®æˆ˜](#ä¸ƒbertopic-åŸç†ä¸å®æˆ˜)
  - [1. BERTopic æµç¨‹](#1-bertopic-æµç¨‹)
  - [2. å®Œæ•´ä»£ç å®æˆ˜](#2-å®Œæ•´ä»£ç å®æˆ˜)
  - [3. ä¸­æ–‡ BERTopic](#3-ä¸­æ–‡-bertopic)
  - [4. åŠ¨æ€ä¸»é¢˜å»ºæ¨¡ï¼ˆæ—¶é—´ç»´åº¦ï¼‰](#4-åŠ¨æ€ä¸»é¢˜å»ºæ¨¡æ—¶é—´ç»´åº¦)
- [å…«ã€å±‚æ¬¡åŒ–èšç±»å®æˆ˜](#å…«å±‚æ¬¡åŒ–èšç±»å®æˆ˜)
  - [1. å‡èšå±‚æ¬¡èšç±» (Agglomerative)](#1-å‡èšå±‚æ¬¡èšç±»-agglomerative)
  - [2. å®Œæ•´ä»£ç ](#2-å®Œæ•´ä»£ç )
  - [3. å±‚æ¬¡åŒ–ä¸»é¢˜å»ºæ¨¡](#3-å±‚æ¬¡åŒ–ä¸»é¢˜å»ºæ¨¡)
- [ä¹ã€åˆ†ç±»ä¸èšç±»è¯„ä¼°æŒ‡æ ‡](#ä¹åˆ†ç±»ä¸èšç±»è¯„ä¼°æŒ‡æ ‡)
  - [1. åˆ†ç±»è¯„ä¼°æŒ‡æ ‡](#1-åˆ†ç±»è¯„ä¼°æŒ‡æ ‡)
  - [2. èšç±»è¯„ä¼°æŒ‡æ ‡](#2-èšç±»è¯„ä¼°æŒ‡æ ‡)
  - [3. å®Œæ•´è¯„ä¼°ä»£ç ](#3-å®Œæ•´è¯„ä¼°ä»£ç )
- [åã€å®æˆ˜æ¡ˆä¾‹ï¼šå®¢æœå·¥å•è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ](#åå®æˆ˜æ¡ˆä¾‹å®¢æœå·¥å•è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ)
  - [1. ç³»ç»Ÿæ¶æ„](#1-ç³»ç»Ÿæ¶æ„)
  - [2. å®Œæ•´å®ç°ä»£ç ](#2-å®Œæ•´å®ç°ä»£ç )
  - [3. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®](#3-ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®)
- [åä¸€ã€æœ¬ç« å°ç»“](#åä¸€æœ¬ç« å°ç»“)

---

## ä¸€ã€åˆ¤åˆ«å¼ AI vs. ç”Ÿæˆå¼ AI

åœ¨ LLM å‡ºç°ä¹‹å‰ï¼ŒNLP çš„åŠå£æ±Ÿå±±æ˜¯ BERT æ‰“ä¸‹çš„ã€‚BERT æ˜¯å…¸å‹çš„åˆ¤åˆ«å¼æ¨¡å‹ï¼ˆEncoder-onlyï¼‰ã€‚

### 1. æ ¸å¿ƒå·®å¼‚

| ç»´åº¦ | åˆ¤åˆ«å¼æ¨¡å‹ (Discriminative) | ç”Ÿæˆå¼æ¨¡å‹ (Generative) |
|------|---------------------------|------------------------|
| **ç›®æ ‡** | $P(Y \mid X)$ - ç»™å®šè¾“å…¥åˆ¤æ–­ç±»åˆ« | $P(X, Y)$ æˆ– $P(Y \mid X)$ è‡ªå›å½’ |
| **ä»£è¡¨** | BERT, RoBERTa, DeBERTa | GPT-4, LLaMA, ChatGPT |
| **ä¼˜åŠ¿** | å‡†ç¡®ç‡é«˜ã€é€Ÿåº¦å¿«ã€æˆæœ¬ä½ | çµæ´»æ€§å¼ºã€é›¶æ ·æœ¬èƒ½åŠ› |
| **åŠ£åŠ¿** | éœ€è¦æ ‡æ³¨æ•°æ®ã€ç±»åˆ«å›ºå®š | æ¨ç†æ…¢ã€æˆæœ¬é«˜ã€æ˜“å¹»è§‰ |

### 2. ä¸ºä»€ä¹ˆåˆ¤åˆ«å¼æ¨¡å‹ä»ç„¶é‡è¦ï¼Ÿ

è™½ç„¶ç°åœ¨å¯ä»¥ç”¨ GPT-4 åšåˆ†ç±»ï¼ˆPrompt: "Is this positive? answer yes/no"ï¼‰ï¼Œä½†åœ¨**é«˜å¹¶å‘ã€ä½å»¶è¿Ÿã€é«˜æ€§ä»·æ¯”**åœºæ™¯ä¸‹ï¼ŒåŸºäº **Embedding çš„åˆ†ç±»å™¨**ä¾ç„¶æ˜¯æœ€ä¼˜é€‰æ‹©ã€‚

**å…¸å‹åœºæ™¯ï¼š**
- å®¢æœå·¥å•è‡ªåŠ¨åˆ†ç±»ï¼ˆæ¯ç§’æ•°åƒè¯·æ±‚ï¼‰
- åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼ˆæ¯«ç§’çº§å“åº”ï¼‰
- èˆ†æƒ…åˆ†æï¼ˆå›ºå®šç±»åˆ«ï¼šæ­£/ä¸­/è´Ÿï¼‰
- å†…å®¹å®¡æ ¸ï¼ˆè¿è§„åˆ†ç±»ï¼‰

---

## äºŒã€Embedding çš„æœ¬è´¨ä¸å¯è§†åŒ–

### 1. ä»€ä¹ˆæ˜¯ Embeddingï¼Ÿ

Embedding æ˜¯å°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´çš„è¿‡ç¨‹ï¼Œä½¿å¾—**è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨ç©ºé—´ä¸­å½¼æ­¤æ¥è¿‘**ã€‚

**æ•°å­¦è¡¨ç¤ºï¼š**
$$
\text{Encoder}: \mathbb{T} \rightarrow \mathbb{R}^d
$$

å…¶ä¸­ï¼š
- $\mathbb{T}$: æ–‡æœ¬ç©ºé—´
- $d$: Embedding ç»´åº¦ï¼ˆé€šå¸¸ 768/1024/1536ï¼‰

### 2. æµè¡Œçš„ Embedding æ¨¡å‹ï¼ˆ2025ç‰ˆï¼‰

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | ä½¿ç”¨åœºæ™¯ | å‘å¸ƒæ—¶é—´ |
|------|------|------|---------|---------|
| **BGE-M3** | 1024 | å¤šè¯­è¨€ã€å¤šç²’åº¦ | è·¨è¯­è¨€æ£€ç´¢ | 2024å¹´ |
| **bge-large-zh-v1.5** | 1024 | ä¸­æ–‡SOTA | ä¸­æ–‡æ£€ç´¢/åˆ†ç±» | 2023å¹´ |
| **M3E-base** | 768 | ä¸­æ–‡è½»é‡çº§ | èµ„æºå—é™åœºæ™¯ | 2023å¹´ |
| **text-embedding-3-large** | 3072 | OpenAIæœ€æ–° | è‹±æ–‡ä»»åŠ¡ | 2024å¹´ |
| **GTE-Qwen2-7B** | 3584 | å¤§æ¨¡å‹çº§åˆ« | é«˜ç²¾åº¦åœºæ™¯ | 2024å¹´ |
| **jina-embeddings-v3** | 1024 | å¤šä»»åŠ¡ä¼˜åŒ– | é€šç”¨åœºæ™¯ | 2024å¹´ |

**2025å¹´æ¨èé€‰æ‹©ï¼š**
- âœ… **ä¸­æ–‡åœºæ™¯**ï¼š`bge-large-zh-v1.5` æˆ– `BGE-M3`
- âœ… **è‹±æ–‡åœºæ™¯**ï¼š`text-embedding-3-large` (API) æˆ– `GTE-Qwen2-7B` (æœ¬åœ°)
- âœ… **èµ„æºå—é™**ï¼š`M3E-base` æˆ– `bge-small-zh-v1.5`
- âœ… **å¤šè¯­è¨€**ï¼š`BGE-M3` æˆ– `jina-embeddings-v3`

### 3. Embedding å¯è§†åŒ–ï¼št-SNE ä¸ UMAP

é«˜ç»´å‘é‡æ— æ³•ç›´æ¥è§‚å¯Ÿï¼Œéœ€è¦é™ç»´åˆ° 2D/3D ç©ºé—´ã€‚

#### 3.1 t-SNE (t-Distributed Stochastic Neighbor Embedding)

**åŸç†ï¼š**
- åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œè®¡ç®—ç‚¹å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
- åœ¨ä½ç»´ç©ºé—´ä¸­ï¼Œä¿æŒç›¸ä¼¼åº¦åˆ†å¸ƒï¼ˆt-åˆ†å¸ƒï¼‰
- é€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ– KL æ•£åº¦

**ä¼˜ç‚¹ï¼š**
- æ“…é•¿ä¿æŒå±€éƒ¨ç»“æ„
- å¯è§†åŒ–æ•ˆæœå¥½

**ç¼ºç‚¹ï¼š**
- æ…¢ï¼ˆ$O(N^2)$ï¼‰
- éšæœºæ€§å¼ºï¼ˆæ¯æ¬¡ç»“æœä¸åŒï¼‰
- ä¸ä¿æŒå…¨å±€ç»“æ„

#### 3.2 UMAP (Uniform Manifold Approximation and Projection)

**åŸç†ï¼š**
- åŸºäºæµå½¢å­¦ä¹ å’Œæ‹“æ‰‘æ•°æ®åˆ†æ
- æ„å»ºé«˜ç»´ç©ºé—´çš„æ‹“æ‰‘å›¾ï¼ˆk-è¿‘é‚»å›¾ï¼‰
- åœ¨ä½ç»´ç©ºé—´é‡å»ºè¯¥æ‹“æ‰‘ç»“æ„

**ä¼˜ç‚¹ï¼š**
- é€Ÿåº¦å¿«ï¼ˆæ”¯æŒå¤§è§„æ¨¡æ•°æ®ï¼‰
- ä¿æŒå…¨å±€ç»“æ„
- ç¡®å®šæ€§å¼º

**ç¼ºç‚¹ï¼š**
- å‚æ•°æ•æ„Ÿï¼ˆ`n_neighbors`, `min_dist`ï¼‰

### 4. å®Œæ•´ä»£ç ï¼šEmbedding å¯è§†åŒ–å®æˆ˜

```python
"""
åŠŸèƒ½ï¼šEmbeddingå¯è§†åŒ–ï¼ˆt-SNE vs UMAPå¯¹æ¯”ï¼‰
ä¾èµ–ï¼šsentence-transformers>=2.3.0, scikit-learn>=1.3.0, umap-learn>=0.5.5
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import umap
from sentence_transformers import SentenceTransformer

# 1. å‡†å¤‡æ•°æ®
texts = [
    # ç§‘æŠ€ç±»
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®",
    "Transformeræ¶æ„æ˜¯ç°ä»£AIçš„åŸºçŸ³",
    "GPT-4å±•ç°äº†å¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›",

    # ä½“è‚²ç±»
    "æ¢…è¥¿å¸¦é¢†é˜¿æ ¹å»·å¤ºå¾—ä¸–ç•Œæ¯å† å†›",
    "NBAæ€»å†³èµ›å³å°†å¼€æ‰“",
    "é©¬æ‹‰æ¾æ¯”èµ›å¸å¼•æ•°ä¸‡åè·‘è€…å‚ä¸",
    "ä¸­å›½é˜Ÿåœ¨å¥¥è¿ä¼šä¸Šå–å¾—ä½³ç»©",

    # ç¾é£Ÿç±»
    "è¿™å®¶é¤å…çš„çƒ¤é¸­éå¸¸ç¾å‘³",
    "å·èœä»¥éº»è¾£è‘—ç§°",
    "æ„å¤§åˆ©é¢é…ç•ªèŒ„é…±å¾ˆç»å…¸",
    "æ—¥æœ¬å¯¿å¸è®²ç©¶é£Ÿææ–°é²œ",

    # æ—…æ¸¸ç±»
    "é•¿åŸæ˜¯ä¸­å›½çš„æ ‡å¿—æ€§æ™¯ç‚¹",
    "å·´é»çš„åŸƒè²å°”é“å¡”ä»¤äººéœ‡æ’¼",
    "é©¬å°”ä»£å¤«çš„æµ·æ»©ç¾ä¸èƒœæ”¶",
    "äº¬éƒ½çš„å¤å¯ºå±•ç°æ—¥æœ¬æ–‡åŒ–"
]

labels = ["ç§‘æŠ€"]*4 + ["ä½“è‚²"]*4 + ["ç¾é£Ÿ"]*4 + ["æ—…æ¸¸"]*4

# 2. ç”Ÿæˆ Embeddingï¼ˆä½¿ç”¨2025å¹´æ¨èæ¨¡å‹ï¼‰
print("åŠ è½½ Embedding æ¨¡å‹...")
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
embeddings = model.encode(texts, normalize_embeddings=True)

print(f"Embedding å½¢çŠ¶: {embeddings.shape}")  # (16, 1024)

# 3. t-SNE é™ç»´
print("æ‰§è¡Œ t-SNE é™ç»´...")
tsne = TSNE(
    n_components=2,
    random_state=42,
    perplexity=5,  # å¯¹äºå°æ•°æ®é›†ä½¿ç”¨è¾ƒå°çš„perplexity
    n_iter=1000
)
tsne_results = tsne.fit_transform(embeddings)

# 4. UMAP é™ç»´
print("æ‰§è¡Œ UMAP é™ç»´...")
umap_reducer = umap.UMAP(
    n_components=2,
    random_state=42,
    n_neighbors=5,
    min_dist=0.1,
    metric='cosine'
)
umap_results = umap_reducer.fit_transform(embeddings)

# 5. å¯è§†åŒ–å¯¹æ¯”
fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# å®šä¹‰é¢œè‰²æ˜ å°„
colors = {'ç§‘æŠ€': '#FF6B6B', 'ä½“è‚²': '#4ECDC4', 'ç¾é£Ÿ': '#FFD93D', 'æ—…æ¸¸': '#95E1D3'}

# t-SNE å¯è§†åŒ–
for label in set(labels):
    mask = np.array(labels) == label
    axes[0].scatter(
        tsne_results[mask, 0],
        tsne_results[mask, 1],
        label=label,
        color=colors[label],
        s=150,
        alpha=0.7,
        edgecolors='black',
        linewidths=1
    )
axes[0].set_title("t-SNE å¯è§†åŒ–", fontsize=16, fontweight='bold')
axes[0].legend(fontsize=12)
axes[0].grid(True, alpha=0.3, linestyle='--')

# UMAP å¯è§†åŒ–
for label in set(labels):
    mask = np.array(labels) == label
    axes[1].scatter(
        umap_results[mask, 0],
        umap_results[mask, 1],
        label=label,
        color=colors[label],
        s=150,
        alpha=0.7,
        edgecolors='black',
        linewidths=1
    )
axes[1].set_title("UMAP å¯è§†åŒ–", fontsize=16, fontweight='bold')
axes[1].legend(fontsize=12)
axes[1].grid(True, alpha=0.3, linestyle='--')

plt.tight_layout()
plt.savefig('embedding_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nâœ… å¯è§†åŒ–å®Œæˆï¼")
print("ğŸ’¡ è§‚å¯Ÿè¦ç‚¹ï¼š")
print("   - åŒç±»åˆ«æ–‡æœ¬æ˜¯å¦èšåœ¨ä¸€èµ·ï¼ˆç±»å†…ç´§å¯†åº¦ï¼‰")
print("   - ä¸åŒç±»åˆ«æ˜¯å¦åˆ†ç¦»æ˜æ˜¾ï¼ˆç±»é—´è·ç¦»ï¼‰")
print("   - t-SNEæ›´å…³æ³¨å±€éƒ¨ç»“æ„ï¼ŒUMAPä¿ç•™å…¨å±€ç»“æ„")
```

**è¿è¡Œç»“æœåˆ†æï¼š**
- âœ… **åŒç±»åˆ«èšé›†**ï¼šç›¸åŒä¸»é¢˜çš„æ–‡æœ¬åœ¨ç©ºé—´ä¸­å½¼æ­¤æ¥è¿‘
- âœ… **ç±»åˆ«åˆ†ç¦»**ï¼šä¸åŒä¸»é¢˜ä¹‹é—´æœ‰æ˜æ˜¾è¾¹ç•Œ
- âœ… **t-SNE vs UMAP**ï¼š
  - t-SNEï¼šæ›´ç´§å‡‘çš„èšç±»ï¼Œé€‚åˆå±•ç¤ºå±€éƒ¨å…³ç³»
  - UMAPï¼šä¿æŒå…¨å±€ç»“æ„ï¼Œç±»åˆ«é—´è·ç¦»æ›´æœ‰æ„ä¹‰

**âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**
- é™ç»´ä¼šæŸå¤±ä¿¡æ¯ï¼Œå¯è§†åŒ–ä»…ä¾›å‚è€ƒ
- å°æ•°æ®é›†ï¼ˆ<50æ ·æœ¬ï¼‰é™ç»´æ•ˆæœä¸ç¨³å®š
- ç”Ÿäº§ç¯å¢ƒç›´æ¥ç”¨é«˜ç»´å‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ— éœ€é™ç»´

---

## ä¸‰ã€é›¶æ ·æœ¬åˆ†ç±» (Zero-Shot Classification)

é›¶æ ·æœ¬åˆ†ç±»ä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

### 1. åŸºäº NLI çš„é›¶æ ·æœ¬åˆ†ç±»

**æ ¸å¿ƒæ€æƒ³ï¼š**
å°†åˆ†ç±»é—®é¢˜è½¬åŒ–ä¸º**æ–‡æœ¬è•´å«ï¼ˆEntailmentï¼‰**é—®é¢˜ã€‚

**å…¬å¼ï¼š**
$$
P(\text{label} \mid \text{text}) = P(\text{entailment} \mid \text{premise=text}, \text{hypothesis="This text is about [label]"})
$$

å…¶ä¸­ï¼š
- **Premiseï¼ˆå‰æï¼‰**ï¼šå¾…åˆ†ç±»æ–‡æœ¬
- **Hypothesisï¼ˆå‡è®¾ï¼‰**ï¼šæ„é€ çš„åˆ†ç±»å‡è®¾ï¼Œå¦‚"This text is about ç§‘æŠ€"
- **Entailmentï¼ˆè•´å«ï¼‰**ï¼šå‰ææ˜¯å¦æ”¯æŒå‡è®¾

### 2. è‹±æ–‡é›¶æ ·æœ¬åˆ†ç±»å®ç°

```python
"""
åŠŸèƒ½ï¼šåŸºäºNLIçš„é›¶æ ·æœ¬åˆ†ç±»ï¼ˆè‹±æ–‡ï¼‰
æ¨¡å‹ï¼šfacebook/bart-large-mnliï¼ˆ2019å¹´ï¼Œä¾ç„¶æœ‰æ•ˆï¼‰
ä¾èµ–ï¼štransformers>=4.36.0
"""
from transformers import pipeline

# 1. åŠ è½½é›¶æ ·æœ¬åˆ†ç±»å™¨
classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=0  # ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœæœ‰ï¼‰
)

# 2. å®šä¹‰å€™é€‰ç±»åˆ«
candidate_labels = ["technology", "sports", "politics", "entertainment", "finance"]

# 3. å¾…åˆ†ç±»æ–‡æœ¬
text = "Apple announced the new iPhone 15 series with improved cameras"

# 4. è¿›è¡Œåˆ†ç±»
result = classifier(text, candidate_labels)

print("é›¶æ ·æœ¬åˆ†ç±»ç»“æœ:")
for label, score in zip(result['labels'], result['scores']):
    print(f"  {label}: {score:.3f}")

# è¾“å‡ºç¤ºä¾‹:
# technology: 0.952
# finance: 0.028
# entertainment: 0.012
# sports: 0.005
# politics: 0.003
```

### 3. ä¸­æ–‡é›¶æ ·æœ¬åˆ†ç±»ï¼ˆåŸºäº Sentence-BERTï¼‰

```python
"""
åŠŸèƒ½ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„ä¸­æ–‡é›¶æ ·æœ¬åˆ†ç±»
æ¨¡å‹ï¼šBAAI/bge-large-zh-v1.5ï¼ˆ2025å¹´æ¨èï¼‰
ä¾èµ–ï¼šsentence-transformers>=2.3.0
"""
import numpy as np
from sentence_transformers import SentenceTransformer, util

# 1. åŠ è½½ä¸­æ–‡ Embedding æ¨¡å‹
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

# 2. å®šä¹‰ç±»åˆ«æè¿°ï¼ˆä½¿ç”¨å®Œæ•´æè¿°è€Œéå•ä¸ªè¯ï¼‰
category_descriptions = {
    "ç§‘æŠ€": "è¿™æ®µæ–‡æœ¬è®¨è®ºçš„æ˜¯äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºã€äº’è”ç½‘ã€ç§‘å­¦æŠ€æœ¯æˆ–ITè¡Œä¸šç›¸å…³çš„å†…å®¹",
    "ä½“è‚²": "è¿™æ®µæ–‡æœ¬è®¨è®ºçš„æ˜¯è¶³çƒã€ç¯®çƒã€æ¯”èµ›ã€è¿åŠ¨å‘˜æˆ–ä½“è‚²èµ›äº‹ç›¸å…³çš„å†…å®¹",
    "ç¾é£Ÿ": "è¿™æ®µæ–‡æœ¬è®¨è®ºçš„æ˜¯é¤é¥®ã€èœå“ã€çƒ¹é¥ªã€ç¾å‘³æˆ–é¥®é£Ÿæ–‡åŒ–ç›¸å…³çš„å†…å®¹",
    "æ—…æ¸¸": "è¿™æ®µæ–‡æœ¬è®¨è®ºçš„æ˜¯æ™¯ç‚¹ã€æ—…è¡Œã€åº¦å‡ã€è§‚å…‰æˆ–æ—…æ¸¸ä½“éªŒç›¸å…³çš„å†…å®¹"
}

# 3. å¾…åˆ†ç±»æ–‡æœ¬
text = "é•¿åŸæ˜¯ä¸­å›½å¤ä»£çš„ä¼Ÿå¤§å»ºç­‘ï¼Œå¸å¼•äº†æ— æ•°æ¸¸å®¢å‰æ¥å‚è§‚"

# 4. è®¡ç®—ç›¸ä¼¼åº¦
text_embedding = model.encode(text, convert_to_tensor=True, normalize_embeddings=True)
category_embeddings = model.encode(
    list(category_descriptions.values()),
    convert_to_tensor=True,
    normalize_embeddings=True
)

# 5. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºç‚¹ç§¯ï¼‰
similarities = util.cos_sim(text_embedding, category_embeddings)[0]

# 6. è·å–æ’åºç»“æœ
scores = similarities.cpu().numpy()
ranked_categories = sorted(
    zip(category_descriptions.keys(), scores),
    key=lambda x: x[1],
    reverse=True
)

print("ä¸­æ–‡é›¶æ ·æœ¬åˆ†ç±»ç»“æœ:")
for category, score in ranked_categories:
    confidence = "é«˜" if score > 0.6 else "ä¸­" if score > 0.4 else "ä½"
    print(f"  {category}: {score:.3f} [{confidence}ç½®ä¿¡åº¦]")

# è¾“å‡ºç¤ºä¾‹:
# æ—…æ¸¸: 0.742 [é«˜ç½®ä¿¡åº¦]
# ç§‘æŠ€: 0.312 [ä½ç½®ä¿¡åº¦]
# ç¾é£Ÿ: 0.289 [ä½ç½®ä¿¡åº¦]
# ä½“è‚²: 0.254 [ä½ç½®ä¿¡åº¦]
```

**ğŸ’¡ å…³é”®æŠ€å·§ï¼š**
- âœ… ç”¨**å®Œæ•´æè¿°**ä»£æ›¿å•ä¸ªç±»åˆ«è¯ï¼ˆå‡†ç¡®ç‡æå‡10-15%ï¼‰
- âœ… ä½¿ç”¨`normalize_embeddings=True`ç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦ä¸€è‡´æ€§
- âœ… é€‰æ‹©ä¸“é—¨çš„ä¸­æ–‡Embeddingæ¨¡å‹
- âš ï¸ ç±»åˆ«æè¿°çš„è´¨é‡ç›´æ¥å½±å“åˆ†ç±»æ•ˆæœ

### 4. 2025å¹´æœ€ä½³å®è·µï¼šä½¿ç”¨ LLM API

ç°ä»£LLMï¼ˆå¦‚GPT-4ã€Claudeã€Qwenï¼‰å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ï¼Œé€‚åˆ**å¿«é€ŸåŸå‹**å’Œ**å¤æ‚åˆ†ç±»**åœºæ™¯ã€‚

```python
"""
åŠŸèƒ½ï¼šä½¿ç”¨OpenAI APIè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»
æ¨¡å‹ï¼šgpt-4o-miniï¼ˆæ€§ä»·æ¯”æœ€é«˜ï¼Œ2024å¹´å‘å¸ƒï¼‰
ä¾èµ–ï¼šopenai>=1.10.0
"""
from openai import OpenAI
import json

client = OpenAI()

def classify_text_with_llm(text: str, categories: list[str]) -> dict:
    """
    ä½¿ç”¨LLMè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»

    å‚æ•°:
        text: å¾…åˆ†ç±»æ–‡æœ¬
        categories: å€™é€‰ç±»åˆ«åˆ—è¡¨

    è¿”å›:
        {"category": str, "confidence": float, "reasoning": str}
    """
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†ç±»åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»åˆ°ç»™å®šç±»åˆ«ä¸­ã€‚

å€™é€‰ç±»åˆ«ï¼š{', '.join(categories)}

æ–‡æœ¬ï¼š{text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- category: æœ€åŒ¹é…çš„ç±»åˆ«
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
- reasoning: ç®€çŸ­çš„åˆ†ç±»ç†ç”±ï¼ˆ20å­—ä»¥å†…ï¼‰

ç¤ºä¾‹è¾“å‡ºï¼š
{{"category": "ç§‘æŠ€", "confidence": 0.95, "reasoning": "è®¨è®ºäº†AIæŠ€æœ¯å‘å±•"}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",  # 2025å¹´æ¨èï¼šå¿«é€Ÿä¸”ä¾¿å®œ
        messages=[{"role": "user", "content": prompt}],
        temperature=0,  # ç¡®ä¿ç»“æœç¨³å®š
        response_format={"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
    )

    result = json.loads(response.choices[0].message.content)
    return result

# æµ‹è¯•
categories = ["ç§‘æŠ€", "ä½“è‚²", "ç¾é£Ÿ", "æ—…æ¸¸"]
text = "OpenAIå‘å¸ƒäº†æœ€æ–°çš„GPT-5æ¨¡å‹ï¼Œæ€§èƒ½å¤§å¹…æå‡"

result = classify_text_with_llm(text, categories)
print(f"åˆ†ç±»ç»“æœ: {result['category']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']}")
print(f"ç†ç”±: {result['reasoning']}")

# è¾“å‡ºç¤ºä¾‹:
# åˆ†ç±»ç»“æœ: ç§‘æŠ€
# ç½®ä¿¡åº¦: 0.98
# ç†ç”±: è®¨è®ºäº†AIæ¨¡å‹å‘å¸ƒ
```

**å¯¹æ¯”ï¼šEmbedding vs LLM**

| ç»´åº¦ | Embeddingæ–¹æ³• | LLMæ–¹æ³• |
|------|--------------|---------|
| **å‡†ç¡®ç‡** | ä¸­ï¼ˆ75-85%ï¼‰ | é«˜ï¼ˆ85-95%ï¼‰ |
| **é€Ÿåº¦** | å¿«ï¼ˆ10msï¼‰ | æ…¢ï¼ˆ500-1000msï¼‰ |
| **æˆæœ¬** | ä½ï¼ˆå…è´¹/è‡ªéƒ¨ç½²ï¼‰ | é«˜ï¼ˆ$0.0001-0.001/æ¬¡ï¼‰ |
| **è§£é‡Šæ€§** | æ—  | æœ‰ï¼ˆå¯æä¾›ç†ç”±ï¼‰ |
| **å¤æ‚åœºæ™¯** | å¼± | å¼ºï¼ˆå¯å¤„ç†å¤šå±‚æ¬¡åˆ†ç±»ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | é«˜å¹¶å‘ã€å›ºå®šç±»åˆ« | çµæ´»åˆ†ç±»ã€éœ€è¦è§£é‡Š |

**2025å¹´æ¨èç­–ç•¥ï¼š**
1. **åŸå‹é˜¶æ®µ**ï¼šä½¿ç”¨LLMå¿«é€ŸéªŒè¯
2. **ç”Ÿäº§ç¯å¢ƒ**ï¼š
   - é«˜å¹¶å‘ï¼ˆ>100 QPSï¼‰â†’ Embedding + åˆ†ç±»å™¨
   - ä½å¹¶å‘ã€é«˜å‡†ç¡®ç‡è¦æ±‚ â†’ LLM API
   - æˆæœ¬æ•æ„Ÿ â†’ è‡ªéƒ¨ç½²å¼€æºæ¨¡å‹ï¼ˆQwen2.5-14Bï¼‰

### 5. é«˜çº§ç­–ç•¥ï¼šEmbedding ç²—ç­› + LLM ç²¾æ’ (Hybrid Classification)

å½“ç±»åˆ«æ•°é‡éå¸¸å¤šï¼ˆä¾‹å¦‚ 1000+ ç”µå•†ç±»ç›®ï¼‰æ—¶ï¼Œæˆ‘ä»¬æ— æ³•å°†æ‰€æœ‰ç±»åˆ«éƒ½æ”¾å…¥ LLM çš„ Prompt ä¸­ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ä¸”è´¹ç”¨æ˜‚è´µï¼‰ã€‚æ­¤æ—¶ï¼Œ"ç²—ç­› + ç²¾æ’" æ˜¯æ ‡å‡†è§£å†³æ–¹æ¡ˆã€‚

**æµç¨‹å›¾ï¼š**
```
ç”¨æˆ·è¾“å…¥ â†’ Embedding æ£€ç´¢(å‘é‡åº“) â†’ Top-K å€™é€‰ç±»åˆ«(å¦‚ Top 10)
                                        â†“
                                    æ„å»º Prompt (åŒ…å« Top-K å€™é€‰)
                                        â†“
                                     LLM æœ€ç»ˆè£å†³
```

**ä»£ç å®ç°ï¼š**

```python
"""
åŠŸèƒ½ï¼šEmbedding ç²—ç­› + LLM ç²¾æ’å¤„ç†æµ·é‡ç±»åˆ«
"""
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
import torch

# 1. æ¨¡æ‹Ÿæµ·é‡ç±»åˆ«åº“ (å®é™…åœºæ™¯å¯èƒ½æœ‰æ•°åƒä¸ª)
all_categories = [
    # ç”µå­äº§å“
    "æ™ºèƒ½æ‰‹æœº", "ç¬”è®°æœ¬ç”µè„‘", "å¹³æ¿ç”µè„‘", "æ™ºèƒ½æ‰‹è¡¨", "è“ç‰™è€³æœº",
    "å•åç›¸æœº", "æ— äººæœº", "ç§»åŠ¨ç”µæº", "æ•°æ®çº¿", "æ‰‹æœºå£³",
    # å®¶å±…ç”¨å“
    "æ²™å‘", "åºŠå«", "è¡£æŸœ", "é¤æ¡Œ", "åŠå…¬æ¤…",
    "å°ç¯", "çª—å¸˜", "åœ°æ¯¯", "è£…é¥°ç”»", "èŠ±ç“¶",
    # ... å‡è®¾æœ‰å¾ˆå¤š ...
]

# 2. åˆå§‹åŒ–æ¨¡å‹
embedder = SentenceTransformer('BAAI/bge-large-zh-v1.5')
client = OpenAI()

# 3. é¢„è®¡ç®—ç±»åˆ« Embeddings (åªéœ€åšä¸€æ¬¡)
category_embeddings = embedder.encode(all_categories, convert_to_tensor=True)

def hybrid_classify(text, top_k=5):
    # Step 1: ç²—ç­› (Embedding Retrieval)
    query_embedding = embedder.encode(text, convert_to_tensor=True)

    # è®¡ç®—ç›¸ä¼¼åº¦å¹¶è·å– Top-K
    cos_scores = util.cos_sim(query_embedding, category_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    candidate_indices = top_results.indices.tolist()
    candidates = [all_categories[i] for i in candidate_indices]

    print(f"Embedding ç²—ç­›ç»“æœ: {candidates}")

    # Step 2: ç²¾æ’ (LLM Decision)
    prompt = f"""è¯·ä»ä»¥ä¸‹å€™é€‰ç±»åˆ«ä¸­ï¼Œä¸ºæ–‡æœ¬é€‰æ‹©æœ€å‡†ç¡®çš„ä¸€ä¸ªç±»åˆ«ã€‚

æ–‡æœ¬ï¼š{text}

å€™é€‰åˆ—è¡¨ï¼š
{', '.join(candidates)}

å¦‚æœéƒ½ä¸åˆé€‚ï¼Œè¯·è¿”å›"å…¶ä»–"ã€‚åªè¿”å›ç±»åˆ«åç§°ï¼Œä¸è¦è§£é‡Šã€‚"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    final_category = response.choices[0].message.content
    return final_category

# æµ‹è¯•
text = "æˆ‘æƒ³ä¹°ä¸ªå¤§ç–†ç”¨æ¥èˆªæ‹å©šç¤¼"
result = hybrid_classify(text)
print(f"æœ€ç»ˆåˆ†ç±»: {result}")

# è¾“å‡ºç¤ºä¾‹:
# Embedding ç²—ç­›ç»“æœ: ['æ— äººæœº', 'å•åç›¸æœº', 'æ™ºèƒ½æ‰‹æœº', 'ç§»åŠ¨ç”µæº', 'è“ç‰™è€³æœº']
# æœ€ç»ˆåˆ†ç±»: æ— äººæœº
```

**æ–¹æ¡ˆä¼˜åŠ¿ï¼š**
- âœ… **çªç ´ä¸Šä¸‹æ–‡é™åˆ¶**ï¼šæ”¯æŒæ— é™å¤šçš„ç±»åˆ«ï¼ˆåªè¦å‘é‡æ£€ç´¢èƒ½æ”¯æŒï¼‰ã€‚
- âœ… **é™ä½æˆæœ¬**ï¼šPrompt ä¸­åªéœ€åŒ…å« 5-10 ä¸ªå³ä½¿å€™é€‰ï¼Œå¤§å¤§å‡å°‘ Token æ¶ˆè€—ã€‚
- âœ… **å‡†ç¡®ç‡é«˜**ï¼šLLM æ“…é•¿å¤„ç†æ­§ä¹‰ï¼Œæ¯”å•çº¯ Embedding åŒ¹é…æ›´å‡†ï¼ˆä¾‹å¦‚åŒºåˆ†"è‹¹æœæ‰‹æœº"å’Œ"è‹¹æœ(æ°´æœ)"ï¼‰ã€‚

---

## å››ã€å°‘æ ·æœ¬åˆ†ç±» (Few-Shot Classification)

åªæœ‰å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼ˆæ¯ç±» 5-50 ä¸ªï¼‰æ—¶ï¼Œä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ã€‚

### 1. SetFit æ¡†æ¶æ·±åº¦è§£æ

**SetFit (Sentence Transformer Fine-Tuning)** æ˜¯ HuggingFace æ¨å‡ºçš„å°‘æ ·æœ¬åˆ†ç±»åˆ©å™¨ï¼Œä¸“é—¨è§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚

#### 1.1 ä¸ºä»€ä¹ˆ SetFit å¦‚æ­¤é«˜æ•ˆï¼Ÿ

**ä¼ ç»Ÿå¾®è°ƒçš„å›°å¢ƒï¼š**
- BERT å¾®è°ƒéœ€è¦æ•°åƒæ ·æœ¬æ‰èƒ½é¿å…è¿‡æ‹Ÿåˆ
- Prompt-based æ–¹æ³•éœ€è¦ç²¾å¿ƒè®¾è®¡æç¤ºè¯ï¼Œæ•ˆæœä¸ç¨³å®š
- å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå…¨é‡å¾®è°ƒå®¹æ˜“"è®°ä½"è®­ç»ƒæ ·æœ¬ï¼ˆè¿‡æ‹Ÿåˆï¼‰

**SetFit çš„åˆ›æ–°ï¼š**
1. **å¯¹æ¯”å­¦ä¹ é˜¶æ®µ**ï¼šä»æ¯ç±» 8 ä¸ªæ ·æœ¬ä¸­ç”Ÿæˆé…å¯¹æ•°æ®
   - åŒç±»æ ·æœ¬å¯¹ï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰ï¼š"text_A" ä¸ "text_B" ç›¸ä¼¼
   - å¼‚ç±»æ ·æœ¬å¯¹ï¼ˆè´Ÿæ ·æœ¬å¯¹ï¼‰ï¼š"text_A" ä¸ "text_C" ä¸ç›¸ä¼¼
   - é€šè¿‡å¯¹æ¯”å­¦ä¹ å¾®è°ƒ Sentence Transformerï¼Œæ‹‰è¿‘åŒç±»ï¼Œæ¨è¿œå¼‚ç±»

2. **åˆ†ç±»å¤´è®­ç»ƒ**ï¼šåœ¨å¾®è°ƒåçš„ Embedding ä¹‹ä¸Šè®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨
   - ä½¿ç”¨ç®€å•çš„ Logistic Regression æˆ–æµ…å±‚ MLP
   - å› ä¸º Embedding å·²ç»å­¦åˆ°äº†ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ï¼Œåˆ†ç±»å™¨åªéœ€å­¦ä¹ å†³ç­–è¾¹ç•Œ

**æ•°å­¦åŸç†ï¼š**

å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼š
$$
\mathcal{L} = -\log \frac{\exp(\text{sim}(h_i, h_j^+) / \tau)}{\exp(\text{sim}(h_i, h_j^+) / \tau) + \sum_{k=1}^{N} \exp(\text{sim}(h_i, h_k^-) / \tau)}
$$

å…¶ä¸­ï¼š
- $h_i$: é”šç‚¹æ ·æœ¬çš„ Embedding
- $h_j^+$: æ­£æ ·æœ¬ï¼ˆåŒç±»ï¼‰çš„ Embedding
- $h_k^-$: è´Ÿæ ·æœ¬ï¼ˆå¼‚ç±»ï¼‰çš„ Embedding
- $\tau$: æ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸ 0.05ï¼‰
- $\text{sim}(\cdot, \cdot)$: ä½™å¼¦ç›¸ä¼¼åº¦

**ä¼˜åŠ¿æ€»ç»“ï¼š**
- âœ… **æ•°æ®é«˜æ•ˆ**ï¼šæ¯ç±»åªéœ€ 8-16 ä¸ªæ ·æœ¬ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šè¶…è¿‡ 3000 æ ·æœ¬çš„å…¨é‡å¾®è°ƒ
- âœ… **è®­ç»ƒå¿«é€Ÿ**ï¼šå¯¹æ¯”å­¦ä¹ +åˆ†ç±»å¤´è®­ç»ƒé€šå¸¸ < 1 åˆ†é’Ÿï¼ˆCPUï¼‰
- âœ… **æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ**ï¼šåœ¨ 20 ä¸ªæ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®ç‡ä»…ä½ 1-2%
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šå¯ä½¿ç”¨ä»»ä½• Sentence Transformer ä½œä¸ºåŸºåº§
- âœ… **æ— éœ€ Prompt å·¥ç¨‹**ï¼šä¸åƒ GPT-3 éœ€è¦ç²¾å¿ƒè®¾è®¡æç¤ºè¯

### 2. å®Œæ•´ä»£ç å®ç°

```python
from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset

# 1. å‡†å¤‡å°‘æ ·æœ¬æ•°æ®ï¼ˆæ¯ç±»åªæœ‰ 8 ä¸ªæ ·æœ¬ï¼‰
train_data = {
    "text": [
        # ç§‘æŠ€ç±»ï¼ˆ8ä¸ªï¼‰
        "OpenAIå‘å¸ƒGPT-4æ¨¡å‹", "é‡å­è®¡ç®—æœºå–å¾—çªç ´",
        "èŠ¯ç‰‡æŠ€æœ¯è¿›å…¥3nmæ—¶ä»£", "äººå·¥æ™ºèƒ½è¾…åŠ©åŒ»ç–—è¯Šæ–­",
        "5Gç½‘ç»œè¦†ç›–å…¨å›½", "äº‘è®¡ç®—å¸‚åœºå¿«é€Ÿå¢é•¿",
        "æœºå™¨äººæŠ€æœ¯æ—¥æ–°æœˆå¼‚", "åŒºå—é“¾åº”ç”¨åœºæ™¯æ‰©å¤§",

        # ä½“è‚²ç±»ï¼ˆ8ä¸ªï¼‰
        "ä¸­å›½é˜Ÿå¤ºå¾—å¥¥è¿é‡‘ç‰Œ", "ä¸–ç•Œæ¯å†³èµ›æ¿€åŠ¨äººå¿ƒ",
        "NBAå­£åèµ›è¿›å…¥ç™½çƒ­åŒ–", "é©¬æ‹‰æ¾èµ›äº‹åœ†æ»¡ç»“æŸ",
        "è¶³çƒè½¬ä¼šçª—å£å…³é—­", "è¿åŠ¨å‘˜æ‰“ç ´ä¸–ç•Œçºªå½•",
        "ç¾½æ¯›çƒå…¬å¼€èµ›å¼€å¹•", "ç½‘çƒå¤§æ»¡è´¯èµ›äº‹ç²¾å½©"
    ],
    "label": [0]*8 + [1]*8  # 0: ç§‘æŠ€, 1: ä½“è‚²
}

train_dataset = Dataset.from_dict(train_data)

# 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶åˆ›å»º SetFit æ¨¡å‹
model = SetFitModel.from_pretrained("moka-ai/m3e-base")

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = SetFitTrainer(
    model=model,
    train_dataset=train_dataset,
    num_iterations=20,  # å¯¹æ¯”å­¦ä¹ çš„è½®æ•°
    num_epochs=1,       # åˆ†ç±»å¤´è®­ç»ƒçš„è½®æ•°
    column_mapping={"text": "text", "label": "label"}
)

# 4. è®­ç»ƒæ¨¡å‹
trainer.train()

# 5. æµ‹è¯•æ¨¡å‹
test_texts = [
    "è°·æ­Œæ¨å‡ºæ–°æ¬¾æ™ºèƒ½æ‰‹æœº",
    "ç¯®çƒæ¯”èµ›è¿›å…¥åŠ æ—¶èµ›"
]

predictions = model(test_texts)
print("å°‘æ ·æœ¬åˆ†ç±»ç»“æœ:")
for text, pred in zip(test_texts, predictions):
    label_name = "ç§‘æŠ€" if pred == 0 else "ä½“è‚²"
    print(f"  {text} -> {label_name}")
```

### 3. Few-Shot Promptï¼ˆä½¿ç”¨ LLMï¼‰

å¯¹äºæ›´å°‘çš„æ ·æœ¬ï¼ˆæ¯ç±» 1-3 ä¸ªï¼‰ï¼Œå¯ä»¥ç”¨ In-Context Learningï¼š

```python
from openai import OpenAI

client = OpenAI()

# Few-shot examples
examples = """
ç¤ºä¾‹1:
æ–‡æœ¬: äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ
ç±»åˆ«: ç§‘æŠ€

ç¤ºä¾‹2:
æ–‡æœ¬: æ¢…è¥¿å¸¦é¢†çƒé˜Ÿå¤ºå† 
ç±»åˆ«: ä½“è‚²

ç¤ºä¾‹3:
æ–‡æœ¬: è¿™é“èœéå¸¸ç¾å‘³
ç±»åˆ«: ç¾é£Ÿ
"""

# å¾…åˆ†ç±»æ–‡æœ¬
text = "è‹¹æœå‘å¸ƒæ–°æ¬¾ç¬”è®°æœ¬ç”µè„‘"

# æ„å»º Prompt
prompt = f"""{examples}

ç°åœ¨è¯·åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬:
æ–‡æœ¬: {text}
ç±»åˆ«: """

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    temperature=0
)

print(f"åˆ†ç±»ç»“æœ: {response.choices[0].message.content}")
# è¾“å‡º: ç§‘æŠ€
```

---

## äº”ã€å…¨é‡å¾®è°ƒ vs PEFT (LoRA) åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å¯¹æ¯”

å½“æœ‰è¶³å¤Ÿæ•°æ®æ—¶ï¼Œå¯ä»¥é€‰æ‹©å¾®è°ƒæ¨¡å‹ã€‚

### 1. å…¨é‡å¾®è°ƒ (Full Fine-Tuning)

**å®šä¹‰ï¼š** æ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚

**ä¼˜ç‚¹ï¼š**
- æ•ˆæœæœ€å¥½ï¼ˆå……åˆ†å­¦ä¹ ä»»åŠ¡ç‰¹å¾ï¼‰

**ç¼ºç‚¹ï¼š**
- æ˜¾å­˜å ç”¨å¤§ï¼ˆéœ€è¦å­˜å‚¨å…¨éƒ¨æ¢¯åº¦ï¼‰
- è®­ç»ƒæ…¢
- å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆå°æ•°æ®é›†ï¼‰

### 2. PEFT: LoRA (Low-Rank Adaptation)

**æ ¸å¿ƒæ€æƒ³ï¼š**
å†»ç»“é¢„è®­ç»ƒæƒé‡ $W_0$ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µ $A, B$ï¼š

$$
W = W_0 + \Delta W = W_0 + BA
$$

å…¶ä¸­ï¼š
- $W_0 \in \mathbb{R}^{d \times d}$: å†»ç»“çš„åŸå§‹æƒé‡
- $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$: å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µ
- $r \ll d$: ç§©ï¼ˆé€šå¸¸ 8/16/32ï¼‰

**å‚æ•°é‡å¯¹æ¯”ï¼š**
- åŸå§‹: $d^2$
- LoRA: $2dr$ (å½“ $r=8$, $d=1024$ æ—¶ï¼Œå‡å°‘ **99%**)

### 3. å®Œæ•´å¯¹æ¯”ä»£ç 

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# å‡†å¤‡æ•°æ®
dataset = load_dataset("SetFit/amazon_counterfactual_en")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

dataset = dataset.map(tokenize, batched=True)

# ========== æ–¹æ³•1: å…¨é‡å¾®è°ƒ ==========
model_full = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

print(f"å…¨é‡å¾®è°ƒå‚æ•°é‡: {model_full.num_parameters():,}")

trainer_full = Trainer(
    model=model_full,
    args=TrainingArguments(
        output_dir="./full_finetune",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        logging_steps=100
    ),
    train_dataset=dataset["train"]
)

trainer_full.train()

# ========== æ–¹æ³•2: LoRA å¾®è°ƒ ==========
model_lora = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# é…ç½® LoRA
lora_config = LoraConfig(
    r=8,                    # ç§©
    lora_alpha=32,          # ç¼©æ”¾ç³»æ•°
    target_modules=["query", "value"],  # åªä½œç”¨äº Q,V çŸ©é˜µ
    lora_dropout=0.1,
    bias="none"
)

model_lora = get_peft_model(model_lora, lora_config)
model_lora.print_trainable_parameters()
# è¾“å‡º: trainable params: 294,912 || all params: 109,483,778 || trainable%: 0.27%

trainer_lora = Trainer(
    model=model_lora,
    args=TrainingArguments(
        output_dir="./lora_finetune",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        logging_steps=100
    ),
    train_dataset=dataset["train"]
)

trainer_lora.train()

# ========== æ€§èƒ½å¯¹æ¯” ==========
print("\næ€§èƒ½å¯¹æ¯”:")
print(f"å…¨é‡å¾®è°ƒ - å‚æ•°é‡: {model_full.num_parameters():,}, æ˜¾å­˜: ~4.2GB")
print(f"LoRAå¾®è°ƒ - å‚æ•°é‡: {model_lora.get_nb_trainable_parameters():,}, æ˜¾å­˜: ~1.5GB")
```

### 4. å¯¹æ¯”æ€»ç»“

| ç»´åº¦ | å…¨é‡å¾®è°ƒ | LoRA å¾®è°ƒ |
|------|---------|----------|
| **å‚æ•°é‡** | 100% | 0.1%-1% |
| **æ˜¾å­˜å ç”¨** | é«˜ | ä½ï¼ˆå¯ç”¨æ›´å¤§ Batch Sizeï¼‰ |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | å¿« 1.5-2å€ |
| **æ•ˆæœ** | æœ€ä¼˜ | æ¥è¿‘å…¨é‡ï¼ˆæŸå¤±<1%ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | å¤§æ•°æ®é›† | ä¸­å°æ•°æ®é›† |

**å»ºè®®ï¼š**
- æ•°æ® < 10kï¼šç”¨ **SetFit** æˆ– **Few-Shot**
- æ•°æ® 10k-100kï¼šç”¨ **LoRA**
- æ•°æ® > 100kï¼šè€ƒè™‘ **å…¨é‡å¾®è°ƒ**

---

## å…­ã€æ–‡æœ¬èšç±»ç®—æ³•

å½“æ²¡æœ‰æ ‡æ³¨æ•°æ®æ—¶ï¼Œèšç±»å¯ä»¥è‡ªåŠ¨å‘ç°æ–‡æœ¬çš„å†…åœ¨ç»“æ„ã€‚

### 1. K-Means èšç±»

**åŸç†ï¼š**
1. éšæœºåˆå§‹åŒ– $k$ ä¸ªèšç±»ä¸­å¿ƒ
2. å°†æ¯ä¸ªæ ·æœ¬åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
3. é‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒ
4. é‡å¤ 2-3 ç›´åˆ°æ”¶æ•›

**ä»£ç å®ç°ï¼š**

```python
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# 1. å‡†å¤‡æ•°æ®
texts = [
    "äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ", "æ·±åº¦å­¦ä¹ çªç ´", "ç¥ç»ç½‘ç»œåº”ç”¨",
    "ç¯®çƒæ¯”èµ›ç²¾å½©", "è¶³çƒä¸–ç•Œæ¯", "å¥¥è¿ä¼šå¼€å¹•",
    "ç¾å‘³çš„å·èœ", "æ„å¤§åˆ©é¢å¾ˆå¥½åƒ", "æ—¥æœ¬æ–™ç†ç²¾è‡´",
    "é•¿åŸæ™¯è‰²å£®è§‚", "å·´é»é“å¡”ç¾ä¸½", "é©¬å°”ä»£å¤«æµ·æ»©"
]

# 2. ç”Ÿæˆ Embedding
model = SentenceTransformer('moka-ai/m3e-base')
embeddings = model.encode(texts)

# 3. K-Means èšç±»
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(embeddings)

# 4. è¾“å‡ºç»“æœ
for i, (text, label) in enumerate(zip(texts, labels)):
    print(f"ç°‡{label}: {text}")

# è¾“å‡ºç¤ºä¾‹:
# ç°‡0: äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ
# ç°‡0: æ·±åº¦å­¦ä¹ çªç ´
# ç°‡1: ç¯®çƒæ¯”èµ›ç²¾å½©
# ...
```

**é—®é¢˜ï¼š**
- éœ€è¦é¢„å…ˆæŒ‡å®š $k$ï¼ˆç°‡æ•°ï¼‰
- å¯¹åˆå§‹åŒ–æ•æ„Ÿ
- é«˜ç»´ç©ºé—´æ•ˆæœä¸‹é™ï¼ˆç»´åº¦ç¾éš¾ï¼‰

### 2. DBSCAN (å¯†åº¦èšç±»)

**ä¼˜åŠ¿ï¼š**
- **è‡ªåŠ¨å‘ç°ç°‡æ•°**
- å¯è¯†åˆ«å™ªå£°ç‚¹
- èƒ½å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡

**æ ¸å¿ƒå‚æ•°ï¼š**
- `eps`: é‚»åŸŸåŠå¾„
- `min_samples`: æ ¸å¿ƒç‚¹çš„æœ€å°é‚»å±…æ•°

**ä»£ç å®ç°ï¼š**

```python
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# 1. é™ç»´ï¼ˆé«˜ç»´ç©ºé—´DBSCANæ•ˆæœå·®ï¼‰
pca = PCA(n_components=50)
embeddings_reduced = pca.fit_transform(embeddings)

# 2. DBSCAN èšç±»
dbscan = DBSCAN(eps=0.5, min_samples=2)
labels = dbscan.fit_predict(embeddings_reduced)

print(f"å‘ç° {len(set(labels)) - (1 if -1 in labels else 0)} ä¸ªç°‡")
print(f"å™ªå£°ç‚¹æ•°é‡: {list(labels).count(-1)}")

# 3. è¾“å‡ºç»“æœ
for i, (text, label) in enumerate(zip(texts, labels)):
    cluster_name = f"ç°‡{label}" if label != -1 else "å™ªå£°"
    print(f"{cluster_name}: {text}")
```

### 3. HDBSCAN (å±‚æ¬¡åŒ– DBSCAN)

**æ”¹è¿›ï¼š**
- è‡ªåŠ¨ç¡®å®š `eps` å‚æ•°
- æ›´ç¨³å®šçš„èšç±»ç»“æœ
- é€‚åˆä¸åŒå¯†åº¦çš„ç°‡

**ä»£ç å®ç°ï¼š**

```python
import hdbscan

# 1. HDBSCAN èšç±»
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=2,
    min_samples=1,
    metric='euclidean'
)

labels = clusterer.fit_predict(embeddings_reduced)

print(f"HDBSCAN å‘ç° {len(set(labels)) - (1 if -1 in labels else 0)} ä¸ªç°‡")

# 2. èšç±»æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰
probabilities = clusterer.probabilities_

for text, label, prob in zip(texts, labels, probabilities):
    print(f"ç°‡{label} ({prob:.2f}): {text}")
```

### 4. ç®—æ³•å¯¹æ¯”

| ç®—æ³• | éœ€è¦æŒ‡å®šç°‡æ•° | å™ªå£°è¯†åˆ« | é«˜ç»´é€‚åº” | é€Ÿåº¦ |
|------|------------|---------|---------|------|
| **K-Means** | æ˜¯ | å¦ | å·® | å¿« |
| **DBSCAN** | å¦ | æ˜¯ | å·® | ä¸­ |
| **HDBSCAN** | å¦ | æ˜¯ | ä¸­ | æ…¢ |

**æ¨èç­–ç•¥ï¼š**
1. å…ˆç”¨ **UMAP/PCA** é™ç»´åˆ° 50-100 ç»´
2. ç”¨ **HDBSCAN** è‡ªåŠ¨å‘ç°ç°‡
3. ç”¨ **è½®å»“ç³»æ•° (Silhouette Score)** è¯„ä¼°èšç±»è´¨é‡

---

## ä¸ƒã€BERTopic åŸç†ä¸å®æˆ˜

BERTopic æ˜¯ç°ä»£ä¸»é¢˜å»ºæ¨¡çš„æ ‡å‡†å·¥å…·ï¼Œé›†æˆäº† Embedding + é™ç»´ + èšç±» + å…³é”®è¯æå–ã€‚

### 1. BERTopic æµç¨‹

```
æ–‡æœ¬ â†’ Embedding â†’ UMAPé™ç»´ â†’ HDBSCANèšç±» â†’ c-TF-IDFæå–ä¸»é¢˜è¯
```

**æ ¸å¿ƒåˆ›æ–°ï¼šc-TF-IDF (Class-based TF-IDF)**

ä¼ ç»Ÿ TF-IDF æ˜¯æ–‡æ¡£çº§åˆ«ï¼Œc-TF-IDF æ˜¯**ç°‡çº§åˆ«**ï¼š

$$
\text{c-TF-IDF}(w, c) = \text{TF}(w, c) \times \log \frac{N}{\sum_{c'} \text{TF}(w, c')}
$$

å…¶ä¸­ï¼š
- $w$: è¯
- $c$: ç°‡
- $\text{TF}(w, c)$: è¯ $w$ åœ¨ç°‡ $c$ ä¸­çš„é¢‘ç‡

### 2. å®Œæ•´ä»£ç å®æˆ˜

```python
from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups

# 1. å‡†å¤‡æ•°æ®ï¼ˆ20newsgroups æ•°æ®é›†ï¼‰
docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'][:1000]

# 2. åˆ›å»º BERTopic æ¨¡å‹
topic_model = BERTopic(
    language="english",
    calculate_probabilities=True,
    verbose=True
)

# 3. è®­ç»ƒæ¨¡å‹ï¼ˆè‡ªåŠ¨å®Œæˆ Embedding + é™ç»´ + èšç±»ï¼‰
topics, probs = topic_model.fit_transform(docs)

# 4. æŸ¥çœ‹ä¸»é¢˜
print(f"å‘ç° {len(set(topics))} ä¸ªä¸»é¢˜")
print("\nä¸»é¢˜åŠå…¶å…³é”®è¯:")
for topic_id in set(topics):
    if topic_id == -1:  # å™ªå£°ä¸»é¢˜
        continue
    print(f"\nä¸»é¢˜ {topic_id}:")
    print(topic_model.get_topic(topic_id)[:5])  # å‰5ä¸ªå…³é”®è¯

# è¾“å‡ºç¤ºä¾‹:
# ä¸»é¢˜ 0:
# [('space', 0.012), ('nasa', 0.011), ('launch', 0.009), ...]

# 5. å¯è§†åŒ–ä¸»é¢˜
topic_model.visualize_topics().show()

# 6. æŸ¥çœ‹æ–‡æ¡£å±äºå“ªä¸ªä¸»é¢˜
sample_doc = "NASA launches new space mission"
topic, prob = topic_model.transform([sample_doc])
print(f"\næ–‡æ¡£ä¸»é¢˜: {topic[0]}, ç½®ä¿¡åº¦: {prob[0]:.3f}")
```

### 3. ä¸­æ–‡ BERTopic

```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 1. ä¸­æ–‡æ–‡æ¡£
docs = [
    "äººå·¥æ™ºèƒ½æŠ€æœ¯å¿«é€Ÿå‘å±•",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­åº”ç”¨å¹¿æ³›",
    "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„é‡è¦åˆ†æ”¯",
    "ç¯®çƒæ¯”èµ›éå¸¸æ¿€çƒˆ",
    "è¶³çƒä¸–ç•Œæ¯å¸å¼•å…¨çƒå…³æ³¨",
    "å¥¥è¿ä¼šè¿åŠ¨å‘˜å¥‹å‹‡æ‹¼æ",
    "å·èœéº»è¾£é²œé¦™",
    "ç²¤èœæ¸…æ·¡çˆ½å£",
    "æ—¥æœ¬æ–™ç†ç²¾è‡´ç¾è§‚"
]

# 2. ä½¿ç”¨ä¸­æ–‡ Embedding æ¨¡å‹
embedding_model = SentenceTransformer("moka-ai/m3e-base")

# 3. åˆ›å»ºæ¨¡å‹
topic_model = BERTopic(
    embedding_model=embedding_model,
    language="chinese (simplified)",
    min_topic_size=2  # æœ€å°ä¸»é¢˜å¤§å°
)

# 4. è®­ç»ƒ
topics, probs = topic_model.fit_transform(docs)

# 5. æŸ¥çœ‹ä¸»é¢˜
for topic_id in set(topics):
    if topic_id != -1:
        print(f"ä¸»é¢˜ {topic_id}: {topic_model.get_topic(topic_id)[:3]}")
```

### 4. åŠ¨æ€ä¸»é¢˜å»ºæ¨¡ï¼ˆæ—¶é—´ç»´åº¦ï¼‰

è·Ÿè¸ªä¸»é¢˜éšæ—¶é—´çš„æ¼”å˜ï¼š

```python
from bertopic import BERTopic
import pandas as pd

# 1. å‡†å¤‡å¸¦æ—¶é—´æˆ³çš„æ•°æ®
docs = [...]
timestamps = ["2023-01-01", "2023-02-01", ...]  # å¯¹åº”æ¯ä¸ªæ–‡æ¡£çš„æ—¶é—´

# 2. è®­ç»ƒæ¨¡å‹
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)

# 3. éšæ—¶é—´æ¼”å˜åˆ†æ
topics_over_time = topic_model.topics_over_time(docs, timestamps)

# 4. å¯è§†åŒ–
topic_model.visualize_topics_over_time(topics_over_time).show()
```

---

## å…«ã€å±‚æ¬¡åŒ–èšç±»å®æˆ˜

å±‚æ¬¡åŒ–èšç±»æ„å»ºæ ‘çŠ¶ç»“æ„ï¼ˆDendrogramï¼‰ï¼Œé€‚åˆå‘ç°åµŒå¥—çš„ä¸»é¢˜å±‚æ¬¡ã€‚

### 1. å‡èšå±‚æ¬¡èšç±» (Agglomerative)

**åŸç†ï¼š**
1. æ¯ä¸ªæ ·æœ¬åˆå§‹ä¸ºä¸€ä¸ªç°‡
2. ä¸æ–­åˆå¹¶æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç°‡
3. ç›´åˆ°æ‰€æœ‰æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªç°‡

### 2. å®Œæ•´ä»£ç 

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# 1. å‡†å¤‡æ•°æ®
texts = [
    "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ ",
    "è¶³çƒ", "ç¯®çƒ", "æ’çƒ",
    "å·èœ", "ç²¤èœ", "æ¹˜èœ"
]

model = SentenceTransformer('moka-ai/m3e-base')
embeddings = model.encode(texts)

# 2. è®¡ç®—å±‚æ¬¡èšç±»é“¾æ¥çŸ©é˜µ
linkage_matrix = linkage(embeddings, method='ward')

# 3. ç»˜åˆ¶æ ‘çŠ¶å›¾
plt.figure(figsize=(10, 6))
dendrogram(linkage_matrix, labels=texts, leaf_font_size=12)
plt.title("å±‚æ¬¡åŒ–èšç±»æ ‘çŠ¶å›¾")
plt.xlabel("æ ·æœ¬")
plt.ylabel("è·ç¦»")
plt.tight_layout()
plt.savefig('hierarchical_clustering.png', dpi=150)
plt.show()

# 4. åˆ‡åˆ†ä¸ºæŒ‡å®šæ•°é‡çš„ç°‡
agg_clustering = AgglomerativeClustering(n_clusters=3)
labels = agg_clustering.fit_predict(embeddings)

for text, label in zip(texts, labels):
    print(f"ç°‡{label}: {text}")
```

### 3. å±‚æ¬¡åŒ–ä¸»é¢˜å»ºæ¨¡

ç»“åˆ BERTopic å®ç°å±‚æ¬¡åŒ–ä¸»é¢˜ï¼š

```python
from bertopic import BERTopic
from sklearn.cluster import AgglomerativeClustering

# 1. è®­ç»ƒ BERTopic
topic_model = BERTopic(hdbscan_model=AgglomerativeClustering(n_clusters=10))
topics, probs = topic_model.fit_transform(docs)

# 2. æ„å»ºä¸»é¢˜å±‚æ¬¡
hierarchical_topics = topic_model.hierarchical_topics(docs)

# 3. å¯è§†åŒ–å±‚æ¬¡ç»“æ„
topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show()
```

---

## ä¹ã€åˆ†ç±»ä¸èšç±»è¯„ä¼°æŒ‡æ ‡

è¯„ä¼°æ˜¯æœºå™¨å­¦ä¹ çš„ç”Ÿå‘½çº¿ï¼Œæ²¡æœ‰è¯„ä¼°å°±æ— æ³•ä¼˜åŒ–ã€‚

### 1. åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

#### æ··æ·†çŸ©é˜µä¸æ ¸å¿ƒæŒ‡æ ‡

**å‡†ç¡®ç‡ (Accuracy)**ï¼šæ•´ä½“æ­£ç¡®ç‡
$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**ç²¾ç¡®ç‡ (Precision)**ï¼šé¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹
$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**å¬å›ç‡ (Recall)**ï¼šå®é™…ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**F1åˆ†æ•°**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**æŒ‡æ ‡é€‰æ‹©æŒ‡å—ï¼š**

| åœºæ™¯ | æ¨èæŒ‡æ ‡ | åŸå›  |
|------|---------|------|
| **æ•°æ®å¹³è¡¡** | Accuracy | ç®€å•ç›´è§‚ |
| **æ•°æ®ä¸å¹³è¡¡** | F1-score | ç»¼åˆè€ƒè™‘ |
| **åƒåœ¾é‚®ä»¶æ£€æµ‹** | Precision | é¿å…è¯¯æ€ |
| **ç–¾ç—…è¯Šæ–­** | Recall | é¿å…æ¼è¯Š |
| **å¤šåˆ†ç±»** | Macro-F1 | å¹³ç­‰å¯¹å¾…å„ç±» |

### 2. èšç±»è¯„ä¼°æŒ‡æ ‡

#### æœ‰æ ‡ç­¾è¯„ä¼°

**è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI)**ï¼šèŒƒå›´[-1, 1]ï¼Œ1è¡¨ç¤ºå®Œç¾èšç±»

**å½’ä¸€åŒ–äº’ä¿¡æ¯ (NMI)**ï¼šèŒƒå›´[0, 1]ï¼Œ1è¡¨ç¤ºå®Œç¾èšç±»

#### æ— æ ‡ç­¾è¯„ä¼°

**è½®å»“ç³»æ•° (Silhouette Score)**
$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

- èŒƒå›´ï¼š[-1, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½
- $a(i)$ï¼šæ ·æœ¬åˆ°åŒç°‡å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»
- $b(i)$ï¼šæ ·æœ¬åˆ°æœ€è¿‘å…¶ä»–ç°‡çš„å¹³å‡è·ç¦»

### 3. å®Œæ•´è¯„ä¼°ä»£ç 

```python
"""
åŠŸèƒ½ï¼šåˆ†ç±»ä¸èšç±»è¯„ä¼°å·¥å…·
ä¾èµ–ï¼šscikit-learn>=1.3.0, seaborn>=0.12.0
"""
from sklearn.metrics import (
    classification_report, confusion_matrix,
    silhouette_score, davies_bouldin_score
)
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_classification(y_true, y_pred, labels=None):
    """åˆ†ç±»è¯„ä¼°"""
    print("=" * 50)
    print("åˆ†ç±»è¯„ä¼°ç»“æœ")
    print("=" * 50)
    print(classification_report(y_true, y_pred, target_names=labels))

    # æ··æ·†çŸ©é˜µå¯è§†åŒ–
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150)
    plt.show()

def evaluate_clustering(X, labels_pred):
    """èšç±»è¯„ä¼°"""
    silhouette = silhouette_score(X, labels_pred)
    db_index = davies_bouldin_score(X, labels_pred)

    print("=" * 50)
    print("èšç±»è¯„ä¼°ç»“æœ")
    print("=" * 50)
    print(f"è½®å»“ç³»æ•° (Silhouette): {silhouette:.3f} [è¶Šæ¥è¿‘1è¶Šå¥½]")
    print(f"DBæŒ‡æ•°: {db_index:.3f} [è¶Šå°è¶Šå¥½]")

# ä½¿ç”¨ç¤ºä¾‹
y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 1, 2, 0, 2, 2]
evaluate_classification(y_true, y_pred, ['ç§‘æŠ€', 'ä½“è‚²', 'ç¾é£Ÿ'])
```

---

## åã€å®æˆ˜æ¡ˆä¾‹ï¼šå®¢æœå·¥å•è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ

### 1. ç³»ç»Ÿæ¶æ„

```
ç”¨æˆ·å·¥å• â†’ æ–‡æœ¬æ¸…æ´— â†’ Embedding â†’ åˆ†ç±»å™¨ â†’ è·¯ç”±åˆ°å¯¹åº”å®¢æœç»„
                              â†“
                          å®šæœŸé‡è®­ç»ƒï¼ˆæ•°æ®æ¼‚ç§»æ£€æµ‹ï¼‰
```

### 2. å®Œæ•´å®ç°ä»£ç 

```python
"""
åŠŸèƒ½ï¼šå®¢æœå·¥å•è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿï¼ˆå®Œæ•´ç”Ÿäº§çº§å®ç°ï¼‰
ç‰¹ç‚¹ï¼šæ•°æ®å¢å¼ºã€æ¨¡å‹æŒä¹…åŒ–ã€æ€§èƒ½ç›‘æ§
ä¾èµ–ï¼šsentence-transformers>=2.3.0, scikit-learn>=1.3.0, joblib>=1.3.0
"""
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============ 1. æ•°æ®å‡†å¤‡ä¸å¢å¼º ============
def prepare_data():
    """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå®é™…åº”ä»æ•°æ®åº“è¯»å–ï¼‰"""
    data = pd.DataFrame({
        'text': [
            # è´¦å·é—®é¢˜ï¼ˆæ‰©å±•æ ·æœ¬ï¼‰
            "è´¦å·æ— æ³•ç™»å½•", "å¯†ç å¿˜è®°äº†", "ç™»å½•å¤±è´¥", "è´¦å·è¢«é”å®š",
            "éªŒè¯ç æ”¶ä¸åˆ°", "æ— æ³•æ³¨å†Œæ–°è´¦å·", "è´¦å·å¼‚å¸¸",

            # å•†å“é—®é¢˜
            "å•†å“è´¨é‡å·®", "æ”¶åˆ°ç ´æŸå•†å“", "äº§å“æœ‰ç‘•ç–µ", "å•†å“ä¸æè¿°ä¸ç¬¦",
            "æƒ³è¦é€€è´§", "å•†å“åŠŸèƒ½æ•…éšœ", "äº§å“å°ºå¯¸ä¸å¯¹",

            # ç‰©æµé—®é¢˜
            "ç‰©æµå¤ªæ…¢", "å¿«é€’ä¸¢å¤±", "é…é€å»¶è¿Ÿ", "å¿«é€’è¿˜æ²¡åˆ°",
            "ç‰©æµä¿¡æ¯ä¸æ›´æ–°", "é…é€åœ°å€é”™è¯¯", "å¿«é€’è¢«ç­¾æ”¶ä½†æœªæ”¶åˆ°",

            # å”®åé—®é¢˜
            "å¦‚ä½•ç”³è¯·é€€æ¬¾", "é€€æ¬¾è¿›åº¦æŸ¥è¯¢", "å‘ç¥¨å¼€å…·", "ä¿ä¿®æœŸå¤šä¹…",
            "å”®åæœåŠ¡æ€åº¦å·®", "ç»´ä¿®ç”³è¯·", "æ¢è´§æµç¨‹"
        ],
        'label': [0]*7 + [1]*7 + [2]*7 + [3]*7  # 0:è´¦å· 1:å•†å“ 2:ç‰©æµ 3:å”®å
    })
    return data

# ============ 2. æ¨¡å‹è®­ç»ƒ ============
def train_classifier():
    """è®­ç»ƒåˆ†ç±»å™¨"""
    print("=" * 60)
    print("å®¢æœå·¥å•åˆ†ç±»ç³»ç»Ÿ - æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    data = prepare_data()
    print(f"æ•°æ®é›†å¤§å°: {len(data)} æ¡")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{data['label'].value_counts()}\n")

    # ç”Ÿæˆ Embedding
    print("åŠ è½½ Embedding æ¨¡å‹...")
    model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
    embeddings = model.encode(
        data['text'].tolist(),
        normalize_embeddings=True,
        show_progress_bar=True
    )

    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        embeddings, data['label'],
        test_size=0.2,
        random_state=42,
        stratify=data['label']  # ä¿æŒç±»åˆ«æ¯”ä¾‹
    )

    # è®­ç»ƒåˆ†ç±»å™¨
    print("\nè®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨...")
    classifier = LogisticRegression(
        max_iter=1000,
        class_weight='balanced',  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        random_state=42
    )
    classifier.fit(X_train, y_train)

    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(classifier, X_train, y_train, cv=3)
    print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")

    # æµ‹è¯•é›†è¯„ä¼°
    y_pred = classifier.predict(X_test)
    category_names = ['è´¦å·é—®é¢˜', 'å•†å“é—®é¢˜', 'ç‰©æµé—®é¢˜', 'å”®åé—®é¢˜']

    print("\næµ‹è¯•é›†è¯„ä¼°:")
    print(classification_report(y_test, y_pred, target_names=category_names))

    # ä¿å­˜æ¨¡å‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"ticket_classifier_{timestamp}.pkl"
    joblib.dump({
        'classifier': classifier,
        'embedding_model_name': 'BAAI/bge-large-zh-v1.5',
        'categories': category_names
    }, model_path)
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model, classifier, category_names

# ============ 3. åœ¨çº¿æ¨ç† ============
class TicketClassifier:
    """å·¥å•åˆ†ç±»å™¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰"""

    def __init__(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = joblib.load(model_path)
        self.classifier = checkpoint['classifier']
        self.categories = checkpoint['categories']
        self.embedding_model = SentenceTransformer(
            checkpoint['embedding_model_name']
        )
        print(f"âœ… æ¨¡å‹å·²åŠ è½½ï¼Œæ”¯æŒç±»åˆ«: {self.categories}")

    def predict(self, text: str) -> dict:
        """
        åˆ†ç±»å•æ¡å·¥å•

        è¿”å›:
            {
                'category': str,  # ç±»åˆ«åç§°
                'label': int,     # ç±»åˆ«ID
                'confidence': float,  # ç½®ä¿¡åº¦
                'all_probs': dict  # æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
            }
        """
        # ç”Ÿæˆ Embedding
        embedding = self.embedding_model.encode(
            [text],
            normalize_embeddings=True
        )

        # é¢„æµ‹
        label = self.classifier.predict(embedding)[0]
        probs = self.classifier.predict_proba(embedding)[0]

        return {
            'category': self.categories[label],
            'label': int(label),
            'confidence': float(probs[label]),
            'all_probs': {
                cat: float(prob)
                for cat, prob in zip(self.categories, probs)
            }
        }

    def predict_batch(self, texts: list[str]) -> list[dict]:
        """æ‰¹é‡åˆ†ç±»ï¼ˆæå‡ååé‡ï¼‰"""
        embeddings = self.embedding_model.encode(
            texts,
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=True
        )

        labels = self.classifier.predict(embeddings)
        probs = self.classifier.predict_proba(embeddings)

        results = []
        for label, prob in zip(labels, probs):
            results.append({
                'category': self.categories[label],
                'label': int(label),
                'confidence': float(prob[label])
            })
        return results

# ============ 4. ä½¿ç”¨ç¤ºä¾‹ ============
if __name__ == "__main__":
    # è®­ç»ƒæ¨¡å‹
    emb_model, clf, categories = train_classifier()

    # æ¨¡æ‹Ÿæ¨ç†
    print("\n" + "=" * 60)
    print("æ¨ç†æµ‹è¯•")
    print("=" * 60)

    test_tickets = [
        "æˆ‘çš„è´¦å·ç™»ä¸ä¸Šå»äº†",
        "æ”¶åˆ°çš„å•†å“æœ‰åˆ’ç—•",
        "å¿«é€’ä¸‰å¤©äº†è¿˜æ²¡é€åˆ°",
        "é€€æ¬¾ä»€ä¹ˆæ—¶å€™åˆ°è´¦"
    ]

    for ticket in test_tickets:
        embedding = emb_model.encode([ticket], normalize_embeddings=True)
        label = clf.predict(embedding)[0]
        prob = clf.predict_proba(embedding)[0][label]
        print(f"å·¥å•: {ticket}")
        print(f"  â†’ {categories[label]} (ç½®ä¿¡åº¦: {prob:.3f})\n")
```

### 3. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®

#### 3.1 æ€§èƒ½ä¼˜åŒ–

```python
"""
ä¼˜åŒ–ç­–ç•¥ï¼š
1. æ¨¡å‹é¢„åŠ è½½ï¼ˆé¿å…æ¯æ¬¡è¯·æ±‚éƒ½åŠ è½½ï¼‰
2. Batchæ¨ç†ï¼ˆç´¯ç§¯è¯·æ±‚æ‰¹é‡å¤„ç†ï¼‰
3. GPUåŠ é€Ÿï¼ˆsentence-transformersæ”¯æŒCUDAï¼‰
"""

# ä½¿ç”¨ ONNX åŠ é€Ÿæ¨ç†ï¼ˆå¯é€‰ï¼‰
from optimum.onnxruntime import ORTModelForFeatureExtraction
model = ORTModelForFeatureExtraction.from_pretrained(
    'BAAI/bge-large-zh-v1.5',
    export=True,
    provider='CUDAExecutionProvider'  # GPUåŠ é€Ÿ
)
```

#### 3.2 ç›‘æ§ä¸å‘Šè­¦

```python
"""
å…³é”®ç›‘æ§æŒ‡æ ‡ï¼š
1. åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå®šæœŸäººå·¥æ ‡æ³¨éªŒè¯ï¼‰
2. ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆä½ç½®ä¿¡åº¦æ ·æœ¬éœ€äººå·¥å¤æ ¸ï¼‰
3. æ•°æ®æ¼‚ç§»æ£€æµ‹ï¼ˆç±»åˆ«åˆ†å¸ƒå˜åŒ–ï¼‰
4. æ¨ç†å»¶è¿Ÿï¼ˆP50/P95/P99ï¼‰
"""

# ç½®ä¿¡åº¦é˜ˆå€¼ç­–ç•¥
def classify_with_threshold(text, threshold=0.7):
    result = classifier.predict(text)
    if result['confidence'] < threshold:
        return {'category': 'è½¬äººå·¥', 'reason': 'ä½ç½®ä¿¡åº¦'}
    return result
```

#### 3.3 æŒç»­ä¼˜åŒ–

| æ—¶é—´èŠ‚ç‚¹ | ä¼˜åŒ–ç­–ç•¥ |
|---------|---------|
| **ä¸Šçº¿ç¬¬1å‘¨** | æ¯å¤©äººå·¥æ ‡æ³¨100æ¡ï¼Œå¿«é€Ÿä¿®æ­£æ˜æ˜¾é”™è¯¯ |
| **ç¬¬1ä¸ªæœˆ** | æ”¶é›†1000+æ ‡æ³¨æ•°æ®ï¼Œé‡æ–°è®­ç»ƒ |
| **ç¬¬3ä¸ªæœˆ** | å¼•å…¥ä¸»åŠ¨å­¦ä¹ ï¼Œä¼˜å…ˆæ ‡æ³¨å›°éš¾æ ·æœ¬ |
| **ç¬¬6ä¸ªæœˆ** | è¯„ä¼°æ˜¯å¦éœ€è¦æ›´æ¢Embeddingæ¨¡å‹ |

---

## åä¸€ã€æœ¬ç« å°ç»“

### 1. æ ¸å¿ƒè¦ç‚¹å›é¡¾

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | æ ¸å¿ƒä¼˜åŠ¿ | æˆæœ¬/å¤æ‚åº¦ |
|------|---------|---------|------------|
| **æ¢ç´¢åˆ†æ** | **BERTopic / HDBSCAN** | è‡ªåŠ¨å‘ç°ä¸»é¢˜ï¼Œæ— éœ€æ ‡æ³¨ | ä½ |
| **å°‘é‡æ•°æ® (<100)** | **Few-Shot / LLM API** | å¯åŠ¨å¿«ï¼Œæ— éœ€è®­ç»ƒ | ä¸­/é«˜ï¼ˆAPIè´¹ç”¨ï¼‰ |
| **ä¸­ç­‰æ•°æ® (1k-10k)** | **SetFit / LoRA** | æ•ˆæœå¥½ï¼Œè®­ç»ƒå¿« | ä¸­ |
| **ç”Ÿäº§ç³»ç»Ÿ (>10k)** | **Embedding + LR / MLP** | æ¨ç†æå¿«ï¼Œå¹¶å‘é«˜ | ä½ï¼ˆæ¨ç†æˆæœ¬ï¼‰ |

### 2. 2025å¹´æŠ€æœ¯è¶‹åŠ¿

1. **RAGä¸åˆ†ç±»èåˆ**ï¼šåˆ†ç±»ä¸å†æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯RAGçš„è·¯ç”±èŠ‚ç‚¹ï¼ˆè¯­ä¹‰è·¯ç”±ï¼‰ã€‚
2. **å°æ¨¡å‹å´›èµ·**ï¼šå¯¹äºç‰¹å®šåˆ†ç±»ä»»åŠ¡ï¼Œå¾®è°ƒåçš„0.5Bæ¨¡å‹ï¼ˆå¦‚Qwen2-0.5Bï¼‰æ•ˆæœç”±äºé€šç”¨å¤§æ¨¡å‹ã€‚
3. **åˆæˆæ•°æ®å¢å¼º**ï¼šä½¿ç”¨GPT-4ç”Ÿæˆå›°éš¾æ ·æœ¬ï¼ˆHard Negativesï¼‰æ¥è®­ç»ƒå°æ¨¡å‹ï¼Œæˆä¸ºæ ‡å‡†èŒƒå¼ã€‚

### 3. åº”ç”¨å»ºè®®

- âœ… **ä»ç®€å•å¼€å§‹**ï¼šå…ˆç”¨ LLM API è·‘é€šæµç¨‹ï¼ŒéªŒè¯ä»·å€¼ã€‚
- âœ… **æ•°æ®ä¸ºç‹**ï¼šä¸å…¶çº ç»“æ¨¡å‹æ¶æ„ï¼Œä¸å¦‚æ¸…æ´—å‡ ç™¾æ¡é«˜è´¨é‡æ•°æ®ã€‚
- âœ… **æŒç»­ç›‘æ§**ï¼šåˆ†ç±»æ¨¡å‹ä¼šéšæ—¶é—´é€€åŒ–ï¼Œå¿…é¡»å»ºç«‹ Human-in-the-loop æœºåˆ¶ã€‚

### 4. å»¶ä¼¸é˜…è¯»

- **è®ºæ–‡**ï¼š[SetFit: Efficient Few-Shot Learning Without Prompts](https://arxiv.org/abs/2209.11055)
- **å·¥å…·**ï¼š[MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)
- **æ•™ç¨‹**ï¼š[Hugging Face Text Classification Guide](https://huggingface.co/docs/transformers/tasks/sequence_classification)

---

**ä¸‹ä¸€ç« é¢„å‘Šï¼š** [ç¬¬2ç« ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) åŸç†](../ç¬¬2ç« _æ£€ç´¢å¢å¼ºç”Ÿæˆ_RAG_åŸç†.md)

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥ RAG æ¶æ„ï¼Œæ¢è®¨å¦‚ä½•è®© LLM "å¤–æŒ‚å¤§è„‘"ï¼Œè§£å†³å¹»è§‰å’ŒçŸ¥è¯†æ—¶æ•ˆæ€§é—®é¢˜ã€‚
