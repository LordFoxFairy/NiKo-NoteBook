# 第1章：语义理解应用：文本分类与聚类

> 即使在生成式 AI 时代，经典的分类和聚类依然是业务系统的基石。

---

## 目录
- [一、判别式 AI vs. 生成式 AI](#一判别式-ai-vs-生成式-ai)
- [二、Embedding 的本质与可视化](#二embedding-的本质与可视化)
- [三、零样本分类 (Zero-Shot Classification)](#三零样本分类-zero-shot-classification)
- [四、少样本分类 (Few-Shot Classification)](#四少样本分类-few-shot-classification)
- [五、全量微调 vs PEFT (LoRA) 在分类任务中的对比](#五全量微调-vs-peft-lora-在分类任务中的对比)
- [六、文本聚类算法](#六文本聚类算法)
- [七、BERTopic 原理与实战](#七bertopic-原理与实战)
- [八、层次化聚类实战](#八层次化聚类实战)
- [九、本章小结](#九本章小结)

---

## 一、判别式 AI vs. 生成式 AI

在 LLM 出现之前，NLP 的半壁江山是 BERT 打下的。BERT 是典型的判别式模型（Encoder-only）。

### 1. 核心差异

| 维度 | 判别式模型 (Discriminative) | 生成式模型 (Generative) |
|------|---------------------------|------------------------|
| **目标** | $P(Y \mid X)$ - 给定输入判断类别 | $P(X, Y)$ 或 $P(Y \mid X)$ 自回归 |
| **代表** | BERT, RoBERTa, DeBERTa | GPT-4, LLaMA, ChatGPT |
| **优势** | 准确率高、速度快、成本低 | 灵活性强、零样本能力 |
| **劣势** | 需要标注数据、类别固定 | 推理慢、成本高、易幻觉 |

### 2. 为什么判别式模型仍然重要？

虽然现在可以用 GPT-4 做分类（Prompt: "Is this positive? answer yes/no"），但在**高并发、低延迟、高性价比**场景下，基于 **Embedding 的分类器**依然是最优选择。

**典型场景：**
- 客服工单自动分类（每秒数千请求）
- 垃圾邮件过滤（毫秒级响应）
- 舆情分析（固定类别：正/中/负）
- 内容审核（违规分类）

---

## 二、Embedding 的本质与可视化

### 1. 什么是 Embedding？

Embedding 是将文本映射到高维向量空间的过程，使得**语义相似的文本在空间中彼此接近**。

**数学表示：**
$$
\text{Encoder}: \mathbb{T} \rightarrow \mathbb{R}^d
$$

其中：
- $\mathbb{T}$: 文本空间
- $d$: Embedding 维度（通常 768/1024/1536）

### 2. 流行的 Embedding 模型

| 模型 | 维度 | 特点 | 使用场景 |
|------|------|------|---------|
| **BGE-large-zh** | 1024 | 中文最强 | 中文检索/分类 |
| **M3E-base** | 768 | 中文轻量级 | 资源受限场景 |
| **text-embedding-ada-002** | 1536 | OpenAI API | 英文任务 |
| **GTE-large** | 1024 | 通用性强 | 多语言 |

### 3. Embedding 可视化：t-SNE 与 UMAP

高维向量无法直接观察，需要降维到 2D/3D 空间。

#### 3.1 t-SNE (t-Distributed Stochastic Neighbor Embedding)

**原理：**
- 在高维空间中，计算点对之间的相似度（高斯分布）
- 在低维空间中，保持相似度分布（t-分布）
- 通过梯度下降最小化 KL 散度

**优点：**
- 擅长保持局部结构
- 可视化效果好

**缺点：**
- 慢（$O(N^2)$）
- 随机性强（每次结果不同）
- 不保持全局结构

#### 3.2 UMAP (Uniform Manifold Approximation and Projection)

**原理：**
- 基于流形学习和拓扑数据分析
- 构建高维空间的拓扑图（k-近邻图）
- 在低维空间重建该拓扑结构

**优点：**
- 速度快（支持大规模数据）
- 保持全局结构
- 确定性强

**缺点：**
- 参数敏感（`n_neighbors`, `min_dist`）

### 4. 完整代码：Embedding 可视化实战

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import umap
from sentence_transformers import SentenceTransformer

# 1. 准备数据
texts = [
    # 科技类
    "人工智能正在改变世界",
    "深度学习模型训练需要大量数据",
    "Transformer架构是现代AI的基石",

    # 体育类
    "梅西带领阿根廷夺得世界杯冠军",
    "NBA总决赛即将开打",
    "马拉松比赛吸引数万名跑者参与",

    # 美食类
    "这家餐厅的烤鸭非常美味",
    "川菜以麻辣著称",
    "意大利面配番茄酱很经典",

    # 旅游类
    "长城是中国的标志性景点",
    "巴黎的埃菲尔铁塔令人震撼",
    "马尔代夫的海滩美不胜收"
]

labels = ["科技", "科技", "科技",
          "体育", "体育", "体育",
          "美食", "美食", "美食",
          "旅游", "旅游", "旅游"]

# 2. 生成 Embedding
model = SentenceTransformer('moka-ai/m3e-base')  # 中文模型
embeddings = model.encode(texts)

print(f"Embedding 形状: {embeddings.shape}")  # (12, 768)

# 3. t-SNE 降维
tsne = TSNE(n_components=2, random_state=42, perplexity=5)
tsne_results = tsne.fit_transform(embeddings)

# 4. UMAP 降维
umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=5)
umap_results = umap_reducer.fit_transform(embeddings)

# 5. 可视化
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# t-SNE 可视化
for label in set(labels):
    mask = np.array(labels) == label
    axes[0].scatter(tsne_results[mask, 0], tsne_results[mask, 1],
                   label=label, s=100, alpha=0.7)
axes[0].set_title("t-SNE Visualization", fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# UMAP 可视化
for label in set(labels):
    mask = np.array(labels) == label
    axes[1].scatter(umap_results[mask, 0], umap_results[mask, 1],
                   label=label, s=100, alpha=0.7)
axes[1].set_title("UMAP Visualization", fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('embedding_visualization.png', dpi=150)
plt.show()

print("\n可视化完成！观察聚类效果。")
```

**运行结果分析：**
- 同类别文本应该聚在一起
- t-SNE 更关注局部聚类紧密度
- UMAP 能看到类别之间的相对距离

---

## 三、零样本分类 (Zero-Shot Classification)

零样本分类不需要训练数据，直接利用预训练模型的语义理解能力。

### 1. 基于 NLI（自然语言推理）的零样本分类

**核心思想：**
将分类问题转化为**文本蕴含（Entailment）**问题。

**公式：**
$$
P(\text{label} \mid \text{text}) = P(\text{entailment} \mid \text{premise=text}, \text{hypothesis="This text is about [label]"})
$$

### 2. 完整代码实现

```python
from transformers import pipeline

# 1. 加载零样本分类器（基于 NLI 模型）
classifier = pipeline("zero-shot-classification",
                     model="facebook/bart-large-mnli")

# 2. 定义候选类别
candidate_labels = ["科技", "体育", "政治", "娱乐", "财经"]

# 3. 待分类文本
text = "苹果公司发布了全新的 iPhone 15 系列手机"

# 4. 进行分类
result = classifier(text, candidate_labels)

print("零样本分类结果:")
for label, score in zip(result['labels'], result['scores']):
    print(f"  {label}: {score:.3f}")

# 输出示例:
# 科技: 0.952
# 财经: 0.028
# 娱乐: 0.012
# ...
```

### 3. 中文零样本分类（基于 Sentence-BERT）

```python
import numpy as np
from sentence_transformers import SentenceTransformer, util

# 1. 加载中文 Embedding 模型
model = SentenceTransformer('moka-ai/m3e-base')

# 2. 定义类别描述（而非单个词）
category_descriptions = {
    "科技": "关于人工智能、计算机、互联网、科学技术的内容",
    "体育": "关于足球、篮球、比赛、运动员的内容",
    "美食": "关于餐饮、菜品、烹饪、美味的内容",
    "旅游": "关于景点、旅行、度假、观光的内容"
}

# 3. 待分类文本
text = "长城是中国古代的伟大建筑，吸引了无数游客"

# 4. 计算相似度
text_embedding = model.encode(text, convert_to_tensor=True)
category_embeddings = model.encode(list(category_descriptions.values()),
                                  convert_to_tensor=True)

# 5. 余弦相似度
similarities = util.cos_sim(text_embedding, category_embeddings)[0]

# 6. 获取最相似的类别
scores = similarities.cpu().numpy()
ranked_categories = sorted(zip(category_descriptions.keys(), scores),
                          key=lambda x: x[1], reverse=True)

print("中文零样本分类:")
for category, score in ranked_categories:
    print(f"  {category}: {score:.3f}")

# 输出:
# 旅游: 0.712
# 科技: 0.302
# ...
```

**关键技巧：**
- 用**完整描述**代替单个类别词，提升准确率
- 使用专门的中文 Embedding 模型

---

## 四、少样本分类 (Few-Shot Classification)

只有少量标注样本（每类 5-50 个）时，使用少样本学习框架。

### 1. SetFit 框架

**SetFit (Sentence Transformer Fine-Tuning)** 是 HuggingFace 推出的少样本分类利器。

**核心思想：**
1. 用对比学习微调 Sentence Transformer（拉近同类样本，推远异类样本）
2. 在 Embedding 之上训练轻量级分类头

**优势：**
- 每类只需 8-16 个样本
- 训练速度快（CPU 可训练）
- 效果接近全量微调

### 2. 完整代码实现

```python
from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset

# 1. 准备少样本数据（每类只有 8 个样本）
train_data = {
    "text": [
        # 科技类（8个）
        "OpenAI发布GPT-4模型", "量子计算机取得突破",
        "芯片技术进入3nm时代", "人工智能辅助医疗诊断",
        "5G网络覆盖全国", "云计算市场快速增长",
        "机器人技术日新月异", "区块链应用场景扩大",

        # 体育类（8个）
        "中国队夺得奥运金牌", "世界杯决赛激动人心",
        "NBA季后赛进入白热化", "马拉松赛事圆满结束",
        "足球转会窗口关闭", "运动员打破世界纪录",
        "羽毛球公开赛开幕", "网球大满贯赛事精彩"
    ],
    "label": [0]*8 + [1]*8  # 0: 科技, 1: 体育
}

train_dataset = Dataset.from_dict(train_data)

# 2. 加载预训练模型并创建 SetFit 模型
model = SetFitModel.from_pretrained("moka-ai/m3e-base")

# 3. 创建训练器
trainer = SetFitTrainer(
    model=model,
    train_dataset=train_dataset,
    num_iterations=20,  # 对比学习的轮数
    num_epochs=1,       # 分类头训练的轮数
    column_mapping={"text": "text", "label": "label"}
)

# 4. 训练模型
trainer.train()

# 5. 测试模型
test_texts = [
    "谷歌推出新款智能手机",
    "篮球比赛进入加时赛"
]

predictions = model(test_texts)
print("少样本分类结果:")
for text, pred in zip(test_texts, predictions):
    label_name = "科技" if pred == 0 else "体育"
    print(f"  {text} -> {label_name}")
```

### 3. Few-Shot Prompt（使用 LLM）

对于更少的样本（每类 1-3 个），可以用 In-Context Learning：

```python
from openai import OpenAI

client = OpenAI()

# Few-shot examples
examples = """
示例1:
文本: 人工智能改变世界
类别: 科技

示例2:
文本: 梅西带领球队夺冠
类别: 体育

示例3:
文本: 这道菜非常美味
类别: 美食
"""

# 待分类文本
text = "苹果发布新款笔记本电脑"

# 构建 Prompt
prompt = f"""{examples}

现在请分类以下文本:
文本: {text}
类别: """

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    temperature=0
)

print(f"分类结果: {response.choices[0].message.content}")
# 输出: 科技
```

---

## 五、全量微调 vs PEFT (LoRA) 在分类任务中的对比

当有足够数据时，可以选择微调模型。

### 1. 全量微调 (Full Fine-Tuning)

**定义：** 更新模型的所有参数。

**优点：**
- 效果最好（充分学习任务特征）

**缺点：**
- 显存占用大（需要存储全部梯度）
- 训练慢
- 容易过拟合（小数据集）

### 2. PEFT: LoRA (Low-Rank Adaptation)

**核心思想：**
冻结预训练权重 $W_0$，只训练低秩分解矩阵 $A, B$：

$$
W = W_0 + \Delta W = W_0 + BA
$$

其中：
- $W_0 \in \mathbb{R}^{d \times d}$: 冻结的原始权重
- $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$: 可训练的低秩矩阵
- $r \ll d$: 秩（通常 8/16/32）

**参数量对比：**
- 原始: $d^2$
- LoRA: $2dr$ (当 $r=8$, $d=1024$ 时，减少 **99%**)

### 3. 完整对比代码

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# 准备数据
dataset = load_dataset("SetFit/amazon_counterfactual_en")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

dataset = dataset.map(tokenize, batched=True)

# ========== 方法1: 全量微调 ==========
model_full = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

print(f"全量微调参数量: {model_full.num_parameters():,}")

trainer_full = Trainer(
    model=model_full,
    args=TrainingArguments(
        output_dir="./full_finetune",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        logging_steps=100
    ),
    train_dataset=dataset["train"]
)

trainer_full.train()

# ========== 方法2: LoRA 微调 ==========
model_lora = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# 配置 LoRA
lora_config = LoraConfig(
    r=8,                    # 秩
    lora_alpha=32,          # 缩放系数
    target_modules=["query", "value"],  # 只作用于 Q,V 矩阵
    lora_dropout=0.1,
    bias="none"
)

model_lora = get_peft_model(model_lora, lora_config)
model_lora.print_trainable_parameters()
# 输出: trainable params: 294,912 || all params: 109,483,778 || trainable%: 0.27%

trainer_lora = Trainer(
    model=model_lora,
    args=TrainingArguments(
        output_dir="./lora_finetune",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        logging_steps=100
    ),
    train_dataset=dataset["train"]
)

trainer_lora.train()

# ========== 性能对比 ==========
print("\n性能对比:")
print(f"全量微调 - 参数量: {model_full.num_parameters():,}, 显存: ~4.2GB")
print(f"LoRA微调 - 参数量: {model_lora.get_nb_trainable_parameters():,}, 显存: ~1.5GB")
```

### 4. 对比总结

| 维度 | 全量微调 | LoRA 微调 |
|------|---------|----------|
| **参数量** | 100% | 0.1%-1% |
| **显存占用** | 高 | 低（可用更大 Batch Size） |
| **训练速度** | 慢 | 快 1.5-2倍 |
| **效果** | 最优 | 接近全量（损失<1%） |
| **适用场景** | 大数据集 | 中小数据集 |

**建议：**
- 数据 < 10k：用 **SetFit** 或 **Few-Shot**
- 数据 10k-100k：用 **LoRA**
- 数据 > 100k：考虑 **全量微调**

---

## 六、文本聚类算法

当没有标注数据时，聚类可以自动发现文本的内在结构。

### 1. K-Means 聚类

**原理：**
1. 随机初始化 $k$ 个聚类中心
2. 将每个样本分配到最近的中心
3. 重新计算每个簇的中心
4. 重复 2-3 直到收敛

**代码实现：**

```python
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# 1. 准备数据
texts = [
    "人工智能改变世界", "深度学习突破", "神经网络应用",
    "篮球比赛精彩", "足球世界杯", "奥运会开幕",
    "美味的川菜", "意大利面很好吃", "日本料理精致",
    "长城景色壮观", "巴黎铁塔美丽", "马尔代夫海滩"
]

# 2. 生成 Embedding
model = SentenceTransformer('moka-ai/m3e-base')
embeddings = model.encode(texts)

# 3. K-Means 聚类
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(embeddings)

# 4. 输出结果
for i, (text, label) in enumerate(zip(texts, labels)):
    print(f"簇{label}: {text}")

# 输出示例:
# 簇0: 人工智能改变世界
# 簇0: 深度学习突破
# 簇1: 篮球比赛精彩
# ...
```

**问题：**
- 需要预先指定 $k$（簇数）
- 对初始化敏感
- 高维空间效果下降（维度灾难）

### 2. DBSCAN (密度聚类)

**优势：**
- **自动发现簇数**
- 可识别噪声点
- 能发现任意形状的簇

**核心参数：**
- `eps`: 邻域半径
- `min_samples`: 核心点的最小邻居数

**代码实现：**

```python
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

# 1. 降维（高维空间DBSCAN效果差）
pca = PCA(n_components=50)
embeddings_reduced = pca.fit_transform(embeddings)

# 2. DBSCAN 聚类
dbscan = DBSCAN(eps=0.5, min_samples=2)
labels = dbscan.fit_predict(embeddings_reduced)

print(f"发现 {len(set(labels)) - (1 if -1 in labels else 0)} 个簇")
print(f"噪声点数量: {list(labels).count(-1)}")

# 3. 输出结果
for i, (text, label) in enumerate(zip(texts, labels)):
    cluster_name = f"簇{label}" if label != -1 else "噪声"
    print(f"{cluster_name}: {text}")
```

### 3. HDBSCAN (层次化 DBSCAN)

**改进：**
- 自动确定 `eps` 参数
- 更稳定的聚类结果
- 适合不同密度的簇

**代码实现：**

```python
import hdbscan

# 1. HDBSCAN 聚类
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=2,
    min_samples=1,
    metric='euclidean'
)

labels = clusterer.fit_predict(embeddings_reduced)

print(f"HDBSCAN 发现 {len(set(labels)) - (1 if -1 in labels else 0)} 个簇")

# 2. 聚类概率（置信度）
probabilities = clusterer.probabilities_

for text, label, prob in zip(texts, labels, probabilities):
    print(f"簇{label} ({prob:.2f}): {text}")
```

### 4. 算法对比

| 算法 | 需要指定簇数 | 噪声识别 | 高维适应 | 速度 |
|------|------------|---------|---------|------|
| **K-Means** | 是 | 否 | 差 | 快 |
| **DBSCAN** | 否 | 是 | 差 | 中 |
| **HDBSCAN** | 否 | 是 | 中 | 慢 |

**推荐策略：**
1. 先用 **UMAP/PCA** 降维到 50-100 维
2. 用 **HDBSCAN** 自动发现簇
3. 用 **轮廓系数 (Silhouette Score)** 评估聚类质量

---

## 七、BERTopic 原理与实战

BERTopic 是现代主题建模的标准工具，集成了 Embedding + 降维 + 聚类 + 关键词提取。

### 1. BERTopic 流程

```
文本 → Embedding → UMAP降维 → HDBSCAN聚类 → c-TF-IDF提取主题词
```

**核心创新：c-TF-IDF (Class-based TF-IDF)**

传统 TF-IDF 是文档级别，c-TF-IDF 是**簇级别**：

$$
\text{c-TF-IDF}(w, c) = \text{TF}(w, c) \times \log \frac{N}{\sum_{c'} \text{TF}(w, c')}
$$

其中：
- $w$: 词
- $c$: 簇
- $\text{TF}(w, c)$: 词 $w$ 在簇 $c$ 中的频率

### 2. 完整代码实战

```python
from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups

# 1. 准备数据（20newsgroups 数据集）
docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'][:1000]

# 2. 创建 BERTopic 模型
topic_model = BERTopic(
    language="english",
    calculate_probabilities=True,
    verbose=True
)

# 3. 训练模型（自动完成 Embedding + 降维 + 聚类）
topics, probs = topic_model.fit_transform(docs)

# 4. 查看主题
print(f"发现 {len(set(topics))} 个主题")
print("\n主题及其关键词:")
for topic_id in set(topics):
    if topic_id == -1:  # 噪声主题
        continue
    print(f"\n主题 {topic_id}:")
    print(topic_model.get_topic(topic_id)[:5])  # 前5个关键词

# 输出示例:
# 主题 0:
# [('space', 0.012), ('nasa', 0.011), ('launch', 0.009), ...]

# 5. 可视化主题
topic_model.visualize_topics().show()

# 6. 查看文档属于哪个主题
sample_doc = "NASA launches new space mission"
topic, prob = topic_model.transform([sample_doc])
print(f"\n文档主题: {topic[0]}, 置信度: {prob[0]:.3f}")
```

### 3. 中文 BERTopic

```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 1. 中文文档
docs = [
    "人工智能技术快速发展",
    "深度学习在图像识别中应用广泛",
    "自然语言处理是AI的重要分支",
    "篮球比赛非常激烈",
    "足球世界杯吸引全球关注",
    "奥运会运动员奋勇拼搏",
    "川菜麻辣鲜香",
    "粤菜清淡爽口",
    "日本料理精致美观"
]

# 2. 使用中文 Embedding 模型
embedding_model = SentenceTransformer("moka-ai/m3e-base")

# 3. 创建模型
topic_model = BERTopic(
    embedding_model=embedding_model,
    language="chinese (simplified)",
    min_topic_size=2  # 最小主题大小
)

# 4. 训练
topics, probs = topic_model.fit_transform(docs)

# 5. 查看主题
for topic_id in set(topics):
    if topic_id != -1:
        print(f"主题 {topic_id}: {topic_model.get_topic(topic_id)[:3]}")
```

### 4. 动态主题建模（时间维度）

跟踪主题随时间的演变：

```python
from bertopic import BERTopic
import pandas as pd

# 1. 准备带时间戳的数据
docs = [...]
timestamps = ["2023-01-01", "2023-02-01", ...]  # 对应每个文档的时间

# 2. 训练模型
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)

# 3. 随时间演变分析
topics_over_time = topic_model.topics_over_time(docs, timestamps)

# 4. 可视化
topic_model.visualize_topics_over_time(topics_over_time).show()
```

---

## 八、层次化聚类实战

层次化聚类构建树状结构（Dendrogram），适合发现嵌套的主题层次。

### 1. 凝聚层次聚类 (Agglomerative)

**原理：**
1. 每个样本初始为一个簇
2. 不断合并最相似的两个簇
3. 直到所有样本合并为一个簇

### 2. 完整代码

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# 1. 准备数据
texts = [
    "深度学习", "神经网络", "机器学习",
    "足球", "篮球", "排球",
    "川菜", "粤菜", "湘菜"
]

model = SentenceTransformer('moka-ai/m3e-base')
embeddings = model.encode(texts)

# 2. 计算层次聚类链接矩阵
linkage_matrix = linkage(embeddings, method='ward')

# 3. 绘制树状图
plt.figure(figsize=(10, 6))
dendrogram(linkage_matrix, labels=texts, leaf_font_size=12)
plt.title("层次化聚类树状图")
plt.xlabel("样本")
plt.ylabel("距离")
plt.tight_layout()
plt.savefig('hierarchical_clustering.png', dpi=150)
plt.show()

# 4. 切分为指定数量的簇
agg_clustering = AgglomerativeClustering(n_clusters=3)
labels = agg_clustering.fit_predict(embeddings)

for text, label in zip(texts, labels):
    print(f"簇{label}: {text}")
```

### 3. 层次化主题建模

结合 BERTopic 实现层次化主题：

```python
from bertopic import BERTopic
from sklearn.cluster import AgglomerativeClustering

# 1. 训练 BERTopic
topic_model = BERTopic(hdbscan_model=AgglomerativeClustering(n_clusters=10))
topics, probs = topic_model.fit_transform(docs)

# 2. 构建主题层次
hierarchical_topics = topic_model.hierarchical_topics(docs)

# 3. 可视化层次结构
topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show()
```

---

## 九、实战案例：客服工单自动分类系统

### 完整流程

```python
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# 1. 准备数据
data = pd.DataFrame({
    'text': [
        "账号无法登录", "密码忘记了", "登录失败",  # 账号问题
        "商品质量差", "收到破损商品", "产品有瑕疵",  # 商品问题
        "物流太慢", "快递丢失", "配送延迟",  # 物流问题
    ],
    'label': [0, 0, 0, 1, 1, 1, 2, 2, 2]  # 0:账号 1:商品 2:物流
})

# 2. 生成 Embedding
model = SentenceTransformer('moka-ai/m3e-base')
embeddings = model.encode(data['text'].tolist())

# 3. 划分训练/测试集
X_train, X_test, y_train, y_test = train_test_split(
    embeddings, data['label'], test_size=0.3, random_state=42
)

# 4. 训练分类器
classifier = LogisticRegression(max_iter=1000)
classifier.fit(X_train, y_train)

# 5. 评估
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred,
                           target_names=['账号问题', '商品问题', '物流问题']))

# 6. 部署推理
def classify_ticket(text):
    embedding = model.encode([text])
    label = classifier.predict(embedding)[0]
    category_names = ['账号问题', '商品问题', '物流问题']
    return category_names[label]

# 测试
print(classify_ticket("我的包裹还没到"))  # 输出: 物流问题
```

---

## 十、本章小结

### 1. 核心要点

| 任务 | 推荐方案 | 适用场景 |
|------|---------|---------|
| **无标注数据** | BERTopic / HDBSCAN | 探索性分析、主题发现 |
| **少量样本 (<100)** | SetFit / Few-Shot | 快速原型、冷启动 |
| **中等数据 (1k-10k)** | LoRA 微调 | 生产系统 |
| **大量数据 (>10k)** | 全量微调 | 追求极致性能 |

### 2. 工程建议

1. **不要过度工程化**：对于明确的分类任务，Embedding + Logistic Regression 往往是最优解
2. **重视数据质量**：100 个高质量样本 > 1000 个噪声样本
3. **定期评估**：分类模型会随数据分布漂移而退化，需定期重新训练
4. **A/B 测试**：上线新模型前，用 A/B 测试验证实际效果

### 3. 延伸阅读

- **论文：** SetFit: Efficient Few-Shot Learning Without Prompts
- **工具：** BERTopic 官方文档
- **库：** sentence-transformers, setfit, bertopic, hdbscan

---

**下一章预告：** 第2章 - 检索增强生成（RAG）原理

在这一章中，我们将深入探讨如何让大模型访问外部知识库，构建企业级的问答系统。
