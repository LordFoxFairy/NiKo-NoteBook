# ç¬¬4ç« ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹åŸç†

> **æ ¸å¿ƒå®šä½**ï¼šç†è§£æ–‡æœ¬-å›¾åƒç­‰å¤šæ¨¡æ€äº¤äº’çš„æ ¸å¿ƒæŠ€æœ¯ï¼ˆCLIPã€ViTã€LLaVAï¼‰
>
> **è¾¹ç•Œçº¦æŸ**ï¼š
> - âœ… åŒ…å«ï¼šCLIP å¯¹æ¯”å­¦ä¹ ã€ViT æ¶æ„ã€LLaVA è¿æ¥å™¨ã€å¤šæ¨¡æ€æ¨ç†å®æˆ˜
> - âŒ ä¸åŒ…å«ï¼šTransformer åŸºç¡€æœºåˆ¶ï¼ˆå·²åœ¨ Part 2 ç¬¬1ç« ï¼‰ã€å¯¹æ¯”å­¦ä¹ åŸºç¡€ç†è®ºï¼ˆå·²åœ¨ Part 3 ç¬¬4ç« ï¼‰

---

## ç›®å½•

1. [å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£ï¼šå›¾åƒä½œä¸º"å¤–è¯­"](#ä¸€å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£å›¾åƒä½œä¸ºå¤–è¯­)
2. [ç»Ÿä¸€ Token åŒ–ï¼šOmni æ¨¡å‹çš„åŸºçŸ³](#äºŒç»Ÿä¸€-token-åŒ–omni-æ¨¡å‹çš„åŸºçŸ³)
3. [è§†è§‰ç¼–ç å™¨ï¼šVision Transformer (ViT)](#ä¸‰è§†è§‰ç¼–ç å™¨vision-transformer-vit)
4. [å›¾æ–‡å¯¹é½ï¼šCLIP](#å››å›¾æ–‡å¯¹é½clip)
5. [å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼šLLaVA](#äº”å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„llava)
6. [è§†é¢‘ç†è§£ï¼šVideo as Frames](#å…­è§†é¢‘ç†è§£video-as-frames)
7. [å®æˆ˜ï¼šå¤šæ¨¡æ€ç†è§£åº”ç”¨](#ä¸ƒå®æˆ˜å¤šæ¨¡æ€ç†è§£åº”ç”¨)
8. [2025è§†è§’ï¼šConnector vs Native Multimodal](#å…«2025è§†è§’connector-vs-native-multimodal)
9. [æ€»ç»“ä¸å±•æœ›](#ä¹æ€»ç»“ä¸å±•æœ›)

---

## ä¸€ã€å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£ï¼šå›¾åƒä½œä¸º"å¤–è¯­"

### 1.1 Token Space Alignmentï¼šä¸ºä»€ä¹ˆå›¾åƒå¯ä»¥è¢«è§†ä¸º"å¤–è¯­"

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªåªæ‡‚ä¸­æ–‡çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°åœ¨ï¼Œæœ‰äººæ‹¿ç€ä¸€å¼ å›¾ç‰‡ï¼Œç”¨ä¸€ç§ä½ ä»æœªè§è¿‡çš„è¯­è¨€ï¼ˆ"å›¾åƒè¯­"ï¼‰å‘ä½ æè¿°ã€‚ä½ è¯¥æ€ä¹ˆåŠï¼Ÿ

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šLLM åªç†è§£æ–‡æœ¬ Tokenï¼Œè€Œå›¾åƒæ˜¯åƒç´ çŸ©é˜µã€‚å°±åƒä¸­æ–‡å’Œè‹±æ–‡ä¸€æ ·ï¼Œå®ƒä»¬æ˜¯**ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„"è¯­è¨€ç©ºé—´"**ã€‚

**è§£å†³æ–¹æ¡ˆï¼šè·¨æ¨¡æ€å¯¹é½ï¼ˆCross-Modal Alignmentï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å›¾åƒç©ºé—´    â”‚                    â”‚  æ–‡æœ¬ç©ºé—´    â”‚
â”‚  (åƒç´ çŸ©é˜µ)  â”‚                    â”‚  (Token åºåˆ—) â”‚
â”‚             â”‚                    â”‚             â”‚
â”‚   [255, 0]  â”‚                    â”‚ "ä¸€åªçŒ«"     â”‚
â”‚   [128, 64] â”‚                    â”‚ "åœ¨è‰åœ°ä¸Š"   â”‚
â”‚   [...]     â”‚                    â”‚ "èººç€"       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â”‚    é€šè¿‡å¯¹é½è®­ç»ƒ                   â”‚
       â”‚    (CLIPã€LLaVA ç­‰)              â”‚
       â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å…±äº«è¯­ä¹‰ç©ºé—´ (Shared Latent Space)   â”‚
â”‚                                            â”‚
â”‚   "çŒ«" â‰ˆ [0.8, -0.3, 0.5, ...]           â”‚
â”‚   ğŸ±   â‰ˆ [0.82, -0.28, 0.51, ...]        â”‚
â”‚                                            â”‚
â”‚   è·ç¦»å¾ˆè¿‘ â†’ è¯­ä¹‰ç›¸ä¼¼ï¼                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. **å›¾åƒç¼–ç å™¨**ï¼šå°†å›¾åƒç¿»è¯‘æˆå‘é‡ï¼ˆå°±åƒæŠŠè‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼‰
2. **æ–‡æœ¬ç¼–ç å™¨**ï¼šå°†æ–‡æœ¬ä¹Ÿç¿»è¯‘æˆå‘é‡
3. **å¯¹é½è®­ç»ƒ**ï¼šè®©æè¿°åŒä¸€äº‹ç‰©çš„å›¾åƒå’Œæ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­**é è¿‘**

### 1.2 æ•°å­¦æœ¬è´¨ï¼šä½™å¼¦ç›¸ä¼¼åº¦

å‡è®¾æˆ‘ä»¬æœ‰ä¸€å¼ çŒ«çš„å›¾ç‰‡ $I$ å’Œæ–‡æœ¬ "a photo of a cat" $T$ã€‚

**ç¼–ç è¿‡ç¨‹**ï¼š
$$
\mathbf{v}_{\text{image}} = E_{\text{vision}}(I) \in \mathbb{R}^d
$$
$$
\mathbf{v}_{\text{text}} = E_{\text{text}}(T) \in \mathbb{R}^d
$$

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼š
$$
\text{sim}(\mathbf{v}_{\text{image}}, \mathbf{v}_{\text{text}}) = \frac{\mathbf{v}_{\text{image}} \cdot \mathbf{v}_{\text{text}}}{\|\mathbf{v}_{\text{image}}\| \|\mathbf{v}_{\text{text}}\|} \in [-1, 1]
$$

- **æ¥è¿‘ 1**ï¼šé«˜åº¦ç›¸å…³ï¼ˆå›¾ç‰‡ç¡®å®æ˜¯çŒ«ï¼‰
- **æ¥è¿‘ 0**ï¼šæ— å…³ï¼ˆå›¾ç‰‡å¯èƒ½æ˜¯ç‹—ï¼‰
- **æ¥è¿‘ -1**ï¼šè´Ÿç›¸å…³ï¼ˆå®é™…åº”ç”¨ä¸­è¾ƒå°‘è§ï¼‰

**ç›´è§‰ç†è§£**ï¼š
- å°±åƒåœ¨é«˜ç»´ç©ºé—´ä¸­æµ‹é‡ä¸¤ä¸ªå‘é‡çš„å¤¹è§’
- å¤¹è§’è¶Šå°ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼

---

## äºŒã€ç»Ÿä¸€ Token åŒ–ï¼šOmni æ¨¡å‹çš„åŸºçŸ³

> **æ ¸å¿ƒå®šä½**ï¼šç†è§£ GPT-4o æ—¶ä»£çš„ "Omni" ç†å¿µ - å¦‚ä½•è®© LLM åƒå¤„ç†æ–‡æœ¬ä¸€æ ·å¤„ç†å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚

### 2.1 ä»è¿ç»­åˆ°ç¦»æ•£ï¼šä¸ºä»€ä¹ˆéœ€è¦ Token åŒ–

**æ ¸å¿ƒé—®é¢˜**ï¼šè¯­è¨€æ¨¡å‹åªç†è§£ç¦»æ•£çš„ Tokenï¼ˆå¦‚æ–‡å­—ï¼‰ï¼Œä½†å›¾åƒã€éŸ³é¢‘æ˜¯è¿ç»­ä¿¡å·ã€‚

```
æ–‡æœ¬ï¼šå¤©ç”Ÿç¦»æ•£
"æˆ‘çˆ±çŒ«" â†’ ["æˆ‘", "çˆ±", "çŒ«"] â†’ [101, 203, 456] (Token IDs)
         âœ… LLM å¯ä»¥ç›´æ¥å¤„ç†

å›¾åƒï¼šè¿ç»­ä¿¡å·
[[[255, 0, 128], [64, 32, 200], ...]]  # åƒç´ çŸ©é˜µ
âŒ LLM æ— æ³•ç›´æ¥å¤„ç† â†’ éœ€è¦è½¬æ¢ä¸ºç¦»æ•£ Token

éŸ³é¢‘ï¼šè¿ç»­ä¿¡å·
[0.023, -0.145, 0.089, ...]  # æ³¢å½¢é‡‡æ ·ç‚¹
âŒ LLM æ— æ³•ç›´æ¥å¤„ç† â†’ éœ€è¦è½¬æ¢ä¸ºç¦»æ•£ Token
```

**Omni æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ‰€æœ‰æ¨¡æ€ç»Ÿä¸€åˆ°ç¦»æ•£ Token ç©ºé—´ï¼Œè®© LLM ç”¨åŒä¸€å¥—æœºåˆ¶å¤„ç†æ‰€æœ‰ä¿¡æ¯ã€‚

### 2.2 è§†è§‰ Token åŒ–ï¼šVQ-VAEï¼ˆVector Quantized Variational AutoEncoderï¼‰

> VQ-VAE æ˜¯å°†è¿ç»­å›¾åƒè½¬æ¢ä¸ºç¦»æ•£ Token çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äº DALL-Eã€Parti ç­‰ç”Ÿæˆæ¨¡å‹ã€‚

#### 2.2.1 VQ-VAE æ ¸å¿ƒåŸç†

**æ¶æ„æµç¨‹**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       VQ-VAE æ¶æ„                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥å›¾åƒ                     ç¼–ç å™¨                    é‡åŒ–å™¨
[256Ã—256Ã—3]                                           Codebook
    â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                              â”‚ eâ‚€=[...]  â”‚
    â–¼                                              â”‚ eâ‚=[...]  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ eâ‚‚=[...]  â”‚
â”‚         â”‚        â”‚          â”‚  è¿ç»­ç¼–ç            â”‚   ...     â”‚
â”‚  åŸå§‹   â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚  Encoder â”‚ [32Ã—32Ã—256]        â”‚ eâ‚ˆâ‚â‚‰â‚=[..]â”‚
â”‚  å›¾åƒ   â”‚        â”‚   (CNN)  â”‚      â”‚             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
â”‚         â”‚        â”‚          â”‚      â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                   â”‚
                                     â–¼                   â–¼
                                æŸ¥æ‰¾æœ€è¿‘çš„ Codebook å‘é‡
                                     â”‚
                                     â–¼
                          ç¦»æ•£ Token åºåˆ—
                          [142, 783, 45, 892, ...]
                          [1024 ä¸ª Token IDs]
                                     â”‚
                                     â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Decoder    â”‚
                          â”‚    (CNN)     â”‚â”€â”€â”€â”€â”€â”€â”€> é‡å»ºå›¾åƒ
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         [256Ã—256Ã—3]
```

**å·¥ä½œæµç¨‹**ï¼š

1. **Encoder**ï¼šå°†å›¾åƒ $(256 \times 256 \times 3)$ å‹ç¼©ä¸ºä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ $(32 \times 32 \times 256)$
2. **Quantization**ï¼šå°†æ¯ä¸ªç‰¹å¾å‘é‡ $(256ç»´)$ æ˜ å°„åˆ°æœ€è¿‘çš„ Codebook å‘é‡
3. **ç¦»æ•£åŒ–**ï¼šå¾—åˆ° $32 \times 32 = 1024$ ä¸ªç¦»æ•£ Token
4. **Decoder**ï¼šä»ç¦»æ•£ Token é‡å»ºå›¾åƒï¼ˆè®­ç»ƒæ—¶ç”¨äºä¼˜åŒ–ï¼‰

#### 2.2.2 Codebook çš„å·¥ä½œæœºåˆ¶

**Codebook**ï¼šé¢„å®šä¹‰çš„å‘é‡å­—å…¸ï¼Œç±»ä¼¼äº"è§†è§‰è¯æ±‡è¡¨"ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VectorQuantizer(nn.Module):
    """VQ-VAE çš„é‡åŒ–å±‚"""
    def __init__(self, num_embeddings=8192, embedding_dim=256):
        """
        Args:
            num_embeddings: Codebook å¤§å°ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼‰
            embedding_dim: æ¯ä¸ªå‘é‡çš„ç»´åº¦
        """
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim

        # Codebook: [8192, 256] - 8192 ä¸ª 256 ç»´çš„å‘é‡
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

    def forward(self, z):
        """
        Args:
            z: ç¼–ç å™¨è¾“å‡º [B, H, W, D] ä¾‹å¦‚ [B, 32, 32, 256]
        Returns:
            quantized: é‡åŒ–åçš„ç‰¹å¾ [B, H, W, D]
            token_ids: ç¦»æ•£ Token IDs [B, H, W]
        """
        B, H, W, D = z.shape

        # 1. å±•å¹³ç©ºé—´ç»´åº¦: [B, H, W, D] -> [B*H*W, D]
        z_flat = z.reshape(-1, D)

        # 2. è®¡ç®—ä¸æ‰€æœ‰ Codebook å‘é‡çš„è·ç¦»
        # |z - e|Â² = |z|Â² + |e|Â² - 2zÂ·e
        distances = (
            torch.sum(z_flat**2, dim=1, keepdim=True) +  # [B*H*W, 1]
            torch.sum(self.embedding.weight**2, dim=1) -  # [8192]
            2 * torch.matmul(z_flat, self.embedding.weight.t())  # [B*H*W, 8192]
        )

        # 3. æ‰¾åˆ°æœ€è¿‘çš„ Codebook å‘é‡ï¼ˆè´ªå¿ƒåŒ¹é…ï¼‰
        token_ids = torch.argmin(distances, dim=1)  # [B*H*W]

        # 4. æŸ¥è¡¨è·å–é‡åŒ–åçš„å‘é‡
        quantized_flat = self.embedding(token_ids)  # [B*H*W, D]

        # 5. æ¢å¤ç©ºé—´ç»´åº¦
        quantized = quantized_flat.view(B, H, W, D)  # [B, H, W, D]
        token_ids = token_ids.view(B, H, W)  # [B, H, W]

        # 6. Straight-through estimatorï¼ˆè®­ç»ƒæŠ€å·§ï¼‰
        # å‰å‘ä¼ æ’­ï¼šä½¿ç”¨é‡åŒ–åçš„å‘é‡
        # åå‘ä¼ æ’­ï¼šæ¢¯åº¦ç›´æ¥ä¼ ç»™ç¼–ç å™¨
        quantized = z + (quantized - z).detach()

        return quantized, token_ids

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    vq = VectorQuantizer(num_embeddings=8192, embedding_dim=256)

    # æ¨¡æ‹Ÿç¼–ç å™¨è¾“å‡º
    z = torch.randn(2, 32, 32, 256)  # batch=2, å›¾åƒç¼–ç ä¸º 32Ã—32 çš„ç‰¹å¾å›¾

    quantized, token_ids = vq(z)

    print(f"è¾“å…¥: {z.shape}")
    print(f"é‡åŒ–å: {quantized.shape}")
    print(f"Token IDs: {token_ids.shape}")
    print(f"Token èŒƒå›´: [{token_ids.min()}, {token_ids.max()}]")

    # è¾“å‡º:
    # è¾“å…¥: torch.Size([2, 32, 32, 256])
    # é‡åŒ–å: torch.Size([2, 32, 32, 256])
    # Token IDs: torch.Size([2, 32, 32])
    # Token èŒƒå›´: [0, 8191]
```

**ä¸ºä»€ä¹ˆè¿™æ ·æœ‰æ•ˆ**ï¼Ÿ

```
å›¾åƒ: [256Ã—256Ã—3] â†’ Encoder â†’ [32Ã—32Ã—256] â†’ VQ â†’ [32Ã—32] Token IDs
                                 â†‘
                          æ¯ä¸ªä½ç½®é€‰æ‹© 8192 ä¸ªå€™é€‰ä¸­æœ€åŒ¹é…çš„ Token

æœ€ç»ˆï¼šå›¾åƒè¢«è¡¨ç¤ºä¸º 1024 ä¸ªç¦»æ•£ Tokenï¼ˆå°±åƒ 1024 ä¸ªå•è¯ï¼ï¼‰
```

#### 2.2.3 è§†è§‰ Token åŒ–çš„ç›´è§‚ç†è§£

```
åŸå§‹å›¾åƒ                 VQ-VAE Token åŒ–              æ–‡æœ¬ç±»æ¯”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ±        â”‚   â†’     â”‚ [142, 783]  â”‚   â‰ˆ         â”‚ "ä¸€åªçŒ«"     â”‚
â”‚             â”‚         â”‚ [45, 892]   â”‚              â”‚             â”‚
â”‚             â”‚         â”‚ [234, 1023] â”‚              â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  åƒç´ çŸ©é˜µ                 ç¦»æ•£ Token                   æ–‡æœ¬ Token

ç°åœ¨ï¼ŒLLM å¯ä»¥ç”¨åŒæ ·çš„ Transformer å¤„ç†è¿™ä¸¤ç§ Tokenï¼
```

### 2.3 éŸ³é¢‘ Token åŒ–ï¼šAudio Codec

> éŸ³é¢‘çš„ Token åŒ–ä¸å›¾åƒç±»ä¼¼ï¼Œä½†ä½¿ç”¨ä¸“é—¨çš„éŸ³é¢‘ç¼–è§£ç å™¨ã€‚

#### 2.3.1 å¸¸ç”¨éŸ³é¢‘ Codec

**1. EnCodecï¼ˆMetaï¼Œ2022ï¼‰**
- å°†éŸ³é¢‘å‹ç¼©ä¸ºç¦»æ•£ Token
- æ”¯æŒå¤šç§ç ç‡ï¼ˆ1.5-12 kbpsï¼‰
- åº”ç”¨ï¼šAudioLMã€MusicGen

**2. SoundStreamï¼ˆGoogleï¼Œ2021ï¼‰**
- é«˜è´¨é‡éŸ³é¢‘å‹ç¼©
- åº”ç”¨ï¼šAudioPaLM

**æ¶æ„æµç¨‹**ï¼ˆä»¥ EnCodec ä¸ºä¾‹ï¼‰ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EnCodec éŸ³é¢‘ Token åŒ–                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åŸå§‹éŸ³é¢‘æ³¢å½¢                ç¼–ç å™¨                  é‡åŒ–å™¨
[1ç§’ @ 24kHz]                                      Codebook
= 24000 é‡‡æ ·ç‚¹                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                            â”‚ eâ‚€=[...]  â”‚
    â–¼                                            â”‚ eâ‚=[...]  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   ...     â”‚
â”‚  éŸ³é¢‘   â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚  Conv    â”‚  å‹ç¼©ç‰¹å¾        â”‚ eâ‚â‚€â‚‚â‚ƒ=[..]â”‚
â”‚  æ³¢å½¢   â”‚        â”‚  Encoder â”‚  [75, 128]      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                â”‚
                                     â–¼                â–¼
                              é‡åŒ–ä¸ºç¦»æ•£ Token
                                     â”‚
                                     â–¼
                          [523, 12, 945, 234, ...]
                          [75 ä¸ª Token / ç§’]
                                     â”‚
                                     â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Decoder    â”‚
                          â”‚   (Conv)     â”‚â”€â”€â”€â”€â”€â”€â”€> é‡å»ºéŸ³é¢‘
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å‚æ•°**ï¼š
- **é™é‡‡æ ·å€ç‡**ï¼š320å€ï¼ˆ24000 Hz â†’ 75 Token/ç§’ï¼‰
- **Codebook å¤§å°**ï¼š1024ï¼ˆ10 ä½ï¼‰
- **å‹ç¼©ç‡**ï¼š1 ç§’éŸ³é¢‘ â‰ˆ 75 ä¸ª Token

#### 2.3.2 éŸ³é¢‘ Token åŒ–ç¤ºä¾‹ä»£ç 

```python
"""
ä½¿ç”¨ EnCodec è¿›è¡ŒéŸ³é¢‘ Token åŒ–
éœ€è¦: pip install encodec
"""
import torch
import torchaudio
from encodec import EncodecModel
from encodec.utils import convert_audio

# 1. åŠ è½½é¢„è®­ç»ƒçš„ EnCodec æ¨¡å‹
model = EncodecModel.encodec_model_24khz()
model.set_target_bandwidth(6.0)  # è®¾ç½®ç›®æ ‡ç ç‡ 6 kbps

# 2. åŠ è½½éŸ³é¢‘æ–‡ä»¶
wav, sr = torchaudio.load("audio.wav")

# 3. è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼ˆ24kHz, monoï¼‰
wav = convert_audio(wav, sr, model.sample_rate, model.channels)
wav = wav.unsqueeze(0)  # [1, channels, time]

# 4. ç¼–ç ä¸ºç¦»æ•£ Token
with torch.no_grad():
    encoded_frames = model.encode(wav)

# 5. æå– Token IDs
# encoded_frames æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« [codes, scale]
# codes: [batch, num_quantizers, time]
codes = encoded_frames[0][0]  # [1, num_quantizers, time]

print(f"åŸå§‹éŸ³é¢‘: {wav.shape[2]} é‡‡æ ·ç‚¹ ({wav.shape[2]/model.sample_rate:.2f} ç§’)")
print(f"Token åºåˆ—: {codes.shape}")
print(f"å‹ç¼©ç‡: {wav.shape[2] / codes.shape[2]:.1f}x")

# è¾“å‡ºç¤ºä¾‹:
# åŸå§‹éŸ³é¢‘: 48000 é‡‡æ ·ç‚¹ (2.00 ç§’)
# Token åºåˆ—: torch.Size([1, 8, 150])
# å‹ç¼©ç‡: 320.0x

# 6. è§£ç å›éŸ³é¢‘ï¼ˆéªŒè¯ï¼‰
with torch.no_grad():
    reconstructed = model.decode(encoded_frames)

print(f"é‡å»ºéŸ³é¢‘: {reconstructed.shape}")
```

### 2.4 ç»Ÿä¸€ Token ç©ºé—´ï¼šOmni æ¨¡å‹çš„å®ç°

**GPT-4o / Gemini 1.5 çš„æ¨æµ‹æ¶æ„**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Unified Token Space                       â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   æ–‡æœ¬      â”‚  â”‚    å›¾åƒ      â”‚  â”‚    éŸ³é¢‘      â”‚    â”‚
â”‚  â”‚  "ä½ å¥½"     â”‚  â”‚   ğŸ–¼ï¸         â”‚  â”‚   ğŸ”Š         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                â”‚                  â”‚             â”‚
â”‚    Text Tokenizer   VQ-VAE Encoder    Audio Codec        â”‚
â”‚         â”‚                â”‚                  â”‚             â”‚
â”‚         â–¼                â–¼                  â–¼             â”‚
â”‚    [1024, 2045]    [256000+142]       [264000+523]       â”‚
â”‚         â”‚                â”‚                  â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                          â”‚                                â”‚
â”‚                          â–¼                                â”‚
â”‚              Unified Transformer (GPT-4o)                 â”‚
â”‚              - è¯æ±‡è¡¨: [0, 300000)                        â”‚
â”‚                [0, 256k):  æ–‡æœ¬ Token                     â”‚
â”‚                [256k, 264k): è§†è§‰ Token (VQ-VAE)         â”‚
â”‚                [264k, 300k): éŸ³é¢‘ Token (Codec)          â”‚
â”‚              - æ— éœ€æŠ•å½±å±‚ï¼ŒåŸç”Ÿç»Ÿä¸€å¤„ç†                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

1. **çœŸæ­£çš„ Any-to-Any**
   ```
   è¾“å…¥:  [æ–‡æœ¬ Token] + [å›¾åƒ Token] + [éŸ³é¢‘ Token]
   è¾“å‡º:  [æ–‡æœ¬ Token] æˆ– [å›¾åƒ Token] æˆ– [éŸ³é¢‘ Token]
   ```

2. **æ— ä¿¡æ¯ç“¶é¢ˆ**
   - ä¸éœ€è¦æŠ•å½±å±‚ï¼ˆLLaVA çš„ç“¶é¢ˆï¼‰
   - æ¯å±‚ Transformer éƒ½èƒ½å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯

3. **ç»Ÿä¸€è®­ç»ƒèŒƒå¼**
   - æ‰€æœ‰æ¨¡æ€ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒç›®æ ‡ï¼ˆNext Token Predictionï¼‰
   - è‡ªç„¶æ”¯æŒæ¨¡æ€é—´çš„ç»†ç²’åº¦äº¤äº’

### 2.5 å®æˆ˜ï¼šæ„å»ºç®€æ˜“çš„è§†è§‰ Token åŒ–å™¨

```python
"""
ä½¿ç”¨é¢„è®­ç»ƒçš„ VQGAN è¿›è¡Œå›¾åƒ Token åŒ–
éœ€è¦: pip install taming-transformers-rom1504
"""
import torch
from omegaconf import OmegaConf
from taming.models.vqgan import VQModel
from PIL import Image
import torchvision.transforms as transforms

class ImageTokenizer:
    """å›¾åƒ Token åŒ–å™¨ï¼ˆåŸºäº VQGANï¼‰"""

    def __init__(self, config_path, checkpoint_path):
        # åŠ è½½é…ç½®å’Œæ¨¡å‹
        config = OmegaConf.load(config_path)
        self.model = VQModel(**config.model.params)

        state_dict = torch.load(checkpoint_path, map_location="cpu")["state_dict"]
        self.model.load_state_dict(state_dict, strict=False)
        self.model.eval()

        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(256),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])

    def encode(self, image_path):
        """å°†å›¾åƒç¼–ç ä¸ºç¦»æ•£ Token"""
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        image = Image.open(image_path).convert("RGB")
        x = self.transform(image).unsqueeze(0)  # [1, 3, 256, 256]

        # ç¼–ç 
        with torch.no_grad():
            z = self.model.encode(x)  # è¿ç»­ç‰¹å¾
            _, _, [_, _, indices] = self.model.quantize(z)  # é‡åŒ–

        # indices: [1, 16*16] = [1, 256] (16x16 ä¸ª Token)
        return indices.squeeze(0).cpu().numpy()

    def decode(self, token_ids):
        """ä» Token é‡å»ºå›¾åƒ"""
        with torch.no_grad():
            z_q = self.model.quantize.get_codebook_entry(
                torch.tensor(token_ids).unsqueeze(0),
                shape=(1, 16, 16, 256)
            )
            reconstructed = self.model.decode(z_q)

        # è½¬ä¸º PIL å›¾åƒ
        img = reconstructed.squeeze(0).permute(1, 2, 0).cpu().numpy()
        img = ((img + 1) / 2 * 255).clip(0, 255).astype('uint8')
        return Image.fromarray(img)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    tokenizer = ImageTokenizer(
        config_path="vqgan_config.yaml",
        checkpoint_path="vqgan.ckpt"
    )

    # ç¼–ç 
    token_ids = tokenizer.encode("cat.jpg")
    print(f"Token åºåˆ—: {token_ids}")
    print(f"Token æ•°é‡: {len(token_ids)}")
    print(f"Token èŒƒå›´: [{token_ids.min()}, {token_ids.max()}]")

    # è§£ç ï¼ˆéªŒè¯ï¼‰
    reconstructed = tokenizer.decode(token_ids)
    reconstructed.save("cat_reconstructed.jpg")
```

**å…³é”®ç‚¹**ï¼š
- å›¾åƒ $(256 \times 256)$ â†’ $16 \times 16 = 256$ ä¸ª Token
- æ¯ä¸ª Token æ¥è‡ªå¤§å°ä¸º 8192 çš„ Codebook
- å‹ç¼©ç‡ï¼š$(256 \times 256 \times 3)$ åƒç´  â†’ 256 ä¸ª Tokenï¼ˆçº¦ 768:1ï¼‰

---

## ä¸‰ã€è§†è§‰ç¼–ç å™¨ï¼šVision Transformer (ViT)

> è¯¦è§ [Part 2 ç¬¬1ç« ] Transformer æ ¸å¿ƒæœºåˆ¶ã€‚æœ¬ç« ä»…è®²è§£ ViT å¦‚ä½•å°† Transformer åº”ç”¨äºå›¾åƒã€‚

### 3.1 æ ¸å¿ƒæ€æƒ³ï¼šå›¾åƒæ˜¯ 16Ã—16 çš„å•è¯

**é—®é¢˜**ï¼šTransformer å¤„ç†ä¸€ç»´åºåˆ—ï¼Œä½†å›¾åƒæ˜¯äºŒç»´çš„ $(H \times W)$ã€‚

**ViT çš„è§£å†³æ–¹æ¡ˆ**ï¼šæŠŠå›¾åƒåˆ‡æˆå°æ–¹å—ï¼ˆPatchesï¼‰ï¼Œåƒå¤„ç†å•è¯ä¸€æ ·å¤„ç†å®ƒä»¬ã€‚

```python
# 1. åŸå§‹å›¾åƒ
image = [224, 224, 3]  # é«˜Ã—å®½Ã—é€šé“

# 2. åˆ‡æˆ Patches (æ¯å— 16Ã—16)
num_patches = (224 // 16) * (224 // 16) = 14 * 14 = 196
patches = split_image(image, patch_size=16)  # [196, 16, 16, 3]

# 3. å±•å¹³æ¯ä¸ª Patch
patch_vectors = patches.reshape(196, 16*16*3)  # [196, 768]

# 4. çº¿æ€§æŠ•å½±åˆ° Embedding ç»´åº¦
embeddings = Linear(768, 768)(patch_vectors)  # [196, 768]

# 5. åŠ å…¥ä½ç½®ç¼–ç ï¼ˆå‘Šè¯‰æ¨¡å‹æ¯ä¸ª Patch çš„ä½ç½®ï¼‰
position_embeddings = learnable_params([196, 768])
final_input = embeddings + position_embeddings

# 6. å–‚ç»™ Transformerï¼
```

**ç±»æ¯”**ï¼š
- **NLP**ï¼šä¸€å¥è¯ = ["æˆ‘", "çˆ±", "çŒ«"] â†’ 3 ä¸ª Token
- **ViT**ï¼šä¸€å¼ å›¾ = [Patchâ‚, Patchâ‚‚, ..., Patchâ‚â‚‰â‚†] â†’ 196 ä¸ª Token

### 3.2 ViT ä»£ç å®ç°

```python
import torch
import torch.nn as nn

class PatchEmbedding(nn.Module):
    """å°†å›¾åƒåˆ‡åˆ†æˆ Patches å¹¶æ˜ å°„åˆ° Embedding ç©ºé—´"""
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.n_patches = (img_size // patch_size) ** 2

        # ä½¿ç”¨å·ç§¯å®ç°åˆ†å—+æŠ•å½±ï¼ˆæœ€é«˜æ•ˆçš„æ–¹å¼ï¼‰
        # kernel_size=patch_size, stride=patch_size å®ç°éé‡å åˆ†å—
        self.proj = nn.Conv2d(in_channels, embed_dim,
                              kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        # x: [batch, 3, 224, 224]
        x = self.proj(x)         # [batch, 768, 14, 14]
        x = x.flatten(2)         # [batch, 768, 196]
        x = x.transpose(1, 2)    # [batch, 196, 768]
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, embed_dim=768,
                 num_heads=12, num_layers=12, num_classes=1000):
        super().__init__()

        # 1. Patch Embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)

        # 2. CLS Tokenï¼ˆç±»ä¼¼ BERT çš„ [CLS]ï¼Œç”¨äºåˆ†ç±»ï¼‰
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # 3. Position Embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)
        )

        # 4. Transformer Encoderï¼ˆè¯¦è§ Part 2 ç¬¬1ç« ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads,
            activation='gelu', batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # 5. Classification Head
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        B = x.shape[0]

        # Patch Embedding
        x = self.patch_embed(x)  # [B, 196, 768]

        # æ·»åŠ  CLS Token
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, 768]
        x = torch.cat((cls_tokens, x), dim=1)           # [B, 197, 768]

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed

        # Transformer Encoder
        x = self.encoder(x)

        # å– CLS Token è¿›è¡Œåˆ†ç±»
        cls_output = x[:, 0]        # [B, 768]
        logits = self.head(cls_output)  # [B, 1000]

        return logits

# æµ‹è¯•
if __name__ == "__main__":
    model = VisionTransformer()
    dummy_img = torch.randn(2, 3, 224, 224)  # æ‰¹é‡å¤§å°=2
    output = model(dummy_img)
    print(f"è¾“å…¥: {dummy_img.shape}, è¾“å‡º: {output.shape}")
    # è¾“å…¥: torch.Size([2, 3, 224, 224]), è¾“å‡º: torch.Size([2, 1000])
```

**å…³é”®ç‚¹**ï¼š
- **Patch Embedding**ï¼šç”¨å·ç§¯é«˜æ•ˆå®ç°åˆ†å—
- **CLS Token**ï¼šå…¨å±€ç‰¹å¾èšåˆï¼ˆå¯é€‰ï¼Œæœ‰äº› ViT ç”¨å…¨å±€å¹³å‡æ± åŒ–ï¼‰
- **ä½ç½®ç¼–ç **ï¼šViT é€šå¸¸ä½¿ç”¨**å¯å­¦ä¹ **çš„ä½ç½®ç¼–ç ï¼ˆä¸ Transformer çš„æ­£å¼¦ç¼–ç ä¸åŒï¼‰

---

## å››ã€å›¾æ–‡å¯¹é½ï¼šCLIP

> **CLIP (Contrastive Language-Image Pre-training)** æ˜¯ OpenAI 2021 å¹´æå‡ºçš„çªç ´æ€§å·¥ä½œï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®©å›¾åƒå’Œæ–‡æœ¬åœ¨åŒä¸€ç©ºé—´ä¸­å¯¹é½ã€‚

### 4.1 æ ¸å¿ƒæœºåˆ¶ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰

> è¯¦è§ [Part 3 ç¬¬4ç« ] å¯¹æ¯”å­¦ä¹ è¯¦è§£ã€‚æœ¬èŠ‚ä»…è®²è§£ CLIP çš„å…·ä½“å®ç°ã€‚

**è®­ç»ƒæ•°æ®**ï¼š4 äº¿ä¸ªï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹ï¼Œä»äº’è”ç½‘çˆ¬å–ã€‚

**è®­ç»ƒç›®æ ‡**ï¼š
- **æ­£æ ·æœ¬å¯¹** $(I_i, T_i)$ï¼šç›¸ä¼¼åº¦**æœ€å¤§åŒ–**
- **è´Ÿæ ·æœ¬å¯¹** $(I_i, T_j)_{i \neq j}$ï¼šç›¸ä¼¼åº¦**æœ€å°åŒ–**

```python
import torch
import torch.nn.functional as F

def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    CLIP çš„ InfoNCE æŸå¤±

    Args:
        image_embeddings: [N, D] - N å¼ å›¾åƒçš„ç‰¹å¾å‘é‡
        text_embeddings: [N, D] - N ä¸ªæ–‡æœ¬çš„ç‰¹å¾å‘é‡
        temperature: æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶ softmax åˆ†å¸ƒçš„å¹³æ»‘åº¦
    """
    # 1. å½’ä¸€åŒ–ï¼ˆç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ­£ç¡®ï¼‰
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)

    # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [N, N]
    # logits[i, j] = sim(image_i, text_j)
    logits = (image_embeddings @ text_embeddings.T) / temperature

    # 3. å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä½™æ˜¯è´Ÿæ ·æœ¬
    labels = torch.arange(len(logits)).to(logits.device)

    # 4. åŒå‘æŸå¤±ï¼ˆå›¾åƒâ†’æ–‡æœ¬ + æ–‡æœ¬â†’å›¾åƒï¼‰
    loss_i2t = F.cross_entropy(logits, labels)        # å›¾åƒæŸ¥æ–‡æœ¬
    loss_t2i = F.cross_entropy(logits.T, labels)      # æ–‡æœ¬æŸ¥å›¾åƒ

    loss = (loss_i2t + loss_t2i) / 2
    return loss
```

**æ•°å­¦è¡¨è¾¾**ï¼ˆå›¾åƒâ†’æ–‡æœ¬æ–¹å‘ï¼‰ï¼š
$$
\mathcal{L}_{i \to t} = -\log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^N \exp(\text{sim}(I_i, T_j) / \tau)}
$$

**ç›´è§‰è§£é‡Š**ï¼š
- **åˆ†å­**ï¼šæ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
- **åˆ†æ¯**ï¼šæ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆæ­£æ ·æœ¬åº”è¯¥è¿œå¤§äºè´Ÿæ ·æœ¬ï¼‰
- **$\tau$ (æ¸©åº¦)**ï¼šè¶Šå°ï¼Œæ¨¡å‹å¯¹éš¾è´Ÿæ ·æœ¬è¶Šæ•æ„Ÿ

### 4.2 CLIP çš„å®é™…ä½¿ç”¨

**é›¶æ ·æœ¬å›¾åƒåˆ†ç±»**ï¼ˆZero-shot Classificationï¼‰

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

# 1. åŠ è½½é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹
model_name = "openai/clip-vit-base-patch32"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 2. å‡†å¤‡å›¾åƒ
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 3. å®šä¹‰å€™é€‰ç±»åˆ«ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼ï¼‰
candidates = [
    "a photo of a cat",
    "a photo of a dog",
    "a photo of a bird",
    "a photo of remote controls"  # å›¾ä¸­å®é™…æœ‰é¥æ§å™¨
]

# 4. ç¼–ç 
inputs = processor(text=candidates, images=image,
                   return_tensors="pt", padding=True)

# 5. å‰å‘ä¼ æ’­
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # [1, 4]
probs = logits_per_image.softmax(dim=1)      # è½¬ä¸ºæ¦‚ç‡

# 6. è¾“å‡ºç»“æœ
print("å€™é€‰ç±»åˆ«:", candidates)
print("åŒ¹é…æ¦‚ç‡:", probs.detach().numpy()[0])

# é¢„æœŸè¾“å‡ºï¼ˆå®é™…å›¾ç‰‡æ˜¯ä¸¤åªçŒ«å’Œä¸€äº›é¥æ§å™¨ï¼‰ï¼š
# å€™é€‰ç±»åˆ«: ['a photo of a cat', 'a photo of a dog', 'a photo of a bird', 'a photo of remote controls']
# åŒ¹é…æ¦‚ç‡: [0.85, 0.02, 0.01, 0.12]  (çŒ«çš„æ¦‚ç‡æœ€é«˜)
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- **é›¶æ ·æœ¬èƒ½åŠ›**ï¼šä¸éœ€è¦ä¸“é—¨è®­ç»ƒåˆ†ç±»å™¨ï¼Œç›´æ¥ç”¨æ–‡æœ¬æè¿°ç±»åˆ«
- **çµæ´»æ€§**ï¼šå¯ä»¥éšæ—¶æ”¹å˜å€™é€‰ç±»åˆ«ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ

### 4.3 CLIP çš„åº”ç”¨åœºæ™¯

1. **é›¶æ ·æœ¬å›¾åƒåˆ†ç±»**ï¼ˆå¦‚ä¸Šä¾‹ï¼‰
2. **å›¾æ–‡æ£€ç´¢**ï¼ˆè¯¦è§ç¬¬äº”èŠ‚å®æˆ˜ï¼‰
3. **å¤šæ¨¡æ€æœç´¢**ï¼šè¾“å…¥æ–‡å­—æœå›¾ç‰‡ï¼Œæˆ–è¾“å…¥å›¾ç‰‡æœç›¸ä¼¼å›¾ç‰‡
4. **å›¾åƒç”Ÿæˆå¼•å¯¼**ï¼šStable Diffusionã€DALL-E ä½¿ç”¨ CLIP å¼•å¯¼ç”Ÿæˆ

---

## äº”ã€å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼šLLaVA

> **LLaVA (Large Language and Vision Assistant)** æ˜¯å½“å‰æœ€æµè¡Œçš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼Œè®¾è®¡ç†å¿µç®€å•ä¼˜é›…ã€‚

### 5.1 æ¶æ„è®¾è®¡ï¼šä¸‰ä¸ªç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚  è¾“å…¥: å›¾åƒ + æ–‡æœ¬æŒ‡ä»¤                          â”‚
â”‚  "è¯·æè¿°è¿™å¼ å›¾ç‰‡"                               â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£ Vision Encoder (CLIP ViT-L/14)              â”‚
â”‚     - å†»ç»“å‚æ•°ï¼Œä¸è®­ç»ƒ                           â”‚
â”‚     - è¾“å‡º: [576, 1024] è§†è§‰ Token              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£ Projection Layer (æŠ•å½±å±‚)                   â”‚
â”‚     - å¯è®­ç»ƒçš„ MLP: 1024 â†’ 4096 ç»´              â”‚
â”‚     - å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM çš„ Token ç©ºé—´         â”‚
â”‚     - è¾“å‡º: [576, 4096] "ä¼ªè£…"æˆæ–‡æœ¬ Token      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3ï¸âƒ£ LLM (Vicuna-7B / LLaMA-7B)                 â”‚
â”‚     - å¤„ç†: [è§†è§‰ Token] + [æ–‡æœ¬ Token]         â”‚
â”‚     - ç”Ÿæˆ: "è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†..."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- **Vision Encoder**ï¼šæå–è§†è§‰ç‰¹å¾ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„ CLIPï¼‰
- **Projection Layer**ï¼šæ¡¥æ¥è§†è§‰å’Œè¯­è¨€ç©ºé—´ï¼ˆ**å…³é”®åˆ›æ–°**ï¼‰
- **LLM**ï¼šç†è§£å¹¶ç”Ÿæˆæ–‡æœ¬

### 5.2 Projection Layerï¼šToken Space Alignment çš„å®ç°

**é—®é¢˜**ï¼š
- CLIP ViT è¾“å‡ºç»´åº¦ï¼š1024
- LLaMA-7B Token ç»´åº¦ï¼š4096

**è§£å†³æ–¹æ¡ˆ**ï¼šç®€å•çš„ MLP

```python
class ProjectionLayer(nn.Module):
    """å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ° LLM çš„ Token Embedding ç©ºé—´"""
    def __init__(self, vision_dim=1024, llm_dim=4096):
        super().__init__()
        # ä¸¤å±‚ MLP
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, vision_features):
        # vision_features: [B, N, 1024] (N=576 ä¸ªè§†è§‰ Token)
        # è¾“å‡º: [B, N, 4096]
        return self.proj(vision_features)

# ä½¿ç”¨ç¤ºä¾‹
proj = ProjectionLayer()
vision_tokens = torch.randn(1, 576, 1024)  # CLIP è¾“å‡º
llm_tokens = proj(vision_tokens)           # [1, 576, 4096]

# ç°åœ¨å¯ä»¥ä¸æ–‡æœ¬ Token æ‹¼æ¥ï¼
text_tokens = torch.randn(1, 20, 4096)     # æ–‡æœ¬ Token
combined = torch.cat([llm_tokens, text_tokens], dim=1)  # [1, 596, 4096]
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼Ÿ
- è§†è§‰ç‰¹å¾ç»è¿‡æŠ•å½±åï¼Œ"ä¼ªè£…"æˆäº† LLM å¯ä»¥ç†è§£çš„ Token
- LLM å°±åƒåœ¨å¤„ç†ä¸€æ®µ"ç‰¹æ®Šè¯­è¨€"ï¼ˆå›¾åƒè¯­ï¼‰ï¼Œä½†ä½¿ç”¨ç›¸åŒçš„ Transformer æœºåˆ¶

### 5.3 LLaVA çš„ä¸¤é˜¶æ®µè®­ç»ƒ

> è¿™ç§**ä¸¤é˜¶æ®µè®­ç»ƒæ³•**å·²æˆä¸ºè¡Œä¸šæ ‡å‡†ã€‚

#### é˜¶æ®µä¸€ï¼šç‰¹å¾å¯¹é½é¢„è®­ç»ƒï¼ˆFeature Alignment Pre-trainingï¼‰

**è®­ç»ƒç­–ç•¥**ï¼š
- ğŸ”’ **å†»ç»“**ï¼šVision Encoder + LLM
- ğŸ”¥ **è®­ç»ƒ**ï¼šä»… Projection Layer

**æ•°æ®**ï¼šCC3Mï¼ˆ300 ä¸‡å›¾åƒ-æ ‡é¢˜å¯¹ï¼‰

**ç›®çš„**ï¼šè®© Projection Layer å­¦ä¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM èƒ½ç†è§£çš„ç©ºé—´

```python
# ä¼ªä»£ç 
for image, caption in dataset:
    # 1. æå–è§†è§‰ç‰¹å¾ï¼ˆå†»ç»“ï¼‰
    vision_features = vision_encoder(image)  # [B, 576, 1024]

    # 2. æŠ•å½±åˆ° LLM ç©ºé—´ï¼ˆè®­ç»ƒï¼‰
    visual_tokens = projection_layer(vision_features)  # [B, 576, 4096]

    # 3. æ‹¼æ¥æ–‡æœ¬ Tokenï¼ˆå†»ç»“ï¼‰
    text_tokens = llm.tokenize(caption)
    combined_tokens = concat([visual_tokens, text_tokens])

    # 4. è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä»…åå‘ä¼ æ’­åˆ° Projection Layerï¼‰
    loss = llm.forward(combined_tokens, labels=caption)
    loss.backward()  # åªæ›´æ–° Projection Layer çš„å‚æ•°
```

#### é˜¶æ®µäºŒï¼šè§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVisual Instruction Tuningï¼‰

**è®­ç»ƒç­–ç•¥**ï¼š
- ğŸ”’ **å†»ç»“**ï¼šVision Encoder
- ğŸ”¥ **è®­ç»ƒ**ï¼šProjection Layer + LLM

**æ•°æ®**ï¼šé«˜è´¨é‡è§†è§‰æŒ‡ä»¤æ•°æ®ï¼ˆå¦‚ LLaVA-Instruct-150Kï¼‰

**ç¤ºä¾‹æ•°æ®**ï¼š
```json
{
    "image": "beach.jpg",
    "conversations": [
        {
            "role": "human",
            "content": "è¿™å¼ å›¾ç‰‡ä¸­å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªç¾ä¸½çš„æµ·æ»©æ—¥è½åœºæ™¯ã€‚å¤©ç©ºå‘ˆç°å‡ºæ©™è‰²å’Œç´«è‰²çš„æ¸å˜ï¼Œæµ·æµªè½»æŸ”åœ°æ‹æ‰“ç€æ²™æ»©ã€‚è¿œå¤„å¯ä»¥çœ‹åˆ°å‡ åªæµ·é¸¥åœ¨é£ç¿”ã€‚æ•´ä½“æ°›å›´å®é™è€Œç¥¥å’Œã€‚"
        }
    ]
}
```

**ç›®çš„**ï¼š
- æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›
- å­¦ä¼šéµå¾ªè§†è§‰ç›¸å…³çš„æŒ‡ä»¤
- ç”Ÿæˆæ›´è¯¦ç»†ã€å‡†ç¡®çš„æè¿°

### 5.4 å…¶ä»–è¿æ¥å™¨æ–¹æ¡ˆï¼šPerceiver Resampler (Flamingo/IDEFICS)

LLaVA ä½¿ç”¨ç®€å•çš„ MLPï¼Œä½†æœ‰äº›æ¨¡å‹ä½¿ç”¨æ›´å¤æ‚çš„è¿æ¥å™¨ã€‚

**Perceiver Resampler**ï¼ˆFlamingo æ¶æ„ï¼‰ï¼š

```python
class PerceiverResampler(nn.Module):
    """
    ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å‹ç¼©è§†è§‰ Token
    """
    def __init__(self, num_queries=64, vision_dim=1024, llm_dim=4096, depth=6):
        super().__init__()
        # å¯å­¦ä¹ çš„ Query Token
        self.queries = nn.Parameter(torch.randn(num_queries, llm_dim))

        # å¤šå±‚äº¤å‰æ³¨æ„åŠ›
        self.layers = nn.ModuleList([
            nn.MultiheadAttention(llm_dim, num_heads=16)
            for _ in range(depth)
        ])

    def forward(self, vision_features):
        # vision_features: [B, 576, 1024]

        # 1. å…ˆæŠ•å½±åˆ° LLM ç»´åº¦
        vision_features = nn.Linear(1024, 4096)(vision_features)  # [B, 576, 4096]

        # 2. ä½¿ç”¨å›ºå®šæ•°é‡çš„ Queries æå–ä¿¡æ¯
        B = vision_features.size(0)
        queries = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, 64, 4096]

        # 3. å¤šå±‚äº¤å‰æ³¨æ„åŠ›
        for layer in self.layers:
            queries, _ = layer(
                query=queries.transpose(0, 1),        # [64, B, 4096]
                key=vision_features.transpose(0, 1),  # [576, B, 4096]
                value=vision_features.transpose(0, 1)
            )
            queries = queries.transpose(0, 1)         # [B, 64, 4096]

        # è¾“å‡º: [B, 64, 4096] (ä» 576 å‹ç¼©åˆ° 64 ä¸ª Token!)
        return queries
```

**å¯¹æ¯”**ï¼š
| æ–¹æ¡ˆ | è¾“å‡º Token æ•° | å¤æ‚åº¦ | ä»£è¡¨æ¨¡å‹ |
|------|--------------|--------|----------|
| **MLP (LLaVA)** | 576 | ä½ | LLaVA, Qwen-VL |
| **Perceiver Resampler** | 64 | ä¸­ | Flamingo, IDEFICS |
| **Q-Former (BLIP-2)** | 32 | é«˜ | BLIP-2, InstructBLIP |

**æƒè¡¡**ï¼š
- **æ›´å¤š Token**ï¼šä¿ç•™æ›´å¤šè§†è§‰ç»†èŠ‚ï¼Œä½†å¢åŠ  LLM è®¡ç®—é‡
- **æ›´å°‘ Token**ï¼šè®¡ç®—é«˜æ•ˆï¼Œä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚

---

## å…­ã€è§†é¢‘ç†è§£ï¼šVideo as Frames

> **æ ¸å¿ƒå®šä½**ï¼šç†è§£å¤šæ¨¡æ€æ¨¡å‹å¦‚ä½•å¤„ç†è§†é¢‘ - æœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯å°†è§†é¢‘è§†ä¸ºä¸€ç³»åˆ—é™æ€å›¾åƒã€‚

### 6.1 è§†é¢‘çš„æœ¬è´¨ï¼šæ—¶åºå›¾åƒåºåˆ—

**æ ¸å¿ƒæ€æƒ³**ï¼šè§†é¢‘ = è¿ç»­çš„å›¾åƒå¸§ + æ—¶é—´ç»´åº¦

```
è§†é¢‘æ–‡ä»¶ (30 FPS, 10ç§’)
    â†“
300 ä¸ªå›¾åƒå¸§
    â†“
æŠ½å¸§ç­–ç•¥ (é™ä½è®¡ç®—æˆæœ¬)
    â†“
é€‰æ‹©å…³é”®å¸§ (ä¾‹å¦‚: æ¯ç§’ 1 å¸§)
    â†“
10 ä¸ªå›¾åƒ Token åºåˆ—
    â†“
è¾“å…¥åˆ°å¤šæ¨¡æ€æ¨¡å‹
```

### 6.2 è§†é¢‘æŠ½å¸§ç­–ç•¥

**å¸¸è§ç­–ç•¥**ï¼š

1. **å‡åŒ€æŠ½å¸§ï¼ˆUniform Samplingï¼‰**
   ```python
   def uniform_sample_frames(video_path, num_frames=8):
       """å‡åŒ€æŠ½å– N å¸§"""
       cap = cv2.VideoCapture(video_path)
       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

       # è®¡ç®—é‡‡æ ·é—´éš”
       indices = np.linspace(0, total_frames-1, num_frames, dtype=int)

       frames = []
       for idx in indices:
           cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
           ret, frame = cap.read()
           if ret:
               frames.append(frame)

       cap.release()
       return frames

   # ä½¿ç”¨ç¤ºä¾‹
   # 10ç§’è§†é¢‘(300å¸§) â†’ æŠ½å– 8 å¸§ â†’ æ¯ 37 å¸§æŠ½ä¸€æ¬¡
   frames = uniform_sample_frames("video.mp4", num_frames=8)
   ```

2. **FPS å›ºå®šæŠ½å¸§ï¼ˆFixed FPSï¼‰**
   ```python
   def sample_by_fps(video_path, target_fps=1):
       """æŒ‰å›ºå®š FPS æŠ½å¸§ï¼ˆä¾‹å¦‚æ¯ç§’ 1 å¸§ï¼‰"""
       cap = cv2.VideoCapture(video_path)
       original_fps = cap.get(cv2.CAP_PROP_FPS)

       # è®¡ç®—æ¯éš”å¤šå°‘å¸§æŠ½ä¸€æ¬¡
       frame_interval = int(original_fps / target_fps)

       frames = []
       frame_idx = 0
       while True:
           ret, frame = cap.read()
           if not ret:
               break

           if frame_idx % frame_interval == 0:
               frames.append(frame)

           frame_idx += 1

       cap.release()
       return frames

   # ä½¿ç”¨ç¤ºä¾‹
   # 30 FPS è§†é¢‘ â†’ æ¯ 30 å¸§æŠ½ä¸€æ¬¡ â†’ 1 FPS
   frames = sample_by_fps("video.mp4", target_fps=1)
   ```

3. **å…³é”®å¸§æ£€æµ‹ï¼ˆKeyframe Detectionï¼‰**
   - åŸºäºåœºæ™¯å˜åŒ–æ£€æµ‹ï¼ˆScene Change Detectionï¼‰
   - æ£€æµ‹å¸§é—´å·®å¼‚ï¼Œä¿ç•™å˜åŒ–æ˜¾è‘—çš„å¸§
   - æ›´æ™ºèƒ½ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜

### 6.3 è§†é¢‘ Token åŒ–ï¼šä¸¤ç§èŒƒå¼

#### èŒƒå¼ 1ï¼šè¿æ¥å™¨æ–¹æ¡ˆï¼ˆLLaVA-Videoï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaVA-Video æ¶æ„                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è§†é¢‘è¾“å…¥ (10ç§’, 30 FPS)
    â†“
æŠ½å– 8 å¸§ (Uniform Sampling)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frame â”‚ Frame â”‚ Frame â”‚ ... â”‚ Frame â”‚
â”‚   1   â”‚   2   â”‚   3   â”‚     â”‚   8   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚       â”‚       â”‚             â”‚
    â–¼       â–¼       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Vision Encoder (CLIP ViT)      â”‚
â”‚    æ¯å¸§ â†’ [576, 1024]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
    8 Ã— [576, 1024] = [4608, 1024]
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Projection Layer                â”‚
â”‚    [4608, 1024] â†’ [4608, 4096]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LLM (å¤„ç† 4608 ä¸ªè§†è§‰ Token)    â”‚
â”‚    + æ–‡æœ¬ Token                     â”‚
â”‚    â†’ ç”Ÿæˆè§†é¢‘æè¿°                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**ï¼š
- æ¯å¸§ç‹¬ç«‹ç¼–ç ï¼ˆæ— æ—¶åºå»ºæ¨¡ï¼‰
- æ‹¼æ¥æ‰€æœ‰å¸§çš„ Tokenï¼ˆçº¿æ€§å¢é•¿ï¼‰
- ä¾èµ– LLM çš„è‡ªæ³¨æ„åŠ›å­¦ä¹ æ—¶åºå…³ç³»

#### èŒƒå¼ 2ï¼šåŸç”Ÿç»Ÿä¸€æ–¹æ¡ˆï¼ˆGPT-4oï¼‰

```
è§†é¢‘ â†’ VQ-VAE ç¼–ç å™¨ï¼ˆ3D å·ç§¯ï¼‰â†’ æ—¶ç©ºç¦»æ•£ Token
                â†“
    ç›´æ¥è¾“å…¥ç»Ÿä¸€ Transformer
    ï¼ˆè§†è§‰ Token æœ¬èº«åŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
```

### 6.4 å®æˆ˜ï¼šä½¿ç”¨ LLaVA-Video ç†è§£è§†é¢‘

```python
"""
ä½¿ç”¨ Video-LLaVA è¿›è¡Œè§†é¢‘é—®ç­”
éœ€è¦: pip install transformers accelerate opencv-python
"""
import torch
import cv2
import numpy as np
from transformers import VideoLlavaForConditionalGeneration, AutoProcessor
from PIL import Image

def load_video_frames(video_path, num_frames=8):
    """ä»è§†é¢‘ä¸­å‡åŒ€æŠ½å– N å¸§"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    indices = np.linspace(0, total_frames-1, num_frames, dtype=int)

    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            # OpenCV è¯»å–çš„æ˜¯ BGRï¼Œè½¬ä¸º RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(frame))

    cap.release()
    return frames

# 1. åŠ è½½æ¨¡å‹
model_id = "LanguageBind/Video-LLaVA-7B"
model = VideoLlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_id)

# 2. åŠ è½½è§†é¢‘å¹¶æŠ½å¸§
video_path = "cooking.mp4"
frames = load_video_frames(video_path, num_frames=8)

print(f"æŠ½å–äº† {len(frames)} å¸§")

# 3. å‡†å¤‡é—®é¢˜
prompt = "USER: <video>\nDescribe what's happening in this video.\nASSISTANT:"

# 4. å¤„ç†è¾“å…¥
inputs = processor(
    text=prompt,
    images=frames,  # Video-LLaVA ä½¿ç”¨ images å‚æ•°å¤„ç†è§†é¢‘å¸§
    return_tensors="pt"
).to("cuda")

# 5. ç”Ÿæˆå›ç­”
with torch.inference_mode():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=False
    )

# 6. è§£ç è¾“å‡º
output = processor.decode(generated_ids[0], skip_special_tokens=True)
answer = output.split("ASSISTANT:")[-1].strip()

print(f"\nè§†é¢‘: {video_path}")
print(f"é—®é¢˜: Describe what's happening in this video.")
print(f"å›ç­”: {answer}")

# é¢„æœŸè¾“å‡ºç¤ºä¾‹:
# "This video shows a person cooking in a kitchen. They start by chopping vegetables,
#  then heat oil in a pan, add the vegetables, and stir-fry them. The person appears
#  to be preparing a healthy meal."
```

### 6.5 è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ä¸ä¼˜åŒ–

**æŒ‘æˆ˜ 1ï¼šToken æ•°é‡çˆ†ç‚¸**
- é—®é¢˜ï¼š8 å¸§ Ã— 576 Token/å¸§ = 4608 Tokenï¼ˆæ¥è¿‘æŸäº›æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
- è§£å†³æ–¹æ¡ˆï¼š
  - ä½¿ç”¨ Perceiver Resampler å‹ç¼©ï¼ˆ576 â†’ 64 Token/å¸§ï¼‰
  - å‡å°‘æŠ½å¸§æ•°é‡ï¼ˆç‰ºç‰²æ—¶åºç»†èŠ‚ï¼‰

**æŒ‘æˆ˜ 2ï¼šç¼ºä¹çœŸæ­£çš„æ—¶åºå»ºæ¨¡**
- é—®é¢˜ï¼šç‹¬ç«‹ç¼–ç æ¯å¸§ï¼Œæ— æ³•æ•æ‰è¿ç»­åŠ¨ä½œ
- è§£å†³æ–¹æ¡ˆï¼š
  - ä½¿ç”¨ 3D å·ç§¯ï¼ˆC3Dã€I3Dï¼‰
  - æ—¶åº Transformerï¼ˆTimeSformerï¼‰
  - åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼ˆGPT-4oï¼‰

**æŒ‘æˆ˜ 3ï¼šé•¿è§†é¢‘å¤„ç†**
- é—®é¢˜ï¼š10 åˆ†é’Ÿè§†é¢‘æŠ½å¸§åä»æœ‰æ•°ç™¾å¸§
- è§£å†³æ–¹æ¡ˆï¼š
  - åˆ†æ®µå¤„ç†ï¼ˆæ¯ 30 ç§’ä¸€æ®µï¼‰
  - å±‚æ¬¡åŒ–é‡‡æ ·ï¼ˆå…ˆç²—é‡‡æ ·å®šä½å…³é”®ç‰‡æ®µï¼Œå†ç»†é‡‡æ ·ï¼‰
  - ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆGemini 1.5ï¼š1M Tokenï¼‰

### 6.6 è§†é¢‘ç†è§£çš„åº”ç”¨åœºæ™¯

1. **è§†é¢‘æ‘˜è¦ç”Ÿæˆ**
   ```python
   prompt = "Summarize the main events in this video in 3 sentences."
   ```

2. **æ—¶é—´æˆ³å®šä½**
   ```python
   prompt = "At what timestamp does the person start cooking? Answer in format MM:SS."
   ```

3. **åŠ¨ä½œè¯†åˆ«**
   ```python
   prompt = "What actions does the person perform in this video? List them step by step."
   ```

4. **è§†é¢‘é—®ç­”**
   ```python
   prompt = "How many people appear in this video?"
   ```

---

## ä¸ƒã€å®æˆ˜ï¼šå¤šæ¨¡æ€ç†è§£åº”ç”¨

### 7.1 ä½¿ç”¨å¼€æºæ¨¡å‹ï¼šLLaVA å›¾åƒé—®ç­”

```python
"""
ä½¿ç”¨ Hugging Face çš„ LLaVA-1.5-7B è¿›è¡Œå›¾åƒç†è§£
éœ€è¦: pip install transformers accelerate pillow
"""
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests
import torch

# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
model_id = "llava-hf/llava-1.5-7b-hf"
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"  # è‡ªåŠ¨åˆ†é… GPU/CPU
)
processor = AutoProcessor.from_pretrained(model_id)

# 2. å‡†å¤‡å›¾åƒå’Œé—®é¢˜
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image = Image.open(requests.get(url, stream=True).raw)

prompt = "USER: <image>\nWhat's the content of this image?\nASSISTANT:"

# 3. å¤„ç†è¾“å…¥
inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")

# 4. ç”Ÿæˆå›ç­”
with torch.inference_mode():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=False
    )

# 5. è§£ç è¾“å‡º
output = processor.decode(generated_ids[0], skip_special_tokens=True)
answer = output.split("ASSISTANT:")[-1].strip()

print("å›¾åƒ:", url)
print("é—®é¢˜:", "What's the content of this image?")
print("å›ç­”:", answer)

# é¢„æœŸè¾“å‡º:
# "This image shows a red stop sign at a street intersection in Australia.
#  The sign features white text in English and additional text in Chinese characters."
```

### 7.2 å®æˆ˜ï¼šæ„å»ºæœ¬åœ°å›¾æ–‡æ£€ç´¢å¼•æ“

ä½¿ç”¨ CLIP æ„å»ºä¸€ä¸ªç®€å•çš„å›¾ç‰‡æœç´¢å¼•æ“ã€‚

```python
import os
import torch
import glob
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import numpy as np

class ImageSearchEngine:
    """åŸºäº CLIP çš„å›¾æ–‡æ£€ç´¢å¼•æ“"""

    def __init__(self, model_id="openai/clip-vit-base-patch32"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½ CLIP æ¨¡å‹
        self.model = CLIPModel.from_pretrained(model_id).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_id)

        # ç´¢å¼•æ•°æ®
        self.image_paths = []
        self.image_features = None

    def index_images(self, image_dir):
        """ä¸ºæŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡å»ºç«‹ç‰¹å¾ç´¢å¼•"""
        # 1. æ”¶é›†å›¾ç‰‡è·¯å¾„
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.webp']
        for ext in extensions:
            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))

        print(f"æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å»ºç«‹ç´¢å¼•...")

        # 2. æ‰¹é‡æå–ç‰¹å¾
        all_features = []
        batch_size = 32

        for i in range(0, len(self.image_paths), batch_size):
            batch_paths = self.image_paths[i:i+batch_size]
            images = []

            # åŠ è½½å›¾ç‰‡
            for path in batch_paths:
                try:
                    images.append(Image.open(path).convert("RGB"))
                except Exception as e:
                    print(f"è¯»å–å¤±è´¥ {path}: {e}")
                    continue

            if not images:
                continue

            # æå–ç‰¹å¾
            with torch.no_grad():
                inputs = self.processor(images=images, return_tensors="pt",
                                       padding=True).to(self.device)
                features = self.model.get_image_features(**inputs)
                # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                features = features / features.norm(p=2, dim=-1, keepdim=True)
                all_features.append(features.cpu())

            print(f"å·²ç´¢å¼•: {min(i+batch_size, len(self.image_paths))}/{len(self.image_paths)}")

        # 3. åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.image_features = torch.cat(all_features)
        print("ç´¢å¼•å®Œæˆ!")

    def search(self, query_text, top_k=5):
        """ä½¿ç”¨æ–‡æœ¬æœç´¢å›¾ç‰‡"""
        if self.image_features is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ index_images() å»ºç«‹ç´¢å¼•")

        # 1. ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        with torch.no_grad():
            inputs = self.processor(text=[query_text], return_tensors="pt",
                                   padding=True).to(self.device)
            text_features = self.model.get_text_features(**inputs)
            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)

        # 2. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        # [1, D] @ [N, D]^T = [1, N]
        similarities = (text_features.cpu() @ self.image_features.T).squeeze(0)

        # 3. è·å– Top-K
        values, indices = similarities.topk(top_k)

        results = []
        for val, idx in zip(values, indices):
            results.append({
                'path': self.image_paths[idx],
                'score': val.item()
            })

        return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»ºæœç´¢å¼•æ“å¹¶å»ºç«‹ç´¢å¼•
    engine = ImageSearchEngine()
    engine.index_images("./my_photos")  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹

    # 2. æœç´¢
    queries = [
        "a dog playing in the park",
        "sunset at the beach",
        "a person reading a book"
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: '{query}'")
        results = engine.search(query, top_k=3)

        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['path']} (ç›¸ä¼¼åº¦: {result['score']:.4f})")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ‰¾åˆ° 1523 å¼ å›¾ç‰‡ï¼Œå¼€å§‹å»ºç«‹ç´¢å¼•...
å·²ç´¢å¼•: 32/1523
å·²ç´¢å¼•: 64/1523
...
ç´¢å¼•å®Œæˆ!

æŸ¥è¯¢: 'a dog playing in the park'
  1. ./my_photos/IMG_2023.jpg (ç›¸ä¼¼åº¦: 0.8752)
  2. ./my_photos/IMG_1845.jpg (ç›¸ä¼¼åº¦: 0.8231)
  3. ./my_photos/IMG_2091.jpg (ç›¸ä¼¼åº¦: 0.7963)
```

### 7.3 å®æˆ˜ï¼šä½¿ç”¨ GPT-4V è¿›è¡Œé«˜çº§è§†è§‰ç†è§£

```python
"""
è°ƒç”¨ GPT-4V API è¿›è¡Œå›¾åƒç†è§£
éœ€è¦: pip install openai
"""
from openai import OpenAI
import base64

client = OpenAI(api_key="your-api-key")

def encode_image(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image(image_path, question):
    """ä½¿ç”¨ GPT-4V åˆ†æå›¾åƒ"""
    base64_image = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",  # æˆ– "gpt-4-turbo"
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": question
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"  # é«˜åˆ†è¾¨ç‡æ¨¡å¼
                        }
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å›¾åƒæè¿°
    description = analyze_image(
        "chart.png",
        "è¯¦ç»†æè¿°è¿™å¼ å›¾è¡¨ï¼ŒåŒ…æ‹¬ç±»å‹ã€è¶‹åŠ¿å’Œå…³é”®æ•°æ®ç‚¹"
    )
    print("å›¾è¡¨åˆ†æ:", description)

    # 2. OCR + ç»“æ„åŒ–è¾“å‡º
    ocr_result = analyze_image(
        "receipt.jpg",
        "æå–è¿™å¼ æ”¶æ®ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…æ‹¬ï¼šå•†å®¶åç§°ã€æ—¥æœŸã€å•†å“åˆ—è¡¨ã€æ€»é‡‘é¢"
    )
    print("æ”¶æ®ä¿¡æ¯:", ocr_result)

    # 3. è§†è§‰æ¨ç†
    reasoning = analyze_image(
        "scene.jpg",
        "è¿™å¼ å›¾ç‰‡ä¸­æœ‰å“ªäº›æ½œåœ¨çš„å®‰å…¨éšæ‚£ï¼Ÿè¯·åˆ—ä¸¾å¹¶è§£é‡Š"
    )
    print("å®‰å…¨åˆ†æ:", reasoning)
```

---

## å…«ã€å½“å‰è§†è§’ï¼šConnector vs Native Multimodal

> **æ ¸å¿ƒå®šä½**ï¼šæ·±å…¥ç†è§£ä¸¤ç§å¤šæ¨¡æ€æ¶æ„èŒƒå¼çš„æœ¬è´¨åŒºåˆ« - Connectorï¼ˆè¿æ¥å™¨ï¼‰æ˜¯"å¤–æŒ‚çœ¼ç›"ï¼ŒNativeï¼ˆåŸç”Ÿï¼‰æ˜¯"å…¨èº«ç¥ç»ç³»ç»Ÿ"ã€‚

### 8.1 æ¶æ„èŒƒå¼å¯¹æ¯”ï¼šçœ¼ç› vs ç¥ç»ç³»ç»Ÿ

#### Connector æ–¹æ¡ˆï¼ˆLLaVAï¼‰ï¼šå¤–æŒ‚çš„"çœ¼ç›"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Connector æ¶æ„ (LLaVA)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   å¤§è„‘      â”‚
                  â”‚   (LLM)     â”‚  â† åªæ‡‚"è¯­è¨€"
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
                      æŠ•å½±å±‚
                    (ç¿»è¯‘å™¨)
                         â†‘
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   çœ¼ç›       â”‚
                  â”‚ (CLIP ViT)   â”‚  â† åªæ‡‚"è§†è§‰"
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
                     ğŸ–¼ï¸ å›¾åƒ

æœ¬è´¨ï¼šä¸¤ä¸ªç‹¬ç«‹è®­ç»ƒçš„ç³»ç»Ÿï¼Œé€šè¿‡"ç¿»è¯‘å±‚"å‹‰å¼ºæ²Ÿé€š
ç±»æ¯”ï¼šç»™åªä¼šä¸­æ–‡çš„äººé…ä¸€ä¸ªè‹±è¯­ç¿»è¯‘
```

**å·¥ä½œæµç¨‹**ï¼š
1. **è§†è§‰ç¼–ç å™¨**ï¼ˆCLIP ViTï¼‰ï¼šç‹¬ç«‹é¢„è®­ç»ƒï¼Œå†»ç»“å‚æ•°
   - è®­ç»ƒæ•°æ®ï¼š4äº¿å›¾æ–‡å¯¹ï¼ˆCLIP æ•°æ®é›†ï¼‰
   - ç›®æ ‡ï¼šå›¾æ–‡å¯¹æ¯”å­¦ä¹ 
   - è¾“å‡ºï¼š1024 ç»´è§†è§‰ç‰¹å¾

2. **æŠ•å½±å±‚**ï¼ˆProjectionï¼‰ï¼šæ¡¥æ¥å±‚ï¼Œå”¯ä¸€å¯è®­ç»ƒ
   - ä½œç”¨ï¼šå°† 1024 ç»´è§†è§‰ç‰¹å¾"ä¼ªè£…"æˆ 4096 ç»´æ–‡æœ¬ Token
   - è®­ç»ƒæ•°æ®ï¼šå°‘é‡ï¼ˆ30ä¸‡-150ä¸‡ï¼‰å›¾æ–‡å¯¹
   - æŒ‘æˆ˜ï¼šå¿…é¡»åœ¨æœ‰é™æ•°æ®ä¸‹å®Œæˆ"ç¿»è¯‘"ä»»åŠ¡

3. **è¯­è¨€æ¨¡å‹**ï¼ˆLLMï¼‰ï¼šç‹¬ç«‹é¢„è®­ç»ƒï¼Œå¾®è°ƒ
   - è®­ç»ƒæ•°æ®ï¼šæ•°ä¸‡äº¿ Token çš„çº¯æ–‡æœ¬
   - ç›®æ ‡ï¼šè¯­è¨€å»ºæ¨¡
   - é—®é¢˜ï¼šä»æœªåœ¨é¢„è®­ç»ƒä¸­"è§è¿‡"çœŸå®å›¾åƒ

#### Native æ–¹æ¡ˆï¼ˆGPT-4oï¼‰ï¼šåŸç”Ÿçš„"ç¥ç»ç³»ç»Ÿ"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Native æ¶æ„ (GPT-4o)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   ç»Ÿä¸€å¤§è„‘      â”‚
                  â”‚  (Transformer)  â”‚
                  â”‚                 â”‚
                  â”‚  ä»å‡ºç”Ÿå°±åŒæ—¶  â”‚
                  â”‚  "çœ‹""å¬""è¯´" â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†‘
                    ç»Ÿä¸€ Token æµ
                           â†‘
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
     ğŸ–¼ï¸ å›¾åƒ            ğŸ“ æ–‡æœ¬            ğŸ”Š éŸ³é¢‘
    (VQ-VAE)         (BPE)            (Codec)
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              æ‰€æœ‰æ¨¡æ€å…±äº«åŒä¸€è¯æ±‡è¡¨

æœ¬è´¨ï¼šä»é›¶å¼€å§‹ï¼Œç”¨å¤šæ¨¡æ€æ•°æ®è”åˆè®­ç»ƒçš„å•ä¸€ç³»ç»Ÿ
ç±»æ¯”ï¼šä»å°åœ¨åŒè¯­ç¯å¢ƒé•¿å¤§çš„äººï¼Œå¤©ç”Ÿå°±ä¼šä¸­è‹±æ–‡
```

**å·¥ä½œæµç¨‹**ï¼š
1. **ç»Ÿä¸€ Token åŒ–**ï¼šæ‰€æœ‰æ¨¡æ€è½¬ä¸ºç¦»æ•£ Token
   ```
   æ–‡æœ¬: "çŒ«" â†’ Token ID 1024
   å›¾åƒ: ğŸ±  â†’ Token ID 256142
   éŸ³é¢‘: å–µ  â†’ Token ID 264523
   ```

2. **ç»Ÿä¸€ Transformer**ï¼šå•ä¸€æ¨¡å‹å¤„ç†æ‰€æœ‰ Token
   - è®­ç»ƒæ•°æ®ï¼šæ··åˆæ•°æ®ï¼ˆæ–‡æœ¬ + å›¾åƒ + éŸ³é¢‘ + ...ï¼‰
   - è®­ç»ƒç›®æ ‡ï¼šç»Ÿä¸€çš„ Next Token Prediction
   - ä¼˜åŠ¿ï¼šæ¯å±‚éƒ½èƒ½å­¦ä¹ è·¨æ¨¡æ€äº¤äº’

3. **æ— éœ€æ¡¥æ¥å±‚**ï¼šæ‰€æœ‰æ¨¡æ€å¤©ç”Ÿåœ¨åŒä¸€ç©ºé—´
   - æ— ä¿¡æ¯ç“¶é¢ˆ
   - æ— éœ€"ç¿»è¯‘"
   - è‡ªç„¶æ”¯æŒæ¨¡æ€æ··åˆ

### 8.2 æ ¸å¿ƒå·®å¼‚ï¼šæ·±å…¥æŠ€æœ¯å¯¹æ¯”

| ç»´åº¦ | Connector (LLaVA) | Native (GPT-4o) |
|------|-------------------|-----------------|
| **è®­ç»ƒèŒƒå¼** | ğŸ”§ **ç»„è£…å¼**ï¼šå…ˆå•æ¨¡æ€ â†’ åæ‹¼æ¥ | ğŸŒ± **åŸç”Ÿå¼**ï¼šä»é›¶å¤šæ¨¡æ€è”åˆè®­ç»ƒ |
| **Token ç©ºé—´** | ğŸ”€ **åˆ†ç¦»åå¯¹é½**ï¼š<br>- CLIP: $\mathbb{R}^{1024}$<br>- LLM: $\mathbb{R}^{4096}$<br>- æŠ•å½±å±‚å¼ºè¡Œå¯¹é½ | âœ¨ **å¤©ç„¶ç»Ÿä¸€**ï¼š<br>- æ‰€æœ‰æ¨¡æ€å…±äº«åŒä¸€è¯æ±‡è¡¨<br>- [0, 300k) åŒ…å«æ–‡æœ¬/å›¾åƒ/éŸ³é¢‘ |
| **ä¿¡æ¯æµåŠ¨** | ğŸš§ **å•å‘å—é™**ï¼š<br>è§†è§‰ â†’ æŠ•å½±å±‚ â†’ LLM<br>æŠ•å½±å±‚æ˜¯ç“¶é¢ˆï¼ˆ576 Tokenï¼‰ | ğŸŒŠ **å…¨å‘æµåŠ¨**ï¼š<br>ä»»æ„æ¨¡æ€å¯åœ¨ä»»æ„å±‚äº¤äº’<br>æ— ç“¶é¢ˆ |
| **ç»†ç²’åº¦äº¤äº’** | âŒ **æµ…å±‚äº¤äº’**ï¼š<br>- åªæœ‰ LLM çš„è‡ªæ³¨æ„åŠ›èƒ½è·¨æ¨¡æ€<br>- è§†è§‰ç¼–ç å™¨å®Œå…¨ä¸çŸ¥é“æ–‡æœ¬ | âœ… **æ·±å±‚èåˆ**ï¼š<br>- æ¯å±‚ Transformer éƒ½è·¨æ¨¡æ€<br>- å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘äº’ç›¸"ç†è§£" |
| **é•¿ä¸Šä¸‹æ–‡** | ğŸ“ **å—é™**ï¼š<br>- 8K Tokenï¼ˆLLaMA-7Bï¼‰<br>- è§†è§‰ Token å å¤§å¤´ï¼ˆ576Ã—N å¸§ï¼‰ | ğŸš€ **è¶…é•¿**ï¼š<br>- 128K+ (GPT-4o)<br>- 1M Token (Gemini 1.5) |
| **æ¨¡æ€æ•°é‡** | ğŸ”¢ **å—é™**ï¼š<br>- é€šå¸¸åªæ”¯æŒæ–‡æœ¬+å›¾åƒ<br>- æ·»åŠ æ–°æ¨¡æ€éœ€è¦æ–°çš„ç¼–ç å™¨+æŠ•å½±å±‚ | âˆ **æ— é™æ‰©å±•**ï¼š<br>- æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç»Ÿä¸€<br>- æ·»åŠ æ–°æ¨¡æ€åªéœ€æ‰©å±•è¯æ±‡è¡¨ |
| **è®­ç»ƒæˆæœ¬** | ğŸ’° **ä½**ï¼š<br>- 100ä¸‡ç¾å…ƒçº§åˆ«<br>- åªè®­ç»ƒæŠ•å½±å±‚+å¾®è°ƒ LLM | ğŸ’¸ **æé«˜**ï¼š<br>- æ•°äº¿ç¾å…ƒçº§åˆ«<br>- ä»é›¶è®­ç»ƒæ•´ä¸ªæ¨¡å‹ |
| **æ¨ç†æ•ˆç‡** | âš¡ **é«˜**ï¼š<br>- 7B-13B å‚æ•°<br>- å¯æœ¬åœ°éƒ¨ç½² | ğŸ¢ **ä½**ï¼š<br>- æ•°ç™¾ B å‚æ•°ï¼ˆæ¨æµ‹ï¼‰<br>- åªèƒ½ API è°ƒç”¨ |

### 8.3 èƒ½åŠ›å¯¹æ¯”ï¼šå®é™…åœºæ™¯æµ‹è¯•

#### åœºæ™¯ 1ï¼šç»†ç²’åº¦è§†è§‰æ¨ç†

**ä»»åŠ¡**ï¼šå›¾ä¸­å·¦ä¸‹è§’çš„å’–å•¡æ¯æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ

**Connectorï¼ˆLLaVAï¼‰è¡¨ç°**ï¼š
```
é—®ï¼šå›¾ä¸­å·¦ä¸‹è§’çš„å’–å•¡æ¯æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ
ç­”ï¼šå›¾ç‰‡ä¸­æœ‰ä¸€ä¸ªå’–å•¡æ¯ï¼Œå®ƒæ˜¯ç™½è‰²çš„ã€‚

é—®é¢˜ï¼šæ— æ³•ç²¾ç¡®å®šä½"å·¦ä¸‹è§’"ï¼Œå› ä¸ºï¼š
- CLIP ViT åªè¾“å‡ºå…¨å±€ç‰¹å¾ï¼ˆ576 ä¸ª patchï¼‰
- æŠ•å½±å±‚å‹ç¼©åï¼Œç©ºé—´ä¿¡æ¯è¿›ä¸€æ­¥æŸå¤±
- LLM éš¾ä»¥ç†è§£ç²¾ç¡®çš„ç©ºé—´ä½ç½®
```

**Nativeï¼ˆGPT-4oï¼‰è¡¨ç°**ï¼š
```
é—®ï¼šå›¾ä¸­å·¦ä¸‹è§’çš„å’–å•¡æ¯æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ
ç­”ï¼šå·¦ä¸‹è§’çš„å’–å•¡æ¯æ˜¯æ·±è“è‰²ï¼Œæ¯èº«ä¸Šæœ‰ç™½è‰²çš„å“ç‰Œlogoã€‚

ä¼˜åŠ¿ï¼š
- åŸç”Ÿè§†è§‰ç¼–ç ä¿ç•™æ›´å¤šç©ºé—´ä¿¡æ¯
- ç»Ÿä¸€ Transformer èƒ½åœ¨å¤šå±‚ç»†åŒ–ç©ºé—´ç†è§£
- æ— ä¿¡æ¯ç“¶é¢ˆ
```

#### åœºæ™¯ 2ï¼šè·¨æ¨¡æ€æ¨ç†

**ä»»åŠ¡**ï¼šçœ‹å›¾è¯´è¯ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆè¿™å¼ å›¾ä»¤äººæ„ŸåŠ¨ã€‚

**Connectorï¼ˆLLaVAï¼‰è¡¨ç°**ï¼š
```
ç­”ï¼šå›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä½æ¯äº²æŠ±ç€å¥¹çš„å­©å­ã€‚è¿™å¯èƒ½ä»¤äººæ„ŸåŠ¨ï¼Œ
    å› ä¸ºå®ƒå±•ç¤ºäº†æ¯çˆ±ã€‚

ç‰¹ç‚¹ï¼š
- æè¿°å‡†ç¡®ä½†æµ…å±‚
- ç¼ºä¹æƒ…æ„Ÿç»†èŠ‚ï¼ˆè¡¨æƒ…ã€å§¿æ€ã€èƒŒæ™¯ï¼‰
- æ¨ç†é“¾è¾ƒçŸ­
```

**Nativeï¼ˆGPT-4oï¼‰è¡¨ç°**ï¼š
```
ç­”ï¼šå›¾ç‰‡ä¸­ï¼Œä¸€ä½æ¯äº²æ­£ç´§ç´§æ‹¥æŠ±ç€å¥¹çš„å­©å­ï¼Œä¸¤äººéƒ½é—­ç€çœ¼ç›ï¼Œ
    è„¸ä¸Šæ´‹æº¢ç€å¹¸ç¦çš„å¾®ç¬‘ã€‚é˜³å…‰ä»çª—æˆ·æ´’è¿›æ¥ï¼Œæ¸©æš–åœ°ç…§åœ¨å¥¹ä»¬
    èº«ä¸Šã€‚è¿™å¼ å›¾ä»¤äººæ„ŸåŠ¨ï¼Œå› ä¸ºï¼š
    1. è‚¢ä½“è¯­è¨€ï¼šç´§å¯†çš„æ‹¥æŠ±ä¼ é€’å‡ºæ·±æ·±çš„çˆ±
    2. é¢éƒ¨è¡¨æƒ…ï¼šæ»¡è¶³ä¸”å®‰å¿ƒçš„å¾®ç¬‘
    3. å…‰çº¿æ°›å›´ï¼šæš–è‰²è°ƒçƒ˜æ‰˜å‡ºæ¸©é¦¨æ„Ÿ
    è¿™ç§æ¯å­é—´çš„çº¯ç²¹æƒ…æ„Ÿè¿æ¥è§¦åŠ¨äººå¿ƒã€‚

ä¼˜åŠ¿ï¼š
- å¤šå±‚æ¬¡ç»†èŠ‚è§‚å¯Ÿ
- è§†è§‰-æƒ…æ„Ÿ-è¯­è¨€æ·±åº¦èåˆ
- æ¨ç†é“¾æ›´é•¿æ›´è¿è´¯
```

#### åœºæ™¯ 3ï¼šè§†é¢‘+éŸ³é¢‘ç†è§£

**ä»»åŠ¡**ï¼šåˆ†æè¿™æ®µé’¢ç´æ¼”å¥è§†é¢‘ï¼Œè¯„ä»·æ¼”å¥è€…çš„æŠ€å·§ã€‚

**Connectorï¼ˆLLaVA-Videoï¼‰è¡¨ç°**ï¼š
```
ç­”ï¼šæ— æ³•å®Œæˆä»»åŠ¡
åŸå› ï¼š
- éœ€è¦å•ç‹¬çš„éŸ³é¢‘ç¼–ç å™¨ï¼ˆå¢åŠ å¤æ‚åº¦ï¼‰
- è§†é¢‘å¸§å’ŒéŸ³é¢‘ Token å¦‚ä½•åŒæ­¥ï¼Ÿ
- æŠ•å½±å±‚å¦‚ä½•å¤„ç†ä¸‰æ¨¡æ€ï¼ˆè§†é¢‘+éŸ³é¢‘+æ–‡æœ¬ï¼‰ï¼Ÿ
```

**Nativeï¼ˆGPT-4oï¼‰è¡¨ç°**ï¼š
```
è¾“å…¥ï¼š[è§†é¢‘å¸§ Token] + [éŸ³é¢‘ Token] + [æ–‡æœ¬ Token]

ç­”ï¼šæ¼”å¥è€…å±•ç°äº†å‡ºè‰²çš„æŠ€å·§ï¼š
    - è§†è§‰è§‚å¯Ÿï¼šæ‰‹æŒ‡åŠ¨ä½œæµç•…ï¼Œè¸æ¿ä½¿ç”¨ç²¾å‡†
    - å¬è§‰åˆ†æï¼šéŸ³è‰²é¥±æ»¡ï¼ŒèŠ‚å¥ç¨³å®šï¼Œå¼ºå¼±å¯¹æ¯”æ˜æ˜¾
    - ç»¼åˆè¯„ä»·ï¼šè¿™æ˜¯ä¸€åœºé«˜æ°´å¹³çš„æ¼”å‡º

ä¼˜åŠ¿ï¼š
- Any-to-Any åŸç”Ÿæ”¯æŒ
- ç»Ÿä¸€ Token ç©ºé—´æ— éœ€å¤æ‚å·¥ç¨‹
```

### 8.4 ç±»æ¯”ç†è§£ï¼šä¸¤ç§æ¶æ„çš„æœ¬è´¨

#### Connectorï¼šæ‹¼æ¥æ±½è½¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è‡ªè¡Œè½¦  â”‚ â†’ â”‚  æ”¹è£…å¥—ä»¶ â”‚ â†’ â”‚ ç”µåŠ¨è½¦  â”‚
â”‚ å¼•æ“    â”‚    â”‚ (æŠ•å½±å±‚)  â”‚    â”‚ (èƒ½è·‘)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¼˜ç‚¹ï¼š
âœ… ä¾¿å®œï¼šåˆ©ç”¨ç°æˆéƒ¨ä»¶
âœ… å¿«é€Ÿï¼šç»„è£…å³å¯ä¸Šè·¯

ç¼ºç‚¹ï¼š
âŒ æ€§èƒ½å—é™ï¼šå¼•æ“å’Œç”µæ± ä¸åŒ¹é…
âŒ æ•ˆç‡ä½ï¼šèƒ½é‡åœ¨è½¬æ¢ä¸­æŸå¤±
âŒ æ‰©å±•éš¾ï¼šåŠ è£…éŸ³å“ç³»ç»Ÿå¾ˆéº»çƒ¦
```

#### Nativeï¼šåŸç”Ÿç”µåŠ¨è½¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ç‰¹æ–¯æ‹‰ (Tesla)                â”‚
â”‚  - ç”µæ± ã€å¼•æ“ã€æ§åˆ¶ç³»ç»Ÿä¸€ä½“åŒ–è®¾è®¡    â”‚
â”‚  - ä»é›¶å¼€å§‹ä¸ºç”µåŠ¨ä¼˜åŒ–                â”‚
â”‚  - è½¯ç¡¬ä»¶æ·±åº¦é›†æˆ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¼˜ç‚¹ï¼š
âœ… æ€§èƒ½å¼ºï¼šä¸“ä¸ºç›®æ ‡è®¾è®¡
âœ… æ•ˆç‡é«˜ï¼šæ— èƒ½é‡è½¬æ¢æŸå¤±
âœ… æ‰©å±•æ˜“ï¼šæ·»åŠ åŠŸèƒ½åªéœ€è½¯ä»¶å‡çº§

ç¼ºç‚¹ï¼š
âŒ æ˜‚è´µï¼šç ”å‘æˆæœ¬æ•°åäº¿ç¾å…ƒ
âŒ è€—æ—¶ï¼šéœ€è¦å¤šå¹´è¿­ä»£
```

### 8.5 æœªæ¥è¶‹åŠ¿é¢„æµ‹

**çŸ­æœŸï¼ˆ1-2å¹´ï¼‰**ï¼šConnector ä»æ˜¯ä¸»æµ
- âœ… å¼€æºç¤¾åŒºæŒç»­ä¼˜åŒ– LLaVA ç±»æ¨¡å‹
- âœ… Qwen-VLã€InternVL ç­‰å›½äº§æ–¹æ¡ˆæˆç†Ÿ
- âœ… ä¼ä¸šä¼˜å…ˆé€‰æ‹©æˆæœ¬ä½ã€æ˜“éƒ¨ç½²çš„æ–¹æ¡ˆ

**ä¸­æœŸï¼ˆ2-3å¹´ï¼‰**ï¼šNative å¼€å§‹æ™®åŠ
- ğŸš€ æ–°ä¸€ä»£é—­æºæ¨¡å‹è¿›ä¸€æ­¥æå‡èƒ½åŠ›
- ğŸš€ å¼€æºç¤¾åŒºå°è¯•å°è§„æ¨¡ Native æ¨¡å‹ï¼ˆ7B-13Bï¼‰
- ğŸš€ Omni æ¨¡å‹æˆä¸ºé«˜ç«¯åº”ç”¨æ ‡é…

**é•¿æœŸï¼ˆ3å¹´ä»¥ä¸Šï¼‰**ï¼šNative å®Œå…¨ä¸»å¯¼
- ğŸŒŸ è®­ç»ƒæˆæœ¬é™ä½ï¼ˆæ›´é«˜æ•ˆçš„ç®—æ³•ï¼‰
- ğŸŒŸ å¼€æº Native æ¨¡å‹æ€§èƒ½è¿½ä¸Š Connector
- ğŸŒŸ Connector æ–¹æ¡ˆé€æ¸è¢«æ·˜æ±°ï¼ˆå°±åƒå•æ¨¡æ€æ¨¡å‹æ·˜æ±°ä¼ ç»Ÿ CV/NLP pipelineï¼‰

### 8.6 é€‰å‹å»ºè®®ï¼ˆæœ€æ–°ç‰ˆï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å†³ç­–æ ‘                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½ çš„é¢„ç®—æ˜¯å¦ > $10ä¸‡/å¹´ï¼Ÿ
    â”‚
    â”œâ”€ å¦ â†’ ç”¨ Connector (LLaVA, Qwen-VL)
    â”‚       - å¼€æºå…è´¹
    â”‚       - å¯æœ¬åœ°éƒ¨ç½²
    â”‚       - é€‚åˆï¼šç ”ç©¶ã€åŸå‹ã€ä¸­å°ä¼ä¸š
    â”‚
    â””â”€ æ˜¯ â†’ ä½ éœ€è¦é¡¶çº§æ€§èƒ½å—ï¼Ÿ
            â”‚
            â”œâ”€ æ˜¯ â†’ Native (GPT-4o, Claude 3.5)
            â”‚       - API è°ƒç”¨
            â”‚       - é€‚åˆï¼šé«˜ä»·å€¼åº”ç”¨ï¼ˆåŒ»ç–—ã€é‡‘èï¼‰
            â”‚
            â””â”€ å¦ â†’ æ··åˆæ–¹æ¡ˆ
                    - ç®€å•ä»»åŠ¡ï¼šConnector (è‡ªéƒ¨ç½²)
                    - å¤æ‚ä»»åŠ¡ï¼šNative (API)
                    - é€‚åˆï¼šæˆæœ¬æ•æ„Ÿçš„ç”Ÿäº§ç¯å¢ƒ
```

**å…·ä½“åœºæ™¯æ¨è**ï¼š

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|------|---------|------|
| **å­¦æœ¯ç ”ç©¶** | LLaVA-1.5-7B | å¼€æºã€å¯å¤ç°ã€ç¤¾åŒºæ´»è·ƒ |
| **äº§å“åŸå‹** | Qwen-VL-Chat | ä¸­æ–‡å‹å¥½ã€éƒ¨ç½²ç®€å• |
| **å†…å®¹å®¡æ ¸** | Connector è‡ªéƒ¨ç½² | éšç§ä¿æŠ¤ã€ä½å»¶è¿Ÿ |
| **åŒ»ç–—è¯Šæ–­** | GPT-4o / Claude 3.5 | æœ€é«˜ç²¾åº¦ã€å¯è§£é‡Šæ€§å¼º |
| **é•¿è§†é¢‘åˆ†æ** | Gemini 1.5 Pro | 1M Token ä¸Šä¸‹æ–‡ |
| **å®æ—¶è¯­éŸ³äº¤äº’** | GPT-4o | Any-to-Any åŸç”Ÿæ”¯æŒ |

### 8.7 å®æˆ˜ï¼šå¯¹æ¯”æµ‹è¯•ä¸¤ç§æ¶æ„

```python
"""
å¯¹æ¯” Connector (LLaVA) å’Œ Native (GPT-4o) åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°
"""
from transformers import LlavaForConditionalGeneration, AutoProcessor
from openai import OpenAI
from PIL import Image
import base64
import torch

# ========== Connector æ–¹æ¡ˆ (LLaVA) ==========
def test_connector(image_path, question):
    model_id = "llava-hf/llava-1.5-7b-hf"
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id, torch_dtype=torch.float16, device_map="auto"
    )
    processor = AutoProcessor.from_pretrained(model_id)

    image = Image.open(image_path)
    prompt = f"USER: <image>\n{question}\nASSISTANT:"

    inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")

    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=200)

    output = processor.decode(generated_ids[0], skip_special_tokens=True)
    return output.split("ASSISTANT:")[-1].strip()

# ========== Native æ–¹æ¡ˆ (GPT-4o) ==========
def test_native(image_path, question):
    client = OpenAI(api_key="your-api-key")

    with open(image_path, "rb") as f:
        base64_image = base64.b64encode(f.read()).decode('utf-8')

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                        "detail": "high"
                    }
                }
            ]
        }],
        max_tokens=200
    )

    return response.choices[0].message.content

# ========== å¯¹æ¯”æµ‹è¯• ==========
if __name__ == "__main__":
    test_cases = [
        {
            "image": "complex_scene.jpg",
            "question": "è¯·è¯¦ç»†æè¿°å›¾ç‰‡å³ä¸Šè§’çš„ç‰©ä½“ï¼Œå¹¶è§£é‡Šå®ƒä¸ºä»€ä¹ˆé‡è¦ã€‚"
        },
        {
            "image": "chart.png",
            "question": "åˆ†æè¿™å¼ å›¾è¡¨çš„è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºæŠ•èµ„å»ºè®®ã€‚"
        },
        {
            "image": "medical_scan.jpg",
            "question": "è¯†åˆ«å›¾ä¸­çš„å¼‚å¸¸åŒºåŸŸï¼Œå¹¶è¯„ä¼°ä¸¥é‡ç¨‹åº¦ï¼ˆä»…ä¾›å‚è€ƒï¼‰ã€‚"
        }
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i}: {test['question']}")
        print(f"{'='*60}")

        print("\n[Connector - LLaVA]")
        connector_answer = test_connector(test['image'], test['question'])
        print(connector_answer)

        print("\n[Native - GPT-4o]")
        native_answer = test_native(test['image'], test['question'])
        print(native_answer)

        print(f"\n{'='*60}\n")
```

**é¢„æœŸè§‚å¯Ÿ**ï¼š
- **ç»†èŠ‚ä¸°å¯Œåº¦**ï¼šNative > Connector
- **æ¨ç†æ·±åº¦**ï¼šNative > Connector
- **ç©ºé—´ç†è§£**ï¼šNative > Connector
- **å“åº”é€Ÿåº¦**ï¼šConnector > Nativeï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
- **æˆæœ¬**ï¼šConnector < Native

---

## ä¹ã€æ€»ç»“ä¸å±•æœ›

### 9.1 æ ¸å¿ƒçŸ¥è¯†ç‚¹å›é¡¾

| æŠ€æœ¯ | æ ¸å¿ƒæ€æƒ³ | å…³é”®åˆ›æ–° |
|------|---------|---------|
| **ViT** | å›¾åƒåˆ†å— â†’ Transformer | è¯æ˜ Transformer å¯å¤„ç†è§†è§‰ |
| **CLIP** | å¯¹æ¯”å­¦ä¹ å¯¹é½å›¾æ–‡ | é›¶æ ·æœ¬èƒ½åŠ›ï¼Œè·¨æ¨¡æ€æ£€ç´¢ |
| **LLaVA** | æŠ•å½±å±‚è¿æ¥è§†è§‰å’Œè¯­è¨€ | ç®€å•é«˜æ•ˆï¼Œæ˜“äºè®­ç»ƒ |
| **Native Multimodal** | ç»Ÿä¸€ Token ç©ºé—´ | æ›´å¼ºäº¤äº’ï¼Œæ›´é•¿ä¸Šä¸‹æ–‡ |

### 9.2 å¤šæ¨¡æ€æŠ€æœ¯æ¼”è¿›è·¯çº¿

```
2017: Transformer è¯ç”Ÿï¼ˆçº¯æ–‡æœ¬ï¼‰
  â†“
2020: ViT è¯æ˜ Transformer å¯å¤„ç†å›¾åƒ
  â†“
2021: CLIP å®ç°å›¾æ–‡å¯¹é½ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
  â†“
2023: LLaVA è¿æ¥ LLM å’Œè§†è§‰ï¼ˆæŠ•å½±å±‚æ–¹æ¡ˆï¼‰
  â†“
SOTA: GPT-4V/Gemini åŸç”Ÿå¤šæ¨¡æ€ï¼ˆç«¯åˆ°ç«¯è®­ç»ƒï¼‰
  â†“
å½“å‰: Omni æ¨¡å‹æˆæ ‡é…ï¼ˆå›¾æ–‡éŸ³è§†é¢‘ç»Ÿä¸€ï¼‰
```

### 9.3 æœªæ¥è¶‹åŠ¿

1. **Any-to-Any æ¨¡å‹**
   - è¾“å…¥ï¼šå›¾/æ–‡/éŸ³/è§†é¢‘
   - è¾“å‡ºï¼šå›¾/æ–‡/éŸ³/è§†é¢‘
   - ä»£è¡¨ï¼šGPT-4oï¼ˆå®æ—¶è¯­éŸ³å¯¹è¯ + è§†è§‰ï¼‰

2. **å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰**
   - å¤šæ¨¡æ€ + æœºå™¨äººæ§åˆ¶
   - æ„ŸçŸ¥ï¼ˆè§†è§‰ï¼‰+ ç†è§£ï¼ˆè¯­è¨€ï¼‰+ è¡ŒåŠ¨ï¼ˆæ§åˆ¶ï¼‰
   - ä»£è¡¨ï¼šRT-2ã€PaLM-E

3. **æ›´é•¿ä¸Šä¸‹æ–‡**
   - å¤„ç†å®Œæ•´ç”µå½±ã€é•¿æ–‡æ¡£
   - Gemini 1.5ï¼š1M Tokenï¼ˆçº¦ 1 å°æ—¶è§†é¢‘ï¼‰

4. **æ›´é«˜æ•ˆçš„è®­ç»ƒ**
   - å°æ¨¡å‹ + å¤§æ•°æ® > å¤§æ¨¡å‹ + å°æ•°æ®
   - LoRAã€QLoRA ç­‰é«˜æ•ˆå¾®è°ƒæŠ€æœ¯

### 9.4 å­¦ä¹ èµ„æº

**è®ºæ–‡**ï¼š
- ViT: [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
- CLIP: [Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020)
- LLaVA: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
- GPT-4o: [Omni Technical Report](https://openai.com/index/hello-gpt-4o/)

**ä»£ç **ï¼š
- LLaVA: https://github.com/haotian-liu/LLaVA
- CLIP: https://github.com/openai/CLIP
- Video-LLaVA: https://github.com/PKU-YuanGroup/Video-LLaVA

**å®è·µå»ºè®®**ï¼š
1. å…ˆç”¨ CLIP ç†Ÿæ‚‰å›¾æ–‡å¯¹é½
2. å°è¯•éƒ¨ç½² LLaVA-1.5-7Bï¼ˆæœ¬åœ° GPUï¼‰
3. ä½¿ç”¨ GPT-4o/Gemini API ä½“éªŒåŸç”Ÿå¤šæ¨¡æ€
4. å°è¯•å¤„ç†ç®€å•è§†é¢‘ï¼ˆæŠ½å¸§æ–¹æ¡ˆï¼‰

---

> **ä¸‹ä¸€æ­¥**ï¼šè¯¦è§ [Part 6 ç¬¬4ç« ] å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ï¼Œå­¦ä¹ å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚
