# ç¬¬4ç« ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹åŸç†

> **æ ¸å¿ƒå®šä½**ï¼šç†è§£æ–‡æœ¬-å›¾åƒç­‰å¤šæ¨¡æ€äº¤äº’çš„æ ¸å¿ƒæŠ€æœ¯ï¼ˆCLIPã€ViTã€LLaVAï¼‰
>
> **è¾¹ç•Œçº¦æŸ**ï¼š
> - âœ… åŒ…å«ï¼šCLIP å¯¹æ¯”å­¦ä¹ ã€ViT æ¶æ„ã€LLaVA è¿æ¥å™¨ã€å¤šæ¨¡æ€æ¨ç†å®æˆ˜
> - âŒ ä¸åŒ…å«ï¼šTransformer åŸºç¡€æœºåˆ¶ï¼ˆå·²åœ¨ Part 2 ç¬¬1ç« ï¼‰ã€å¯¹æ¯”å­¦ä¹ åŸºç¡€ç†è®ºï¼ˆå·²åœ¨ Part 3 ç¬¬4ç« ï¼‰

---

## ç›®å½•

1. [å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£ï¼šå›¾åƒä½œä¸º"å¤–è¯­"](#ä¸€å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£å›¾åƒä½œä¸ºå¤–è¯­)
2. [è§†è§‰ç¼–ç å™¨ï¼šVision Transformer (ViT)](#äºŒè§†è§‰ç¼–ç å™¨vision-transformer-vit)
3. [å›¾æ–‡å¯¹é½ï¼šCLIP](#ä¸‰å›¾æ–‡å¯¹é½clip)
4. [å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼šLLaVA](#å››å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„llava)
5. [å®æˆ˜ï¼šå¤šæ¨¡æ€ç†è§£åº”ç”¨](#äº”å®æˆ˜å¤šæ¨¡æ€ç†è§£åº”ç”¨)
6. [2025è§†è§’ï¼šNative Multimodal çš„è¿›åŒ–](#å…­2025è§†è§’native-multimodal-çš„è¿›åŒ–)
7. [æ€»ç»“ä¸å±•æœ›](#ä¸ƒæ€»ç»“ä¸å±•æœ›)

---

## ä¸€ã€å¤šæ¨¡æ€çš„ç›´è§‰ç†è§£ï¼šå›¾åƒä½œä¸º"å¤–è¯­"

### 1.1 Token Space Alignmentï¼šä¸ºä»€ä¹ˆå›¾åƒå¯ä»¥è¢«è§†ä¸º"å¤–è¯­"

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªåªæ‡‚ä¸­æ–‡çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç°åœ¨ï¼Œæœ‰äººæ‹¿ç€ä¸€å¼ å›¾ç‰‡ï¼Œç”¨ä¸€ç§ä½ ä»æœªè§è¿‡çš„è¯­è¨€ï¼ˆ"å›¾åƒè¯­"ï¼‰å‘ä½ æè¿°ã€‚ä½ è¯¥æ€ä¹ˆåŠï¼Ÿ

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šLLM åªç†è§£æ–‡æœ¬ Tokenï¼Œè€Œå›¾åƒæ˜¯åƒç´ çŸ©é˜µã€‚å°±åƒä¸­æ–‡å’Œè‹±æ–‡ä¸€æ ·ï¼Œå®ƒä»¬æ˜¯**ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„"è¯­è¨€ç©ºé—´"**ã€‚

**è§£å†³æ–¹æ¡ˆï¼šè·¨æ¨¡æ€å¯¹é½ï¼ˆCross-Modal Alignmentï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å›¾åƒç©ºé—´    â”‚                    â”‚  æ–‡æœ¬ç©ºé—´    â”‚
â”‚  (åƒç´ çŸ©é˜µ)  â”‚                    â”‚  (Token åºåˆ—) â”‚
â”‚             â”‚                    â”‚             â”‚
â”‚   [255, 0]  â”‚                    â”‚ "ä¸€åªçŒ«"     â”‚
â”‚   [128, 64] â”‚                    â”‚ "åœ¨è‰åœ°ä¸Š"   â”‚
â”‚   [...]     â”‚                    â”‚ "èººç€"       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â”‚    é€šè¿‡å¯¹é½è®­ç»ƒ                   â”‚
       â”‚    (CLIPã€LLaVA ç­‰)              â”‚
       â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å…±äº«è¯­ä¹‰ç©ºé—´ (Shared Latent Space)   â”‚
â”‚                                            â”‚
â”‚   "çŒ«" â‰ˆ [0.8, -0.3, 0.5, ...]           â”‚
â”‚   ğŸ±   â‰ˆ [0.82, -0.28, 0.51, ...]        â”‚
â”‚                                            â”‚
â”‚   è·ç¦»å¾ˆè¿‘ â†’ è¯­ä¹‰ç›¸ä¼¼ï¼                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. **å›¾åƒç¼–ç å™¨**ï¼šå°†å›¾åƒç¿»è¯‘æˆå‘é‡ï¼ˆå°±åƒæŠŠè‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼‰
2. **æ–‡æœ¬ç¼–ç å™¨**ï¼šå°†æ–‡æœ¬ä¹Ÿç¿»è¯‘æˆå‘é‡
3. **å¯¹é½è®­ç»ƒ**ï¼šè®©æè¿°åŒä¸€äº‹ç‰©çš„å›¾åƒå’Œæ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­**é è¿‘**

### 1.2 æ•°å­¦æœ¬è´¨ï¼šä½™å¼¦ç›¸ä¼¼åº¦

å‡è®¾æˆ‘ä»¬æœ‰ä¸€å¼ çŒ«çš„å›¾ç‰‡ $I$ å’Œæ–‡æœ¬ "a photo of a cat" $T$ã€‚

**ç¼–ç è¿‡ç¨‹**ï¼š
$$
\mathbf{v}_{\text{image}} = E_{\text{vision}}(I) \in \mathbb{R}^d
$$
$$
\mathbf{v}_{\text{text}} = E_{\text{text}}(T) \in \mathbb{R}^d
$$

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼š
$$
\text{sim}(\mathbf{v}_{\text{image}}, \mathbf{v}_{\text{text}}) = \frac{\mathbf{v}_{\text{image}} \cdot \mathbf{v}_{\text{text}}}{\|\mathbf{v}_{\text{image}}\| \|\mathbf{v}_{\text{text}}\|} \in [-1, 1]
$$

- **æ¥è¿‘ 1**ï¼šé«˜åº¦ç›¸å…³ï¼ˆå›¾ç‰‡ç¡®å®æ˜¯çŒ«ï¼‰
- **æ¥è¿‘ 0**ï¼šæ— å…³ï¼ˆå›¾ç‰‡å¯èƒ½æ˜¯ç‹—ï¼‰
- **æ¥è¿‘ -1**ï¼šè´Ÿç›¸å…³ï¼ˆå®é™…åº”ç”¨ä¸­è¾ƒå°‘è§ï¼‰

**ç›´è§‰ç†è§£**ï¼š
- å°±åƒåœ¨é«˜ç»´ç©ºé—´ä¸­æµ‹é‡ä¸¤ä¸ªå‘é‡çš„å¤¹è§’
- å¤¹è§’è¶Šå°ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼

---

## äºŒã€è§†è§‰ç¼–ç å™¨ï¼šVision Transformer (ViT)

> è¯¦è§ [Part 2 ç¬¬1ç« ] Transformer æ ¸å¿ƒæœºåˆ¶ã€‚æœ¬ç« ä»…è®²è§£ ViT å¦‚ä½•å°† Transformer åº”ç”¨äºå›¾åƒã€‚

### 2.1 æ ¸å¿ƒæ€æƒ³ï¼šå›¾åƒæ˜¯ 16Ã—16 çš„å•è¯

**é—®é¢˜**ï¼šTransformer å¤„ç†ä¸€ç»´åºåˆ—ï¼Œä½†å›¾åƒæ˜¯äºŒç»´çš„ $(H \times W)$ã€‚

**ViT çš„è§£å†³æ–¹æ¡ˆ**ï¼šæŠŠå›¾åƒåˆ‡æˆå°æ–¹å—ï¼ˆPatchesï¼‰ï¼Œåƒå¤„ç†å•è¯ä¸€æ ·å¤„ç†å®ƒä»¬ã€‚

```python
# 1. åŸå§‹å›¾åƒ
image = [224, 224, 3]  # é«˜Ã—å®½Ã—é€šé“

# 2. åˆ‡æˆ Patches (æ¯å— 16Ã—16)
num_patches = (224 // 16) * (224 // 16) = 14 * 14 = 196
patches = split_image(image, patch_size=16)  # [196, 16, 16, 3]

# 3. å±•å¹³æ¯ä¸ª Patch
patch_vectors = patches.reshape(196, 16*16*3)  # [196, 768]

# 4. çº¿æ€§æŠ•å½±åˆ° Embedding ç»´åº¦
embeddings = Linear(768, 768)(patch_vectors)  # [196, 768]

# 5. åŠ å…¥ä½ç½®ç¼–ç ï¼ˆå‘Šè¯‰æ¨¡å‹æ¯ä¸ª Patch çš„ä½ç½®ï¼‰
position_embeddings = learnable_params([196, 768])
final_input = embeddings + position_embeddings

# 6. å–‚ç»™ Transformerï¼
```

**ç±»æ¯”**ï¼š
- **NLP**ï¼šä¸€å¥è¯ = ["æˆ‘", "çˆ±", "çŒ«"] â†’ 3 ä¸ª Token
- **ViT**ï¼šä¸€å¼ å›¾ = [Patchâ‚, Patchâ‚‚, ..., Patchâ‚â‚‰â‚†] â†’ 196 ä¸ª Token

### 2.2 ViT ä»£ç å®ç°

```python
import torch
import torch.nn as nn

class PatchEmbedding(nn.Module):
    """å°†å›¾åƒåˆ‡åˆ†æˆ Patches å¹¶æ˜ å°„åˆ° Embedding ç©ºé—´"""
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.n_patches = (img_size // patch_size) ** 2

        # ä½¿ç”¨å·ç§¯å®ç°åˆ†å—+æŠ•å½±ï¼ˆæœ€é«˜æ•ˆçš„æ–¹å¼ï¼‰
        # kernel_size=patch_size, stride=patch_size å®ç°éé‡å åˆ†å—
        self.proj = nn.Conv2d(in_channels, embed_dim,
                              kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        # x: [batch, 3, 224, 224]
        x = self.proj(x)         # [batch, 768, 14, 14]
        x = x.flatten(2)         # [batch, 768, 196]
        x = x.transpose(1, 2)    # [batch, 196, 768]
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, embed_dim=768,
                 num_heads=12, num_layers=12, num_classes=1000):
        super().__init__()

        # 1. Patch Embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)

        # 2. CLS Tokenï¼ˆç±»ä¼¼ BERT çš„ [CLS]ï¼Œç”¨äºåˆ†ç±»ï¼‰
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # 3. Position Embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)
        )

        # 4. Transformer Encoderï¼ˆè¯¦è§ Part 2 ç¬¬1ç« ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads,
            activation='gelu', batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # 5. Classification Head
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        B = x.shape[0]

        # Patch Embedding
        x = self.patch_embed(x)  # [B, 196, 768]

        # æ·»åŠ  CLS Token
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, 768]
        x = torch.cat((cls_tokens, x), dim=1)           # [B, 197, 768]

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed

        # Transformer Encoder
        x = self.encoder(x)

        # å– CLS Token è¿›è¡Œåˆ†ç±»
        cls_output = x[:, 0]        # [B, 768]
        logits = self.head(cls_output)  # [B, 1000]

        return logits

# æµ‹è¯•
if __name__ == "__main__":
    model = VisionTransformer()
    dummy_img = torch.randn(2, 3, 224, 224)  # æ‰¹é‡å¤§å°=2
    output = model(dummy_img)
    print(f"è¾“å…¥: {dummy_img.shape}, è¾“å‡º: {output.shape}")
    # è¾“å…¥: torch.Size([2, 3, 224, 224]), è¾“å‡º: torch.Size([2, 1000])
```

**å…³é”®ç‚¹**ï¼š
- **Patch Embedding**ï¼šç”¨å·ç§¯é«˜æ•ˆå®ç°åˆ†å—
- **CLS Token**ï¼šå…¨å±€ç‰¹å¾èšåˆï¼ˆå¯é€‰ï¼Œæœ‰äº› ViT ç”¨å…¨å±€å¹³å‡æ± åŒ–ï¼‰
- **ä½ç½®ç¼–ç **ï¼šViT é€šå¸¸ä½¿ç”¨**å¯å­¦ä¹ **çš„ä½ç½®ç¼–ç ï¼ˆä¸ Transformer çš„æ­£å¼¦ç¼–ç ä¸åŒï¼‰

---

## ä¸‰ã€å›¾æ–‡å¯¹é½ï¼šCLIP

> **CLIP (Contrastive Language-Image Pre-training)** æ˜¯ OpenAI 2021 å¹´æå‡ºçš„çªç ´æ€§å·¥ä½œï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®©å›¾åƒå’Œæ–‡æœ¬åœ¨åŒä¸€ç©ºé—´ä¸­å¯¹é½ã€‚

### 3.1 æ ¸å¿ƒæœºåˆ¶ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰

> è¯¦è§ [Part 3 ç¬¬4ç« ] å¯¹æ¯”å­¦ä¹ è¯¦è§£ã€‚æœ¬èŠ‚ä»…è®²è§£ CLIP çš„å…·ä½“å®ç°ã€‚

**è®­ç»ƒæ•°æ®**ï¼š4 äº¿ä¸ªï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹ï¼Œä»äº’è”ç½‘çˆ¬å–ã€‚

**è®­ç»ƒç›®æ ‡**ï¼š
- **æ­£æ ·æœ¬å¯¹** $(I_i, T_i)$ï¼šç›¸ä¼¼åº¦**æœ€å¤§åŒ–**
- **è´Ÿæ ·æœ¬å¯¹** $(I_i, T_j)_{i \neq j}$ï¼šç›¸ä¼¼åº¦**æœ€å°åŒ–**

```python
import torch
import torch.nn.functional as F

def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    CLIP çš„ InfoNCE æŸå¤±

    Args:
        image_embeddings: [N, D] - N å¼ å›¾åƒçš„ç‰¹å¾å‘é‡
        text_embeddings: [N, D] - N ä¸ªæ–‡æœ¬çš„ç‰¹å¾å‘é‡
        temperature: æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶ softmax åˆ†å¸ƒçš„å¹³æ»‘åº¦
    """
    # 1. å½’ä¸€åŒ–ï¼ˆç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ­£ç¡®ï¼‰
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)

    # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [N, N]
    # logits[i, j] = sim(image_i, text_j)
    logits = (image_embeddings @ text_embeddings.T) / temperature

    # 3. å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä½™æ˜¯è´Ÿæ ·æœ¬
    labels = torch.arange(len(logits)).to(logits.device)

    # 4. åŒå‘æŸå¤±ï¼ˆå›¾åƒâ†’æ–‡æœ¬ + æ–‡æœ¬â†’å›¾åƒï¼‰
    loss_i2t = F.cross_entropy(logits, labels)        # å›¾åƒæŸ¥æ–‡æœ¬
    loss_t2i = F.cross_entropy(logits.T, labels)      # æ–‡æœ¬æŸ¥å›¾åƒ

    loss = (loss_i2t + loss_t2i) / 2
    return loss
```

**æ•°å­¦è¡¨è¾¾**ï¼ˆå›¾åƒâ†’æ–‡æœ¬æ–¹å‘ï¼‰ï¼š
$$
\mathcal{L}_{i \to t} = -\log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^N \exp(\text{sim}(I_i, T_j) / \tau)}
$$

**ç›´è§‰è§£é‡Š**ï¼š
- **åˆ†å­**ï¼šæ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
- **åˆ†æ¯**ï¼šæ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆæ­£æ ·æœ¬åº”è¯¥è¿œå¤§äºè´Ÿæ ·æœ¬ï¼‰
- **$\tau$ (æ¸©åº¦)**ï¼šè¶Šå°ï¼Œæ¨¡å‹å¯¹éš¾è´Ÿæ ·æœ¬è¶Šæ•æ„Ÿ

### 3.2 CLIP çš„å®é™…ä½¿ç”¨

**é›¶æ ·æœ¬å›¾åƒåˆ†ç±»**ï¼ˆZero-shot Classificationï¼‰

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

# 1. åŠ è½½é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹
model_name = "openai/clip-vit-base-patch32"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 2. å‡†å¤‡å›¾åƒ
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 3. å®šä¹‰å€™é€‰ç±»åˆ«ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼ï¼‰
candidates = [
    "a photo of a cat",
    "a photo of a dog",
    "a photo of a bird",
    "a photo of remote controls"  # å›¾ä¸­å®é™…æœ‰é¥æ§å™¨
]

# 4. ç¼–ç 
inputs = processor(text=candidates, images=image,
                   return_tensors="pt", padding=True)

# 5. å‰å‘ä¼ æ’­
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # [1, 4]
probs = logits_per_image.softmax(dim=1)      # è½¬ä¸ºæ¦‚ç‡

# 6. è¾“å‡ºç»“æœ
print("å€™é€‰ç±»åˆ«:", candidates)
print("åŒ¹é…æ¦‚ç‡:", probs.detach().numpy()[0])

# é¢„æœŸè¾“å‡ºï¼ˆå®é™…å›¾ç‰‡æ˜¯ä¸¤åªçŒ«å’Œä¸€äº›é¥æ§å™¨ï¼‰ï¼š
# å€™é€‰ç±»åˆ«: ['a photo of a cat', 'a photo of a dog', 'a photo of a bird', 'a photo of remote controls']
# åŒ¹é…æ¦‚ç‡: [0.85, 0.02, 0.01, 0.12]  (çŒ«çš„æ¦‚ç‡æœ€é«˜)
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- **é›¶æ ·æœ¬èƒ½åŠ›**ï¼šä¸éœ€è¦ä¸“é—¨è®­ç»ƒåˆ†ç±»å™¨ï¼Œç›´æ¥ç”¨æ–‡æœ¬æè¿°ç±»åˆ«
- **çµæ´»æ€§**ï¼šå¯ä»¥éšæ—¶æ”¹å˜å€™é€‰ç±»åˆ«ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ

### 3.3 CLIP çš„åº”ç”¨åœºæ™¯

1. **é›¶æ ·æœ¬å›¾åƒåˆ†ç±»**ï¼ˆå¦‚ä¸Šä¾‹ï¼‰
2. **å›¾æ–‡æ£€ç´¢**ï¼ˆè¯¦è§ç¬¬äº”èŠ‚å®æˆ˜ï¼‰
3. **å¤šæ¨¡æ€æœç´¢**ï¼šè¾“å…¥æ–‡å­—æœå›¾ç‰‡ï¼Œæˆ–è¾“å…¥å›¾ç‰‡æœç›¸ä¼¼å›¾ç‰‡
4. **å›¾åƒç”Ÿæˆå¼•å¯¼**ï¼šStable Diffusionã€DALL-E ä½¿ç”¨ CLIP å¼•å¯¼ç”Ÿæˆ

---

## å››ã€å¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼šLLaVA

> **LLaVA (Large Language and Vision Assistant)** æ˜¯å½“å‰æœ€æµè¡Œçš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹æ¶æ„ï¼Œè®¾è®¡ç†å¿µç®€å•ä¼˜é›…ã€‚

### 4.1 æ¶æ„è®¾è®¡ï¼šä¸‰ä¸ªç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚  è¾“å…¥: å›¾åƒ + æ–‡æœ¬æŒ‡ä»¤                          â”‚
â”‚  "è¯·æè¿°è¿™å¼ å›¾ç‰‡"                               â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£ Vision Encoder (CLIP ViT-L/14)              â”‚
â”‚     - å†»ç»“å‚æ•°ï¼Œä¸è®­ç»ƒ                           â”‚
â”‚     - è¾“å‡º: [576, 1024] è§†è§‰ Token              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£ Projection Layer (æŠ•å½±å±‚)                   â”‚
â”‚     - å¯è®­ç»ƒçš„ MLP: 1024 â†’ 4096 ç»´              â”‚
â”‚     - å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM çš„ Token ç©ºé—´         â”‚
â”‚     - è¾“å‡º: [576, 4096] "ä¼ªè£…"æˆæ–‡æœ¬ Token      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3ï¸âƒ£ LLM (Vicuna-7B / LLaMA-7B)                 â”‚
â”‚     - å¤„ç†: [è§†è§‰ Token] + [æ–‡æœ¬ Token]         â”‚
â”‚     - ç”Ÿæˆ: "è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†..."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- **Vision Encoder**ï¼šæå–è§†è§‰ç‰¹å¾ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„ CLIPï¼‰
- **Projection Layer**ï¼šæ¡¥æ¥è§†è§‰å’Œè¯­è¨€ç©ºé—´ï¼ˆ**å…³é”®åˆ›æ–°**ï¼‰
- **LLM**ï¼šç†è§£å¹¶ç”Ÿæˆæ–‡æœ¬

### 4.2 Projection Layerï¼šToken Space Alignment çš„å®ç°

**é—®é¢˜**ï¼š
- CLIP ViT è¾“å‡ºç»´åº¦ï¼š1024
- LLaMA-7B Token ç»´åº¦ï¼š4096

**è§£å†³æ–¹æ¡ˆ**ï¼šç®€å•çš„ MLP

```python
class ProjectionLayer(nn.Module):
    """å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ° LLM çš„ Token Embedding ç©ºé—´"""
    def __init__(self, vision_dim=1024, llm_dim=4096):
        super().__init__()
        # ä¸¤å±‚ MLP
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, vision_features):
        # vision_features: [B, N, 1024] (N=576 ä¸ªè§†è§‰ Token)
        # è¾“å‡º: [B, N, 4096]
        return self.proj(vision_features)

# ä½¿ç”¨ç¤ºä¾‹
proj = ProjectionLayer()
vision_tokens = torch.randn(1, 576, 1024)  # CLIP è¾“å‡º
llm_tokens = proj(vision_tokens)           # [1, 576, 4096]

# ç°åœ¨å¯ä»¥ä¸æ–‡æœ¬ Token æ‹¼æ¥ï¼
text_tokens = torch.randn(1, 20, 4096)     # æ–‡æœ¬ Token
combined = torch.cat([llm_tokens, text_tokens], dim=1)  # [1, 596, 4096]
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼Ÿ
- è§†è§‰ç‰¹å¾ç»è¿‡æŠ•å½±åï¼Œ"ä¼ªè£…"æˆäº† LLM å¯ä»¥ç†è§£çš„ Token
- LLM å°±åƒåœ¨å¤„ç†ä¸€æ®µ"ç‰¹æ®Šè¯­è¨€"ï¼ˆå›¾åƒè¯­ï¼‰ï¼Œä½†ä½¿ç”¨ç›¸åŒçš„ Transformer æœºåˆ¶

### 4.3 LLaVA çš„ä¸¤é˜¶æ®µè®­ç»ƒ

> è¿™ç§**ä¸¤é˜¶æ®µè®­ç»ƒæ³•**å·²æˆä¸ºè¡Œä¸šæ ‡å‡†ã€‚

#### é˜¶æ®µä¸€ï¼šç‰¹å¾å¯¹é½é¢„è®­ç»ƒï¼ˆFeature Alignment Pre-trainingï¼‰

**è®­ç»ƒç­–ç•¥**ï¼š
- ğŸ”’ **å†»ç»“**ï¼šVision Encoder + LLM
- ğŸ”¥ **è®­ç»ƒ**ï¼šä»… Projection Layer

**æ•°æ®**ï¼šCC3Mï¼ˆ300 ä¸‡å›¾åƒ-æ ‡é¢˜å¯¹ï¼‰

**ç›®çš„**ï¼šè®© Projection Layer å­¦ä¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM èƒ½ç†è§£çš„ç©ºé—´

```python
# ä¼ªä»£ç 
for image, caption in dataset:
    # 1. æå–è§†è§‰ç‰¹å¾ï¼ˆå†»ç»“ï¼‰
    vision_features = vision_encoder(image)  # [B, 576, 1024]

    # 2. æŠ•å½±åˆ° LLM ç©ºé—´ï¼ˆè®­ç»ƒï¼‰
    visual_tokens = projection_layer(vision_features)  # [B, 576, 4096]

    # 3. æ‹¼æ¥æ–‡æœ¬ Tokenï¼ˆå†»ç»“ï¼‰
    text_tokens = llm.tokenize(caption)
    combined_tokens = concat([visual_tokens, text_tokens])

    # 4. è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä»…åå‘ä¼ æ’­åˆ° Projection Layerï¼‰
    loss = llm.forward(combined_tokens, labels=caption)
    loss.backward()  # åªæ›´æ–° Projection Layer çš„å‚æ•°
```

#### é˜¶æ®µäºŒï¼šè§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVisual Instruction Tuningï¼‰

**è®­ç»ƒç­–ç•¥**ï¼š
- ğŸ”’ **å†»ç»“**ï¼šVision Encoder
- ğŸ”¥ **è®­ç»ƒ**ï¼šProjection Layer + LLM

**æ•°æ®**ï¼šé«˜è´¨é‡è§†è§‰æŒ‡ä»¤æ•°æ®ï¼ˆå¦‚ LLaVA-Instruct-150Kï¼‰

**ç¤ºä¾‹æ•°æ®**ï¼š
```json
{
    "image": "beach.jpg",
    "conversations": [
        {
            "role": "human",
            "content": "è¿™å¼ å›¾ç‰‡ä¸­å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªç¾ä¸½çš„æµ·æ»©æ—¥è½åœºæ™¯ã€‚å¤©ç©ºå‘ˆç°å‡ºæ©™è‰²å’Œç´«è‰²çš„æ¸å˜ï¼Œæµ·æµªè½»æŸ”åœ°æ‹æ‰“ç€æ²™æ»©ã€‚è¿œå¤„å¯ä»¥çœ‹åˆ°å‡ åªæµ·é¸¥åœ¨é£ç¿”ã€‚æ•´ä½“æ°›å›´å®é™è€Œç¥¥å’Œã€‚"
        }
    ]
}
```

**ç›®çš„**ï¼š
- æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›
- å­¦ä¼šéµå¾ªè§†è§‰ç›¸å…³çš„æŒ‡ä»¤
- ç”Ÿæˆæ›´è¯¦ç»†ã€å‡†ç¡®çš„æè¿°

### 4.4 å…¶ä»–è¿æ¥å™¨æ–¹æ¡ˆï¼šPerceiver Resampler (Flamingo/IDEFICS)

LLaVA ä½¿ç”¨ç®€å•çš„ MLPï¼Œä½†æœ‰äº›æ¨¡å‹ä½¿ç”¨æ›´å¤æ‚çš„è¿æ¥å™¨ã€‚

**Perceiver Resampler**ï¼ˆFlamingo æ¶æ„ï¼‰ï¼š

```python
class PerceiverResampler(nn.Module):
    """
    ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å‹ç¼©è§†è§‰ Token
    """
    def __init__(self, num_queries=64, vision_dim=1024, llm_dim=4096, depth=6):
        super().__init__()
        # å¯å­¦ä¹ çš„ Query Token
        self.queries = nn.Parameter(torch.randn(num_queries, llm_dim))

        # å¤šå±‚äº¤å‰æ³¨æ„åŠ›
        self.layers = nn.ModuleList([
            nn.MultiheadAttention(llm_dim, num_heads=16)
            for _ in range(depth)
        ])

    def forward(self, vision_features):
        # vision_features: [B, 576, 1024]

        # 1. å…ˆæŠ•å½±åˆ° LLM ç»´åº¦
        vision_features = nn.Linear(1024, 4096)(vision_features)  # [B, 576, 4096]

        # 2. ä½¿ç”¨å›ºå®šæ•°é‡çš„ Queries æå–ä¿¡æ¯
        B = vision_features.size(0)
        queries = self.queries.unsqueeze(0).expand(B, -1, -1)  # [B, 64, 4096]

        # 3. å¤šå±‚äº¤å‰æ³¨æ„åŠ›
        for layer in self.layers:
            queries, _ = layer(
                query=queries.transpose(0, 1),        # [64, B, 4096]
                key=vision_features.transpose(0, 1),  # [576, B, 4096]
                value=vision_features.transpose(0, 1)
            )
            queries = queries.transpose(0, 1)         # [B, 64, 4096]

        # è¾“å‡º: [B, 64, 4096] (ä» 576 å‹ç¼©åˆ° 64 ä¸ª Token!)
        return queries
```

**å¯¹æ¯”**ï¼š
| æ–¹æ¡ˆ | è¾“å‡º Token æ•° | å¤æ‚åº¦ | ä»£è¡¨æ¨¡å‹ |
|------|--------------|--------|----------|
| **MLP (LLaVA)** | 576 | ä½ | LLaVA, Qwen-VL |
| **Perceiver Resampler** | 64 | ä¸­ | Flamingo, IDEFICS |
| **Q-Former (BLIP-2)** | 32 | é«˜ | BLIP-2, InstructBLIP |

**æƒè¡¡**ï¼š
- **æ›´å¤š Token**ï¼šä¿ç•™æ›´å¤šè§†è§‰ç»†èŠ‚ï¼Œä½†å¢åŠ  LLM è®¡ç®—é‡
- **æ›´å°‘ Token**ï¼šè®¡ç®—é«˜æ•ˆï¼Œä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚

---

## äº”ã€å®æˆ˜ï¼šå¤šæ¨¡æ€ç†è§£åº”ç”¨

### 5.1 ä½¿ç”¨å¼€æºæ¨¡å‹ï¼šLLaVA å›¾åƒé—®ç­”

```python
"""
ä½¿ç”¨ Hugging Face çš„ LLaVA-1.5-7B è¿›è¡Œå›¾åƒç†è§£
éœ€è¦: pip install transformers accelerate pillow
"""
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests
import torch

# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
model_id = "llava-hf/llava-1.5-7b-hf"
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"  # è‡ªåŠ¨åˆ†é… GPU/CPU
)
processor = AutoProcessor.from_pretrained(model_id)

# 2. å‡†å¤‡å›¾åƒå’Œé—®é¢˜
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image = Image.open(requests.get(url, stream=True).raw)

prompt = "USER: <image>\nWhat's the content of this image?\nASSISTANT:"

# 3. å¤„ç†è¾“å…¥
inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")

# 4. ç”Ÿæˆå›ç­”
with torch.inference_mode():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=False
    )

# 5. è§£ç è¾“å‡º
output = processor.decode(generated_ids[0], skip_special_tokens=True)
answer = output.split("ASSISTANT:")[-1].strip()

print("å›¾åƒ:", url)
print("é—®é¢˜:", "What's the content of this image?")
print("å›ç­”:", answer)

# é¢„æœŸè¾“å‡º:
# "This image shows a red stop sign at a street intersection in Australia.
#  The sign features white text in English and additional text in Chinese characters."
```

### 5.2 å®æˆ˜ï¼šæ„å»ºæœ¬åœ°å›¾æ–‡æ£€ç´¢å¼•æ“

ä½¿ç”¨ CLIP æ„å»ºä¸€ä¸ªç®€å•çš„å›¾ç‰‡æœç´¢å¼•æ“ã€‚

```python
import os
import torch
import glob
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import numpy as np

class ImageSearchEngine:
    """åŸºäº CLIP çš„å›¾æ–‡æ£€ç´¢å¼•æ“"""

    def __init__(self, model_id="openai/clip-vit-base-patch32"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½ CLIP æ¨¡å‹
        self.model = CLIPModel.from_pretrained(model_id).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_id)

        # ç´¢å¼•æ•°æ®
        self.image_paths = []
        self.image_features = None

    def index_images(self, image_dir):
        """ä¸ºæŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡å»ºç«‹ç‰¹å¾ç´¢å¼•"""
        # 1. æ”¶é›†å›¾ç‰‡è·¯å¾„
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.webp']
        for ext in extensions:
            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))

        print(f"æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å»ºç«‹ç´¢å¼•...")

        # 2. æ‰¹é‡æå–ç‰¹å¾
        all_features = []
        batch_size = 32

        for i in range(0, len(self.image_paths), batch_size):
            batch_paths = self.image_paths[i:i+batch_size]
            images = []

            # åŠ è½½å›¾ç‰‡
            for path in batch_paths:
                try:
                    images.append(Image.open(path).convert("RGB"))
                except Exception as e:
                    print(f"è¯»å–å¤±è´¥ {path}: {e}")
                    continue

            if not images:
                continue

            # æå–ç‰¹å¾
            with torch.no_grad():
                inputs = self.processor(images=images, return_tensors="pt",
                                       padding=True).to(self.device)
                features = self.model.get_image_features(**inputs)
                # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                features = features / features.norm(p=2, dim=-1, keepdim=True)
                all_features.append(features.cpu())

            print(f"å·²ç´¢å¼•: {min(i+batch_size, len(self.image_paths))}/{len(self.image_paths)}")

        # 3. åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.image_features = torch.cat(all_features)
        print("ç´¢å¼•å®Œæˆ!")

    def search(self, query_text, top_k=5):
        """ä½¿ç”¨æ–‡æœ¬æœç´¢å›¾ç‰‡"""
        if self.image_features is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ index_images() å»ºç«‹ç´¢å¼•")

        # 1. ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        with torch.no_grad():
            inputs = self.processor(text=[query_text], return_tensors="pt",
                                   padding=True).to(self.device)
            text_features = self.model.get_text_features(**inputs)
            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)

        # 2. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        # [1, D] @ [N, D]^T = [1, N]
        similarities = (text_features.cpu() @ self.image_features.T).squeeze(0)

        # 3. è·å– Top-K
        values, indices = similarities.topk(top_k)

        results = []
        for val, idx in zip(values, indices):
            results.append({
                'path': self.image_paths[idx],
                'score': val.item()
            })

        return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»ºæœç´¢å¼•æ“å¹¶å»ºç«‹ç´¢å¼•
    engine = ImageSearchEngine()
    engine.index_images("./my_photos")  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹

    # 2. æœç´¢
    queries = [
        "a dog playing in the park",
        "sunset at the beach",
        "a person reading a book"
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: '{query}'")
        results = engine.search(query, top_k=3)

        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['path']} (ç›¸ä¼¼åº¦: {result['score']:.4f})")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ‰¾åˆ° 1523 å¼ å›¾ç‰‡ï¼Œå¼€å§‹å»ºç«‹ç´¢å¼•...
å·²ç´¢å¼•: 32/1523
å·²ç´¢å¼•: 64/1523
...
ç´¢å¼•å®Œæˆ!

æŸ¥è¯¢: 'a dog playing in the park'
  1. ./my_photos/IMG_2023.jpg (ç›¸ä¼¼åº¦: 0.8752)
  2. ./my_photos/IMG_1845.jpg (ç›¸ä¼¼åº¦: 0.8231)
  3. ./my_photos/IMG_2091.jpg (ç›¸ä¼¼åº¦: 0.7963)
```

### 5.3 å®æˆ˜ï¼šä½¿ç”¨ GPT-4V è¿›è¡Œé«˜çº§è§†è§‰ç†è§£

```python
"""
è°ƒç”¨ GPT-4V API è¿›è¡Œå›¾åƒç†è§£
éœ€è¦: pip install openai
"""
from openai import OpenAI
import base64

client = OpenAI(api_key="your-api-key")

def encode_image(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image(image_path, question):
    """ä½¿ç”¨ GPT-4V åˆ†æå›¾åƒ"""
    base64_image = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",  # æˆ– "gpt-4-turbo"
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": question
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"  # é«˜åˆ†è¾¨ç‡æ¨¡å¼
                        }
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. å›¾åƒæè¿°
    description = analyze_image(
        "chart.png",
        "è¯¦ç»†æè¿°è¿™å¼ å›¾è¡¨ï¼ŒåŒ…æ‹¬ç±»å‹ã€è¶‹åŠ¿å’Œå…³é”®æ•°æ®ç‚¹"
    )
    print("å›¾è¡¨åˆ†æ:", description)

    # 2. OCR + ç»“æ„åŒ–è¾“å‡º
    ocr_result = analyze_image(
        "receipt.jpg",
        "æå–è¿™å¼ æ”¶æ®ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…æ‹¬ï¼šå•†å®¶åç§°ã€æ—¥æœŸã€å•†å“åˆ—è¡¨ã€æ€»é‡‘é¢"
    )
    print("æ”¶æ®ä¿¡æ¯:", ocr_result)

    # 3. è§†è§‰æ¨ç†
    reasoning = analyze_image(
        "scene.jpg",
        "è¿™å¼ å›¾ç‰‡ä¸­æœ‰å“ªäº›æ½œåœ¨çš„å®‰å…¨éšæ‚£ï¼Ÿè¯·åˆ—ä¸¾å¹¶è§£é‡Š"
    )
    print("å®‰å…¨åˆ†æ:", reasoning)
```

---

## å…­ã€2025è§†è§’ï¼šNative Multimodal çš„è¿›åŒ–

### 6.1 LLaVA è¿æ¥å™¨æ–¹æ¡ˆ vs Native Multimodal

**LLaVA æ–¹æ¡ˆ**ï¼ˆæ‹¼æ¥å¼ï¼‰ï¼š
```
[é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨] â†’ [æŠ•å½±å±‚] â†’ [é¢„è®­ç»ƒ LLM]
         â†‘                â†‘            â†‘
      å†»ç»“å‚æ•°         å¯è®­ç»ƒ        å¾®è°ƒ
```

**ä¼˜åŠ¿**ï¼š
- âœ… è®­ç»ƒé«˜æ•ˆï¼ˆåªéœ€è®­ç»ƒå°éƒ¨åˆ†å‚æ•°ï¼‰
- âœ… å¯å¤ç”¨ç°æœ‰æ¨¡å‹ï¼ˆCLIP + LLaMAï¼‰
- âœ… æ˜“äºå®ç°å’Œè°ƒè¯•

**å±€é™**ï¼š
- âŒ ä¸¤ä¸ªæ¨¡æ€çš„è®­ç»ƒæ•°æ®ä¸åŒï¼ˆè§†è§‰å’Œè¯­è¨€åˆ†ç¦»ï¼‰
- âŒ æŠ•å½±å±‚å¯èƒ½æˆä¸ºä¿¡æ¯ç“¶é¢ˆ
- âŒ éš¾ä»¥å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’

---

**Native Multimodal**ï¼ˆåŸç”Ÿå¤šæ¨¡æ€ï¼‰ï¼šGPT-4oã€Gemini 1.5

```
ä»é›¶å¼€å§‹ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è”åˆè®­ç»ƒ
```

**æ ¸å¿ƒåŒºåˆ«**ï¼š

| ç»´åº¦ | LLaVA (è¿æ¥å™¨) | GPT-4o/Gemini (åŸç”Ÿ) |
|------|----------------|---------------------|
| **è®­ç»ƒèŒƒå¼** | å…ˆå•æ¨¡æ€åå¤šæ¨¡æ€ | å¤šæ¨¡æ€è”åˆè®­ç»ƒ |
| **æ¶æ„** | ç‹¬ç«‹ç¼–ç å™¨ + æŠ•å½±å±‚ | ç»Ÿä¸€ Transformer |
| **Token ç©ºé—´** | åˆ†ç¦»åå¯¹é½ | åŸç”Ÿç»Ÿä¸€ |
| **é•¿ä¸Šä¸‹æ–‡** | ~8K (å— LLM é™åˆ¶) | 128K+ |
| **ç»†ç²’åº¦äº¤äº’** | å—é™äºæŠ•å½±å±‚ | ä»»æ„å±‚çº§äº¤äº’ |
| **è®­ç»ƒæˆæœ¬** | ä½ï¼ˆæ•°ç™¾ä¸‡ç¾å…ƒï¼‰ | æé«˜ï¼ˆæ•°äº¿ç¾å…ƒï¼‰ |

### 6.2 Native Multimodal çš„æŠ€æœ¯æ¨æµ‹

**Gemini çš„ç»Ÿä¸€ Token åŒ–**ï¼ˆæ¨æµ‹ï¼‰ï¼š

```python
class UnifiedTokenizer:
    """åŸç”Ÿå¤šæ¨¡æ€ Tokenizer"""

    def __init__(self):
        # æ–‡æœ¬è¯è¡¨
        self.text_vocab_size = 256000

        # è§†è§‰ Tokenï¼ˆä½¿ç”¨ VQ-VAE é‡åŒ–ï¼‰
        self.visual_vocab_size = 8192

        # ç»Ÿä¸€è¯è¡¨
        self.total_vocab_size = self.text_vocab_size + self.visual_vocab_size

    def tokenize(self, text=None, image=None):
        tokens = []

        if text:
            # æ–‡æœ¬ Token: [0, 256000)
            text_tokens = self.tokenize_text(text)
            tokens.extend(text_tokens)

        if image:
            # è§†è§‰ Token: [256000, 264192)
            visual_tokens = self.vq_vae_encode(image)  # é‡åŒ–ä¸ºç¦»æ•£ ID
            visual_tokens = visual_tokens + self.text_vocab_size
            tokens.extend(visual_tokens)

        return tokens
```

**ä¼˜åŠ¿**ï¼š
1. **æ— ç¼é›†æˆ**ï¼šå›¾åƒå’Œæ–‡æœ¬åœ¨åŒä¸€ Token ç©ºé—´ï¼Œæ— éœ€æŠ•å½±å±‚
2. **ä»»æ„äº¤é”™**ï¼šå¯ä»¥åœ¨åŒä¸€åºåˆ—ä¸­è‡ªç”±æ··åˆå›¾åƒå’Œæ–‡æœ¬
3. **ç»†ç²’åº¦äº¤äº’**ï¼šæ¯å±‚ Transformer éƒ½èƒ½å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯

### 6.3 é€‰å‹å»ºè®®ï¼ˆ2025ï¼‰

**åœºæ™¯ 1ï¼šç ”ç©¶å’ŒåŸå‹å¼€å‘**
- æ¨èï¼š**LLaVA**ã€Qwen-VLï¼ˆå¼€æºï¼Œæ˜“éƒ¨ç½²ï¼‰
- ç†ç”±ï¼šå¿«é€Ÿè¿­ä»£ï¼Œæˆæœ¬ä½

**åœºæ™¯ 2ï¼šç”Ÿäº§ç¯å¢ƒï¼ˆé«˜ç²¾åº¦éœ€æ±‚ï¼‰**
- æ¨èï¼š**GPT-4o**ã€Claude 3.5 Sonnetï¼ˆAPIï¼‰
- ç†ç”±ï¼šæ€§èƒ½æœ€å¼ºï¼Œæ”¯æŒå¤æ‚æ¨ç†

**åœºæ™¯ 3ï¼šç”Ÿäº§ç¯å¢ƒï¼ˆæˆæœ¬æ•æ„Ÿï¼‰**
- æ¨èï¼š**Gemini 1.5 Flash**ï¼ˆAPIï¼‰æˆ–è‡ªéƒ¨ç½² LLaVA
- ç†ç”±ï¼šæ€§ä»·æ¯”é«˜

**åœºæ™¯ 4ï¼šé•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€**
- æ¨èï¼š**Gemini 1.5 Pro**ï¼ˆ1M Token ä¸Šä¸‹æ–‡ï¼‰
- ç†ç”±ï¼šå”¯ä¸€æ”¯æŒè¶…é•¿è§†é¢‘/æ–‡æ¡£åˆ†æ

---

## ä¸ƒã€æ€»ç»“ä¸å±•æœ›

### 7.1 æ ¸å¿ƒçŸ¥è¯†ç‚¹å›é¡¾

| æŠ€æœ¯ | æ ¸å¿ƒæ€æƒ³ | å…³é”®åˆ›æ–° |
|------|---------|---------|
| **ViT** | å›¾åƒåˆ†å— â†’ Transformer | è¯æ˜ Transformer å¯å¤„ç†è§†è§‰ |
| **CLIP** | å¯¹æ¯”å­¦ä¹ å¯¹é½å›¾æ–‡ | é›¶æ ·æœ¬èƒ½åŠ›ï¼Œè·¨æ¨¡æ€æ£€ç´¢ |
| **LLaVA** | æŠ•å½±å±‚è¿æ¥è§†è§‰å’Œè¯­è¨€ | ç®€å•é«˜æ•ˆï¼Œæ˜“äºè®­ç»ƒ |
| **Native Multimodal** | ç»Ÿä¸€ Token ç©ºé—´ | æ›´å¼ºäº¤äº’ï¼Œæ›´é•¿ä¸Šä¸‹æ–‡ |

### 7.2 å¤šæ¨¡æ€æŠ€æœ¯æ¼”è¿›è·¯çº¿

```
2017: Transformer è¯ç”Ÿï¼ˆçº¯æ–‡æœ¬ï¼‰
  â†“
2020: ViT è¯æ˜ Transformer å¯å¤„ç†å›¾åƒ
  â†“
2021: CLIP å®ç°å›¾æ–‡å¯¹é½ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
  â†“
2023: LLaVA è¿æ¥ LLM å’Œè§†è§‰ï¼ˆæŠ•å½±å±‚æ–¹æ¡ˆï¼‰
  â†“
2024: GPT-4V/Gemini åŸç”Ÿå¤šæ¨¡æ€ï¼ˆç«¯åˆ°ç«¯è®­ç»ƒï¼‰
  â†“
2025: å¤šæ¨¡æ€æˆä¸ºæ ‡é…ï¼ˆå›¾æ–‡éŸ³è§†é¢‘ç»Ÿä¸€ï¼‰
```

### 7.3 æœªæ¥è¶‹åŠ¿

1. **Any-to-Any æ¨¡å‹**
   - è¾“å…¥ï¼šå›¾/æ–‡/éŸ³/è§†é¢‘
   - è¾“å‡ºï¼šå›¾/æ–‡/éŸ³/è§†é¢‘
   - ä»£è¡¨ï¼šGPT-4oï¼ˆå®æ—¶è¯­éŸ³å¯¹è¯ + è§†è§‰ï¼‰

2. **å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰**
   - å¤šæ¨¡æ€ + æœºå™¨äººæ§åˆ¶
   - æ„ŸçŸ¥ï¼ˆè§†è§‰ï¼‰+ ç†è§£ï¼ˆè¯­è¨€ï¼‰+ è¡ŒåŠ¨ï¼ˆæ§åˆ¶ï¼‰
   - ä»£è¡¨ï¼šRT-2ã€PaLM-E

3. **æ›´é•¿ä¸Šä¸‹æ–‡**
   - å¤„ç†å®Œæ•´ç”µå½±ã€é•¿æ–‡æ¡£
   - Gemini 1.5ï¼š1M Tokenï¼ˆçº¦ 1 å°æ—¶è§†é¢‘ï¼‰

4. **æ›´é«˜æ•ˆçš„è®­ç»ƒ**
   - å°æ¨¡å‹ + å¤§æ•°æ® > å¤§æ¨¡å‹ + å°æ•°æ®
   - LoRAã€QLoRA ç­‰é«˜æ•ˆå¾®è°ƒæŠ€æœ¯

### 7.4 å­¦ä¹ èµ„æº

**è®ºæ–‡**ï¼š
- ViT: [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
- CLIP: [Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020)
- LLaVA: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

**ä»£ç **ï¼š
- LLaVA: https://github.com/haotian-liu/LLaVA
- CLIP: https://github.com/openai/CLIP

**å®è·µå»ºè®®**ï¼š
1. å…ˆç”¨ CLIP ç†Ÿæ‚‰å›¾æ–‡å¯¹é½
2. å°è¯•éƒ¨ç½² LLaVA-1.5-7Bï¼ˆæœ¬åœ° GPUï¼‰
3. ä½¿ç”¨ GPT-4V/Gemini API ä½“éªŒå‰æ²¿èƒ½åŠ›
4. é˜…è¯» Flamingoã€BLIP-2 äº†è§£å…¶ä»–æ¶æ„

---

> **ä¸‹ä¸€æ­¥**ï¼šè¯¦è§ [Part 6 ç¬¬4ç« ] å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°ï¼Œå­¦ä¹ å¦‚ä½•è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚
