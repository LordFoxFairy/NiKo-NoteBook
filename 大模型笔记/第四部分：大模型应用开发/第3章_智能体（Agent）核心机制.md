# ç¬¬3ç« :æ™ºèƒ½ä½“(Agent)æ ¸å¿ƒæœºåˆ¶

> "The future of AI is not just about better models, but about better systems." - Andrew Ng
>
> æ™ºèƒ½ä½“(Agent)å°† LLM ä»"å¤§è„‘"å˜æˆäº†"åŒæ‰‹",è®© AI å…·å¤‡äº†ä¸ä¸–ç•Œäº¤äº’çš„èƒ½åŠ›ã€‚

---

## æœ¬ç« å¯¼è¯»

æœ¬ç« ä¸“æ³¨äº **Agent è®¾è®¡æ¨¡å¼ä¸å·¥ç¨‹å®ç°**,æ˜¯æ„å»ºè‡ªä¸»æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨:
- **ReAct/Plan-and-Solve** ç­‰è§„åˆ’æ¨¡å¼çš„ä»£ç å®ç°
- **Tool Use / Function Calling** çš„ JSON Schema å®šä¹‰ä¸è§£æ
- **MCP (Model Context Protocol)** åè®®æ ‡å‡†ä¸å®æˆ˜
- **LangGraph** çš„ StateGraph ç¼–ç¨‹èŒƒå¼
- **Memory ç³»ç»Ÿ**çš„çŸ­æœŸ/é•¿æœŸè®°å¿†è®¾è®¡
- **Multi-Agent** åä½œæ¨¡å¼ (Supervisor/Hierarchical)

**è¾¹ç•Œè¯´æ˜** (å‚è€ƒ chapter-boundaries.md):
- âœ… **æœ¬ç« åŒ…å«**: Agent æ¶æ„è®¾è®¡ã€å·¥å…·è°ƒç”¨ã€MCP åè®®ã€å¤šæ™ºèƒ½ä½“åä½œã€Memory æœºåˆ¶
- âŒ **ä¸åŒ…å«**: CoT æ•°å­¦åŸç† (â†’ Part 7 Ch3)ã€æ¨ç†æ—¶æœç´¢/MCTS (â†’ Part 7 Ch4)ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ Agent (â†’ Part 7 Ch4)

---

## ç›®å½•
- [ä¸€ã€ä» Prompt Engineering åˆ° Agentic Workflow](#ä¸€ä»-prompt-engineering-åˆ°-agentic-workflow)
- [äºŒã€è§„åˆ’ (Planning):ReAct ä¸ Plan-and-Solve](#äºŒè§„åˆ’-planningreact-ä¸-plan-and-solve)
- [ä¸‰ã€å·¥å…·ä½¿ç”¨ (Tool Use) ä¸ Function Calling](#ä¸‰å·¥å…·ä½¿ç”¨-tool-use-ä¸-function-calling)
- [å››ã€MCP (Model Context Protocol) é©å‘½](#å››mcp-model-context-protocol-é©å‘½)
- [äº”ã€è®°å¿†ç³»ç»Ÿ (Memory) è®¾è®¡](#äº”è®°å¿†ç³»ç»Ÿ-memory-è®¾è®¡)
- [å…­ã€LangGraph:çŠ¶æ€æœºç¼–ç¨‹èŒƒå¼](#å…­langgraphçŠ¶æ€æœºç¼–ç¨‹èŒƒå¼)
  - 6.1 StateGraph æ ¸å¿ƒæ¦‚å¿µ
  - 6.2 å®æˆ˜:åŸºäº LangGraph çš„ ReAct Agent
  - 6.3 æ¡ä»¶è¾¹ä¸å¾ªç¯æ§åˆ¶
  - 6.4 æŒä¹…åŒ– (Persistence): Multi-turn å¯¹è¯çš„åŸºç¡€
  - 6.5 Human-in-the-loop: æ•æ„Ÿæ“ä½œçš„å®¡æ‰¹æœºåˆ¶
- [ä¸ƒã€å¤šæ™ºèƒ½ä½“åä½œ (Multi-Agent)](#ä¸ƒå¤šæ™ºèƒ½ä½“åä½œ-multi-agent)
- [å…«ã€Output Parser:ç»“æ„åŒ–è¾“å‡ºè§£æ](#å…«output-parserç»“æ„åŒ–è¾“å‡ºè§£æ)
- [ä¹ã€æœ¬ç« å°ç»“](#ä¹æœ¬ç« å°ç»“)

---

## ä¸€ã€ä» Prompt Engineering åˆ° Agentic Workflow

Andrew Ng æœ€è¿‘æå‡ºä¸€ä¸ªé‡è¦è§‚ç‚¹:**ä¸å…¶è¿½æ±‚æ›´å¼ºçš„æ¨¡å‹ (GPT-5),ä¸å¦‚ä¼˜åŒ– Agent å·¥ä½œæµ (Agentic Workflow)**ã€‚
GPT-3.5 + è‰¯å¥½çš„å·¥ä½œæµ,å¾€å¾€èƒ½è¶…è¶Šé›¶æ ·æœ¬çš„ GPT-4ã€‚

### 1. ä»€ä¹ˆæ˜¯ Agentic Workflow?

**å¯¹æ¯”ç†è§£**:
- **Zero-Shot**: å°±åƒè®©ä¸€ä¸ªäºº"ä¸€å£æ°”"å†™å®Œä¸€ç¯‡è®ºæ–‡,ä¸è®¸æŸ¥èµ„æ–™,ä¸è®¸ä¿®æ”¹
- **Agentic Workflow**: è®© LLM è¿›è¡Œ**è¿­ä»£å¤„ç†** - åˆ—æçº² â†’ æŸ¥èµ„æ–™ â†’ å†™åˆç¨¿ â†’ è‡ªæˆ‘ä¿®æ”¹ â†’ å®šç¨¿

**æ ¸å¿ƒå·®å¼‚**: å…è®¸ LLM è¿›è¡Œå¤šè½®è¿­ä»£,æ¯ä¸€æ­¥éƒ½å¯ä»¥åæ€å’Œçº é”™ã€‚

### 2. å››ç§æ ¸å¿ƒè®¾è®¡æ¨¡å¼

| æ¨¡å¼ | æ ¸å¿ƒæ€æƒ³ | å…¸å‹åœºæ™¯ |
|------|----------|----------|
| **Reflection** | è®©æ¨¡å‹æ£€æŸ¥è‡ªå·±çš„è¾“å‡º | ä»£ç å®¡æŸ¥ã€è®ºæ–‡æ¶¦è‰² |
| **Tool Use** | æ¨¡å‹çŸ¥é“ä½•æ—¶æ±‚åŠ©å¤–éƒ¨å·¥å…· | è®¡ç®—å™¨ã€æœç´¢å¼•æ“ã€æ•°æ®åº“æŸ¥è¯¢ |
| **Planning** | å…ˆæ‹†è§£æ­¥éª¤,å†é€ä¸€æ‰§è¡Œ | å¤æ‚ä»»åŠ¡åˆ†è§£ |
| **Multi-Agent** | ä¸åŒè§’è‰²åä½œå®Œæˆä»»åŠ¡ | è½¯ä»¶å¼€å‘å›¢é˜Ÿæ¨¡æ‹Ÿ |

---

## äºŒã€è§„åˆ’ (Planning):ReAct ä¸ Plan-and-Solve

### 1. ReAct æ¨¡å¼:ä»£ç å®ç°

ReAct (Reason + Act) æ˜¯æœ€ç»å…¸çš„ Agent æ¨¡å¼,ç”± Yao et al. (2022) æå‡ºã€‚

**æ ¸å¿ƒæ€æƒ³**: Thought (æ€è€ƒ) â†’ Action (è¡ŒåŠ¨) â†’ Observation (è§‚å¯Ÿ) å¾ªç¯ã€‚

#### (1) Prompt æ¨¡æ¿

```python
REACT_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªé—®é¢˜æ±‚è§£åŠ©æ‰‹ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”:

Question: {question}

Thought: æˆ‘éœ€è¦æ€è€ƒå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜
Action: [å·¥å…·åç§°][å‚æ•°]
Observation: [å·¥å…·è¿”å›ç»“æœ]
... (é‡å¤ Thought/Action/Observation å¯å¤šæ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: [æœ€ç»ˆç­”æ¡ˆ]

å¯ç”¨å·¥å…·:
- Search[query]: æœç´¢äº’è”ç½‘
- Calculator[expression]: è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
- WikiLookup[term]: æŸ¥è¯¢ç»´åŸºç™¾ç§‘

ç¤ºä¾‹:
Question: åŸƒè²å°”é“å¡”çš„é«˜åº¦æ˜¯å¤šå°‘?å®ƒæ¯”ä¸Šæµ·ä¸­å¿ƒå¤§å¦é«˜å¤šå°‘?
Thought: æˆ‘éœ€è¦å…ˆæŸ¥è¯¢åŸƒè²å°”é“å¡”çš„é«˜åº¦
Action: WikiLookup[åŸƒè²å°”é“å¡”]
Observation: åŸƒè²å°”é“å¡”é«˜ 324 ç±³
Thought: ç°åœ¨æˆ‘éœ€è¦æŸ¥ä¸Šæµ·ä¸­å¿ƒå¤§å¦çš„é«˜åº¦
Action: WikiLookup[ä¸Šæµ·ä¸­å¿ƒå¤§å¦]
Observation: ä¸Šæµ·ä¸­å¿ƒå¤§å¦é«˜ 632 ç±³
Thought: æˆ‘éœ€è¦è®¡ç®—å·®å€¼
Action: Calculator[632 - 324]
Observation: 308
Thought: æˆ‘ç°åœ¨çŸ¥é“ç­”æ¡ˆäº†
Final Answer: åŸƒè²å°”é“å¡”é«˜ 324 ç±³,æ¯”ä¸Šæµ·ä¸­å¿ƒå¤§å¦çŸ® 308 ç±³ã€‚

ç°åœ¨å¼€å§‹:
Question: {question}"""
```

#### (2) å®Œæ•´å®ç° (Python ä¼ªä»£ç )

```python
import re
from typing import Dict, Callable

class ReActAgent:
    def __init__(self, llm, tools: Dict[str, Callable], max_steps=5):
        self.llm = llm
        self.tools = tools
        self.max_steps = max_steps
    
    def run(self, question: str) -> str:
        prompt = REACT_PROMPT_TEMPLATE.format(question=question)
        scratchpad = ""  # è®°å½•æ€è€ƒè¿‡ç¨‹
        
        for step in range(self.max_steps):
            # 1. LLM ç”Ÿæˆä¸‹ä¸€æ­¥æ€è€ƒ
            response = self.llm.invoke(prompt + scratchpad)
            scratchpad += response
            
            # 2. è§£æ Action
            action_match = re.search(r'Action: (\w+)\[(.*?)\]', response)
            
            if "Final Answer:" in response:
                # æ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆ,ç»“æŸå¾ªç¯
                answer = response.split("Final Answer:")[1].strip()
                return answer
            
            if action_match:
                tool_name = action_match.group(1)
                tool_input = action_match.group(2)
                
                # 3. æ‰§è¡Œå·¥å…·
                if tool_name in self.tools:
                    observation = self.tools[tool_name](tool_input)
                else:
                    observation = f"Error: Tool {tool_name} not found"
                
                # 4. æ·»åŠ  Observation åˆ° scratchpad
                scratchpad += f"
Observation: {observation}
"
            else:
                scratchpad += "
[No valid action found. Please try again.]
"
        
        return "Max steps reached. No answer found."

# ç¤ºä¾‹å·¥å…·
def calculator(expr: str) -> str:
    try:
        return str(eval(expr))  # ç”Ÿäº§ç¯å¢ƒè¯·ç”¨ ast.literal_eval!
    except Exception as e:
        return f"Error: {e}"

def search(query: str) -> str:
    # æ¨¡æ‹Ÿæœç´¢
    return f"Mock search result for '{query}'"

# ä½¿ç”¨
agent = ReActAgent(
    llm=your_llm_instance,
    tools={"Calculator": calculator, "Search": search}
)
result = agent.run("å¦‚æœæˆ‘æœ‰ 3 ä¸ªè‹¹æœ,ä¹°äº† 7 ä¸ªæ©™å­,ä¸€å…±å¤šå°‘æ°´æœ?")
```

**å±€é™æ€§åˆ†æ**:
- çŸ­è§†: åªçœ‹ä¸‹ä¸€æ­¥,ç¼ºä¹å…¨å±€è§‚
- æ­»å¾ªç¯é£é™©: å®¹æ˜“åœ¨ä¸¤ä¸ªæ­¥éª¤ä¹‹é—´åå¤æ¨ªè·³
- Token æ¶ˆè€—å¤§: æ¯ä¸€æ­¥éƒ½è¦æŠŠæ•´ä¸ª History å–‚ç»™æ¨¡å‹

### 2. Plan-and-Solve:ç»“æ„åŒ–è§„åˆ’

**æ ¸å¿ƒæ€æƒ³**: å…ˆç”Ÿæˆå®Œæ•´è®¡åˆ’,å†é€ä¸€æ‰§è¡Œ,é¿å… ReAct çš„çŸ­è§†é—®é¢˜ã€‚

#### (1) Prompt æ¨¡æ¿

```python
PLAN_PROMPT = """è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’:

ä»»åŠ¡: {task}

å¯ç”¨å·¥å…·:
{tools_description}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè®¡åˆ’:
{{
  "steps": [
    {{"step": 1, "action": "tool_name", "args": {{"arg1": "value"}}, "reason": "åŸå› "}},
    {{"step": 2, "action": "tool_name", "args": {{"arg1": "value"}}, "reason": "åŸå› "}}
  ]
}}
"""
```

#### (2) å®ç°

```python
import json
from pydantic import BaseModel, Field
from typing import List, Dict

class PlanStep(BaseModel):
    step: int
    action: str
    args: Dict
    reason: str

class Plan(BaseModel):
    steps: List[PlanStep]

class PlanAndSolveAgent:
    def __init__(self, llm, tools: Dict[str, Callable]):
        self.llm = llm
        self.tools = tools
    
    def run(self, task: str) -> str:
        # 1. ç”Ÿæˆè®¡åˆ’
        plan_response = self.llm.invoke(PLAN_PROMPT.format(
            task=task,
            tools_description=self._get_tools_description()
        ))
        
        # 2. è§£æè®¡åˆ’
        try:
            plan_data = json.loads(plan_response)
            plan = Plan(**plan_data)
        except Exception as e:
            return f"Failed to parse plan: {e}"
        
        # 3. æ‰§è¡Œè®¡åˆ’
        results = []
        for step in plan.steps:
            print(f"Step {step.step}: {step.reason}")
            tool_result = self.tools[step.action](**step.args)
            results.append(tool_result)
        
        # 4. ç»¼åˆç»“æœ
        return self._synthesize_results(task, results)
    
    def _get_tools_description(self) -> str:
        return "
".join([f"- {name}: {func.__doc__}" for name, func in self.tools.items()])
    
    def _synthesize_results(self, task: str, results: List) -> str:
        # è®© LLM ç»¼åˆç»“æœ
        prompt = f"Task: {task}

Execution results: {results}

Please provide final answer:"
        return self.llm.invoke(prompt)
```

### 3. Reflection:è‡ªæˆ‘åæ€æœºåˆ¶

Reflection è®© Agent å…·å¤‡"è‡ªæˆ‘çº é”™"èƒ½åŠ›,æ ¸å¿ƒæ˜¯å¼•å…¥ **Evaluator** å’Œ **Self-Reflection** æ­¥éª¤ã€‚

#### (1) Reflexion æ¶æ„

```python
class ReflexionAgent:
    def __init__(self, llm, tools, max_trials=3):
        self.llm = llm
        self.tools = tools
        self.max_trials = max_trials
        self.memory = []  # å­˜å‚¨åæ€ç»éªŒ
    
    def run(self, task: str) -> str:
        for trial in range(self.max_trials):
            print(f"
=== Trial {trial + 1} ===")
            
            # 1. å°è¯•æ‰§è¡Œä»»åŠ¡ (å¸¦ä¸Šä¹‹å‰çš„ç»éªŒ)
            context = self._build_context(task)
            result = self._attempt_task(context)
            
            # 2. è¯„ä¼°ç»“æœ
            is_success, feedback = self._evaluate(task, result)
            
            if is_success:
                return result
            
            # 3. å¤±è´¥åˆ™è¿›è¡Œåæ€
            reflection = self._reflect(task, result, feedback)
            self.memory.append(reflection)
            print(f"Reflection: {reflection}")
        
        return f"Failed after {self.max_trials} trials"
    
    def _build_context(self, task: str) -> str:
        lessons = "
".join([f"- {r}" for r in self.memory])
        return f"""Task: {task}

Previous failures and lessons learned:
{lessons if lessons else "None"}

Please try to solve the task while avoiding previous mistakes."""
    
    def _attempt_task(self, context: str) -> str:
        # ä½¿ç”¨ ReAct æˆ–å…¶ä»–æ–¹å¼æ‰§è¡Œ
        return self.llm.invoke(context)
    
    def _evaluate(self, task: str, result: str) -> tuple[bool, str]:
        """è¯„ä¼°ç»“æœæ˜¯å¦æˆåŠŸ"""
        eval_prompt = f"""Task: {task}
Result: {result}

Is this result correct and complete? Answer with:
- SUCCESS: if correct
- FAILURE: reason why it failed"""
        
        eval_response = self.llm.invoke(eval_prompt)
        
        if "SUCCESS" in eval_response:
            return True, ""
        else:
            return False, eval_response
    
    def _reflect(self, task: str, result: str, feedback: str) -> str:
        """ç”Ÿæˆåæ€"""
        reflect_prompt = f"""You failed at the following task:
Task: {task}
Your result: {result}
Feedback: {feedback}

Please analyze what went wrong and provide a lesson for next time.
Focus on:
1. What assumption was incorrect?
2. What should you do differently next time?

Reflection:"""
        
        return self.llm.invoke(reflect_prompt)
```

#### (2) Output Parser å®ç°

ä¸ºäº†ç¡®ä¿ LLM è¾“å‡ºç¬¦åˆé¢„æœŸæ ¼å¼,æˆ‘ä»¬éœ€è¦ **ç»“æ„åŒ–è§£æå™¨**ã€‚

```python
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field, validator

class ReflectionOutput(BaseModel):
    status: str = Field(description="SUCCESS or FAILURE")
    reason: str = Field(description="Reason for success/failure")
    lesson: str = Field(default="", description="Lesson learned if failed")
    
    @validator('status')
    def validate_status(cls, v):
        if v not in ["SUCCESS", "FAILURE"]:
            raise ValueError("Status must be SUCCESS or FAILURE")
        return v

# ä½¿ç”¨ Parser
parser = PydanticOutputParser(pydantic_object=ReflectionOutput)
format_instructions = parser.get_format_instructions()

# åœ¨ Prompt ä¸­åŠ å…¥æ ¼å¼è¯´æ˜
eval_prompt = f"""Task: {task}
Result: {result}

{format_instructions}"""

# è§£æè¾“å‡º
try:
    parsed = parser.parse(llm_output)
    print(parsed.status, parsed.reason)
except Exception as e:
    print(f"Parsing failed: {e}")
```

---

## ä¸‰ã€å·¥å…·ä½¿ç”¨ (Tool Use) ä¸ Function Calling

### 1. JSON Schema å®šä¹‰æ ‡å‡†

OpenAI çš„ Function Calling ä½¿ç”¨ **JSON Schema** å®šä¹‰å·¥å…·æ¥å£ã€‚

#### (1) æ ‡å‡†æ ¼å¼

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°,ä¾‹å¦‚: Beijing, Shanghai"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æ¸©åº¦å•ä½"
                    }
                },
                "required": ["city"],
                "additionalProperties": False
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "search_web",
            "description": "åœ¨äº’è”ç½‘ä¸Šæœç´¢ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 20
                    }
                },
                "required": ["query"]
            }
        }
    }
]
```

#### (2) ä½¿ç”¨ Pydantic ç®€åŒ–å®šä¹‰

```python
from pydantic import BaseModel, Field

class GetWeatherInput(BaseModel):
    city: str = Field(description="åŸå¸‚åç§°,ä¾‹å¦‚: Beijing, Shanghai")
    unit: str = Field(default="celsius", description="æ¸©åº¦å•ä½", enum=["celsius", "fahrenheit"])

class SearchWebInput(BaseModel):
    query: str = Field(description="æœç´¢å…³é”®è¯")
    max_results: int = Field(default=5, ge=1, le=20, description="è¿”å›ç»“æœæ•°é‡")

# è‡ªåŠ¨ç”Ÿæˆ JSON Schema
from pydantic.json_schema import JsonSchemaValue

def pydantic_to_openai_schema(model: type[BaseModel]) -> dict:
    schema = model.model_json_schema()
    return {
        "type": "function",
        "function": {
            "name": model.__name__,
            "description": model.__doc__ or "",
            "parameters": schema
        }
    }

tools = [
    pydantic_to_openai_schema(GetWeatherInput),
    pydantic_to_openai_schema(SearchWebInput)
]
```

### 2. Function Calling åè®®è¯¦è§£

#### (1) å®Œæ•´è°ƒç”¨æµç¨‹

```python
import openai
import json

def get_weather(city: str, unit: str = "celsius") -> str:
    # æ¨¡æ‹Ÿå¤©æ°” API
    return json.dumps({
        "city": city,
        "temperature": 25,
        "unit": unit,
        "condition": "sunny"
    })

def search_web(query: str, max_results: int = 5) -> str:
    # æ¨¡æ‹Ÿæœç´¢ API
    return json.dumps({
        "query": query,
        "results": [f"Result {i+1} for {query}" for i in range(max_results)]
    })

# å·¥å…·æ˜ å°„
available_functions = {
    "get_weather": get_weather,
    "search_web": search_web
}

# ç¬¬ä¸€è½®: è®©æ¨¡å‹å†³å®šè°ƒç”¨ä»€ä¹ˆå·¥å…·
messages = [
    {"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?"}
]

response = openai.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=tools,
    tool_choice="auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®š
)

# æ£€æŸ¥æ˜¯å¦è¦è°ƒç”¨å·¥å…·
response_message = response.choices[0].message
messages.append(response_message)

if response_message.tool_calls:
    # ç¬¬äºŒè½®: æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
    for tool_call in response_message.tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)
        
        print(f"Calling {function_name} with {function_args}")
        
        # æ‰§è¡Œå·¥å…·
        function_response = available_functions[function_name](**function_args)
        
        # æ·»åŠ å·¥å…·ç»“æœåˆ°å¯¹è¯
        messages.append({
            "role": "tool",
            "tool_call_id": tool_call.id,
            "name": function_name,
            "content": function_response
        })
    
    # ç¬¬ä¸‰è½®: è®©æ¨¡å‹ç»¼åˆå·¥å…·ç»“æœç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    final_response = openai.chat.completions.create(
        model="gpt-4",
        messages=messages
    )
    
    print(final_response.choices[0].message.content)
```

### 3. å·¥å…·è°ƒç”¨å®Œæ•´æµç¨‹å®ç°

#### (1) LangChain å®ç°

```python
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# å®šä¹‰å·¥å…·
@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼,ä¾‹å¦‚: 3 * (2 + 5)"""
    try:
        result = eval(expression)  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ ast.literal_eval
        return f"Result: {result}"
    except Exception as e:
        return f"Error: {e}"

@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´"""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# åˆ›å»º Agent
llm = ChatOpenAI(model="gpt-4", temperature=0)
tools = [calculator, get_current_time]

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹,å¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# è¿è¡Œ
result = agent_executor.invoke({"input": "ç°åœ¨å‡ ç‚¹?è®¡ç®— (3 + 5) * 2 çš„ç»“æœ"})
print(result["output"])
```

---

## å››ã€MCP (Model Context Protocol) é©å‘½

### 1. MCP åè®®æ ‡å‡†

2024å¹´åº•,Anthropic æ¨å‡ºäº† **MCP (Model Context Protocol)**,è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†,æ—¨åœ¨ç»Ÿä¸€ **AI æ¨¡å‹** ä¸ **æ•°æ®æº/å·¥å…·** ä¹‹é—´çš„è¿æ¥ã€‚

**æ ¸å¿ƒä»·å€¼**:
- ç±»ä¼¼äº USB åè®®: ä¸€æ¬¡ç¼–å†™,å¤„å¤„è¿è¡Œ
- æ•°æ®æºå’Œå·¥å…·å¯ä»¥æ— ç¼æ¥å…¥ä»»ä½•æ”¯æŒ MCP çš„ LLM
- é¿å…ä¸ºæ¯ä¸ª LLM å•ç‹¬å¼€å‘æ’ä»¶

**æ¶æ„**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Client     â”‚  (Claude Desktop, Cursor, VS Code)
â”‚  (LLM App)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ MCP Protocol
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Server     â”‚  (Google Drive, Slack, Postgres, File System)
â”‚  (Tool/Data)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ¦‚å¿µ**:
- **Resources**: æ•°æ®æº (æ–‡ä»¶ã€æ•°æ®åº“è®°å½•ã€API ç«¯ç‚¹)
- **Tools**: å¯æ‰§è¡Œçš„å‡½æ•°
- **Prompts**: é¢„å®šä¹‰çš„æç¤ºè¯æ¨¡æ¿

### 2. å®æˆ˜:å®ç°ä¸€ä¸ª MCP Server

æˆ‘ä»¬ç”¨ Python `mcp` åº“å†™ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿ MCP Serverã€‚

#### (1) å®‰è£…ä¾èµ–

```bash
pip install mcp
```

#### (2) æœåŠ¡ç«¯ä»£ç 

```python
# file_system_mcp_server.py
from mcp.server.fastmcp import FastMCP
import os
from pathlib import Path

# åˆ›å»º MCP Server
mcp = FastMCP("FileSystemServer")

@mcp.tool()
def read_file(path: str) -> str:
    """
    è¯»å–æ–‡ä»¶å†…å®¹
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ–‡ä»¶å†…å®¹æˆ–é”™è¯¯ä¿¡æ¯
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            content = f.read()
        return f"File content ({len(content)} chars):
{content}"
    except Exception as e:
        return f"Error reading file: {e}"

@mcp.tool()
def write_file(path: str, content: str) -> str:
    """
    å†™å…¥æ–‡ä»¶
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
        content: è¦å†™å…¥çš„å†…å®¹
    
    Returns:
        æˆåŠŸæˆ–é”™è¯¯ä¿¡æ¯
    """
    try:
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Successfully wrote {len(content)} chars to {path}"
    except Exception as e:
        return f"Error writing file: {e}"

@mcp.tool()
def list_directory(path: str = ".") -> str:
    """
    åˆ—å‡ºç›®å½•å†…å®¹
    
    Args:
        path: ç›®å½•è·¯å¾„ (é»˜è®¤ä¸ºå½“å‰ç›®å½•)
    
    Returns:
        æ–‡ä»¶åˆ—è¡¨
    """
    try:
        items = os.listdir(path)
        files = [f"ğŸ“„ {item}" if os.path.isfile(os.path.join(path, item)) else f"ğŸ“ {item}" for item in items]
        return f"Directory '{path}' contains {len(items)} items:
" + "
".join(files)
    except Exception as e:
        return f"Error listing directory: {e}"

@mcp.resource("file://{path}")
def get_file_resource(path: str) -> str:
    """
    æä¾›æ–‡ä»¶ä½œä¸ºèµ„æº
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"Error: {e}"

# è¿è¡Œ Server
if __name__ == "__main__":
    print("Starting File System MCP Server...")
    mcp.run()
```

#### (3) é…ç½®æ–‡ä»¶ (ç”¨äº Claude Desktop)

åœ¨ `~/Library/Application Support/Claude/claude_desktop_config.json` ä¸­æ·»åŠ :

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "python3",
      "args": ["/path/to/file_system_mcp_server.py"]
    }
  }
}
```

### 3. MCP Client é›†æˆ

#### (1) åœ¨ä»£ç ä¸­è¿æ¥ MCP Server

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def use_mcp_server():
    # è¿æ¥åˆ° MCP Server
    server_params = StdioServerParameters(
        command="python3",
        args=["file_system_mcp_server.py"]
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # åˆå§‹åŒ–
            await session.initialize()
            
            # åˆ—å‡ºå¯ç”¨å·¥å…·
            tools = await session.list_tools()
            print("Available tools:", [t.name for t in tools.tools])
            
            # è°ƒç”¨å·¥å…·
            result = await session.call_tool("list_directory", {"path": "."})
            print(result.content)
            
            # è¯»å–èµ„æº
            resources = await session.list_resources()
            if resources.resources:
                content = await session.read_resource(resources.resources[0].uri)
                print(content.contents)

# è¿è¡Œ
import asyncio
asyncio.run(use_mcp_server())
```

---

## äº”ã€è®°å¿†ç³»ç»Ÿ (Memory) è®¾è®¡

### 1. è®°å¿†ç±»å‹:çŸ­æœŸä¸é•¿æœŸ

**äººç±»è®°å¿†ä½“ç³»**:
- **æ„Ÿå®˜è®°å¿†** (Sensory Memory): æ¯«ç§’çº§,ç¬é—´æ¶ˆå¤±
- **çŸ­æœŸè®°å¿†** (Short-term / Working Memory): ç§’åˆ°åˆ†é’Ÿçº§,å®¹é‡æœ‰é™ (çº¦ 7Â±2 chunks)
- **é•¿æœŸè®°å¿†** (Long-term Memory):
  - **é™ˆè¿°æ€§è®°å¿†** (Declarative): äº‹å®å’Œäº‹ä»¶
    - **è¯­ä¹‰è®°å¿†**: ä¸–ç•ŒçŸ¥è¯† (å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½)
    - **æƒ…æ™¯è®°å¿†**: ä¸ªäººç»å† (æˆ‘ä¸Šå‘¨å»äº†å·´é»)
  - **ç¨‹åºæ€§è®°å¿†** (Procedural): æŠ€èƒ½ (å¦‚ä½•éª‘è‡ªè¡Œè½¦)

**Agent è®°å¿†æ˜ å°„**:
| äººç±»è®°å¿† | Agent å®ç° | æŠ€æœ¯æ–¹æ¡ˆ |
|----------|-----------|----------|
| çŸ­æœŸè®°å¿† | Context Window | ç›´æ¥å­˜å‚¨åœ¨ Prompt ä¸­ (æœ€è¿‘å‡ è½®å¯¹è¯) |
| è¯­ä¹‰è®°å¿† | World Knowledge | æ¨¡å‹é¢„è®­ç»ƒæƒé‡ + RAG çŸ¥è¯†åº“ |
| æƒ…æ™¯è®°å¿† | Experience Buffer | å‘é‡æ•°æ®åº“ (å­˜å‚¨è¿‡å¾€äº¤äº’) |
| ç¨‹åºæ€§è®°å¿† | Skill Library | å¾®è°ƒæƒé‡ + Tool Definitions |

### 2. Memory æ¶æ„è®¾è®¡

#### (1) ç®€å• Memory:ä¼šè¯çº§å­˜å‚¨

```python
from langchain.memory import ConversationBufferMemory

class SimpleMemory:
    def __init__(self, max_turns=10):
        self.messages = []
        self.max_turns = max_turns
    
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        # ä¿æŒçª—å£å¤§å°
        if len(self.messages) > self.max_turns * 2:
            self.messages = self.messages[-self.max_turns * 2:]
    
    def get_history(self) -> str:
        return "
".join([f"{m['role']}: {m['content']}" for m in self.messages])
```

#### (2) æ‘˜è¦è®°å¿†:å‹ç¼©å†å²

```python
from langchain.memory import ConversationSummaryMemory

class SummaryMemory:
    def __init__(self, llm, max_token_limit=2000):
        self.llm = llm
        self.messages = []
        self.summary = ""
        self.max_token_limit = max_token_limit
    
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        total_tokens = self._estimate_tokens()
        if total_tokens > self.max_token_limit:
            self._compress_history()
    
    def _estimate_tokens(self) -> int:
        # ç®€å•ä¼°ç®—: 1 token â‰ˆ 4 chars
        text = self.summary + "
".join([m['content'] for m in self.messages])
        return len(text) // 4
    
    def _compress_history(self):
        """å‹ç¼©å†å²ä¸ºæ‘˜è¦"""
        history_text = "
".join([f"{m['role']}: {m['content']}" for m in self.messages])
        
        prompt = f"""Please summarize the following conversation history concisely:

{history_text}

Summary:"""
        
        new_summary = self.llm.invoke(prompt)
        
        # æ›´æ–°
        self.summary = new_summary if not self.summary else f"{self.summary}
{new_summary}"
        self.messages = []  # æ¸…ç©ºå·²å‹ç¼©çš„æ¶ˆæ¯
    
    def get_context(self) -> str:
        recent = "
".join([f"{m['role']}: {m['content']}" for m in self.messages])
        if self.summary:
            return f"Previous summary:
{self.summary}

Recent messages:
{recent}"
        return recent
```

#### (3) å‘é‡è®°å¿†:è¯­ä¹‰æ£€ç´¢

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document

class VectorMemory:
    def __init__(self, collection_name="agent_memory"):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings
        )
    
    def add_experience(self, content: str, metadata: dict = None):
        """æ·»åŠ ç»éªŒåˆ°é•¿æœŸè®°å¿†"""
        doc = Document(page_content=content, metadata=metadata or {})
        self.vectorstore.add_documents([doc])
    
    def retrieve_relevant(self, query: str, k=3) -> list[str]:
        """æ£€ç´¢ç›¸å…³ç»éªŒ"""
        docs = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in docs]
    
    def get_context_for_query(self, query: str, k=3) -> str:
        """ä¸ºå½“å‰æŸ¥è¯¢æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡"""
        relevant_memories = self.retrieve_relevant(query, k=k)
        if not relevant_memories:
            return ""
        
        return "Relevant past experiences:
" + "
".join(
            [f"- {mem}" for mem in relevant_memories]
        )
```

### 3. MemGPT:è™šæ‹Ÿå†…å­˜ç®¡ç†

MemGPT (Packer et al., 2023) æå‡ºäº†ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„**è™šæ‹Ÿå†…å­˜ç®¡ç†**æœºåˆ¶ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**:
- **Main Context** (ä¸»ä¸Šä¸‹æ–‡): ç›¸å½“äº RAM,å­˜æ”¾å½“å‰æ´»è·ƒä¿¡æ¯
- **Archival Memory** (å½’æ¡£è®°å¿†): ç›¸å½“äº Disk,å­˜æ”¾å†å²ä¿¡æ¯
- **Recall Memory** (å¬å›è®°å¿†): æ ¸å¿ƒå·¥ä½œè®°å¿†

**æ¶æ„å›¾**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Context (Limited Size)   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ System Prompt           â”‚   â”‚
â”‚  â”‚ Recent Messages         â”‚   â”‚
â”‚  â”‚ Recalled Memories       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ memory_insert() / memory_search()
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Archival Memory (Unlimited)   â”‚
â”‚  (Vector Database)              â”‚
â”‚  - Past conversations           â”‚
â”‚  - Knowledge snippets           â”‚
â”‚  - Reflections                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°ç¤ºä¾‹**:

```python
class MemGPTAgent:
    def __init__(self, llm, vector_memory: VectorMemory):
        self.llm = llm
        self.vector_memory = vector_memory
        self.main_context = []
        self.max_context_size = 10
        
        # å®šä¹‰è®°å¿†ç®¡ç†å·¥å…·
        self.tools = {
            "archival_memory_insert": self.archival_memory_insert,
            "archival_memory_search": self.archival_memory_search,
            "core_memory_append": self.core_memory_append,
            "core_memory_replace": self.core_memory_replace
        }
    
    def archival_memory_insert(self, content: str) -> str:
        """å°†ä¿¡æ¯å­˜å…¥é•¿æœŸè®°å¿†"""
        self.vector_memory.add_experience(content)
        return f"Inserted into archival memory: {content[:50]}..."
    
    def archival_memory_search(self, query: str) -> str:
        """æœç´¢é•¿æœŸè®°å¿†"""
        results = self.vector_memory.retrieve_relevant(query, k=3)
        return "
".join(results)
    
    def core_memory_append(self, content: str) -> str:
        """æ·»åŠ åˆ°æ ¸å¿ƒå·¥ä½œè®°å¿†"""
        self.main_context.append(content)
        # å¦‚æœè¶…å‡ºå¤§å°,è§¦å‘å½’æ¡£
        if len(self.main_context) > self.max_context_size:
            archived = self.main_context.pop(0)
            self.archival_memory_insert(archived)
            return f"Archived old memory, added new: {content[:50]}..."
        return f"Added to core memory: {content[:50]}..."
    
    def core_memory_replace(self, old: str, new: str) -> str:
        """æ›¿æ¢æ ¸å¿ƒè®°å¿†å†…å®¹"""
        try:
            idx = self.main_context.index(old)
            self.main_context[idx] = new
            return f"Replaced memory successfully"
        except ValueError:
            return "Old memory not found"
    
    def run(self, user_input: str) -> str:
        # 1. æœç´¢ç›¸å…³é•¿æœŸè®°å¿†
        relevant_memories = self.vector_memory.get_context_for_query(user_input, k=2)
        
        # 2. æ„å»º Prompt
        context = f"""Core memory:
{chr(10).join(self.main_context)}

{relevant_memories}

User: {user_input}

You can use the following tools to manage your memory:
- archival_memory_insert(content): Save information for long-term
- archival_memory_search(query): Search past experiences
- core_memory_append(content): Add to working memory
- core_memory_replace(old, new): Update working memory

Response:"""
        
        # 3. LLM ç”Ÿæˆå“åº”
        response = self.llm.invoke(context)
        
        # 4. è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨ (ç®€åŒ–ç‰ˆ,å®é™…éœ€è¦æ›´å®Œå–„çš„è§£æ)
        # ... tool execution logic ...
        
        return response
```

### 4. å®æˆ˜:å®ç°å¯æŒä¹…åŒ–çš„ Memory

```python
import json
from pathlib import Path
from datetime import datetime

class PersistentMemory:
    def __init__(self, session_id: str, storage_dir="./memory_store"):
        self.session_id = session_id
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        
        self.short_term = []  # çŸ­æœŸè®°å¿†
        self.long_term = []   # é•¿æœŸè®°å¿†
        
        self.load()
    
    def add_interaction(self, user_msg: str, assistant_msg: str):
        """æ·»åŠ äº¤äº’è®°å½•"""
        interaction = {
            "timestamp": datetime.now().isoformat(),
            "user": user_msg,
            "assistant": assistant_msg
        }
        
        self.short_term.append(interaction)
        
        # ä¿æŒçŸ­æœŸè®°å¿†çª—å£å¤§å°
        if len(self.short_term) > 10:
            # å°†æœ€æ—§çš„ç§»åˆ°é•¿æœŸè®°å¿†
            self.long_term.append(self.short_term.pop(0))
    
    def get_context(self, include_long_term=True) -> str:
        """è·å–è®°å¿†ä¸Šä¸‹æ–‡"""
        context = "Recent conversation:
"
        for item in self.short_term:
            context += f"User: {item['user']}
Assistant: {item['assistant']}
"
        
        if include_long_term and self.long_term:
            context += "
Earlier in this session:
"
            for item in self.long_term[-3:]:  # åªå–æœ€è¿‘ 3 æ¡
                context += f"User: {item['user']}
Assistant: {item['assistant']}
"
        
        return context
    
    def save(self):
        """ä¿å­˜åˆ°ç£ç›˜"""
        data = {
            "session_id": self.session_id,
            "short_term": self.short_term,
            "long_term": self.long_term,
            "last_updated": datetime.now().isoformat()
        }
        
        file_path = self.storage_dir / f"{self.session_id}.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self):
        """ä»ç£ç›˜åŠ è½½"""
        file_path = self.storage_dir / f"{self.session_id}.json"
        if file_path.exists():
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                self.short_term = data.get("short_term", [])
                self.long_term = data.get("long_term", [])
    
    def clear(self):
        """æ¸…é™¤è®°å¿†"""
        self.short_term = []
        self.long_term = []
        self.save()
```

---

## å…­ã€LangGraph:çŠ¶æ€æœºç¼–ç¨‹èŒƒå¼

LangGraph æ˜¯å½“å‰æ„å»º Agent çš„æ ¸å¿ƒæ¡†æ¶,ç”± LangChain å›¢é˜Ÿå¼€å‘ã€‚

### 1. StateGraph æ ¸å¿ƒæ¦‚å¿µ

**æ ¸å¿ƒæ€æƒ³**: Agent å·¥ä½œæµæ˜¯ä¸€ä¸ª**æœ‰å‘å›¾** (Directed Graph),å…¶ä¸­:
- **èŠ‚ç‚¹ (Node)**: æ‰§è¡Œç‰¹å®šæ“ä½œ (è°ƒç”¨ LLMã€æ‰§è¡Œå·¥å…·ã€å¤„ç†æ•°æ®)
- **è¾¹ (Edge)**: æ§åˆ¶æµè½¬ (å›ºå®šè¾¹ã€æ¡ä»¶è¾¹)
- **çŠ¶æ€ (State)**: åœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„æ•°æ®

**æ¶æ„å›¾**:
```
START â†’ Agent â†’ should_continue? â”€Yesâ†’ Tools â†’ Agent
                       â”‚
                       No
                       â†“
                      END
```

### 2. å®æˆ˜:åŸºäº LangGraph çš„ ReAct Agent

```python
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_openai import ChatOpenAI
from langchain.tools import tool

# 1. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯å†å²"]
    next_action: str  # "continue" or "end"

# 2. å®šä¹‰å·¥å…·
@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼,ä¾‹å¦‚: 3 * (2 + 5)"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°” (æ¨¡æ‹Ÿ)"""
    return f"{city} çš„å¤©æ°”: æ™´å¤©,æ¸©åº¦ 25Â°C"

tools = [calculator, get_weather]
tool_executor = ToolExecutor(tools)

# 3. åˆ›å»º LLM (æ”¯æŒ function calling)
llm = ChatOpenAI(model="gpt-4", temperature=0)
llm_with_tools = llm.bind_tools(tools)

# 4. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def call_agent(state: AgentState) -> dict:
    """Agent å†³ç­–èŠ‚ç‚¹"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def execute_tools(state: AgentState) -> dict:
    """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_results = []
    for tool_call in last_message.tool_calls:
        result = tool_executor.invoke(tool_call)
        tool_results.append(
            ToolMessage(
                content=str(result),
                tool_call_id=tool_call["id"]
            )
        )
    
    return {"messages": tool_results}

# 5. å®šä¹‰æ¡ä»¶è¾¹
def should_continue(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­"""
    last_message = state["messages"][-1]
    
    # å¦‚æœ LLM è°ƒç”¨äº†å·¥å…·,åˆ™ç»§ç»­
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"
    else:
        return "end"

# 6. æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("agent", call_agent)
workflow.add_node("tools", execute_tools)

# æ·»åŠ è¾¹
workflow.set_entry_point("agent")

# æ¡ä»¶è¾¹: agent â†’ continue â†’ tools æˆ– agent â†’ end
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "tools",
        "end": END
    }
)

# å›ºå®šè¾¹: tools â†’ agent (å½¢æˆå¾ªç¯)
workflow.add_edge("tools", "agent")

# 7. ç¼–è¯‘
app = workflow.compile()

# 8. è¿è¡Œ
initial_state = {
    "messages": [HumanMessage(content="åŒ—äº¬å¤©æ°”å¦‚ä½•?è®¡ç®— (3 + 5) * 2 çš„ç»“æœ")]
}

for output in app.stream(initial_state):
    for key, value in output.items():
        print(f"Output from node '{key}':")
        print(value)
        print("
---
")
```

### 3. æ¡ä»¶è¾¹ä¸å¾ªç¯æ§åˆ¶

#### (1) å¤æ‚æ¡ä»¶è·¯ç”±

```python
def route_query(state: AgentState) -> str:
    """æ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±åˆ°ä¸åŒå¤„ç†å™¨"""
    last_message = state["messages"][-1].content
    
    if "å¤©æ°”" in last_message:
        return "weather_handler"
    elif any(op in last_message for op in ["+", "-", "*", "/"]):
        return "math_handler"
    else:
        return "general_handler"

workflow.add_conditional_edges(
    "classifier",
    route_query,
    {
        "weather_handler": "weather_node",
        "math_handler": "math_node",
        "general_handler": "general_node"
    }
)
```

#### (2) å¾ªç¯æ§åˆ¶ä¸æœ€å¤§æ­¥æ•°

```python
class AgentStateWithCounter(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯å†å²"]
    iteration: int

def should_continue_with_limit(state: AgentStateWithCounter) -> str:
    """å¸¦æœ€å¤§æ­¥æ•°é™åˆ¶çš„å¾ªç¯æ§åˆ¶"""
    MAX_ITERATIONS = 5
    
    if state["iteration"] >= MAX_ITERATIONS:
        return "max_iterations_reached"
    
    last_message = state["messages"][-1]
    
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"
    else:
        return "end"

def increment_counter(state: AgentStateWithCounter) -> dict:
    """å¢åŠ è®¡æ•°å™¨"""
    return {"iteration": state["iteration"] + 1}

# åœ¨å·¥å…·èŠ‚ç‚¹ä¸­å¢åŠ è®¡æ•°
workflow.add_node("tools", lambda state: {
    **execute_tools(state),
    **increment_counter(state)
})
```

#### (3) æ¡ä»¶è¾¹æ·±åº¦å®è·µ: é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

ç”Ÿäº§çº§ Agent å¿…é¡»å¤„ç†å·¥å…·æ‰§è¡Œå¤±è´¥çš„æƒ…å†µã€‚

```python
from typing import Literal, Optional

class RobustAgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯å†å²"]
    retry_count: int
    last_error: Optional[str]

def should_continue_robust(state: RobustAgentState) -> Literal["continue", "retry", "fail", "end"]:
    """
    ç”Ÿäº§çº§æ¡ä»¶åˆ¤æ–­:
    - æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    - æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯éœ€è¦é‡è¯•
    - æ£€æŸ¥é‡è¯•æ¬¡æ•°æ˜¯å¦è¶…é™
    """
    MAX_RETRIES = 3
    last_message = state["messages"][-1]

    # 1. æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if state.get("last_error"):
        if state["retry_count"] >= MAX_RETRIES:
            return "fail"  # è¶…è¿‡é‡è¯•æ¬¡æ•°,ç›´æ¥å¤±è´¥
        else:
            return "retry"  # éœ€è¦é‡è¯•

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"

    # 3. æ­£å¸¸ç»“æŸ
    return "end"

def execute_tools_with_error_handling(state: RobustAgentState) -> dict:
    """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ - å¸¦é”™è¯¯å¤„ç†"""
    messages = state["messages"]
    last_message = messages[-1]

    tool_results = []
    error_occurred = False
    error_msg = None

    for tool_call in last_message.tool_calls:
        try:
            # æ‰§è¡Œå·¥å…·
            result = tool_executor.invoke(tool_call)
            tool_results.append(
                ToolMessage(
                    content=str(result),
                    tool_call_id=tool_call["id"]
                )
            )
        except Exception as e:
            # æ•è·é”™è¯¯
            error_occurred = True
            error_msg = str(e)
            tool_results.append(
                ToolMessage(
                    content=f"Error: {e}",
                    tool_call_id=tool_call["id"]
                )
            )

    return {
        "messages": tool_results,
        "last_error": error_msg if error_occurred else None,
        "retry_count": state["retry_count"] + 1 if error_occurred else 0
    }

def handle_failure(state: RobustAgentState) -> dict:
    """å¤±è´¥å¤„ç†èŠ‚ç‚¹"""
    error_message = AIMessage(
        content=f"æŠ±æ­‰,åœ¨å°è¯• {state['retry_count']} æ¬¡åä»ç„¶å¤±è´¥ã€‚é”™è¯¯: {state['last_error']}"
    )
    return {"messages": [error_message]}

# æ„å»ºå¸¦é”™è¯¯å¤„ç†çš„å·¥ä½œæµ
workflow_robust = StateGraph(RobustAgentState)

workflow_robust.add_node("agent", call_agent)
workflow_robust.add_node("tools", execute_tools_with_error_handling)
workflow_robust.add_node("failure_handler", handle_failure)

workflow_robust.set_entry_point("agent")

# å¤æ‚æ¡ä»¶è¾¹
workflow_robust.add_conditional_edges(
    "agent",
    should_continue_robust,
    {
        "continue": "tools",
        "retry": "agent",      # é‡æ–°è®© LLM ç”Ÿæˆæ–¹æ¡ˆ
        "fail": "failure_handler",
        "end": END
    }
)

workflow_robust.add_edge("tools", "agent")
workflow_robust.add_edge("failure_handler", END)

app_robust = workflow_robust.compile()
```

### 4. æŒä¹…åŒ– (Persistence): Multi-turn å¯¹è¯çš„åŸºç¡€

**æ ¸å¿ƒé—®é¢˜**: é»˜è®¤æƒ…å†µä¸‹,LangGraph çš„çŠ¶æ€æ˜¯"æ— çŠ¶æ€"çš„,æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯å…¨æ–°å¼€å§‹ã€‚
å¯¹äºèŠå¤©æœºå™¨äººã€é•¿æœŸåŠ©æ‰‹ç­‰åœºæ™¯,æˆ‘ä»¬éœ€è¦**è·¨ä¼šè¯ä¿å­˜çŠ¶æ€**ã€‚

#### (1) ä½¿ç”¨ MemorySaver (å†…å­˜çº§æŒä¹…åŒ–)

é€‚ç”¨äºå¼€å‘/æµ‹è¯•ç¯å¢ƒ,è¿›ç¨‹é‡å¯åæ•°æ®ä¼šä¸¢å¤±ã€‚

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END

# å®šä¹‰çŠ¶æ€
class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯å†å²"]
    user_info: dict  # å­˜å‚¨ç”¨æˆ·ä¿¡æ¯

# å®šä¹‰èŠ‚ç‚¹
def chatbot_node(state: ChatState):
    """èŠå¤©æœºå™¨äººèŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4", temperature=0.7)

    # æ„å»ºç³»ç»Ÿæç¤º (åŒ…å«ç”¨æˆ·ä¿¡æ¯)
    system_prompt = f"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚"
    if state.get("user_info"):
        system_prompt += f"
ç”¨æˆ·ä¿¡æ¯: {state['user_info']}"

    messages = [SystemMessage(content=system_prompt)] + list(state["messages"])
    response = llm.invoke(messages)

    return {"messages": [response]}

def extract_user_info(state: ChatState) -> dict:
    """ä»å¯¹è¯ä¸­æå–å¹¶æ›´æ–°ç”¨æˆ·ä¿¡æ¯"""
    last_user_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            last_user_message = msg.content
            break

    # ç®€å•çš„ä¿¡æ¯æå– (ç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”¨ NER æˆ– LLM)
    user_info = state.get("user_info", {})

    if "æˆ‘å«" in last_user_message or "My name is" in last_user_message:
        # æå–åå­— (ç®€åŒ–ç‰ˆ)
        import re
        name_match = re.search(r'æˆ‘å«(.*?)[,ã€‚!]', last_user_message)
        if name_match:
            user_info["name"] = name_match.group(1).strip()

    return {"user_info": user_info}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(ChatState)
workflow.add_node("extract_info", extract_user_info)
workflow.add_node("chatbot", chatbot_node)

workflow.set_entry_point("extract_info")
workflow.add_edge("extract_info", "chatbot")
workflow.add_edge("chatbot", END)

# å…³é”®: æ·»åŠ  Checkpointer
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# ç¬¬ä¸€è½®å¯¹è¯
config = {"configurable": {"thread_id": "user_123"}}  # ä¼šè¯ ID

response1 = app.invoke(
    {"messages": [HumanMessage(content="ä½ å¥½,æˆ‘å«å¼ ä¸‰")]},
    config=config
)
print("Bot:", response1["messages"][-1].content)

# ç¬¬äºŒè½®å¯¹è¯ (ä½¿ç”¨ç›¸åŒçš„ thread_id)
response2 = app.invoke(
    {"messages": [HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—?")]},
    config=config
)
print("Bot:", response2["messages"][-1].content)
# è¾“å‡º: "ä½ å«å¼ ä¸‰" (è®°ä½äº†ä¹‹å‰çš„å¯¹è¯)
```

**å…³é”®ç‚¹**:
- `checkpointer=memory`: å¯ç”¨çŠ¶æ€æŒä¹…åŒ–
- `thread_id`: ç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·/ä¼šè¯çš„å”¯ä¸€æ ‡è¯†ç¬¦
- æ¯æ¬¡è°ƒç”¨ `app.invoke()` æ—¶ä¼ å…¥ç›¸åŒçš„ `config`,å³å¯æ¢å¤ä¹‹å‰çš„çŠ¶æ€

#### (2) ä½¿ç”¨ SqliteSaver (ç£ç›˜çº§æŒä¹…åŒ–)

ç”Ÿäº§ç¯å¢ƒæ¨è,æ•°æ®æŒä¹…åŒ–åˆ° SQLite æ•°æ®åº“ã€‚

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# åˆ›å»º SQLite Checkpointer
db_path = "./checkpoints.db"
memory = SqliteSaver.from_conn_string(db_path)

# æ„å»ºåº”ç”¨ (å…¶ä»–ä»£ç åŒä¸Š)
app = workflow.compile(checkpointer=memory)

# ä½¿ç”¨æ–¹å¼å®Œå…¨ç›¸åŒ
config = {"configurable": {"thread_id": "user_456"}}

response = app.invoke(
    {"messages": [HumanMessage(content="è®°ä½è¿™ä¸ªæ•°å­—: 42")]},
    config=config
)

# å³ä½¿è¿›ç¨‹é‡å¯,æ•°æ®ä»ç„¶ä¿ç•™
# é‡æ–°åˆ›å»º app
memory_new = SqliteSaver.from_conn_string(db_path)
app_new = workflow.compile(checkpointer=memory_new)

response2 = app_new.invoke(
    {"messages": [HumanMessage(content="æˆ‘ä¹‹å‰è®©ä½ è®°ä½çš„æ•°å­—æ˜¯ä»€ä¹ˆ?")]},
    config=config
)
print(response2["messages"][-1].content)  # è¾“å‡º: "42"
```

#### (3) æŸ¥çœ‹å’Œç®¡ç†å†å²çŠ¶æ€

```python
# è·å–æ‰€æœ‰ checkpoint (çŠ¶æ€å¿«ç…§)
checkpoints = list(app.get_state_history(config))

print(f"Total checkpoints: {len(checkpoints)}")

for i, checkpoint in enumerate(checkpoints):
    print(f"
Checkpoint {i}:")
    print(f"  Messages: {len(checkpoint.values['messages'])}")
    print(f"  Config: {checkpoint.config}")

# å›æ»šåˆ°ç‰¹å®šçŠ¶æ€
if len(checkpoints) > 1:
    previous_state = checkpoints[1]
    app.update_state(
        previous_state.config,
        previous_state.values
    )
    print("å›æ»šæˆåŠŸ!")
```

#### (4) å®Œæ•´çš„å¤šè½®å¯¹è¯ç¤ºä¾‹

```python
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.messages import HumanMessage, AIMessage
import uuid

# åˆ›å»ºæŒä¹…åŒ–åº”ç”¨
memory = SqliteSaver.from_conn_string("./chat_history.db")
app = workflow.compile(checkpointer=memory)

def chat_session(user_id: str):
    """æ¨¡æ‹Ÿå¤šè½®å¯¹è¯"""
    config = {"configurable": {"thread_id": user_id}}

    print(f"=== Chat Session: {user_id} ===")

    # ç¬¬ 1 è½®
    print("
User: ä½ å¥½,æˆ‘å«æå››,ä½åœ¨åŒ—äº¬")
    r1 = app.invoke(
        {"messages": [HumanMessage(content="ä½ å¥½,æˆ‘å«æå››,ä½åœ¨åŒ—äº¬")]},
        config=config
    )
    print(f"Bot: {r1['messages'][-1].content}")

    # ç¬¬ 2 è½®
    print("
User: æˆ‘ä½åœ¨å“ªé‡Œ?")
    r2 = app.invoke(
        {"messages": [HumanMessage(content="æˆ‘ä½åœ¨å“ªé‡Œ?")]},
        config=config
    )
    print(f"Bot: {r2['messages'][-1].content}")

    # ç¬¬ 3 è½®
    print("
User: æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ?")
    r3 = app.invoke(
        {"messages": [HumanMessage(content="æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆ?")]},
        config=config
    )
    print(f"Bot: {r3['messages'][-1].content}")

# è¿è¡Œ
chat_session("user_001")

# æ¨¡æ‹Ÿè¿›ç¨‹é‡å¯
print("

=== è¿›ç¨‹é‡å¯ ===
")
memory_new = SqliteSaver.from_conn_string("./chat_history.db")
app_new = workflow.compile(checkpointer=memory_new)

# ç»§ç»­å¯¹è¯
config = {"configurable": {"thread_id": "user_001"}}
print("User: æˆ‘ä»¬ä¹‹å‰èŠè¿‡ä»€ä¹ˆ?")
r4 = app_new.invoke(
    {"messages": [HumanMessage(content="æˆ‘ä»¬ä¹‹å‰èŠè¿‡ä»€ä¹ˆ?")]},
    config=config
)
print(f"Bot: {r4['messages'][-1].content}")
```

### 5. Human-in-the-loop: æ•æ„Ÿæ“ä½œçš„å®¡æ‰¹æœºåˆ¶

åœ¨æ‰§è¡Œå±é™©æ“ä½œ (åˆ é™¤æ–‡ä»¶ã€å‘é€é‚®ä»¶ã€æ”¯ä»˜ç­‰) å‰,Agent åº”è¯¥æš‚åœå¹¶ç­‰å¾…äººç±»å®¡æ‰¹ã€‚

#### (1) ä½¿ç”¨ interrupt_before æš‚åœæ‰§è¡Œ

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END

# å®šä¹‰å±é™©å·¥å…·
@tool
def delete_file(file_path: str) -> str:
    """åˆ é™¤æ–‡ä»¶ (å±é™©æ“ä½œ)"""
    import os
    try:
        os.remove(file_path)
        return f"File {file_path} deleted successfully"
    except Exception as e:
        return f"Error deleting file: {e}"

@tool
def read_file(file_path: str) -> str:
    """è¯»å–æ–‡ä»¶ (å®‰å…¨æ“ä½œ)"""
    try:
        with open(file_path, 'r') as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {e}"

tools = [delete_file, read_file]

# å®šä¹‰çŠ¶æ€
class SafeAgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯å†å²"]
    pending_approval: Optional[str]  # ç­‰å¾…å®¡æ‰¹çš„æ“ä½œ

# å®šä¹‰èŠ‚ç‚¹
def agent_node(state: SafeAgentState):
    """Agent å†³ç­–èŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    llm_with_tools = llm.bind_tools(tools)

    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

def check_if_dangerous(state: SafeAgentState) -> str:
    """æ£€æŸ¥æ˜¯å¦æ˜¯å±é™©æ“ä½œ"""
    last_message = state["messages"][-1]

    if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
        return "safe"

    # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†å±é™©å·¥å…·
    for tool_call in last_message.tool_calls:
        if tool_call["name"] in ["delete_file", "send_email", "make_payment"]:
            return "dangerous"

    return "safe"

def execute_safe_tools(state: SafeAgentState):
    """æ‰§è¡Œå®‰å…¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]

    tool_executor = ToolExecutor(tools)
    tool_results = []

    for tool_call in last_message.tool_calls:
        result = tool_executor.invoke(tool_call)
        tool_results.append(
            ToolMessage(
                content=str(result),
                tool_call_id=tool_call["id"]
            )
        )

    return {"messages": tool_results}

def request_approval(state: SafeAgentState):
    """è¯·æ±‚äººç±»å®¡æ‰¹"""
    last_message = state["messages"][-1]

    # æå–å¾…å®¡æ‰¹çš„æ“ä½œ
    dangerous_ops = []
    for tool_call in last_message.tool_calls:
        dangerous_ops.append(
            f"Tool: {tool_call['name']}, Args: {tool_call['args']}"
        )

    approval_msg = AIMessage(
        content=f"âš ï¸ æ£€æµ‹åˆ°å±é™©æ“ä½œ,éœ€è¦å®¡æ‰¹:
{'
'.join(dangerous_ops)}
è¯·è¾“å…¥ 'approve' æ‰¹å‡†æˆ– 'reject' æ‹’ç»ã€‚"
    )

    return {
        "messages": [approval_msg],
        "pending_approval": "
".join(dangerous_ops)
    }

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(SafeAgentState)

workflow.add_node("agent", agent_node)
workflow.add_node("check_danger", lambda s: s)  # ç©ºèŠ‚ç‚¹,ä»…ç”¨äºæ¡ä»¶åˆ¤æ–­
workflow.add_node("request_approval", request_approval)
workflow.add_node("execute_tools", execute_safe_tools)

workflow.set_entry_point("agent")

# æ¡ä»¶è¾¹: agent â†’ check_danger
workflow.add_edge("agent", "check_danger")

workflow.add_conditional_edges(
    "check_danger",
    check_if_dangerous,
    {
        "safe": "execute_tools",
        "dangerous": "request_approval"
    }
)

workflow.add_edge("execute_tools", END)
workflow.add_edge("request_approval", END)  # æš‚åœ,ç­‰å¾…äººç±»è¾“å…¥

# ç¼–è¯‘ - å…³é”®: interrupt_before
memory = MemorySaver()
app = workflow.compile(
    checkpointer=memory,
    interrupt_before=["execute_tools"]  # åœ¨æ‰§è¡Œå·¥å…·å‰æš‚åœ
)

# ä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "session_001"}}

# ç¬¬ 1 æ­¥: ç”¨æˆ·è¯·æ±‚åˆ é™¤æ–‡ä»¶
print("=== Step 1: User Request ===")
result1 = app.invoke(
    {"messages": [HumanMessage(content="è¯·åˆ é™¤æ–‡ä»¶ /tmp/test.txt")]},
    config=config
)

print(f"Status: {result1['messages'][-1].content}")
# è¾“å‡º: "æ£€æµ‹åˆ°å±é™©æ“ä½œ,éœ€è¦å®¡æ‰¹..."

# ç¬¬ 2 æ­¥: æŸ¥çœ‹å½“å‰çŠ¶æ€
current_state = app.get_state(config)
print(f"
Pending approval: {current_state.values.get('pending_approval')}")
print(f"Next node: {current_state.next}")  # åº”è¯¥æ˜¯ ['execute_tools']

# ç¬¬ 3 æ­¥: äººç±»å®¡æ‰¹
print("
=== Step 2: Human Approval ===")
user_decision = input("Type 'approve' or 'reject': ")

if user_decision.lower() == "approve":
    # ç»§ç»­æ‰§è¡Œ (resume)
    result2 = app.invoke(None, config=config)  # None è¡¨ç¤ºç»§ç»­æ‰§è¡Œ
    print(f"Result: {result2['messages'][-1].content}")
else:
    # æ‹’ç» - æ‰‹åŠ¨ç»“æŸ
    print("Operation rejected by user")
    app.update_state(
        config,
        {"messages": [AIMessage(content="æ“ä½œå·²è¢«ç”¨æˆ·æ‹’ç»")]}
    )
```

**å·¥ä½œæµç¨‹**:
1. Agent å†³å®šè°ƒç”¨ `delete_file`
2. å·¥ä½œæµåœ¨ `execute_tools` å‰æš‚åœ (`interrupt_before`)
3. è¿”å›ç»™ç”¨æˆ·,æ˜¾ç¤ºå¾…å®¡æ‰¹çš„æ“ä½œ
4. äººç±»è¾“å…¥ "approve"
5. è°ƒç”¨ `app.invoke(None, config)` ç»§ç»­æ‰§è¡Œ

#### (2) æ›´ä¼˜é›…çš„å®¡æ‰¹æµç¨‹: åˆ†ç¦»å®¡æ‰¹èŠ‚ç‚¹

```python
def approval_node(state: SafeAgentState):
    """
    å®¡æ‰¹èŠ‚ç‚¹ - ä» state ä¸­è¯»å–äººç±»è¾“å…¥
    """
    messages = state["messages"]

    # æŸ¥æ‰¾æœ€åä¸€æ¡äººç±»æ¶ˆæ¯
    last_human_msg = None
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            last_human_msg = msg.content.lower()
            break

    if last_human_msg == "approve":
        return {"pending_approval": None}  # æ¸…é™¤å®¡æ‰¹çŠ¶æ€
    else:
        # æ‹’ç»
        return {
            "messages": [AIMessage(content="æ“ä½œå·²è¢«æ‹’ç»")],
            "pending_approval": None
        }

def should_execute_after_approval(state: SafeAgentState) -> str:
    """æ£€æŸ¥å®¡æ‰¹ç»“æœ"""
    last_human_msg = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            last_human_msg = msg.content.lower()
            break

    if last_human_msg == "approve":
        return "execute"
    else:
        return "reject"

# æ„å»ºå·¥ä½œæµ (æ”¹è¿›ç‰ˆ)
workflow_v2 = StateGraph(SafeAgentState)

workflow_v2.add_node("agent", agent_node)
workflow_v2.add_node("request_approval", request_approval)
workflow_v2.add_node("approval_gate", approval_node)
workflow_v2.add_node("execute_tools", execute_safe_tools)

workflow_v2.set_entry_point("agent")

workflow_v2.add_conditional_edges(
    "agent",
    check_if_dangerous,
    {
        "safe": "execute_tools",
        "dangerous": "request_approval"
    }
)

workflow_v2.add_edge("request_approval", "approval_gate")

workflow_v2.add_conditional_edges(
    "approval_gate",
    should_execute_after_approval,
    {
        "execute": "execute_tools",
        "reject": END
    }
)

workflow_v2.add_edge("execute_tools", END)

# ç¼–è¯‘ - åœ¨ approval_gate å‰æš‚åœ
app_v2 = workflow_v2.compile(
    checkpointer=MemorySaver(),
    interrupt_before=["approval_gate"]
)

# ä½¿ç”¨
config = {"configurable": {"thread_id": "session_002"}}

# ç¬¬ 1 æ­¥: è¯·æ±‚åˆ é™¤
result = app_v2.invoke(
    {"messages": [HumanMessage(content="åˆ é™¤ /tmp/test.txt")]},
    config=config
)
print(result["messages"][-1].content)

# ç¬¬ 2 æ­¥: äººç±»è¾“å…¥å®¡æ‰¹å†³ç­–
result2 = app_v2.invoke(
    {"messages": [HumanMessage(content="approve")]},
    config=config
)
print(result2["messages"][-1].content)
```

---

## ä¸ƒã€å¤šæ™ºèƒ½ä½“åä½œ (Multi-Agent)

### 1. ä¸ºä»€ä¹ˆéœ€è¦å¤š Agent?

**å• Agent çš„å±€é™**:
- å®¹æ˜“äº§ç”Ÿè§’è‰²æ··ä¹± (æ—¢è¦å†™ä»£ç åˆè¦æµ‹è¯•)
- éš¾ä»¥åŒæ—¶ç²¾é€šå¤šä¸ªé¢†åŸŸ
- ç¼ºä¹"æ‰¹åˆ¤æ€§æ€ç»´" (è‡ªå·±ä¸å®¹æ˜“å‘ç°è‡ªå·±çš„é”™è¯¯)

**å¤š Agent çš„ä¼˜åŠ¿**:
- **ä¸“ä¸šåŒ–**: æ¯ä¸ª Agent ä¸“æ³¨ä¸€ä¸ªè§’è‰²
- **å¯¹æŠ—æ€§**: ä¸åŒ Agent å¯ä»¥äº’ç›¸å®¡æŸ¥
- **å¹¶è¡Œå¤„ç†**: å¤šä¸ª Agent åŒæ—¶å·¥ä½œ

### 2. Supervisor æ¨¡å¼å®ç°

Supervisor æ¨¡å¼: ä¸€ä¸ªä¸»ç®¡ Agent åè°ƒå¤šä¸ªå·¥ä½œ Agentã€‚

```python
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# 1. å®šä¹‰è§’è‰² Prompt
SUPERVISOR_PROMPT = """ä½ æ˜¯é¡¹ç›®ç»ç†,è´Ÿè´£åè°ƒå›¢é˜Ÿå®Œæˆä»»åŠ¡ã€‚

å›¢é˜Ÿæˆå‘˜:
- researcher: è´Ÿè´£ä¿¡æ¯æœç´¢å’Œç ”ç©¶
- coder: è´Ÿè´£ç¼–å†™ä»£ç 
- reviewer: è´Ÿè´£ä»£ç å®¡æŸ¥

æ ¹æ®ç”¨æˆ·éœ€æ±‚,å†³å®šä¸‹ä¸€æ­¥åº”è¯¥ç”±è°æ¥å¤„ç†ã€‚
å¦‚æœä»»åŠ¡å®Œæˆ,å›å¤ "FINISH"ã€‚

å½“å‰ä»»åŠ¡: {task}
å·¥ä½œå†å²: {history}

è¯·è¾“å‡º JSON æ ¼å¼:
{{"next_worker": "researcher|coder|reviewer|FINISH", "instruction": "å…·ä½“æŒ‡ä»¤"}}
"""

RESEARCHER_PROMPT = "ä½ æ˜¯ä¸€åç ”ç©¶å‘˜,è´Ÿè´£æœç´¢ä¿¡æ¯å’Œåˆ†æéœ€æ±‚ã€‚"
CODER_PROMPT = "ä½ æ˜¯ä¸€åå·¥ç¨‹å¸ˆ,è´Ÿè´£æ ¹æ®éœ€æ±‚ç¼–å†™é«˜è´¨é‡ä»£ç ã€‚"
REVIEWER_PROMPT = "ä½ æ˜¯ä¸€åä»£ç å®¡æŸ¥å‘˜,è´Ÿè´£æ£€æŸ¥ä»£ç è´¨é‡å’Œæ­£ç¡®æ€§ã€‚"

# 2. å®šä¹‰çŠ¶æ€
class TeamState(TypedDict):
    task: str
    history: list[dict]
    next_worker: str
    current_result: str

# 3. å®šä¹‰å·¥ä½œèŠ‚ç‚¹
def supervisor_node(state: TeamState):
    print("
>>> Supervisor is deciding...")
    
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    history_text = "
".join([f"{h['worker']}: {h['result']}" for h in state["history"]])
    
    prompt = SUPERVISOR_PROMPT.format(
        task=state["task"],
        history=history_text or "None"
    )
    
    response = llm.invoke([HumanMessage(content=prompt)])
    
    # è§£æ JSON (ç®€åŒ–ç‰ˆ)
    import json
    try:
        decision = json.loads(response.content)
        return {
            "next_worker": decision["next_worker"],
            "current_result": decision["instruction"]
        }
    except:
        return {"next_worker": "FINISH", "current_result": "Error parsing decision"}

def researcher_node(state: TeamState):
    print("
>>> Researcher is working...")
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    prompt = f"{RESEARCHER_PROMPT}

Task: {state['current_result']}"
    result = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {
        "history": state["history"] + [{"worker": "researcher", "result": result}],
        "current_result": result
    }

def coder_node(state: TeamState):
    print("
>>> Coder is working...")
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    prompt = f"{CODER_PROMPT}

Task: {state['current_result']}
Context: {state['history']}"
    result = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {
        "history": state["history"] + [{"worker": "coder", "result": result}],
        "current_result": result
    }

def reviewer_node(state: TeamState):
    print("
>>> Reviewer is working...")
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    prompt = f"{REVIEWER_PROMPT}

Code to review: {state['current_result']}"
    result = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {
        "history": state["history"] + [{"worker": "reviewer", "result": result}],
        "current_result": result
    }

# 4. æ„å»ºå›¾
workflow = StateGraph(TeamState)

workflow.add_node("supervisor", supervisor_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("coder", coder_node)
workflow.add_node("reviewer", reviewer_node)

# è®¾ç½®å…¥å£
workflow.set_entry_point("supervisor")

# æ¡ä»¶è¾¹: supervisor æ ¹æ®å†³ç­–è·¯ç”±
def route_next(state: TeamState) -> str:
    next_worker = state["next_worker"]
    if next_worker == "FINISH":
        return "end"
    return next_worker

workflow.add_conditional_edges(
    "supervisor",
    route_next,
    {
        "researcher": "researcher",
        "coder": "coder",
        "reviewer": "reviewer",
        "end": END
    }
)

# æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å®Œæˆåå›åˆ° supervisor
for node in ["researcher", "coder", "reviewer"]:
    workflow.add_edge(node, "supervisor")

# ç¼–è¯‘
app = workflow.compile()

# è¿è¡Œ
result = app.invoke({
    "task": "å†™ä¸€ä¸ª Python å‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
    "history": [],
    "next_worker": "",
    "current_result": ""
})

print("
=== Final Result ===")
print(result["current_result"])
```

### 3. Hierarchical Agent æ¶æ„

åˆ†å±‚æ¶æ„: é«˜å±‚ Agent è´Ÿè´£æˆ˜ç•¥,åº•å±‚ Agent è´Ÿè´£æ‰§è¡Œã€‚

```python
# é«˜å±‚ Agent: ä»»åŠ¡åˆ†è§£
class HighLevelAgent:
    def __init__(self, llm):
        self.llm = llm
    
    def decompose_task(self, task: str) -> list[dict]:
        """å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡"""
        prompt = f"""è¯·å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡:

Task: {task}

è¾“å‡º JSON æ ¼å¼:
{{
  "subtasks": [
    {{"id": 1, "description": "...", "assigned_to": "worker_type"}},
    {{"id": 2, "description": "...", "assigned_to": "worker_type"}}
  ]
}}
"""
        response = self.llm.invoke([HumanMessage(content=prompt)])
        import json
        return json.loads(response.content)["subtasks"]

# åº•å±‚ Agent: æ‰§è¡Œå…·ä½“ä»»åŠ¡
class LowLevelAgent:
    def __init__(self, llm, role: str):
        self.llm = llm
        self.role = role
    
    def execute(self, subtask: dict) -> str:
        prompt = f"""You are a {self.role}.
Execute this subtask: {subtask['description']}

Output:"""
        return self.llm.invoke([HumanMessage(content=prompt)]).content

# åè°ƒå™¨
class HierarchicalSystem:
    def __init__(self, llm):
        self.high_level = HighLevelAgent(llm)
        self.workers = {
            "researcher": LowLevelAgent(llm, "researcher"),
            "coder": LowLevelAgent(llm, "coder"),
            "tester": LowLevelAgent(llm, "tester")
        }
    
    def run(self, task: str) -> dict:
        # 1. é«˜å±‚åˆ†è§£ä»»åŠ¡
        subtasks = self.high_level.decompose_task(task)
        
        # 2. åˆ†é…å¹¶æ‰§è¡Œ
        results = []
        for subtask in subtasks:
            worker = self.workers[subtask["assigned_to"]]
            result = worker.execute(subtask)
            results.append({"subtask": subtask, "result": result})
        
        return {"task": task, "subtasks": results}
```

### 4. MetaGPT ä¸ AutoGen ç®€ä»‹

#### (1) MetaGPT: SOP é©±åŠ¨

MetaGPT çš„æ ¸å¿ƒæ˜¯**æ ‡å‡†åŒ–æ–‡æ¡£è¾“å‡º**:
- ProductManager è¾“å‡º PRD (Product Requirement Document)
- Architect è¾“å‡º System Design
- Engineer è¾“å‡º Code
- QA Engineer è¾“å‡º Test Report

æ¯ä¸ª Agent çš„è¾“å‡ºéƒ½æœ‰ä¸¥æ ¼çš„æ ¼å¼çº¦æŸ,å‡å°‘è¯¯å·®ç´¯ç§¯ã€‚

#### (2) AutoGen: å¯¹è¯ç¼–ç¨‹

AutoGen æ›´çµæ´»,æ ¸å¿ƒæ˜¯**ä¼šè¯** (Conversation):
- å®šä¹‰å¤šä¸ª Agent,æ¯ä¸ªæœ‰è‡ªå·±çš„ System Prompt
- å°†å®ƒä»¬æ”¾å…¥ GroupChat
- è®©å®ƒä»¬è‡ªå·±å¯¹è¯,ç›´åˆ°è¾¾æˆå…±è¯†æˆ–å®Œæˆä»»åŠ¡

```python
# AutoGen ç¤ºä¾‹ (ç®€åŒ–)
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

llm_config = {"model": "gpt-4", "api_key": "..."}

# å®šä¹‰ Agent
user_proxy = UserProxyAgent(
    name="User",
    human_input_mode="NEVER",
    code_execution_config={"work_dir": "coding"}
)

coder = AssistantAgent(
    name="Coder",
    system_message="ä½ æ˜¯ä¸€åå·¥ç¨‹å¸ˆ,è´Ÿè´£ç¼–å†™ä»£ç ã€‚",
    llm_config=llm_config
)

reviewer = AssistantAgent(
    name="Reviewer",
    system_message="ä½ æ˜¯ä¸€åä»£ç å®¡æŸ¥å‘˜,è´Ÿè´£æ£€æŸ¥ä»£ç è´¨é‡ã€‚",
    llm_config=llm_config
)

# åˆ›å»ºç¾¤èŠ
groupchat = GroupChat(
    agents=[user_proxy, coder, reviewer],
    messages=[],
    max_round=10
)

manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# å¯åŠ¨å¯¹è¯
user_proxy.initiate_chat(
    manager,
    message="è¯·å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•çš„ Python å®ç°,å¹¶è¿›è¡Œä»£ç å®¡æŸ¥ã€‚"
)
```

---

## å…«ã€Output Parser:ç»“æ„åŒ–è¾“å‡ºè§£æ

### 1. Pydantic æ¨¡å‹å®šä¹‰

ä½¿ç”¨ Pydantic å¼ºåˆ¶ LLM è¾“å‡ºç¬¦åˆé¢„æœŸæ ¼å¼ã€‚

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional

class CodeReviewResult(BaseModel):
    """ä»£ç å®¡æŸ¥ç»“æœ"""
    
    overall_score: int = Field(ge=0, le=100, description="æ€»ä½“è¯„åˆ† (0-100)")
    issues: List[str] = Field(description="å‘ç°çš„é—®é¢˜åˆ—è¡¨")
    suggestions: List[str] = Field(description="æ”¹è¿›å»ºè®®")
    approved: bool = Field(description="æ˜¯å¦é€šè¿‡å®¡æŸ¥")
    
    @validator('overall_score')
    def score_determines_approval(cls, v, values):
        if v < 60 and values.get('approved', False):
            raise ValueError("Score < 60 but marked as approved")
        return v

class TaskPlan(BaseModel):
    """ä»»åŠ¡è§„åˆ’"""
    
    goal: str = Field(description="ä»»åŠ¡ç›®æ ‡")
    steps: List[dict] = Field(description="æ‰§è¡Œæ­¥éª¤")
    estimated_time: Optional[int] = Field(default=None, description="é¢„è®¡è€—æ—¶ (åˆ†é’Ÿ)")
    dependencies: List[str] = Field(default_factory=list, description="ä¾èµ–é¡¹")

# ä½¿ç”¨
from langchain.output_parsers import PydanticOutputParser

parser = PydanticOutputParser(pydantic_object=CodeReviewResult)

prompt = f"""è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç :

```python
def add(a, b):
    return a + b
```

{parser.get_format_instructions()}
"""

response = llm.invoke(prompt)
result = parser.parse(response)

print(f"Score: {result.overall_score}")
print(f"Approved: {result.approved}")
```

### 2. è‡ªä¿®å¤è§£æå™¨

å½“ LLM è¾“å‡ºæ ¼å¼é”™è¯¯æ—¶,è‡ªåŠ¨å°è¯•ä¿®å¤ã€‚

```python
from langchain.output_parsers import OutputFixingParser

base_parser = PydanticOutputParser(pydantic_object=CodeReviewResult)
fixing_parser = OutputFixingParser.from_llm(parser=base_parser, llm=llm)

# å³ä½¿ LLM è¾“å‡ºæ ¼å¼æœ‰è¯¯,ä¹Ÿä¼šå°è¯•ä¿®å¤
try:
    result = fixing_parser.parse(llm_output)
except Exception as e:
    print(f"Even fixing failed: {e}")
```

---

## ä¹ã€æœ¬ç« å°ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Workflow > Model**: ä¼˜ç§€çš„å·¥ä½œæµå¯ä»¥è®©å¼±æ¨¡å‹è¡¨ç°å¾—åƒå¼ºæ¨¡å‹
2. **ReAct vs Plan-and-Solve**:
   - ReAct é€‚åˆç®€å•ä»»åŠ¡,çµæ´»ä½†çŸ­è§†
   - Plan-and-Solve é€‚åˆå¤æ‚ä»»åŠ¡,éœ€è¦å…¨å±€è§„åˆ’
3. **Memory æ˜¯é•¿æœŸå¯¹è¯çš„å…³é”®**:
   - çŸ­æœŸè®°å¿†: Context Window
   - é•¿æœŸè®°å¿†: Vector Database + MemGPT è™šæ‹Ÿå†…å­˜
4. **MCP æ˜¯å·¥å…·é›†æˆçš„æœªæ¥**: ä¸€æ¬¡ç¼–å†™,å¤„å¤„è¿è¡Œ
5. **LangGraph æ˜¯å½“å‰æœ€ä½³ Agent æ¡†æ¶**: StateGraph æä¾›æ¸…æ™°çš„æ§åˆ¶æµ
6. **Multi-Agent æ˜¯å¤„ç†å¤æ‚ä»»åŠ¡çš„å¿…ç»ä¹‹è·¯**: Supervisor æ¨¡å¼ç®€å•æœ‰æ•ˆ
7. **ç”Ÿäº§çº§ Agent ä¸‰è¦ç´ ** (æœ¬ç« é‡ç‚¹è¡¥å……):
   - **Conditional Edges (æ¡ä»¶è¾¹)**: å®ç°å¤æ‚çš„å†³ç­–é€»è¾‘,åŒ…æ‹¬é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶
   - **Persistence (æŒä¹…åŒ–)**: MemorySaver/SqliteSaver å®ç°è·¨ä¼šè¯çŠ¶æ€ä¿å­˜,æ˜¯ Multi-turn å¯¹è¯çš„åŸºç¡€
   - **Human-in-the-loop**: interrupt_before æœºåˆ¶è®© Agent åœ¨æ‰§è¡Œæ•æ„Ÿæ“ä½œå‰æš‚åœ,ç­‰å¾…äººç±»å®¡æ‰¹

### æŠ€æœ¯æ ˆæ€»ç»“

| ç»„ä»¶ | æ¨èæŠ€æœ¯ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| Agent æ¡†æ¶ | LangGraph | å¤æ‚å·¥ä½œæµ,éœ€è¦ç²¾ç¡®æ§åˆ¶ |
| æ¡ä»¶è¾¹ | Conditional Edges + Error Handling | ç”Ÿäº§çº§å†³ç­–é€»è¾‘,é”™è¯¯å¤„ç†ä¸é‡è¯• |
| æŒä¹…åŒ– | SqliteSaver (ç”Ÿäº§) / MemorySaver (å¼€å‘) | å¤šè½®å¯¹è¯,ä¼šè¯ç®¡ç†,çŠ¶æ€æ¢å¤ |
| Human-in-the-loop | interrupt_before / interrupt_after | æ•æ„Ÿæ“ä½œå®¡æ‰¹,äººç±»ä»‹å…¥å†³ç­– |
| å·¥å…·é›†æˆ | MCP Protocol | ç»Ÿä¸€å·¥å…·æ¥å£ |
| Memory | Vector DB + MemGPT | é•¿æœŸå¯¹è¯,çŸ¥è¯†ç®¡ç† |
| Output Parsing | Pydantic + LangChain Parsers | ç»“æ„åŒ–è¾“å‡º |
| Multi-Agent | Supervisor Pattern | åˆ†å·¥åä½œ |

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- **Part 7 Chapter 3**: CoT çš„æ•°å­¦æœ¬è´¨ä¸æ¨ç†å¢å¼ºæŠ€æœ¯
- **Part 7 Chapter 4**: æ¨ç†æ¨¡å‹ (o1/R1) çš„è®­ç»ƒä¸å®ç°
- **Part 5 Chapter 1-3**: LangChain/LangGraph ç”Ÿæ€æ·±å…¥

---

**ä¸‹ä¸€ç« é¢„å‘Š**: ç¬¬4ç«  - å¤šæ¨¡æ€å¤§æ¨¡å‹åŸç†

åœ¨ä¸‹ä¸€ç« ä¸­,æˆ‘ä»¬å°†ç»™ Agent è£…ä¸Š"çœ¼ç›"å’Œ"è€³æœµ",æ¢è®¨ LLaVAã€GPT-4V èƒŒåçš„è§†è§‰ç¼–ç åŸç†,ä»¥åŠå¦‚ä½•æ„å»ºå¤šæ¨¡æ€ Agentã€‚
