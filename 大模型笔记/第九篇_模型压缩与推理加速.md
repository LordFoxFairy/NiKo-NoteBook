# 第十篇:模型压缩与推理加速

> 量化、剪枝、蒸馏与高效推理的完整技术栈

**适合人群**: 算法工程师、部署工程师
**预计时间**: 10-12小时
**前置知识**: 第三篇(微调技术)

---

## 本篇概览

整合**模型压缩**和**推理加速**两大方向:
- 量化技术(INT8/INT4/GPTQ/AWQ)
- 剪枝与蒸馏
- vLLM/TensorRT-LLM推理引擎
- Flash Attention/KV Cache优化

---

## 第1章:量化技术

### 1.1 量化数学基础

#### 1.1.1 量化定义

**量化映射**:

将高精度浮点数 $x \in \mathbb{R}$ 映射到低精度整数 $x_q \in \mathbb{Z}$:
$$
\text{Quantize}: \mathbb{R} \rightarrow \{q_0, q_1, \ldots, q_{2^b-1}\}
$$

其中 $b$ 为比特数(如INT8: b=8, INT4: b=4)。

**对称量化**(零点在中心):
$$
x_q = \text{clip}\left(\text{round}\left(\frac{x}{s}\right), -2^{b-1}, 2^{b-1}-1\right)
$$

缩放因子:
$$
s = \frac{\max(|x|)}{2^{b-1}-1}
$$

**反量化**:
$$
\hat{x} = s \cdot x_q
$$

**量化误差**:
$$
\epsilon = x - \hat{x} = x - s \cdot \text{round}(x/s)
$$

**非对称量化**(支持偏移):
$$
x_q = \text{round}\left(\frac{x - z}{s}\right)
$$

其中:
- $s = \frac{x_{\max} - x_{\min}}{2^b - 1}$: 缩放因子
- $z = x_{\min}$: 零点(zero-point)

**优势**: 可以更紧密地拟合非对称分布(如ReLU后的激活值)。

#### 1.1.2 量化误差分析

**均匀量化的期望误差**:

假设 $x$ 在 $[x_{\min}, x_{\max}]$ 均匀分布,量化误差:
$$
\mathbb{E}[\epsilon^2] = \frac{s^2}{12}
$$

证明:
$$
\epsilon \sim \text{Uniform}(-s/2, s/2)
$$
$$
\mathbb{E}[\epsilon^2] = \int_{-s/2}^{s/2} \frac{x^2}{s} dx = \frac{s^2}{12}
$$

**信噪比**(Signal-to-Quantization-Noise Ratio):
$$
\text{SQNR} = 10 \log_{10} \frac{\sigma_x^2}{\sigma_\epsilon^2} \approx 6.02b + 1.76 \text{ dB}
$$

每增加1比特,SQNR提升约6dB!

**神经网络的特殊性**:

权重/激活值通常服从**近似正态分布**,而非均匀分布:
$$
x \sim \mathcal{N}(0, \sigma^2)
$$

此时均匀量化不是最优,需要**非均匀量化**。

### 1.2 PTQ vs QAT

| 方法 | 训练成本 | 精度 | 适用场景 |
|------|---------|-----|---------|
| PTQ | 无需训练 | 中等 | 快速部署 |
| QAT | 需重训练 | 高 | 极致性能 |

**PTQ (Post-Training Quantization)**:

在预训练模型上直接量化,使用校准数据集估计范围:
$$
x_{\max} = \mu + k\sigma, \quad x_{\min} = \mu - k\sigma
$$

典型 $k=6$ (覆盖99.7%的数据)。

**QAT (Quantization-Aware Training)**:

在训练时模拟量化:
$$
\tilde{x} = \text{Quantize}(x) + \text{Straight-Through-Estimator}
$$

梯度通过STE传播:
$$
\frac{\partial \mathcal{L}}{\partial x} \approx \frac{\partial \mathcal{L}}{\partial \tilde{x}}
$$

### 1.3 GPTQ算法深度解析

#### 1.3.1 核心思想

基于**二阶优化** (Optimal Brain Quantization) 的逐层量化。

**优化目标**:

最小化量化前后的输出误差:
$$
\min_{\hat{W}} \|WX - \hat{W}X\|^2_F
$$

其中:
- $W \in \mathbb{R}^{d_{out} \times d_{in}}$: 原始权重
- $\hat{W}$: 量化后的权重
- $X \in \mathbb{R}^{d_{in} \times N}$: 校准数据的激活值

**等价形式** (按行分解):
$$
\min_{\hat{w}_i} \sum_{i=1}^{d_{out}} \|w_i^T X - \hat{w}_i^T X\|^2 = \sum_{i=1}^{d_{out}} (w_i - \hat{w}_i)^T (XX^T) (w_i - \hat{w}_i)
$$

定义 **Hessian矩阵**:
$$
H = \frac{2}{N} XX^T \in \mathbb{R}^{d_{in} \times d_{in}}
$$

目标简化为:
$$
\min_{\hat{w}} (w - \hat{w})^T H (w - \hat{w})
$$

#### 1.3.2 逐列量化算法

**贪心策略**: 逐列量化,并补偿误差到未量化列。

伪代码:
```
for j = 1 to d_in:
    # 量化第j列
    w_q[j] = Quantize(w[j])
    
    # 计算误差
    e_j = w[j] - w_q[j]
    
    # 误差补偿到剩余列
    for i = j+1 to d_in:
        w[i] -= (H[i,j] / H[j,j]) * e_j
```

**量化补偿公式**:
$$
\delta w_i = -\frac{H_{ij}}{H_{jj}} (w_j - \hat{w}_j)
$$

**直觉**: 利用Hessian逆矩阵的结构,将误差分摊到相关参数。

**理论保证**:

如果 $H$ 可逆,最优补偿满足:
$$
\nabla_w \mathcal{L}(\hat{w}) \approx 0
$$

#### 1.3.3 复杂度分析

**时间复杂度**:
- Hessian计算: $O(d_{in}^2 N)$
- Cholesky分解: $O(d_{in}^3)$
- 量化+补偿: $O(d_{in}^2 d_{out})$

**瓶颈**: Hessian计算和存储($d_{in}=4096$ → 64MB/层)

**优化**: 分块处理,减少内存占用。

### 1.4 AWQ - 激活感知量化

#### 1.4.1 核心观察

**激活值的幂律分布**:

统计分析显示,激活值满足:
$$
P(|x| > t) \propto t^{-\alpha}, \quad \alpha \approx 1.5
$$

少数通道贡献巨大能量:
$$
\sum_{c \in \text{top-1\%}} x_c^2 / \sum_c x_c^2 > 0.5
$$

**可视化**:
```
通道重要性分布:
████████████████████  (top 1%: 50% energy)
████                  (next 9%: 30% energy)
██                    (remaining 90%: 20% energy)
```

#### 1.4.2 混合精度策略

**思想**: 保护显著通道,量化普通通道。

**缩放变换**:
$$
Y = (WX) = (W \text{diag}(s)^{-1})(\text{diag}(s) X)
$$

定义:
- $\tilde{W} = W \text{diag}(s)^{-1}$: 缩放后的权重(量化)
- $\tilde{X} = \text{diag}(s) X$: 缩放后的激活(保持FP16)

**最优缩放因子**:

最小化量化误差:
$$
s^* = \arg\min_s \mathbb{E}\left[\left\|\text{Quantize}(W \text{diag}(s)^{-1}) \text{diag}(s) X - WX\right\|^2\right]
$$

**闭式解** (基于简化假设):
$$
s_c = \left(\frac{1}{N}\sum_{n=1}^N |x_{c,n}|\right)^\gamma
$$

其中 $\gamma \in [0, 1]$ 控制保护程度:
- $\gamma=0$: 无缩放
- $\gamma=1$: 完全按激活幅度缩放

**实验最优**: $\gamma \approx 0.5$

#### 1.4.3 与GPTQ的对比

| 维度 | GPTQ | AWQ |
|------|------|-----|
| 优化目标 | 输出误差 | 激活感知 |
| 计算复杂度 | 高(Hessian) | 低(统计量) |
| 精度 | 高 | 中高 |
| 速度 | 慢(小时) | 快(分钟) |

**实践选择**:
- 追求极致精度: GPTQ
- 快速部署: AWQ

### 1.5 NF4量化深度分析

#### 1.5.1 设计思想

**观察**: 神经网络权重通常服从**近似正态分布**:
$$
W \sim \mathcal{N}(0, \sigma^2)
$$

**标准INT4量化**(均匀):
$$
q_i \in \{-8, -7, \ldots, 7\}, \quad \text{间隔恒定}
$$

**问题**: 正态分布在尾部稀疏,均匀量化浪费编码空间。

**NF4量化**(非均匀):

根据正态分布的**分位数**选择量化点:
$$
q_i = \Phi^{-1}\left(\frac{i+0.5}{2^b}\right), \quad i=0, 1, \ldots, 2^b-1
$$

其中 $\Phi^{-1}$ 为标准正态分布的逆累积分布函数。

**直觉**: 在高密度区域(中心)量化点密集,低密度区域(尾部)稀疏。

#### 1.5.2 期望误差分析

**Lloyd-Max量化理论**:

最优量化器满足:
$$
q_i^* = \mathbb{E}[X \mid X \in (t_{i-1}, t_i)]
$$

其中 $t_i$ 为分界点。

对于正态分布 $\mathcal{N}(0, 1)$:
$$
q_i = \frac{\phi(t_{i-1}) - \phi(t_i)}{\Phi(t_i) - \Phi(t_{i-1})}
$$

NF4近似这一最优解!

**量化误差**:
$$
\mathbb{E}[(X - Q(X))^2] \approx \frac{\sigma^2}{2^{2b}} \times c(\text{distribution})
$$

NF4的 $c \approx 0.5$,而均匀量化 $c \approx 1.0$,误差减半!

---

## 第2章:剪枝与蒸馏

### 2.1 结构化剪枝

**通道剪枝**:
- 移除整个通道/注意力头
- 保持密集矩阵
- 实际加速比与理论一致

**敏感度分析**:
$$S_i = \frac{w_i^2 H_{ii}}{2}$$

### 2.2 知识蒸馏

**损失函数**:
$$\mathcal{L}_{KD} = \tau^2 \cdot D_{KL}(p_T \| p_S) + \alpha \cdot L_{CE}(y, p_S)$$

**温度参数**:
- $\tau$ = 2-10: 软化分布,传递暗知识

---

## 第3章:Flash Attention深度解析

### 3.1 标准Attention的IO瓶颈

#### 3.1.1 GPU内存层级

**GPU内存架构**:
```
SRAM (On-chip):  20MB,  19 TB/s  (极快但小)
HBM (Off-chip):  80GB,  1.5 TB/s (大但慢)
```

**访问延迟**:
- SRAM: ~1 cycle
- HBM: ~100-200 cycles

**IO瓶颈**: HBM访问成为主要瓶颈!

#### 3.1.2 标准Attention的IO复杂度

**计算步骤**:
```python
# 标准实现
S = Q @ K.T               # [N, N], 写入HBM
P = softmax(S)            # 读S,写P到HBM  
O = P @ V                 # 读P和V,写O
```

**IO复杂度分析**:

设序列长度 $N$,隐藏维度 $d$,SRAM大小 $M$。

1. **$QK^T$ 计算**: 
   - 读: $Q, K$ → $2Nd$ bytes
   - 写: $S$ → $N^2$ bytes

2. **Softmax**:
   - 读: $S$ → $N^2$ bytes
   - 写: $P$ → $N^2$ bytes

3. **$PV$ 计算**:
   - 读: $P, V$ → $N^2 + Nd$ bytes
   - 写: $O$ → $Nd$ bytes

**总IO量**:
$$
\text{HBM Access} = 2Nd + 4N^2 + 2Nd = 4Nd + 4N^2 = \Theta(N^2)
$$

**FLOPs vs IO**:

计算量:
$$
\text{FLOPs} = 2N^2 d
$$

对于 $N=1024, d=64$:
- FLOPs: $2 \times 1024^2 \times 64 \approx 134\text{M}$
- HBM访问: $4 \times 1024 \times 64 + 4 \times 1024^2 \approx 4.5\text{M}$

**瓶颈**: IO时间 >> 计算时间,GPU利用率低!

**带宽利用率**:
$$
\text{Utilization} = \frac{\text{FLOPs} / \text{Peak FLOPs}}{\text{IO} / \text{Peak BW}} \approx 15\%
$$

### 3.2 Flash Attention核心优化

#### 3.2.1 Tiling分块策略

**核心思想**: 将 $Q, K, V$ 分块,在SRAM中完成计算。

**分块定义**:
- $Q$ 分为 $T_r$ 块: $Q_1, \ldots, Q_{T_r}$,每块大小 $B_r \times d$
- $K, V$ 分为 $T_c$ 块: $K_1, \ldots, K_{T_c}$,每块大小 $B_c \times d$

**约束**: 
$$
B_r d + 2B_c d \leq M \quad (\text{SRAM容量})
$$

**算法流程**:

外层循环(遍历Q块):
1. 加载 $Q_i$ 块到SRAM
2. 初始化输出 $O_i$

内层循环(遍历K,V块):
1. 加载 $K_j, V_j$ 块到SRAM
2. 在SRAM中计算:
   - $S_{ij} = Q_i K_j^T / \sqrt{d}$
   - $P_{ij} = \text{softmax}(S_{ij})$ (使用在线算法)
   - $O_i \mathrel{+}= P_{ij} V_j$

完成内层循环后:
- 将 $O_i$ 写回HBM

#### 3.2.2 在线Softmax算法 (核心创新)

**问题**: 标准softmax需要两次遍历:
$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

1. 第一次: 计算 $\max(x)$
2. 第二次: 计算指数和

**在线算法**: 增量更新统计量!

**数学推导**:

设已处理前 $j-1$ 块,统计量为:
- $m_{old} = \max(x_1, \ldots, x_{j-1})$
- $\ell_{old} = \sum_{i=1}^{j-1} e^{x_i - m_{old}}$

加载新块 $x_j$,更新:
$$
m_{new} = \max(m_{old}, \max(x_j))
$$

**关键**: 重新调整旧的指数和
$$
\ell_{new} = e^{m_{old} - m_{new}} \ell_{old} + \sum_{k \in \text{block}_j} e^{x_k - m_{new}}
$$

**证明正确性**:
$$
\ell_{new} = e^{m_{old} - m_{new}} \sum_{i=1}^{j-1} e^{x_i - m_{old}} + \sum_{k \in j} e^{x_k - m_{new}}
$$
$$
= \sum_{i=1}^{j-1} e^{x_i - m_{new}} + \sum_{k \in j} e^{x_k - m_{new}}
$$
$$
= \sum_{i=1}^{j} e^{x_i - m_{new}} \quad \checkmark
$$

**输出更新**:
$$
O_{new} = \frac{\ell_{old}}{\ell_{new}} e^{m_{old} - m_{new}} O_{old} + \frac{1}{\ell_{new}} \sum_{k \in j} e^{x_k - m_{new}} V_k
$$

#### 3.2.3 IO复杂度分析

**Flash Attention的IO**:

外层循环 $T_r$ 次,内层循环 $T_c$ 次:
- 读 $Q$: $T_c$ 次 × $B_r d$ = $Nd$ (总)
- 读 $K, V$: $T_r$ 次 × $2B_c d$ = $2Nd$ (总)
- 写 $O$: $Nd$

**总IO量**:
$$
\text{HBM Access} = 4Nd = O(Nd)
$$

**对比**:
| 方法 | IO复杂度 |
|------|----------|
| 标准Attention | $O(N^2 + Nd)$ |
| Flash Attention | $O(Nd)$ |

**加速比** (当 $N \gg d$):
$$
\text{Speedup} \approx \frac{N^2}{Nd} = \frac{N}{d}
$$

对于 $N=2048, d=64$:
$$
\text{Speedup} = 2048/64 = 32\times
$$

**实际加速**: 3-5x (受其他因素影响)

### 3.3 反向传播的重计算

**问题**: 反向传播需要 $P$ 矩阵,存储需要 $O(N^2)$ 内存!

**解决**: **重计算** (Recomputation)

在反向传播时:
1. 保存 $Q, K, V$ (只需 $O(Nd)$ 内存)
2. 重新计算 $S, P$
3. 计算梯度

**权衡**:
- 内存: $O(N^2) \rightarrow O(Nd)$
- 计算: 增加1倍(可接受,因为IO是瓶颈)

**数学**: 梯度计算公式
$$
\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial O} P V^T
$$

需要 $P$,但可以即时重算!

---

## 第4章:vLLM推理引擎 - 生产部署实战

> vLLM是当前最高效的LLM推理引擎,本章深入架构原理与生产部署

### 4.1 vLLM架构与核心特性

#### 4.1.1 PagedAttention原理

**核心思想**: 借鉴操作系统虚拟内存的分页机制

**传统KV Cache问题**:
- 预分配固定大小内存
- 内存碎片化严重(浪费60-80%)
- 无法动态调整

**PagedAttention解决方案**:

**数学定义**:

将KV Cache分割为固定大小的块(block):
$$
\text{KV}_{\text{total}} = [\text{Block}_1, \text{Block}_2, \ldots, \text{Block}_N]
$$

每个块大小通常为16-64 tokens:
$$
\text{Block}_i \in \mathbb{R}^{b \times d}
$$

**动态内存分配机制**:

核心组件:
- **空闲块池**: 维护可用的KV Cache块
- **分配表**: 记录每个序列占用的块

工作流程:
1. **分配**: 根据序列长度,从空闲池中分配所需数量的块
   - 所需块数 = ⌈序列长度 / 块大小⌉
2. **使用**: 序列生成过程中,动态填充已分配的块
3. **释放**: 序列完成后,将块归还到空闲池,供新请求使用

**内存节省**:
- **传统方法**: 每个序列预分配最大长度(如2048 tokens)
- **PagedAttention**: 按需分配,只用实际长度的内存

**实测数据(2025)**:
- 内存碎片减少: 60-80%
- 批次大小提升: 2-4x
- 吞吐量提升: 2.7x (vLLM 0.6.0)

#### 4.1.2 Continuous Batching

**传统静态批处理的问题**:

```
Batch 1: [Seq1(100 tokens), Seq2(200 tokens), Seq3(50 tokens)]
等待Seq2完成200步 → GPU空闲50%时间
```

**持续批处理(Continuous Batching)**:

```
Step 1-50:  [Seq1, Seq2, Seq3]  (3个序列)
Step 51:    [Seq1, Seq2, Seq4]  (Seq3完成,加入Seq4)
Step 101:   [Seq5, Seq2, Seq4]  (Seq1完成,加入Seq5)
```

**数学表达**:

设批次容量为 $B$,在时刻 $t$:
$$
\mathcal{B}_t = \{s_1, \ldots, s_k\}, \quad k \leq B
$$

若序列 $s_i$ 在 $t$ 时刻完成:
$$
\mathcal{B}_{t+1} = (\mathcal{B}_t \setminus \{s_i\}) \cup \{s_{\text{new}}\}
$$

**优势**:
- GPU利用率 >90%
- 平均延迟降低 40-50%
- 吞吐量提升 2-3x

#### 4.1.3 2025年最新性能数据

**vLLM 0.6.0 Benchmark**:

| 模型 | 配置 | 吞吐量(tokens/s) | 延迟(ms) |
|------|------|-----------------|---------|
| Llama 8B | H100 | 2,300-2,500 | 35 |
| Llama 70B | 4xH100 | 1,800 | 120 |
| Qwen2.5-7B | A100 | 1,500 | 48 |

**对比**:
- vs HuggingFace Transformers: **5-10x** 吞吐提升
- vs TensorRT-LLM: 相当(某些场景略优)

---

### 4.2 vLLM快速入门

#### 4.2.1 安装

```bash
# 方式1: PyPI安装(推荐)
pip install vllm

# 方式2: 从源码安装(最新特性)
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**依赖要求(2025)**:
- Python 3.9+
- PyTorch 2.1+
- CUDA 12.1+ (推荐12.4)
- GPU: V100/A10/A100/H100

#### 4.2.2 关键配置参数

**核心参数说明**:

- **并行策略**:
  - `tensor_parallel_size`: 张量并行度(跨GPU切分权重)
  - `pipeline_parallel_size`: 流水线并行度(按层切分模型)

- **内存管理**:
  - `gpu_memory_utilization`: GPU显存利用率(0.85-0.95)
  - `max_num_batched_tokens`: 批处理token上限
  - `max_num_seqs`: 最大并发序列数

- **PagedAttention配置**:
  - `block_size`: KV Cache块大小(通常16/32/64)
  - `swap_space`: CPU交换空间大小(GB)

- **优化选项(2025)**:
  - `enable_prefix_caching`: 前缀缓存(共享KV)
  - `enable_chunked_prefill`: 分块预填充

- **量化支持**:
  - `quantization`: 量化方法(awq/gptq/None)
  - `dtype`: 数据类型(float16/bfloat16)

**配置建议**:

| 模型规模 | GPU配置 | TP | PP | 显存利用率 |
|---------|---------|----|----|----------|
| 7B | 1xA100 | 1 | 1 | 0.90 |
| 13B | 2xA100 | 2 | 1 | 0.90 |
| 70B | 4xA100 | 4 | 1 | 0.95 |
| 70B | 8xA100 | 4 | 2 | 0.95 |

---

### 4.3 OpenAI兼容API服务

#### 4.3.1 启动API Server

```bash
# 基础启动
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B \
    --port 8000 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.9

# 完整配置
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-70B \
    --port 8000 \
    --host 0.0.0.0 \
    --tensor-parallel-size 4 \
    --pipeline-parallel-size 2 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --enable-prefix-caching \
    --served-model-name llama-70b \
    --trust-remote-code
```

**环境变量配置**:

```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
export VLLM_USE_MODELSCOPE=false
export HF_HOME=/data/models
```

#### 4.3.2 客户端调用

**OpenAI兼容接口**:

vLLM提供与OpenAI API完全兼容的接口,可以使用任何支持OpenAI格式的客户端库。

**关键端点**:
- `/v1/chat/completions`: 对话补全
- `/v1/completions`: 文本补全
- `/health`: 健康检查
- `/metrics`: Prometheus监控指标

**请求格式示例**:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-70b",
    "messages": [
      {"role": "user", "content": "你好"}
    ],
    "temperature": 0.7,
    "max_tokens": 256
  }'
```

---

### 4.4 Docker部署

#### 4.4.1 官方镜像

```bash
# 拉取镜像
docker pull vllm/vllm-openai:latest

# 启动服务
docker run --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model meta-llama/Llama-3.1-8B \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.9
```

#### 4.4.2 自定义Dockerfile

```dockerfile
FROM vllm/vllm-openai:latest

# 预下载模型
RUN python -c "from huggingface_hub import snapshot_download; \
    snapshot_download('meta-llama/Llama-3.1-8B', cache_dir='/models')"

# 设置环境变量
ENV MODEL_NAME=meta-llama/Llama-3.1-8B
ENV TENSOR_PARALLEL_SIZE=2
ENV GPU_MEMORY_UTILIZATION=0.9

# 启动脚本
CMD python -m vllm.entrypoints.openai.api_server \
    --model $MODEL_NAME \
    --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
    --gpu-memory-utilization $GPU_MEMORY_UTILIZATION
```

**构建与运行**:

```bash
docker build -t my-vllm .
docker run --gpus all -p 8000:8000 my-vllm
```

---

### 4.5 Kubernetes生产部署

#### 4.5.1 部署配置(2025最佳实践)

```yaml
# vllm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama
  namespace: ai-inference
spec:
  replicas: 3                    # 多副本
  selector:
    matchLabels:
      app: vllm-llama
  template:
    metadata:
      labels:
        app: vllm-llama
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - --model
          - meta-llama/Llama-3.1-8B
          - --tensor-parallel-size
          - "2"
          - --gpu-memory-utilization
          - "0.9"
          - --max-model-len
          - "8192"
          - --enable-prefix-caching
        resources:
          limits:
            nvidia.com/gpu: 2    # 每个Pod 2卡
          requests:
            nvidia.com/gpu: 2
            memory: "32Gi"
            cpu: "8"
        ports:
        - containerPort: 8000
          name: http
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface
        livenessProbe:          # 健康检查
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:         # 就绪检查
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-pvc

---
# PVC配置(关键:使用ReadWriteMany)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-pvc
  namespace: ai-inference
spec:
  accessModes:
  - ReadWriteMany              # 多Pod共享,加速HPA扩容
  resources:
    requests:
      storage: 100Gi
  storageClassName: nfs-storage

---
# Service配置
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
  namespace: ai-inference
spec:
  selector:
    app: vllm-llama
  ports:
  - port: 8000
    targetPort: 8000
    name: http
  type: LoadBalancer           # 或ClusterIP+Ingress

---
# HPA自动扩缩容
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
  namespace: ai-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-llama
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:                    # 扩缩容策略
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
```

**关键配置说明**:

1. **ReadWriteMany PVC**:
   - 多Pod共享模型缓存
   - HPA扩容时无需重新下载模型
   - 扩容速度提升10-100x

2. **健康检查**:
   - Liveness: 重启失败Pod
   - Readiness: 流量路由控制

3. **HPA策略**:
   - CPU + 自定义指标(QPS)
   - 平稳扩缩容(stabilizationWindow)

#### 4.5.2 Ingress配置

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vllm-ingress
  namespace: ai-inference
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
  - host: llm.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: vllm-service
            port:
              number: 8000
  tls:
  - hosts:
    - llm.example.com
    secretName: llm-tls
```

---

### 4.6 vLLM Production Stack(2025新特性)

#### 4.6.1 前缀缓存与KV共享

**核心优势**:
- **10x性能提升**: 3-10x低延迟, 2-5x高吞吐
- **前缀感知路由**: 相似请求共享KV Cache
- **自动负载均衡**: 智能分配请求

**启用方式**:

通过配置参数启用:
- `enable_prefix_caching=True`: 启用前缀缓存
- `enable_chunked_prefill=True`: 启用分块预填充

**原理**:

```
请求1: "请解释PagedAttention的原理和优势"
请求2: "请解释PagedAttention的实现细节"

共享前缀: "请解释PagedAttention的"
→ 只计算一次KV Cache,两个请求共享!
```

#### 4.6.2 多GPU优化配置

**常见误区**: `--tensor-parallel-size 1` 仍会使用多GPU

**正确配置示例(DeepSeek-R1 70B)**:

关键参数配置:
- `tensor_parallel_size=4`: 4卡张量并行
- `pipeline_parallel_size=2`: 2级流水线并行
- `max_num_batched_tokens=8192`: 批处理上限
- `max_num_seqs=256`: 最大并发序列数
- `enable_prefix_caching=True`: 启用前缀缓存
- `gpu_memory_utilization=0.95`: 高显存利用率

**性能调优表**:

| 模型规模 | GPU配置 | TP | PP | BatchSize | 吞吐量 |
|---------|---------|----|----|----------|--------|
| 7B | 1xA100 | 1 | 1 | 128 | 2,500 |
| 13B | 2xA100 | 2 | 1 | 256 | 3,200 |
| 70B | 4xA100 | 4 | 1 | 512 | 1,800 |
| 70B | 8xA100 | 4 | 2 | 1024 | 2,500 |

---

### 4.7 性能监控

#### 4.7.1 Prometheus Metrics

**vLLM暴露的指标**:

```python
# 访问 http://localhost:8000/metrics

# 关键指标:
# - vllm:num_requests_running       (运行中的请求数)
# - vllm:num_requests_waiting       (等待中的请求数)
# - vllm:gpu_cache_usage_perc       (GPU缓存使用率)
# - vllm:avg_generation_throughput_toks_per_s  (平均吞吐量)
# - vllm:avg_time_to_first_token_seconds       (首token延迟)
# - vllm:avg_time_per_output_token_seconds     (每token延迟)
```

**Prometheus配置**:

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['vllm-service:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s
```

#### 4.7.2 Grafana Dashboard

**关键面板**:

1. **吞吐量**: `rate(vllm:num_requests_running[1m])`
2. **延迟**: `vllm:avg_time_to_first_token_seconds`
3. **队列长度**: `vllm:num_requests_waiting`
4. **GPU利用率**: `vllm:gpu_cache_usage_perc`

---

### 4.8 故障排查

#### 4.8.1 常见问题

**1. OOM(显存不足)**:

```bash
# 降低显存利用率
--gpu-memory-utilization 0.85

# 降低最大长度
--max-model-len 4096

# 降低批次大小
--max-num-seqs 128
```

**2. 低吞吐量**:

```python
# 启用优化选项
enable_prefix_caching=True
enable_chunked_prefill=True

# 增加批次大小
max_num_batched_tokens=16384
max_num_seqs=512
```

**3. 高延迟**:

```bash
# 减少并发
--max-num-seqs 64

# 使用更小的模型
--model meta-llama/Llama-3.1-8B  # 代替70B
```

#### 4.8.2 日志分析

```bash
# 启用详细日志
export VLLM_LOGGING_LEVEL=DEBUG

# 查看日志
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B \
    --log-level debug 2>&1 | tee vllm.log
```

---

### 4.9 最佳实践总结

**部署建议**:

```
单GPU(A100 40GB): Llama-3.1-8B, batch=128
双GPU(2xA100): Llama-3.1-13B, TP=2, batch=256
四GPU(4xA100): Llama-3.1-70B, TP=4, batch=512
八GPU(8xH100): Llama-3.1-405B, TP=8, PP=2
```

**优化清单**:

- ✅ 启用 `enable_prefix_caching`
- ✅ 使用 `ReadWriteMany` PVC (K8s)
- ✅ 配置HPA自动扩缩容
- ✅ 监控Prometheus指标
- ✅ 定期更新vLLM版本

**成本优化**:

- 使用Spot实例(降低60%成本)
- 混合部署(小模型+大模型)
- 智能路由(简单请求→小模型)

---

**参考资源**:
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [vLLM 2025部署指南](https://indusvalley.io/blogs/managing-deployments-of-llms-using-vllm)
- [K8s生产栈](https://blog.lmcache.ai/en/2025/01/21/high-performance-and-easy-deployment-of-vllm-in-k8s-with-vllm-production-stack/)
- [多GPU优化](https://www.databasemart.com/blog/vllm-distributed-inference-optimization-guide)

---

## 第5章:TensorRT-LLM

### 5.1 优化技术

**核心特性**:
- FP16/INT8/INT4混合精度
- Flash Attention集成
- 自定义CUDA Kernel
- 多GPU并行

### 5.2 部署流程

```bash
# 1. 模型转换
python convert_checkpoint.py \
    --model_dir ./llama-7b \
    --output_dir ./trt_ckpt \
    --dtype float16 \
    --tp_size 2

# 2. 构建引擎
trtllm-build \
    --checkpoint_dir ./trt_ckpt \
    --output_dir ./trt_engine \
    --gemm_plugin float16 \
    --max_batch_size 256

# 3. 推理
mpirun -n 2 python run.py --engine_dir ./trt_engine
```

### 5.3 性能对比

| 配置 | 吞吐(tokens/s) | 延迟(ms) |
|------|---------------|---------|
| HF Transformers | 25 | 850 |
| vLLM | 120 | 180 |
| TensorRT-LLM | 180 | 120 |

---

## 第6章:KV Cache优化

### 6.1 问题分析

**内存占用**:
$$\text{Memory}_{KV} = 2 \times L \times N \times d \times b$$

- L: 层数
- N: 序列长度  
- d: 隐藏维度
- b: 字节数

**示例**(LLaMA-7B, 2048 tokens):
- FP16: 2 × 32 × 2048 × 4096 × 2 = 1GB

### 6.2 优化策略

#### 6.2.1 MQA/GQA

**Multi-Query Attention (MQA)**:
- 所有头共享同一组K,V
- 内存降低 H倍(H为头数)

**Grouped-Query Attention (GQA)**:
- 分组共享K,V
- 平衡MQA和MHA

#### 6.2.2 KV Cache量化

```python
# INT8量化KV Cache
kv_cache_int8 = quantize(kv_cache_fp16, scale)
# 内存减半,精度损失<1%
```

---

## 第7章:边缘部署

### 7.1 llama.cpp

**特点**:
- 纯C++实现
- CPU推理优化
- 量化支持(Q4/Q5/Q8)

**使用**:
```bash
# 量化模型
./quantize llama-7b.gguf llama-7b-q4.gguf Q4_K_M

# 推理
./main -m llama-7b-q4.gguf -p "Once upon a time" -n 128
```

### 7.2 MLC-LLM移动端

**支持平台**:
- iOS (iPhone/iPad)
- Android
- WebGPU (浏览器)

**优化**:
- Metal/Vulkan后端
- 4-bit量化
- 分块加载

---

## 第8章:生产部署架构

### 8.1 推理服务设计

**架构概览**:

```
客户端请求 → 负载均衡器 → API Gateway → vLLM引擎
                                        ↓
                                  PagedAttention
                                        ↓
                                  GPU内存池
```

**关键设计要点**:

1. **异步处理**: 使用异步引擎提升并发能力
2. **流式响应**: 支持SSE(Server-Sent Events)流式输出
3. **请求队列**: 实现优先级队列和超时控制
4. **健康检查**: 定期检查GPU状态和内存使用

**推荐架构组件**:
- Web框架: FastAPI/Flask
- 负载均衡: Nginx/HAProxy
- 监控: Prometheus + Grafana
- 日志: ELK Stack

### 8.2 负载均衡

**策略**:
- 按序列长度分流
- GPU利用率感知调度
- 优先级队列

### 8.3 监控指标

**关键指标**:
```python
metrics = {
    'throughput': tokens_per_second,
    'latency_p50': p50_ms,
    'latency_p99': p99_ms,
    'gpu_utilization': gpu_util,
    'kv_cache_usage': cache_ratio,
    'queue_length': pending_requests
}
```

---

## 总结

### 核心要点

1. **量化是主流**: INT8无损,INT4可接受
2. **Flash Attention必备**: 3-5x加速
3. **vLLM是首选**: 生产级吞吐
4. **边缘有llama.cpp**: CPU推理优化

### 优化流程

```
模型选择 → 量化(GPTQ/AWQ) → 引擎选择(vLLM/TRT) 
→ 调优(batch size/KV cache) → 监控优化
```

### 实战建议

**服务器部署**:
```
推荐: vLLM + Flash Attention + INT8量化
吞吐: >100 tokens/s
成本: A100 80GB
```

**边缘部署**:
```
推荐: llama.cpp + Q4量化
模型: 7B
设备: M2 Mac / RTX 4090
```

### 前沿趋势

1. **INT4成为标准**: AWQ/GPTQ广泛应用
2. **MOE稀疏激活**: 激活10-20%参数
3. **投机解码**: 小模型草稿+大模型验证
4. **硬件协同**: NPU专用AI芯片

---

**推荐资源**:
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GPTQ论文](https://arxiv.org/abs/2210.17323)
- [AWQ论文](https://arxiv.org/abs/2306.00978)
