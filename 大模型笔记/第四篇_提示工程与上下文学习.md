# 第五篇:提示工程与上下文学习

> 从基础Prompt到高级推理,掌握LLM应用的核心技能

**适合人群**: 所有开发者、产品经理、应用工程师
**预计时间**: 6-8 小时
**前置知识**: 第一篇(大模型基础)

---

## 本篇概览

本篇整合了**提示工程**、**上下文学习**和**文本生成策略**:
- Prompt设计的四要素框架与最佳实践
- CoT/ToT/ReAct等高级推理技术
- ICL原理与Few-shot技巧
- 文本生成解码算法(Temperature/Top-p/Beam Search)
- 结构化输出(JSON Mode/Function Calling)

---

## 第1章:Prompt基础

### 1.1 什么是Prompt Engineering

**定义**: 通过精心设计输入文本,引导大语言模型生成期望输出的技术。

**核心思想**:
- 将任务需求转化为模型能理解的指令
- 通过示例和上下文引导模型行为  
- 零代码、纯输入优化

**为什么重要**:
```
传统ML:  数据 + 训练 → 模型
Prompt工程: 任务 + Prompt → LLM → 结果
```

### 1.2 Prompt的四要素框架

```
Prompt = 指令(Instruction) 
       + 上下文(Context) 
       + 输入(Input) 
       + 输出指示(Output Indicator)
```

**完整示例**:
```
# 指令
你是一位专业的翻译助手,擅长中英互译。

# 上下文  
用户需要将技术文档翻译成英文,要求专业、准确。

# 输入
中文: 人工智能正在改变世界。

# 输出指示
英文:
```

### 1.3 设计原则

#### 1.3.1 清晰性原则

❌ **坏示例**:
```
写点关于AI的东西
```

✅ **好示例**:
```
请撰写一篇500字的科普文章,介绍人工智能的三个主要应用领域:
1. 医疗诊断
2. 自动驾驶  
3. 智能客服

要求:
- 语言通俗易懂,适合非专业读者
- 每个领域150字左右
- 包含具体案例
```

#### 1.3.2 具体性原则

❌ **模糊指令**:
```
总结这篇文章
```

✅ **具体指令**:
```
用3个要点总结这篇文章的核心观点:
- 每个要点不超过30字
- 按重要性排序
- 使用项目符号格式
```

#### 1.3.3 分步指令

**复杂任务拆解**:
```
请按以下步骤分析客户评论:

步骤1: 识别评论的情感倾向(正面/负面/中性)
步骤2: 提取用户提到的具体问题
步骤3: 建议改进措施
步骤4: 用JSON格式输出结果
```

### 1.4 常用Prompt模板

#### 1.4.1 角色扮演模板

```
你是一位{角色},拥有{专业背景}。

你的职责是{任务描述}。

在回答时,请注意:
- {要求1}
- {要求2}
- {要求3}

现在,请{具体任务}:
{输入内容}
```

#### 1.4.2 任务分解模板

```
任务: {总体目标}

请按以下子任务逐步完成:
1. {子任务1}
2. {子任务2}  
3. {子任务3}

最终输出格式:
{格式说明}
```

#### 1.4.3 Few-shot学习模板

```
以下是{任务}的示例:

示例1:
输入: {输入1}
输出: {输出1}

示例2:
输入: {输入2}
输出: {输出2}

现在请处理新输入:
输入: {新输入}
输出:
```

---

## 第2章:上下文学习(ICL)理论

### 2.1 什么是ICL

**定义**: 模型通过少量示例(无梯度更新)学习新任务的能力。

**数学形式**:
$$
P_{\theta}(y | x, \mathcal{D}_{\text{demo}})
$$

其中:
- $\mathcal{D}_{\text{demo}} = \{(x_1, y_1), \ldots, (x_k, y_k)\}$: 示例集
- $\theta$: 预训练参数(冻结)

**关键**: 无需微调,纯输入改变!

### 2.2 ICL的理论解释

#### 2.2.1 隐式梯度下降

**理论**(von Oswald et al. 2023):

Transformer的前向传播等价于**梯度下降步骤**:

$$
\theta_{\text{ICL}} = \theta_{\text{pretrain}} - \eta \nabla_\theta \mathcal{L}(\mathcal{D}_{\text{demo}})
$$

**证明草图**:

Self-Attention的更新:
$$
h_{t+1} = h_t + \text{Attn}(h_t, \text{context})
$$

类似于优化器更新:
$$
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}
$$

通过精心设计的注意力权重矩阵,可以模拟梯度下降!

#### 2.2.2 贝叶斯推理视角

**Bayesian ICL** (Xie et al. 2022):

模型维护任务分布的隐式后验:
$$
P(\text{task} | \mathcal{D}_{\text{demo}}) \propto P(\mathcal{D}_{\text{demo}} | \text{task}) P(\text{task})
$$

预测时:
$$
P(y|x, \mathcal{D}_{\text{demo}}) = \int P(y|x, \text{task}) P(\text{task}|\mathcal{D}_{\text{demo}}) d\text{task}
$$

**直觉**: 示例帮助模型推断当前任务!

#### 2.2.3 函数学习视角

ICL可视为学习**函数映射**:
$$
f_{\mathcal{D}_{\text{demo}}}: \mathcal{X} \rightarrow \mathcal{Y}
$$

GPT等模型是**元学习器**(meta-learner),能快速适配新函数!

### 2.3 Few-Shot vs Zero-Shot vs Fine-tune

| 方法 | 示例数 | 梯度更新 | 泛化能力 | 成本 |
|------|--------|---------|---------|------|
| **Zero-Shot** | 0 | ✗ | 低 | 极低 |
| **Few-Shot (ICL)** | 1-10 | ✗ | 中 | 低 |
| **Fine-tune** | 100-10K | ✓ | 高 | 高 |

**数学对比**:

Zero-Shot:
$$
P_\theta(y|x)
$$

Few-Shot ICL:
$$
P_\theta(y|x, x_1, y_1, \ldots, x_k, y_k)
$$

Fine-tune:
$$
P_{\theta'}(y|x), \quad \theta' = \arg\min_\theta \mathcal{L}(\mathcal{D}_{\text{train}})
$$

### 2.4 示例选择策略

#### 2.4.1 随机选择

最简单,但性能波动大:
$$
\mathcal{D}_{\text{demo}} \sim \text{Uniform}(\mathcal{D}_{\text{all}})
$$

#### 2.4.2 相似度选择

选择与测试样本最相似的示例:
$$
\mathcal{D}_{\text{demo}} = \text{TopK}_{\text{sim}}(x_{\text{test}}, \mathcal{D}_{\text{all}})
$$

**相似度度量**:
- 余弦相似度: $\text{sim}(x, x') = \frac{x \cdot x'}{\|x\| \|x'\|}$
- BM25分数(文本检索)

**算法伪代码**:
```
函数 select_examples(query, candidates, k):
    输入:
        - query: 查询文本
        - candidates: 候选示例集合
        - k: 选择数量

    1. 文本编码:
       query_embedding = Encoder(query)
       candidate_embeddings = [Encoder(c.text) for c in candidates]

    2. 计算相似度:
       scores = []
       for cand_emb in candidate_embeddings:
           # 余弦相似度
           sim = dot(query_embedding, cand_emb) / (norm(query_embedding) * norm(cand_emb))
           scores.append(sim)

    3. 选择TopK:
       top_k_indices = argsort(scores, descending=True)[:k]

    返回: [candidates[i] for i in top_k_indices]
```

**关键**: 使用预训练的文本编码器(如BERT, Sentence-BERT)获取语义向量

#### 2.4.3 多样性选择

**问题**: 相似度选择可能选到重复示例

**解决**: 最大边际相关性(MMR)
$$
\text{MMR} = \arg\max_{d \in D \setminus S} [\lambda \cdot \text{sim}(d, q) - (1-\lambda) \cdot \max_{d' \in S} \text{sim}(d, d')]
$$

平衡相关性和多样性!

#### 2.4.4 难度递增选择

**策略**: 从简单到困难排序示例

**效果**: 类似课程学习,提升10-20%性能!

### 2.5 示例数量的影响

**经验规律**:
$$
\text{Performance} \approx a + b \log(k)
$$

其中 $k$ 为示例数。

**实验数据** (GPT-3):
| 示例数 | 准确率 |
|--------|--------|
| 0 (Zero-shot) | 65% |
| 1 | 72% |
| 5 | 81% |
| 10 | 84% |
| 50 | 86% |

**饱和现象**: 超过10个示例,收益递减!

**理论解释**: 上下文窗口限制 + 注意力稀释

### 2.6 标签空间的重要性

**发现**(Min et al. 2022): 
即使**标签错误**,ICL仍有效!

**实验**:
```
示例: "I love this movie → Negative"  (标签错误!)
测试: "Great film" → ?
结果: 仍能学会情感分类!
```

**理论**: ICL主要学习**输入空间结构**和**标签格式**,而非输入-标签映射!

数学表达:
$$
P(y|x, \mathcal{D}_{\text{demo}}) \approx P_{\text{format}}(y) \cdot P_{\text{space}}(y|x)
$$

---

## 第3章:思维链(CoT)推理

### 3.1 CoT原理

**标准Prompt**:
```
Q: Roger有5个球,他买了2个网球罐,每罐3个球。他现在有多少球?
A: 11
```

**CoT Prompt**:
```
Q: Roger有5个球,他买了2个网球罐,每罐3个球。他现在有多少球?
A: Roger开始有5个球。
   2个罐子,每个3个球,是6个新球。
   5 + 6 = 11。
   答案是11。
```

**核心思想**: 显式输出中间推理步骤!

### 3.2 为什么CoT有效?

#### 3.2.1 计算图扩展

**理论**(Ling et al. 2023):

没有CoT:
$$
y = f_\theta(x) \quad \text{(单步映射)}
$$

使用CoT:
$$
y = f_\theta^{(n)}(f_\theta^{(n-1)}(\cdots f_\theta^{(1)}(x))) \quad \text{(多步复合)}
$$

**效果**: 增加有效计算深度,类似增加Transformer层数!

#### 3.2.2 工作记忆扩展

**类比**: CoT = 外部草稿纸

无CoT时,模型只能用内部隐藏状态存储中间结果(有限)。

CoT将中间步骤写入上下文(无限扩展)!

**数学**:

记忆容量:
$$
\text{Memory}_{\text{no-CoT}} = O(d_{\text{hidden}})
$$
$$
\text{Memory}_{\text{CoT}} = O(L_{\text{context}})
$$

### 3.3 Zero-Shot CoT

**神奇发现**: 只需在Prompt加 **"Let's think step by step"**!

```python
prompt = f"""
Q: {question}
A: Let's think step by step.
"""

response = llm(prompt)
```

**效果**: 无需示例,性能提升20-50%!

**理论**: 预训练数据包含大量"step by step"推理文本,这个短语激活了模型的推理模式!

### 3.4 自洽性(Self-Consistency)

**方法**:
1. 用CoT生成N个推理路径(temperature > 0)
2. 提取N个答案
3. 多数投票

**数学**:
$$
\hat{y} = \arg\max_y \sum_{i=1}^N \mathbb{1}[y_i = y]
$$

**效果**: GSM8K数学题 74% → 82%!

**为什么有效?**

不同推理路径可能犯不同错误,但正确答案更一致!

类似集成学习的Bagging思想。

### 3.5 CoT的局限

**失败案例**:

1. **错误传播**: 早期步骤错误导致后续全错
2. **幻觉链**: 生成看似合理但错误的推理
3. **过度依赖**: 简单问题反而变差

**解决方案**: 下一节的高级技术!

---

### 2.1 ICL的理论基础

**现象**: 大模型可以从Few-shot示例中"学习",无需梯度更新。

**数学建模**: ICL可视为隐式贝叶斯推断

给定示例 $D = \{(x^{(i)}, y^{(i)})\}_{i=1}^k$,模型推断任务 $\tau$:

$$
P(y | x, D) = \mathbb{E}_{\tau \sim P(\tau | D)} [P(y | x, \tau)]
$$

**直觉解释**:
1. 模型从示例中推断任务类型
2. 在该任务假设下预测新输入
3. 无需显式更新参数

### 2.2 示例选择策略

#### 2.2.1 相似性检索

使用语义相似度选择最相关示例:

**算法流程**:
```
函数 select_examples_by_similarity(query, example_pool, k):
    1. 编码查询和示例:
       query_vector = TextEncoder(query)
       example_vectors = [TextEncoder(ex.input) for ex in example_pool]

    2. 计算相似度矩阵:
       similarities = []
       for ex_vec in example_vectors:
           sim = cosine_similarity(query_vector, ex_vec)
           similarities.append(sim)

    3. 排序并选择TopK:
       sorted_indices = argsort(similarities, reverse=True)
       top_k_indices = sorted_indices[:k]

    返回: [example_pool[i] for i in top_k_indices]
```

**技术要点**:
- 文本编码器可用BERT、RoBERTa等预训练模型
- 余弦相似度计算: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}$

#### 2.2.2 多样性采样

避免示例过于相似,使用**最大边际相关性(MMR)**:

**MMR算法**:
$$
\text{MMR} = \arg\max_{d \in \mathcal{D} \setminus \mathcal{S}} [\lambda \cdot \text{Sim}(d, Q) - (1-\lambda) \cdot \max_{s \in \mathcal{S}} \text{Sim}(d, s)]
$$

**伪代码**:
```
函数 diverse_sampling_mmr(query, examples, k, λ):
    输入:
        - query: 查询文本
        - examples: 候选示例集
        - k: 选择数量
        - λ: 平衡参数(0-1,推荐0.5)

    初始化:
        selected = []
        candidates = examples.copy()

    1. 选择首个最相关示例:
       first = argmax(similarity(query, ex) for ex in candidates)
       selected.add(first)
       candidates.remove(first)

    2. 迭代选择剩余示例:
       while len(selected) < k:
           mmr_scores = []
           for cand in candidates:
               # 相关性分数
               relevance = similarity(query, cand)

               # 多样性分数(与已选示例的最小差异)
               diversity = min(1 - similarity(cand, sel) for sel in selected)

               # MMR综合得分
               mmr_score = λ * relevance + (1 - λ) * diversity
               mmr_scores.append(mmr_score)

           # 选择MMR得分最高的
           best_idx = argmax(mmr_scores)
           selected.add(candidates[best_idx])
           candidates.remove(candidates[best_idx])

    返回: selected
```

**参数调节**:
- $\lambda = 1.0$: 纯相关性,等价于贪心选择
- $\lambda = 0.5$: 平衡相关性与多样性(推荐)
- $\lambda = 0.0$: 纯多样性,可能选到无关示例

### 2.3 示例顺序的影响

**实验发现**: 示例顺序会影响性能!

| 顺序策略 | MMLU准确率 |
|---------|-----------|
| 随机顺序 | 42.3% |
| 难度递增 | 44.1% |
| 相似度排序 | 45.2% |

**最佳实践**: 
- 将最相似的示例放在最后(最接近输入)
- 简单→困难的渐进顺序

### 2.4 ICL的能力边界

**优势**:
- ✅ 快速适应新任务
- ✅ 无需训练数据
- ✅ 灵活可控

**局限**:
- ❌ 上下文长度限制(通常<10个示例)
- ❌ 复杂推理能力不足
- ❌ 对示例质量敏感

---

## 第3章:高级推理技术

### 3.1 Chain-of-Thought (CoT)

#### 3.1.1 核心思想

引导模型生成**中间推理步骤**,而非直接输出答案。

**标准Prompt**:
```
Q: 咖啡店有23杯咖啡,卖出了17杯,又做了8杯。现在有多少杯?
A: 15杯
```

**CoT Prompt**:
```
Q: 咖啡店有23杯咖啡,卖出了17杯,又做了8杯。现在有多少杯?
A: 让我们一步步思考:
1. 初始有23杯
2. 卖出17杯后剩余: 23 - 17 = 6杯
3. 又做了8杯: 6 + 8 = 14杯
因此,现在有14杯咖啡。
```

#### 3.1.2 数学原理

**分解复杂度**: 
$$
P(y | x) = \sum_{r \in \text{Reasoning}} P(y | r) P(r | x)
$$

通过显式生成推理链 $r$,降低单步预测难度。

#### 3.1.3 Zero-shot CoT

**神奇提示词**: "Let's think step by step"

```
Q: 一个数的3倍加上5等于20,这个数是多少?
A: Let's think step by step.
```

模型会自动生成推理过程!

**为什么有效**:
- 预训练数据中存在大量"step by step"推理模式
- 激活了模型的推理能力

#### 3.1.4 CoT实战技巧

**提示模板**:
```python
cot_prompt = """
解决以下问题,请详细说明推理过程:

问题: {question}

推理步骤:
1. 
2.
3.

最终答案:
"""
```

**验证答案**:
```python
def verify_with_cot(question, answer):
    prompt = f"""
    问题: {question}
    给定答案: {answer}
    
    请验证这个答案是否正确,说明理由:
    """
    return llm.generate(prompt)
```

### 3.2 Self-Consistency

**动机**: CoT的单次推理可能出错,如何提高可靠性?

**方法**: 多次采样 + 投票

**算法流程**:
```
函数 self_consistency(question, n_samples=5):
    输入:
        - question: 问题文本
        - n_samples: 采样次数

    1. 生成多个推理路径:
       answers = []
       for i in 1 to n_samples:
           # 使用温度采样增加多样性
           prompt = question + "\nLet's think step by step."
           response = LLM.generate(prompt, temperature=0.7)

           # 提取最终答案
           answer = extract_final_answer(response)
           answers.append(answer)

    2. 多数投票(Majority Voting):
       # 统计每个答案的出现次数
       answer_counts = {}
       for ans in answers:
           answer_counts[ans] = answer_counts.get(ans, 0) + 1

       # 选择出现次数最多的答案
       final_answer = argmax(answer_counts, key=answer_counts.get)

    返回: final_answer
```

**数学形式**:
$$
\hat{y} = \arg\max_y \sum_{i=1}^N \mathbb{1}[y_i = y]
$$

其中 $y_i$ 是第 $i$ 次采样得到的答案。
```

**效果提升**:
- GSM8K数学题: 74.4% → 78.7%
- 代价: 5倍计算量

### 3.3 Tree of Thoughts (ToT)

**思想**: 将推理过程建模为搜索树。

```
         根问题
        /   |   \
    思路1 思路2 思路3
     / \    |    / \
   ... ... ... ... ...
```

**算法流程**:
```python
def tree_of_thoughts(problem, max_depth=3, beam_width=3):
    """
    思维树搜索
    """
    def evaluate_thought(thought):
        # 使用LLM评估思路的潜力
        score = llm.generate(
            f"评价这个思路的正确性(0-10分): {thought}"
        )
        return parse_score(score)
    
    # BFS或DFS搜索
    queue = [(problem, [], 0)]  # (当前状态, 路径, 深度)
    
    while queue:
        state, path, depth = queue.pop(0)
        
        if is_solution(state):
            return path
        
        if depth >= max_depth:
            continue
        
        # 生成候选思路
        thoughts = llm.generate(
            f"针对问题 '{state}',列出{beam_width}种可能的思路:"
        )
        
        # 评分并排序
        scored_thoughts = [(t, evaluate_thought(t)) for t in thoughts]
        scored_thoughts.sort(key=lambda x: x[1], reverse=True)
        
        # 扩展Top-K
        for thought, score in scored_thoughts[:beam_width]:
            new_state = apply_thought(state, thought)
            queue.append((new_state, path + [thought], depth + 1))
    
    return None
```

**适用场景**:
- 创意写作(探索多个情节走向)
- 数学证明(尝试不同证明路径)
- 战略规划(评估多种方案)

### 3.4 ReAct: 推理+行动

**框架**: 交替进行思考(Thought)和行动(Action)。

```
Thought: 我需要查找最新的GDP数据
Action: Search("中国2024年GDP")
Observation: [搜索结果]

Thought: 数据显示GDP为...现在需要计算增长率
Action: Calculate(growth_rate)
Observation: 5.2%

Thought: 已获得所有信息,可以回答
Answer: 2024年中国GDP增长率为5.2%
```

**实现**:
```python
def react_agent(question, max_steps=5):
    """
    ReAct Agent实现
    """
    tools = {
        'Search': search_tool,
        'Calculate': calculator,
        'Lookup': lookup_tool
    }
    
    context = f"Question: {question}\n"
    
    for step in range(max_steps):
        # 生成思考和行动
        response = llm.generate(
            context + "\nThought:"
        )
        
        thought = extract_thought(response)
        action = extract_action(response)  # 格式: Action[arg]
        
        context += f"\nThought: {thought}"
        context += f"\nAction: {action}"
        
        # 执行行动
        if action.startswith('Finish'):
            return extract_answer(action)
        
        tool_name, arg = parse_action(action)
        observation = tools[tool_name](arg)
        
        context += f"\nObservation: {observation}"
    
    return "Max steps reached"
```

---

## 第4章:文本生成解码策略

### 4.1 解码算法数学基础

文本生成的核心是自回归建模:
$$
P(x_{1:n}) = \prod_{t=1}^{n} P(x_t | x_{<t})
$$

**目标**: 找到高概率序列。

### 4.2 Greedy Decoding

**算法**: 每步选择概率最大的token
$$
x_t = \arg\max_{v \in \mathcal{V}} P(v | x_{<t})
$$

**优点**:
- 确定性,可复现
- 速度快

**缺点**:
- 局部最优,可能陷入重复
- 缺乏多样性

### 4.3 Temperature Sampling

**公式**:
$$
P_T(x_t = v) = \frac{\exp(z_v / T)}{\sum_{u} \exp(z_u / T)}
$$

**温度效应**:

| Temperature | 效果 | 应用场景 |
|------------|------|---------|
| T = 0 | 等价于Greedy | 事实性任务(翻译、摘要) |
| T = 0.7 | 平衡分布 | 对话、问答(推荐) |
| T = 1.0 | 原始分布 | 标准采样 |
| T = 1.5 | 创意多样 | 创意写作、头脑风暴 |

**为什么0.7是常用值**:

通过实验发现,0.7在**流畅性**和**多样性**间达到最优平衡:
- T < 0.5: 过于确定,缺乏变化
- T = 0.7: 既保留高概率选项,又允许适度探索
- T > 1.0: 容易产生不连贯文本

### 4.4 Top-k Sampling

**算法**: 仅从概率Top-k的token中采样

$$
P_{\text{top-k}}(v) = \begin{cases}
\frac{P(v)}{\sum_{u \in V_k} P(u)} & v \in V_k \\
0 & \text{otherwise}
\end{cases}
$$

**参数选择**:
- k = 40-50: 常用值
- k = 1: 等价于Greedy
- k过大: 引入噪声
- k过小: 限制多样性

### 4.5 Nucleus Sampling (Top-p)

**核心思想**: 动态选择累积概率达到p的最小token集合

$$
V_p = \min \left\{ V' : \sum_{v \in V'} P(v) \geq p \right\}
$$

**自适应性**:
```
高确定性分布(如"Paris is the capital of ___"):
  P = [0.95(France), 0.02, 0.01, ...]
  Top-0.9自动缩小到1-2个token

低确定性分布(如创意续写):
  P = [0.15, 0.12, 0.11, 0.10, ...]
  Top-0.9扩展到更多token
```

**典型值**: p = 0.9 或 p = 0.95

### 4.6 组合策略

**实践推荐**:
```python
# 平衡质量与多样性
response = llm.generate(
    prompt,
    temperature=0.7,
    top_p=0.9,
    top_k=50
)
```

**不同场景配置**:
```python
# 事实性任务
factual_config = {
    'temperature': 0.2,
    'top_p': 0.95
}

# 对话系统
chat_config = {
    'temperature': 0.7,
    'top_p': 0.9,
    'frequency_penalty': 0.3  # 降低重复
}

# 创意写作
creative_config = {
    'temperature': 1.0,
    'top_p': 0.95,
    'presence_penalty': 0.6  # 鼓励新话题
}
```

---

## 第5章:结构化输出

### 5.1 JSON Mode

**OpenAI API**:
```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "你是数据提取助手,以JSON格式输出"},
        {"role": "user", "content": "提取: 张三,30岁,软件工程师"}
    ],
    response_format={"type": "json_object"}
)
```

**输出保证**:
```json
{
  "name": "张三",
  "age": 30,
  "occupation": "软件工程师"
}
```

### 5.2 Function Calling

**定义函数Schema**:
```python
functions = [
    {
        "name": "get_weather",
        "description": "获取指定城市的天气信息",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "城市名称,如: 北京、上海"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "温度单位"
                }
            },
            "required": ["location"]
        }
    }
]

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "北京今天天气怎么样?"}],
    functions=functions,
    function_call="auto"
)

if response.choices[0].message.function_call:
    function_call = response.choices[0].message.function_call
    args = json.loads(function_call.arguments)
    # 调用实际函数
    result = get_weather(**args)
```

### 5.3 Schema验证原理

**核心思想**: 通过Schema约束模型输出格式

**JSON Schema示例**:
```json
{
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "人名"
    },
    "age": {
      "type": "integer",
      "minimum": 0,
      "maximum": 150,
      "description": "年龄"
    },
    "skills": {
      "type": "array",
      "items": {"type": "string"},
      "description": "技能列表"
    }
  },
  "required": ["name", "age", "skills"]
}
```

**提示模板**:
```
从以下文本中提取人物信息,严格按照JSON Schema输出:

文本: {input_text}

JSON Schema:
{schema_definition}

要求:
1. 输出必须是有效的JSON
2. 所有必填字段必须存在
3. 数据类型必须匹配
4. 数值范围必须符合约束

输出:
```

**验证流程**:
```
1. 生成响应 → 2. 解析JSON → 3. Schema验证 → 4. 类型转换
   ↓失败                ↓失败              ↓失败
   重试或报错          重试或报错        重试或报错
```

---

## 第6章:Prompt优化技术

### 6.1 自动提示词优化(APE)

**思想**: 让LLM生成并优化Prompt

```python
def automatic_prompt_engineer(task_description, examples):
    """
    自动生成优化的Prompt
    """
    # 1. 生成候选Prompts
    meta_prompt = f"""
    任务: {task_description}
    
    示例:
    {examples}
    
    请生成5个不同的Prompt来解决这个任务:
    """
    
    candidate_prompts = llm.generate(meta_prompt, n=5)
    
    # 2. 评估每个Prompt
    scores = []
    for prompt in candidate_prompts:
        score = evaluate_prompt(prompt, validation_set)
        scores.append(score)
    
    # 3. 选择最佳
    best_prompt = candidate_prompts[argmax(scores)]
    
    return best_prompt
```

### 6.2 梯度引导优化

**核心思想**: 将Prompt优化视为参数优化问题

**数学框架**:
$$
\theta^* = \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}} [\mathcal{L}(f_\theta(x), y)]
$$

其中 $\theta$ 代表Prompt模板参数。

**BootstrapFewShot算法原理**:

```
输入: 任务定义T, 训练集D_train
输出: 优化的Few-shot示例集

1. 初始化:
   - 随机选择k个示例作为种子

2. 自举过程(Bootstrap):
   for epoch in range(max_epochs):
       # 使用当前示例集预测训练集
       predictions = [predict(x, current_examples) for x in D_train]

       # 选择高置信度的正确预测
       correct_high_conf = filter(predictions,
                                  lambda p: p.confidence > τ and p.correct)

       # 更新示例集
       current_examples = select_diverse(correct_high_conf, k)

3. 返回最优示例集
```

**关键技术**:
- **置信度筛选**: 只保留模型预测正确且置信的样本
- **多样性采样**: 避免示例过于相似
- **迭代优化**: 通过自举不断改进示例质量

### 6.3 A/B测试

**统计显著性检验原理**:

使用t检验判断两个Prompt的性能差异是否显著。

**假设检验框架**:
```
H0 (零假设): μ_A = μ_B (两个Prompt性能相同)
H1 (备择假设): μ_A ≠ μ_B (两个Prompt性能不同)
```

**t统计量**:
$$
t = \frac{\bar{X}_A - \bar{X}_B}{\sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}}
$$

其中:
- $\bar{X}_A, \bar{X}_B$: 两组样本均值
- $s_A^2, s_B^2$: 样本方差
- $n_A, n_B$: 样本数量

**检验流程**:
```
函数 ab_test_prompts(prompt_a, prompt_b, test_set, α=0.05):
    1. 收集性能数据:
       scores_A = [evaluate(prompt_a, x) for x in test_set]
       scores_B = [evaluate(prompt_b, x) for x in test_set]

    2. 计算统计量:
       mean_A = average(scores_A)
       mean_B = average(scores_B)

       # t统计量
       t_stat = (mean_A - mean_B) / pooled_std_error

    3. 计算p值:
       # 基于t分布计算概率
       p_value = P(|T| >= |t_stat| | H0为真)

    4. 决策:
       if p_value < α:
           结论: "存在显著差异,拒绝H0"
           winner = "A" if mean_A > mean_B else "B"
       else:
           结论: "无显著差异,不能拒绝H0"

    返回: (结论, p_value, winner)
```

**实践建议**:
- 测试集大小: n ≥ 30 (满足中心极限定理)
- 显著性水平: α = 0.05 (标准)
- 多重比较校正: 使用Bonferroni校正避免假阳性

---

## 第7章:实战案例

### 7.1 智能客服系统

```python
customer_service_prompt = """
你是一位专业的客服代表。请遵循以下准则:

1. 始终保持礼貌和耐心
2. 先理解问题,再提供解决方案
3. 如果不确定,承认并转交人工
4. 用简洁的语言回答

客户问题: {question}

回复:
"""
```

### 7.2 代码生成助手

```python
code_generation_prompt = """
请根据以下需求生成Python代码:

需求: {requirement}

要求:
1. 包含完整的类型注解
2. 添加docstring说明
3. 处理边界情况
4. 包含2-3个测试用例

代码:
```python
"""
```

### 7.3 数据分析报告

```python
analysis_prompt = """
基于以下数据生成分析报告:

{data}

报告结构:
1. 数据概览(3-5句话)
2. 关键发现(3个要点)
3. 趋势分析
4. 行动建议

格式: Markdown
"""
```

---

## 总结

### 核心要点

1. **Prompt是输入优化艺术**: 清晰、具体、结构化
2. **ICL无需训练**: Few-shot示例即可适应新任务
3. **CoT提升推理**: 中间步骤比直接答案更可靠
4. **解码策略影响质量**: Temperature/Top-p需根据场景调整
5. **结构化输出是趋势**: JSON Mode/Function Calling成为标配

### 最佳实践

```
任务分析 → 选择合适模板 → 添加Few-shot示例 
→ 使用CoT/ReAct → 配置解码参数 → A/B测试优化
```

### 进阶方向

1. **多模态Prompt**: 图像+文本的混合输入
2. **Prompt压缩**: 减少token消耗
3. **元学习Prompt**: 自动适应新领域
4. **对抗性Prompt**: 提升鲁棒性

---

**推荐资源**:
- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Anthropic Prompt Library](https://docs.anthropic.com/claude/prompt-library)
- [DSPy框架](https://github.com/stanfordnlp/dspy)
- [LangChain Prompt Hub](https://smith.langchain.com/hub)
